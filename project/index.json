{
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#0": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nNotazione asintotica\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#1": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica\ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istitutipubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente\nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il\ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve\nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#10": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNotazione O-grande\n• Denotiamo O( g(n)) (“O grande di gdi n”) l’insieme \ndelle funzioni “ limitate superiormente da g(n) ”\n• Definite come segue:\n• Oppure, più formalmente:f(n) ∈O(g(n)) ⇔esistono due costanti positive c\nedn0tali che per ogni n≥n0si verifica\n0 ≤f(n) ≤c ⋅g(n)f(n) ∈O(g(n)) ⇔esistono due costanti positive c\nedn0tali che per ogni n≥n0si verifica\n0 ≤f(n) ≤c ⋅g(n)\nO(g(n)) = { f(n) : ∃c> 0, ∃n0> 0, tali che ∀n≥n0 \n0 ≤f(n) ≤c ⋅g(n)}O(g(n)) = { f(n) : ∃c> 0, ∃n0> 0, tali che ∀n≥n0 \n0 ≤f(n) ≤c ⋅g(n)}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#11": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNotazione O-grande\nn0c ⋅g(n)\nf(n)\nn",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#12": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itOsservazioni sulla definizione di O( g(n))\n•O ( g(n)) = ∅(l’insieme vuoto) se g(n) è una \nfunzione asintoticamente negativa\n– conveniamo che g(n) non sia mai asintoticamente \nnegativa\n• Le costanti ced n0dipendono dalla specifica f(n)\n• Qual è il ruolo della costante c?\n– se la costante cnon ci fosse \n• correttamente avremmo 2 n∈O(n2)\n• ma avremmo anche 2 n∉O(n), oppure n2+1 ∉O(n2)\n• Vale la proprietà riflessiva: g(n) ∈O(g(n))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#13": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itFunzioni limitate superiormente da g(n)\nO(g(n))\ng(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#14": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsempio di funzione ∈O(n2)\n• dimostriamo che      n2–3n∈O(n2)\n– dobbiamo trovare almeno una c > 0 ed una n0> 0 tali che\n∀n≥n0,  0  ≤f(n) ≤c ⋅g(n)\n0  ≤ n2–3n≤c ⋅n2\n– dividiamo per n2e otteniamo \n– proviamo a fissare c=\nè soddisfatta per   n> 0\nè soddisfatta per   n≥6\n– dunque c = 0.5 e n0= 6 dimostrano l’asserto21\n21\n0≤21–n3≤c\n2 21–n3≤1\n0≤21–n321",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#15": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsempio di funzione ∈O(n2)\n-20020406080\n0123456789 1 0 1 1n2 -3n21n0\n2n2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#16": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itGeneralizzazione\n•p e r  c1>0 e c2>0 si ha c1nk–c2nk-1∈O(nk)\n– dobbiamo trovare almeno una c> 0 ed una n0> 0 tali che\n∀n≥n0,  0  ≤f(n) ≤c ⋅g(n)\n0 ≤c1nk–c2nk-1≤c ⋅nk\n– dividiamo per nk e otteniamo:\n– proviamo a fissare c= c1\nè soddisfatta per   n≥0\nè soddisfatta per   n≥\n– dunque la coppia c = c1e n0=  c2/c1dimostrano l’asserto0≤c1–nc2≤c\n0≤c1–nc2c1–nc2≤c1\nc1c2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#17": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizio: n3∉O(n2)\n• dimostriamo che n3∉O(n2)\n– dovremmo trovare ced n0tali che\n∀n≥n0, 0 ≤f(n) ≤c ⋅g(n)\n0  ≤n3≤c ⋅n2\n– dividiamo per n2\n0 ≤n≤c\n– assurdo\n• quale che sia cesiste sempre un valore di nper cui n> c\n• analogamente, è facile dimostrare che \nnk+1∉O(nk) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#18": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizio: n2∈O(n3)\n• dimostriamo, viceversa che n2∈O(n3)\n– dobbiamo trovare ced n0tali che\n∀n≥n0, 0 ≤f(n) ≤c ⋅g(n)\n0  ≤n2≤c ⋅n3\n– dividiamo per n2\n0 ≤1 ≤c ⋅n\n– che è soddisfatta, per esempio, per c= 1 ed n0= 1\n• analogamente, è facile dimostrare che \nnk∈O(nk+1) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#19": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itFunzioni incommensurabili\n• è sempre vero che: f(n) ∈O(g(n)) \noppure: g(n) ∈O(f(n)) ? \n• consideriamo le seguenti funzioni\n• poiché n2∉O(n)\n–f(n) ∉O(g(n)) per via degli npari\n–g(n) ∉O(f(n)) per via degli ndispari=pari è  sedispari è  se)(2n nn nnf\n=\npari è  sedispari è  se)(2\nn nn nng",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#2": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNotazione Asintotica\n• Definizioni\n• Proprietà delle notazioni asintotiche \n• Uso esteso (o improprio) della notazione",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#20": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itDue funzioni incommensurabili\n020040060080010001200140016001800\n1\n3\n5\n7\n9\n1113\n15\n171921\n23\n25\n27\n2931\n33\n353739g(n)\nf(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#21": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsploriamo O( g(n))\n• quali funzioni (oltre a g(n)) sono in O( g(n)) ?\nO(g(n))\ng(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#22": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itFunzioni in O( g(n))\n• dimostriamo che appartengono ad O( g(n)) le \nseguenti funzioni f(n): \nproprietà transitiva\nf(n) ∈O(h(n))   per quanche   h(n) ∈O(g(n))\nregola dei fattori costanti positivi\nf(n) = d·h(n)   per qualche   h(n) ∈O(g(n)) e d> 0\nregola della somma\nf(n) = h(n) + k(n)   con   h(n) e k(n) ∈O(g(n))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#23": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itProprietà transitiva\n• dimostriamo che:\n• per ipotesi\n∃c'> 0, ∃n'0> 0, t.c. ∀n≥n'0, 0 ≤f(n) ≤c'⋅h(n)\n∃c\" > 0, ∃n\"0> 0, t.c. ∀n≥n\"0, 0 ≤h(n) ≤c\"⋅g(n)\n• componendo le due\n0 ≤f(n) ≤c'⋅c\"⋅g(n)\n• e dunque\n∃c'\" > 0, ∃n'\"0> 0, t.c. ∀n≥n'\"0, 0 ≤f(n) ≤c'\"⋅g(n) \ncon c'\" = c' ⋅c\" e con n'\"0= max( n'0,n\"0)()\n()())( )(\n)( )()( )(\nng nf\nng nhnh nf\nΟ∈ ⇒\n\nΟ∈∧Ο∈",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#24": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itRegola dei fattori costanti positivi\n•s e  d> 0 è una costante\nf(n) ∈O(g(n)) ⇔ d·f(n) ∈O(g(n))\n• infatti, per ipotesi si ha:\n∃c> 0, ∃n0> 0, t.c. ∀n≥n0, 0 ≤f(n) ≤c⋅g(n)\n• definisco \nc’=  c⋅d(c’ > 0 dato che d> 0)\n• sostituendo c= c’/dottengo\n∃c’ > 0, ∃n0> 0, t.c. ∀n≥n0, 0 ≤f(n) ≤c’/d⋅g(n)\n• finalmente moltiplicando per d\n∃c’ > 0, ∃n0> 0, t.c. ∀n≥n0, 0 ≤d⋅f(n) ≤c’⋅g(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#25": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itRegola della somma\n• dimostriamo che:\n• per ipotesi\n∃c'> 0, ∃n'0> 0, t.c. ∀n≥n'0, 0 ≤h(n) ≤c'⋅g(n)\n∃c\" > 0, ∃n\"0> 0, t.c. ∀n≥n\"0, 0 ≤k(n) ≤c\"⋅g(n)\n• sommando le due disequazioni si ottiene\n0 ≤h(n) + k(n) ≤c'⋅g(n) +c\"⋅g(n)\n• da cui\n∃c'\" > 0, ∃n'\"0> 0, t.c. ∀n≥n'\"0, 0 ≤h(n) + k(n) ≤c'\"⋅g(n) \ncon c'\" = c' + c\"   e con   n'\"0= max( n'0,n\"0)( )\n()())( )()(\n)( )()( )(\nng nknh\nng nkng nh\nΟ∈ + ⇒\n\nΟ∈∧Ο∈",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#26": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itUsi estesi (o impropri) della notazione\n• abuso della notazione\n– spesso in luogo di f(n) ∈O(g(n)) si trova \nf(n) = O( g(n))\n– questo corrisponde alla lettura “ f(n) è O( g(n))”\npiuttosto che “ f(n) è un elemento di O( g(n))”\n• operazioni con la notazione asintotica \n3n3+ O(n)   si intende: 3 n3sommata con una\nqualche funzione appartenente adO(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#27": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizio\n• dimostriamo che\n6n4–3n3+ 2n2+ 5n + 6 ∈O(n4)\nO(n4) – O( n3) + O( n2) + O( n) + O(1)\nO(n4)   +    O( n2)   +     O( n)\nO(n4)         +        O( n)\nO(n4)fattori costanti positivi\nappartenenze note \nproprietà transitiva\nregola della somma\nregola della somma",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#28": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sulla notazione O-grande\n• Quali di questi rapporti di contenimento sono \ncorretti?\nO(n) O(n2)O(n)\nO(n2) O(n)O(n2)\nRisposta 1 Risposta 2 Risposta 3",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#29": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNotazione  Ω\n• Denotiamo Ω(g(n)) (“Omega di gdi n”) l’insieme \ndelle funzioni “ limitate inferiormente da g(n) ”\n• Definite come segue:\n• Oppure, più formalmente:f(n) ∈Ω(g(n)) ⇔esistono due costanti positive c\ned n0tali che per ogni n≥n0si verifica\n0 ≤c ⋅g(n) ≤f(n)f(n) ∈Ω(g(n)) ⇔esistono due costanti positive c\ned n0tali che per ogni n≥n0si verifica\n0 ≤c ⋅g(n) ≤f(n)\nΩ(g(n)) = { f(n) : ∃c> 0, ∃n0> 0, tali che ∀n≥n0 \n0 ≤c ⋅g(n) ≤f(n)}Ω(g(n)) = { f(n) : ∃c> 0, ∃n0> 0, tali che ∀n≥n0 \n0 ≤c ⋅g(n) ≤f(n)}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#3": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itStudio di funzioni\n• Intersezioni con gli assi e segno\n• Simmetrie e periodicità\n• Continuità, discontinuità, derivazione\n• Massimi, minimi e punti di flesso• Comportamento agli estremi del dominio \n– asintoti orizzontali, verticali, obliqui\n– notazione asintotica voi siete qui",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#30": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNotazione Ω\nn0c ⋅g(n)f(n)\nn",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#31": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itOsservazioni sulla definizione di Ω(g(n))\n• si può facilmente dimostrare che\nf(n) ∈Ω(g(n)) ⇔g(n) ∈O(f(n))\n• nel caso della notazione Ωoccorre spesso \nricorrere a valori minori di uno per la costante c\n– la costante cnon è necessariamente un intero\n• sarebbe stato analogo scrivere: 0  ≤g(n) ≤c⋅f(n)\n• anche per Ω(g(n)) esistono funzioni \nincommensurabili\n• vale la proprietà riflessiva: g(n) ∈Ω(g(n))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#32": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itFunzioni limitate inferiormente da g(n)\ng(n)Ω(g(n))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#33": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itFunzioni in Ω(g(n))\n•p e r  Ω(g(n)) valgono proprietà analoghe a quelle \nche abbiamo dimostrato per O( g(n))\n• appartengono ad Ω(g(n)) le seguenti funzioni:\nproprietà transitiva\nf(n) ∈Ω(h(n))   per qualche   h(n) ∈Ω(g(n))\nregola dei fattori costanti positivi\nf(n) = d·h(n)   per qualche   h(n) ∈Ω(g(n)) e d> 0\nregola della somma\nf(n) = h(n) + k(n)   con   h(n) e k(n) ∈Ω(g(n))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#34": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sulla notazione Ω\n• Quali di questi rapporti di contenimento sono \ncorretti?\nΩ(n) Ω(n2)Ω(n)\nΩ(n2) Ω(n)Ω(n2)\nRisposta 1 Risposta 2 Risposta 3",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#35": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNotazione  Θ\n• denotiamo Θ(g(n)) (“Teta di gdi n”) l’insieme delle funzioni \n“limitate inferiormente e superiormente da g(n) ”\n• definite come segue:\n• oppure, più formalmente:f(n) ∈Θ(g(n))  ⇔esistono tre costanti positive c1, \nc2, ed n0tali che per ogni n≥n0si verifica\n0 ≤c1⋅g(n) ≤f(n) ≤c2⋅g(n) f(n) ∈Θ(g(n))  ⇔esistono tre costanti positive c1, \nc2, ed n0tali che per ogni n≥n0si verifica\n0 ≤c1⋅g(n) ≤f(n) ≤c2⋅g(n) \nΘ(g(n)) = { f(n) : ∃n0> 0, ∃c1> 0, ∃c2> 0, tali che\n0 ≤c1⋅g(n) ≤f(n) ≤c2⋅g(n) \n∀n≥n0 }Θ(g(n)) = { f(n) : ∃n0> 0, ∃c1> 0, ∃c2> 0, tali che\n0 ≤c1⋅g(n) ≤f(n) ≤c2⋅g(n) \n∀n≥n0 }",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#36": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itNotazione Θ\nn0c2 ⋅g(n)\nf(n)\nc1 ⋅g(n)\nn",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#37": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itOsservazioni sulla definizione di Θ(g(n))\n• dalla definizione si ricava immediatamente che\n– questa considerazione offre una definizione alternativa di \nΘ(g(n))\n• vale la proprietà riflessiva: g(n) ∈Θ(g(n))\n• valgono tutte le proprietà che abbiamo dimostrato per \nO-grande e per Ω()( )\n()\nΩ∈∧Ο∈\n⇔ Θ∈\n)( )()( )(\n)( )(\nng nfng nf\nng nf",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#38": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itFunzioni ∈Θ(g(n))\nO(g(n))\ng(n)Ω(g(n))\nΘ(g(n))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#39": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itProprietà simmetrica\n• è immediato dimostrare che \nf(n) ∈Θ(g(n)) ⇔g(n) ∈Θ(f(n))\n• infatti\nf(n) ∈O(g(n))  ⇒ g(n) ∈Ω(f(n))\nf(n) ∈Ω(g(n))  ⇒ g(n) ∈O(f(n))\n• dunque \nf(n) ∈Θ(g(n)) ⇒g(n) ∈Θ(f(n)) \n• in maniera analoga si dimostra che \ng(n) ∈Θ(f(n)) ⇒f(n) ∈Θ(g(n)) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#4": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itLa funzione lineare 10n\n0100200300400500600\n0 5 10 15 20 25 30 35 40 45 50n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#40": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itLa relazione di equivalenza Θ\n• poiché f(n) ∈Θ(g(n)) ⇔g(n) ∈Θ(f(n)), la \nrelazione f(n) ∈Θ(g(n)) tra f(n) e g(n) gode \ndella proprietà simmetrica\n• dunque la notazione Θdefinisce una relazione \ndi equivalenza\n– valgono infatti le tre proprietà riflessiva, \nsimmetrica e transitiva\n– la notazione Θconsente di classificare tutte le \nfunzioni in classi di equiva lenza, che descrivono il \nloro comportamento al crescere di n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#41": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itRapporti tra classi\nΘ(n)\nO(n) O(n2)Ω(n)Ω(n2)\nΘ(n2)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#42": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itGerarchia delle funzioni\nΘ(n!)\n...\nΘ(2n)\n...\nΘ(n3)\nΘ(n2)\nΘ(nlog n)\nΘ(n)\nΘ(log n)\nΘ(1)• le funzioni nella classe Θ(g(n))\n• sono O( f(n)) per tutte le f(n) \nappartenenti alle classi \nsuperiori a Θ(g(n))\n• sono Ω(f(n)) per tutte le f(n) \nappartenenti alle classi \ninferiori a Θ(g(n))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#5": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itLa funzione lineare 20n\n020040060080010001200\n0 5 10 15 20 25 30 35 40 45 50n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#6": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itla funzione quadratica n2\n050010001500200025003000\n0 5 10 15 20 25 30 35 40 45 50n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#7": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itla funzione cubica n3\n020000400006000080000100000120000140000\n0 5 10 15 20 25 30 35 40 45 50n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#8": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itLa funzione esponenziale 2n\n02E+144E+146E+148E+141E+151.2E+15\n0 5 10 15 20 25 30 35 40 45 50n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\040-notazione-asintotica-14.pdf#9": "040-notazione-asintotica-14 copyright ©2022 maurizio.patrignani@uniroma3.itScopo delle notazioni asintotiche\n• Si applicano alle funzioni f(n) il cui dominio è\nl’insieme Ndei naturali\n– possono essere facilmente estese ai reali\n• Classificano le funzioni dal punto di vista del \nloro comportamento per grandi valori di n\n• Forniscono un limite superiore e/o inferiore \ndella funzione\n– la limitazione avviene per confronto con altre \nfunzioni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#0": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nIl problema dell’ordinamento\nAlgoritmi greedy e algoritmi iterativi\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#1": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright\n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, anim azioni, video, audio, musica e \ntesto) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non a fini\ndi lucro, da università e scuole pubbliche e da istituti pubblici diricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente\nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il contenuto delle\nslides, che sono comunque soggette a cambiamento\n• questa nota di copyright non deve essere mai rimossa e deve essere\nriportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#10": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmi che operano in loco\n• gli algoritimi che operano in loco non \nnecessitano di copiare l’input in strutture di dati \ndiverse da quella utilizzata per l’input\n– questa caratteristica è utile per input di grosse \ndimensioni\n• l’algoritmo SELECTION_SORT opera in loco\n– gli elementi vengono solo scambiati\n– l’algoritmo necessita di una quantità di memoria \ncostante oltre a quella per memorizzare A\n• la memoria utilizzata dall’algoritmo è O(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#11": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmi di ordinamento stabili\n• un algoritmo di ordinamento si dice stabile se \nnon modifica l’ordine degli elementi che hanno \nlo stesso valore\n– in alcune applicazioni ciò può essere utile\n• quando agli elementi sono co llegati dei dati satellite\n• quando gli elementi con la stessa chiave hanno una \nposizione reciproca significativa\n• l’algoritmo SELECTION_SORT è stabile\n–s e  A [ j] = A[ k] con j< k, allora A[ j] viene \nselezionato per primo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#12": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itInsertion sort\nUn algoritmo incrementale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#13": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itGli algoritmi incrementali\n• si basano sulla seguente osservazione\n– la soluzione di un’istanza di dimensione \nn-1 può essere utile per risolvere un’istanza di \ndimensione n\n•e s e m p i o\na) istanza di dimensione cinque:\nb) istanza di dimensione sei:\nla soluzione di (a) mi aiuta a risolvere (b) ? \n+ =   ?5246152461\n341256+3",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#14": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.it♥♥\n♥\n♥♥♥♥♥♥♥\n♥♥\n♥♥\n♥♥Insertion sort\n• manteniamo un sottoinsieme \nordinato di elementi\n– cominciando da un singolo \nelemento\n• inseriamo un elemento alla \nvolta\n– il sottoinsieme ordinato cresce\n– i suoi elementi vengono traslati \nper far posto al nuovo elemento\n• quando tutti gli elementi sono \ninseriti il problema è risolto\n♥ ♥♥\n♥♥♥♥\n♥\n♥♥♥♥♥♥♥\n♥♥\n♥♥\n♥♥ ♥♥♥\n♥♥♥♥\n♥\n♥♥♥♥♥♥♥♥♥\n♥♥\n♥♥♥♥♥\n♥♥♥♥\n♥\n♥♥♥♥\n♥\n♥\n♥♥♥\n♥♥\n♥♥ ♥♥♥\n♥♥♥♥\n♥\n♥♥\n♥\n♥♥\n♥\n♥♥♥\n♥♥\n♥♥ ♥♥♥\n♥♥♥♥\n♥\n♥♥♥♥♥♥♥\n♥♥\n♥♥\n♥♥♥\n♥♥\n♥♥",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#15": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itINSERTION_SORT\nordinatoAj\nda ordinare0 i8. A[i+1] = key7. i = i-16. A[i+1] = A[i]5. while i>-1andA[i]>key4. i = j-13. Zinserisce key nella sequenza ordinata A[0..j-1]2. key = A[j]1. forj = 1toA.length-1INSERTION_SORT(A)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#16": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itInsertion sort con input\n5ji\n2\n4ji5ji\n2key\nji\n4\nji\n44613\n4613\n25613\n25\n25613\n613\n5ji\n4 2613\n5ji\n2461356 2413jikey\n524613ji\n51 2463ji\n51 2463ji\n1 24563ji\n41 2563j i\n41 2563j i412563ij\n43 1256ij\n43 1256ji524613\n43 1256ji\n3 12456j i\n312456j i\n312456jkey",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#17": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itProprietà di INSERTION_SORT\n•INSERTION_SORT è stabile\n– supponiamo che per i< jsi abbia A[ i] = A[ j]\n– quando l’algoritmo inserisce A[ j], l’elemento A[ i] è\ngià stato inserito\n– l’algoritmo inserisce A[ j] dopo A[ i] preservando \ndunque la loro posizione reciproca originale\n•INSERTION_SORT opera in loco\n– richiede O(1) memoria addizionale rispetto alla \nmemoria utilizzata per l’input",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#18": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itComplessità nel caso peggiore\n• il ciclo esterno viene eseguito O( n) volte\n• nel caso peggiore \n– l’elemento corrente deve essere inserito sempre al primo posto\n• l’array A in input è ordinato in maniera decrescente\n– il numero delle operazioni è Θ(n2)8. A[i+1] = key7. i = i-16. A[i+1] = A[i]5. while i>-1andA[i]>key4. i = j-13. Zinserisce key nella sequenza ordinata A[0..j-1]2. key = A[j]1. forj = 1toA.length-1INSERTION_SORT(A)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#19": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itComplessità nel caso migliore\n• il ciclo esterno viene eseguito comunque O( n) volte\n• nel caso migliore \n– l’elemento corrente è già posizionato al punto giusto\n• l’array A in input è già ordinato\n– il numero delle operazioni è Θ(n)8. A[i+1] = key7. i = i-16. A[i+1] = A[i]5. while i>-1andA[i]>key4. i = j-13. Zinserisce key nella sequenza ordinata A[0..j-1]2. key = A[j]1. forj = 1toA.length-1INSERTION_SORT(A)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#2": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itPanoramica\n• il problema dell’ordinamento\n• gli algoritmi greedy\n– l’algoritmo selection sort\n• gli algoritmi iterativi\n– l’algoritmo insertion sort",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#20": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itComplessità nel caso medio\n• il ciclo esterno viene eseguito comunque O( n) volte\n• nel caso medio\n– l’elemento corrente va posizionato nel mezzo di A[0… j]\n– il numero delle operazioni è Θ(n2)8. A[i+1] = key7. i = i-16. A[i+1] = A[i]5. while i>-1andA[i]>key4. i = j-13. Zinserisce key nella sequenza ordinata A[0..j-1]2. key = A[j]1. forj = 1toA.length-1INSERTION_SORT(A)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#21": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itProprietà dell’insertion sort\n• efficente su piccole istanze\n– più efficiente in pratica che il selection sort\n– nel caso migliore ha complessità lineare \n– si può calcolare che la complessità media è n2/4\n• adattivo\n– veloce su instanze già parzialmente ordinate\n• complessità O( n+ d) dove dè il numero delle inversioni\n•s t a b i l e  \n– non cambia l’ordine degli elem enti che hanno lo stesso valore\n•i n  l o c o\n– la memoria addizionale richiesta è O(1) \n• online\n– può essere utilizzato quando i numeri arrivano uno alla volta",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#22": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmi di ordinamento visti finora\nsisi stabile\nsisi in loco\nΘ(n2) Θ(n2) Θ(n) INSERTION-SORTΘ(n2) SELECTION-SORTcaso \npeggiore\ncaso \nmedio\ncaso \nmigliore",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#3": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itIl problema dell’ordinamento\n•input : una sequenza di nnumeri <a1, a2, ..., an>\n•output : una permutazione <a1’, a2’, ..., an’> della \nsequenza tale che a1’≤a2’≤... ≤an’\n• esempio \n– un’istanza\n<31, 41, 59, 26, 41, 58>\n– la soluzione dell’istanza qui sopra\n<26, 31, 41, 41, 58, 59>\n• nel seguito supporremo che l’istanza sia fornita \ntramite un array A con nposizioni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#4": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itSelection sort\nLa tecnica greedy",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#5": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itLa tecnica greedy\n• la tecnica greedy (golosa) consiste nel scegliere \nsempre l’alternativa che al momento sembra piùappetibile\n– corrisponde ad eseguire una scelta localmente ottima\n– ciò non sempre comporta una scelta globalmente ottima\n• esempio in cui greedy no n dà la soluzione ottima\n– trovare il cammino più breve tra ncittà\n– algoritmo greedy: muoviti sempre verso la città più vicina \nnon ancora visitata\nsoluzione \ntrovata \ndall’algoritmo \ngreedysoluzione \nmigliore",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#6": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmo selection sort\n• utilizza una tecnica greedy per ordinare un \narray\n• strategia generale \n– seleziona l’elemento più piccolo \ne mettilo al primo posto\n– seleziona l’elemento più piccolo \ndei rimanenti e mettilo al secondo posto\n–…58471326\n18475326\n12475386",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#7": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmo SELECTION_SORT\n• pseudocodice dell’algortimo\n– si fa uso di due cicli annidati\n8.A[min] = temp7.A[i] = A[min]6.temp = A[i] Zscambio A[i] con A[min]5. min = j4. if A[j] < A[min] Zdevo aggiornare min3. for j = i + 1 toA.length-1 Zscorro l’array2.min = i Zindice elemento minimo in A[i..n-1]1. fori = 0toA.length–2SELECTION_SORT( A)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#8": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itComplessità del SELECTION_SORT\n• i valori dell’input non modificano il numero delle \niterazioni del ciclo esterno e del ciclo interno\n– quindi il caso migliore, il caso peggiore ed il caso medio \nhanno la stessa complessità8.A[min] = temp7.A[i] = A[min]6.temp = A[i] Zscambio A[i] con A[min]5. min = j4. if A[j] < A[min] Zdevo aggiornare min3. for j = i + 1 toA.length-1 Zscorro l’array2.min = i Zindice elemento minimo in A[i..n-1]1. fori = 0toA.length–2SELECTION_SORT( A)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\060-ordinamento-06.pdf#9": "060-ordinamento-06 copyright ©2019 maurizio.patrignani@uniroma3.itComplessità del SELECTION_SORT\n• l’algoritmo esegue O( n) cicli esterni e O( n) cicli interni\n– dunque SELECTION-SORT ha complessità O( n2)\n• la riga 4 viene eseguita ( n-1)+( n-2)+...+1 = [ n(n-1)]/2 volte\n– dunque SELECTION-SORT ha complessità Ω(n2)\n• il tempo di esecuzione dell’algoritmo è Θ(n2)8.A[min] = temp7.A[i] = A[min]6.temp = A[i] Zscambio A[i] con A[min]5. min = j4. if A[j] < A[min] Zdevo aggiornare min3. for j = i + 1 toA.length-1 Zscorro l’array2.min = i Zindice elemento minimo in A[i..n-1]1. fori = 0toA.length–2SELECTION_SORT( A)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#0": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nRicorsione e complessità\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#1": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica\ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istitutipubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente\nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il\ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve\nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#10": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n2 variabile i2 variabile sum 2 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#11": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n2 variabile i2 variabile sum 3 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)3 variabile i2 variabile f 8 istruzioneFACT(2)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#12": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n2 variabile i4 variabile sum 3 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#13": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n3 variabile i4 variabile sum 2 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#14": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n3 variabile i4 variabile sum 3 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)4 variabile i6 variabile f 8 istruzioneFACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#15": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n3 variabile i10 variabile sum 3 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#16": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n4 variabile i10 variabile sum 2 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#17": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n4 variabile i10 variabile sum 4 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#18": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itFunzioni ricorsive\n• abbiamo già visto che l’algoritmo iterativo FACT per \nil calcolo del fattoriale ha complessità Θ(n)\n• il calcolo del fattoriale può essere facilmente realizzato \nanche tramite un algoritmo ricorsivo5. return f4. f=n  *  FACT_RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#19": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di funzioni ricorsive\n• supponiamo di eseguire \nFACT_RIC (3)\n• seguiamo l’evoluzione \ndello stack dei record di attivazione5. return f4. f=n  *  FACT_RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n0 variabile f4 istruzioneFACT_RIC(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#2": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itSommario\n• funzioni e record di attivazione\n• ricorsione e record di attivazione\n• formule di ricorrenza\n– teorema dell’esperto\n• strategie algoritmiche\n– algoritmi divide et impera e merge sort",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#20": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di funzioni ricorsive\n• supponiamo di eseguire \nFACT_RIC (3)\n• seguiamo l’evoluzione \ndello stack dei record di attivazione5. return f4. f=n  *  FACT-RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n0 variabile f4 istruzioneFACT_RIC(3)0 variabile f4 istruzioneFACT_RIC(2)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#21": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di funzioni ricorsive\n• supponiamo di eseguire \nFACT_RIC (3)\n• seguiamo l’evoluzione \ndello stack dei record di attivazione5. return f4. f=n  *  FACT-RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n0 variabile f4 istruzioneFACT_RIC(3)0 variabile f4 istruzioneFACT_RIC(2)0 variabile f4 istruzioneFACT_RIC(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#22": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di funzioni ricorsive\n• supponiamo di eseguire \nFACT_RIC (3)\n• seguiamo l’evoluzione \ndello stack dei record di attivazione5. return f4. f=n  *  FACT-RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n0 variabile f4 istruzioneFACT_RIC(3)0 variabile f4 istruzioneFACT_RIC(2)0 variabile f4 istruzioneFACT_RIC(1)1 variabile f2 istruzioneFACT_RIC(0)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#23": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di funzioni ricorsive\n• supponiamo di eseguire \nFACT_RIC (3)\n• seguiamo l’evoluzione \ndello stack dei record di attivazione5. return f4. f=n  *  FACT-RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n0 variabile f4 istruzioneFACT_RIC(3)0 variabile f4 istruzioneFACT_RIC(2)1 variabile f4 istruzioneFACT_RIC(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#24": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di funzioni ricorsive\n• supponiamo di eseguire \nFACT_RIC (3)\n• seguiamo l’evoluzione \ndello stack dei record di attivazione5. return f4. f=n  *  FACT-RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n0 variabile f4 istruzioneFACT_RIC(3)2 variabile f4 istruzioneFACT_RIC(2)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#25": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di funzioni ricorsive\n• supponiamo di eseguire \nFACT_RIC (3)\n• seguiamo l’evoluzione \ndello stack dei record di attivazione5. return f4. f=n  *  FACT-RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\n6 variabile f4 istruzioneFACT_RIC(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#26": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itCosto di FACT_RIC\n• il costo di FACT_RIC (n) è\n–Θ(1) quando nèz e r o\n– pari al costo di FACT_RIC (n-1) + Θ(1) negli altri \ncasi5. return f4. f=n  *  FACT-RIC( n-1)3. else2.f=11. ifn == 0FACT_RIC(n)\nT(0) = Θ(1) \nT(n) = T( n-1) + Θ(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#27": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itFormule di ricorrenza\n• equazioni o disequazioni che descrivono una \nfunzione in termini del suo valore su input più\npiccoli\n– prevedono sempre dei casi base e dei casi induttivi\n•e s e m p i\na per n= 0\nT(n-1) + g(n)p e r  n> 0\n a per n= 0 o n= 1\n2T(n/2) + f(n) per n> 1T(n) =\nT(n) =",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#28": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itFormule di ricorrenza\n• le soluzioni delle formule di ricorrenza non \nsempre sono facili da trovare\n• quando esprimono delle complessità asintotiche \ntalvolta i casi base vengono omessi\n–s e  T ( n) esprime il tempo di esecuzione di un \nalgoritmo, T( n) è sempre Θ(1) per npiccolo\n•e s e m p i o\nT(n) = 2T( n/2) + Θ(n)\n• è sottointeso che T( n) = Θ(1) per n = 0 e n = 1",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#29": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itSoluzione di una equazione di ricorrenza\n• dimostriamo che l’equazione di ricorrenza\n• ammette come soluzione\n• per dimostrarlo sostituiamo la soluzione \nproposta a destra e sinistra dell’equazione di \nricorrenzak=1g(k) ∑n\nT(n) = a+a per n= 0\nT(n-1) + g(n)p e r  n> 0T(n) =",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#3": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n0 variabile i0 variabile sum3 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#30": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itVerifica della correttezza della soluzione\n• caso base per n=0\nT(n=0) = a+               = a + 0 = a (verificato)\n• caso induttivo \nso chek=1g(k) ∑0\ng(k) T(n-1) = a+ \nk=1∑n-1\ng(k) + g(n) T(n)=  a+ \nk=1∑n-1\ng(k) T(n)=  a+ \nk=1∑n\n(verificato)(ipotesi induttiva)\nT(n) = T( n-1) + g(n)    (dalla definizione)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#31": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itGraficamente\ng(n) n\nn-1\n0\nnn-2\n2\n1g(n-1)\ng(n-2)\ng(2)\ng(1)\na\ng(k) a+ \nk=1∑ndimensione del problemaalbero delle \nchiamate ricorsivecontributi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#32": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itComplessità di FACT_RIC\n• sappiamo che FACT_RIC ha complessità\na per n= 0\nT(n-1) + g(n)p e r  n> 0T(n) =Θ(1) per n= 0\nT(n-1) + Θ(1) per n> 0T(n) =\n• sappiamo che l’equazione di ricorrenza\n• ammette come soluzione g(k) T(n)=  a+ \nk=1∑n\n• la complessità di FACT_RIC è dunque\nΘ(1) T(n)=  Θ(1) + \nk=1∑n\n=  Θ(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#33": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itVersione ricorsiva del selection sort\n9. SELECTION-RIC (A,i+1)8.A[min] = temp7.A[i] = A[min]6.temp = A[i] Zscambio A[i] con A[min]5. min = j4. if A[j] < A[min] Zdevo aggiornare min3. for j = i + 1 toA.length-1 Zscorro l’array2.min = i Zindice elemento minimo in A[i..n-1]1. ifi < A.length–1 Zaltrimenti è già ordinatoSELECTION_RIC( A,i) Zordina A da i a A.length-11. SELECTION_RIC (A,0) Zordina A da 0 in poiSELECTION( A) Zordina A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#34": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itComplessità di SELECTION_RIC\n• possiamo scrivere la seguente equazione di \nricorrenza, in cui nè il numero degli elementi \ndi A ancora da ordinare\nΘ(1) per n= 1\nT(n-1) + Θ(n)p e r  n> 1T(n) =\n• la complessità di SELECTION_RIC è dunque\nΘ(k) T(n)=  Θ(1) + \nk=1∑n\n=  Θ(n2)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#35": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itGraficamente\nn\nn-1\nn-2\n2\n1 Θ(1)Θ(2)Θ(n-2)Θ(n-1)Θ(n)\nΘ(n) Θ(n)\nn/2 · Θ(n) = Θ(n2)dimensione del problema\nalbero delle \nchiamate ricorsivecontributi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#36": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itLa tecnica divide et impera\n• detta anche “divide and conquer”\n• consiste nel suddividere il problema in diversi \nsottoproblemi\n– i sottoproblemi sono dello stes so tipo del problema originale\n• ma di dimensioni più piccole\n– i sottoproblemi possono essere risolti in maniera ricorsiva\n• suddividendoli a loro volta\n–c a s o  b a s e\n• quando i sottoproblemi sono di di mensioni ridottissime la loro \nsoluzione è banale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#37": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itRicorsione del divide et impera\n• a ciascun passo della ricorsione\n– divide\n• l’istanza corrente viene divi sa in due o più istanze più\npiccole\n– impera\n• l’algoritmo viene lanciato sulle istanze più piccole \n– combina\n• le soluzioni delle istanze più piccole vengono utilizzate \nper produrre una soluzione dell’istanza corrente",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#38": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itMerge sort\n• introdotto da John von Neumann nel 1945\n• osservazione elementare\n– due sequenze ordinate possono essere fuse in un’unica \nsequenza ordinata molto facilmente\n• un possibile algoritmo\n– dividere la sequenza di input in due sottosequenze\n– ordinare le due sottosequenze\n• tramite lo stesso merge sort\n– fondere le due sottosequenze ordinate\n• caso base\n– un array di un solo elemento è ordinato per definizione",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#39": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itMerge sort con input 52471326\n52471326\n5247 1326s\n52471326ss ss13 26 52 47ss\n25 4713 26m m m m\n2457 1236m m\n12234567ms\nm= split\n= merge",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#4": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n0 variabile i0 variabile sum 3 istruzionevariabile i1 variabile f 5 istruzioneFACT(0)4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#40": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itFusione: l’algoritmo MERGE\n…(continua nella prossima slide)…9.R[n2] =  ∞Zchiudo con “infinito”8.L[n1] =  ∞Zchiudo con “infinito”7.R[j] = A[q+j+1] Zcopio la 2asequenza6. for j = 0ton2-15.L[i] = A[p+i] Zcopio la 1asequenza4. for i = 0ton1-13. Zcreo array L[0…n1] e R[0…n2] (con una casella in +)2.n2=r  -q  Zlunghezza della seconda sequenza1.n1= q - p + 1 Zlunghezza della prima sequenzaMERGE(A,p,q,r)\nA << <<\n<< << L[] R[] ∞ ∞pq r",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#41": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itFusione (continua)\n18. j = j + 117. A[k] = R[j] Zpesco da R16. else15. i = i + 114. A[k] = L[i] Zpesco da L13. if L[i] ≤R[j] then 12.for k = ptor11.j = 0 Ziteratore per array R10.i = 0 Ziteratore per array L…(dalla slide precedente)…\n• il confronto con “ ≤” sulla riga 13 garantisce la stabilità\ndell’algoritmo\n–s e  L[i]=R[j] allora L[i] ha la precedenza",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#42": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itL’algoritmo MERGE_SORT\n5. MERGE( A,p,q,r)4. MERGE_SORT( A,q+1,r)3. MERGE_SORT( A,p,q)2. q =  (p+r)/2 Zdivido l’array in due1. if p < r then Znel caso base esco subitoMERGE_SORT( A,p,r)\n• all’inizio della computazione lanciamo• l’algoritmo MERGE_SORT esegue la parte “divide”, risolve i \nsottoproblemi ed esegue la parte “combine”\n1. MERGE_SORT( A,0,A.length-1 )MERGE(A) Zordina A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#43": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itTempo di esecuzione di merge sort\n• calcoliamo il costo T( n) di esecuzione del merge sort \nsu un’istanza con nelementi\n• caso base\n–c o s t o Θ(1)\n• divide \n– calcolo di n/2: costo D( n) = Θ(1)\n• impera\n– ogni sottoproblema ha dimensione n/2\n– i sottoproblemi sono 2– costo: 2·T( n/2)\n• combina\n– l’algoritmo MERGE ha costo lineare: C( n) = Θ(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#44": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itTempo di esecuzione di merge sort\n• complessivamente\n• poiché D( n) + C( n) = Θ(1) + Θ(n) = Θ(n) si ha\n• dimostreremo che questa particolare equazione \ndi ricorrenza ammette come soluzione Θ(1) per n= 0 o n= 1\n2·T(n/2) + D( n) + C( n) per n> 1\n Θ(1) per n= 0 o n= 1\n2·T(n/2) + Θ(n) per n> 1T(n) =\nT(n) =\nT(n) = Θ(nlog n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#45": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itMaster theorem (teorema dell’esperto)\n• siano a, b≥1\n• il master theorem considera l’equazione di ricorrenza \nseguente\n• il master theorem afferma che tale equazione di \nricorrenza ammette le soluzioni seguenti\n1. se a< bkallora T( n) = Θ(nk)\n2. se a= bkallora T( n) = Θ(nklog n)\n3. se a> bkallora T( n) = Θ(nlogba)Θ(1) per n= 0\na⋅T(n/b) + O(nk) per n> 0T(n) =",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#46": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itAlbero di ricorsionelog bnn\n1bn\nbn\nbn\nbn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n1 1 11\na\na2\na3\nn/bh= 1 ⇒ n = bh⇒ h = logbnnumero \nnodi\nnbalog",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#47": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itCosto per ogni livellolog bnn\n1bn\nbn\nbn\nbn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n1 1 1costo\nkn\n()kbna/\n()kbna2 2/\n()kbna3 3/\nnbalog",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#48": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itCosto per ogni livellolog bnn\n1bn\nbn\nbn\nbn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n1 1 1costo\nkn\n()kbna/\n()kbna2 2/\n()kbna3 3/\nnbalogMaster Theorem:\nT(n) = a⋅T(n/b) + O( nk) \n1. se a< bkallora T( n) = O( nk)\nIn pratica se a< bkla complessità è\ndominata dal primo livello \ndell’albero di ricorsione",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#49": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itCosto per ogni livellolog bnn\n1bn\nbn\nbn\nbn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n1 1 1costo\nkn\n()kbna/\n()kbna2 2/\n()kbna3 3/\nnbalogMaster Theorem:\nT(n) = a⋅T(n/b) + O( nk) \n2. se a= bkallora T( n) = O( nklog n)\nSe a= bkla complessità è uguale per \ntutti i livelli\n- infatti ai(n/bi)k=bki(n/bi)k=nk",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#5": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n0 variabile i0 variabile sum 3 istruzione2 variabile i1 variabile f 8 istruzioneFACT(0)4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#50": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itCosto per ogni livellolog bnn\n1bn\nbn\nbn\nbn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n2bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n3bn\n1 1 1costo\nkn\n()kbna/\n()kbna2 2/\n()kbna3 3/\nnbalogMaster Theorem:\nT(n) = a⋅T(n/b) + O( nk) \n3. se a> bkallora T( n) = O( nlogb a)\nSe a> bkla complessità è dominata \ndall’ultimo livello\n- infatti alogb n=nlogb a",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#51": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itDimostriamo che xlog y= ylog x\n• Partiamo da\nxlogy= ylogx\n• Facciamo il logaritmo da entrambe le parti\nlog(xlog y) = log( ylogx)  \n• Ricordando che\nlog ab= blog a\n• Otteniamo \n(log y)(log x) = (log x)(log y) \n• Che è vera per la proprietà commutativa del prodotto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#52": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itDimostrazione del primo caso ( a < bk)\n• La somma del costo di tutti i livelli è\n• è una serie geometrica con ragione\n• Se         , cioè se          , la sommatoria, anche se \navesse infiniti termini, sarebbe comunque una costante\n• Dunque∑ ∑ ∑\n= = =\n\n= =\n\n=h\nii\nkkh\niikk\nih\nik\nii\nbanbnabna nT\n0 0 0)(\nkbar=\n1<rkba<\n)1/(1 r−\n()knOnT =)(∑\n=\n\nh\nii\nkba\n0",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#53": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itDimostrazione del terzo caso ( a > bk)\n• Torniamo alla serie geometrica con \n• Se           , cioè se            , la sommatoria vale∑\n=\n\n=h\nii\nkk\nbannT\n0)(kbar=\n1>rkba>\n()hh h\nrOrr\nrr∈−−=−−\n11\n11\nkn\nknn\nn kn n\nkh\nna\nba\nba\nbarb\nbb\nbbb log\nloglog\nloglog log\n) (= = = \n\n=",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#54": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itDimostrazione del terzo caso ( a > bk)\n• Dunque\n•D a  c u i() () ()\n\n\n⋅ = ⋅ =kn\nk h k\nnaOnO rOnOnTblog\n)(\n( )()a nb bnO aOnTlog log)( = =",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#55": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEsempi di applicazione del master theorem\n•T (n) = 9T( n/3) + n\n– abbiamo: a= 9;b= 3; p(nk) = n;k= 1\n– quindi a> bk\n–s ih a  T ( n) = Θ(nlogba) = Θ(nlog39) = Θ(n2)\n•T (n) = T(2 n/3) + 1\n– abbiamo: a= 1; b= 3/2; p(nk) = 1; k= 0\n– quindi a= bk\n–s i  h a  T ( n) = Θ(nk log n) = Θ(n0log n) = Θ(log n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#56": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itComplessità del merge sort\n• la complessità del merge sort è data dalla \nformula di ricorrenza \nT(n) = 2·T( n/2) + Θ(n)\n• applichiamo il teorema dell’esperto\n– abbiamo: a= 2; b= 2; p(nk) = n; k= 1\n– quindi a= bk\n–s i  h a  T ( n) = Θ(nk log n) = Θ(nlog n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#57": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itGraficamente\nΘ(n)log2nn\n1n\n2n\n2\nn\n4n\n4n\n4n\n4\n24Θ(n)\nΘ(n)\nΘ(n)\nΘ(n)\n1\nΘ(n log n)albero delle \nchiamate ricorsivecontributi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#58": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itAlgoritmi di ordinamento visti finora\nsi no Θ(n logn) MERGE-SORTsisi stabile\nsisi in loco\nΘ(n2) Θ(n2) Θ(n) INSERTION-SORTΘ(n2) SELECTION-SORTcaso \npeggiore\ncaso \nmedio\ncaso \nmigliore",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#6": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n0 variabile i1 variabile sum3 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#7": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n1 variabile i1 variabile sum 2 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#8": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n1 variabile i1 variabile sum 3 istruzione2 variabile i1 variabile f 8 istruzioneFACT(1)4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\070-ricorsione-e-complessita-06.pdf#9": "070-ricorsione-e-complessita-06 copyright ©2022 maurizio.patrignani@uniroma3.itEffetti di una chiamata a funzione\n• supponiamo di eseguire \nSUM_OF_FACT (3)\n• seguiamo l’evoluzione \ndello stack dei record di \nattivazione\n1 variabile i2 variabile sum 3 istruzione4. return sum3.sum = sum + FACT(i)2. fori = 0 ton1.sum = 0SUM_OF_FACT(n)\n8. return f7.f = f * i6. fori = 2 ton5.f = 1FACT(n)\nSUM_OF_FACT(3)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#0": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nTipi astratti di dato\n(pile e code realizzate tramite array)\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#1": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#10": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itRealizzazione di una pila con array\n• Una pila ppuò essere realizzata tramite un \noggetto contenente un array p.A e un intero \np.top che specifica l’indice dell’elemento \naffiorante\n4712\n• Quando la pila è vuota p.top vale -1\n-1p.top\n3\np.topp.A\np.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#11": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itPUSH(p,9)Sequenza di operazioni su una pila\np.top\n-1\n3p.top\n0\n31p.top\n13149p.top\n2PUSH(p,3)\nPUSH(p,1)POP(p)PUSH(p,4)\n3149p.top\n3\n3149p.top\n1POP(p)p.A\np.A\np.Ap.Ap.Ap.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#12": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itOperazione NEW_STACK\n• Implementazione in pseudocodice della \nfunzione di creazione\n• L’uso della funzione NEW_STACK èi l  \nseguente4. return p3. p.top = –12.Z p.top intero1.Zcreo un oggetto p con: p.A array di maxsize interiNEW_STACK(maxsize)\n...p = NEW_STACK (10) Zcreo una pila con 10 posizioni...",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#13": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itOperazioni PUSH e POP\n• Implementazione delle funzioni di modifica\n5.return p.A[p.top + 1]4.p.top = p.top – 13. else 2. error (“underflow”)1. if p.top == -1POP(p)5.p.A[p.top] = x4.p.top = p.top + 13. else 2. error (“overflow”)1. if p.top == p.A.length-1PUSH(p,x)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#14": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAltre operazioni sulle pile\n1. return p.top == -1 Ztrue se la pila è vuotaIS_EMPTY(p)\n1.p.top = -1 Zvuoto la pilaEMPTY(p)\n1. return p.A[p.top] Zl’elemento affiorante TOP(p)\n• Il tempo di esecuzione di ogni operazione è Θ(1)1. return p.top + 1 Zil numero di elementiSIZE(p)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#15": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itGestione telescopica della pila\n• Strategia utilizzata per non avere limiti sulla \ndimensione della pila\n• Si adotta una dimensione iniziale di default\n– può essere un numero fissato dal programmatore\n• per esempio 128 posizioni\n– può essere il numero di celle specificato dall’utente \nnella funzione NEW_STACK\n• Quando viene eseguita una PUSH sulla pila \npiena si raddoppia la dimensione della pila \ncorrente per poter inserire ulteriori elementi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#16": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itNuova funzione PUSH\n• Questa funzione PUSH raddoppia la dimensione \ndell’array p.A quando l’array è pieno\n7.p.A[p.top] = x6.p.top = p.top + 1 Zinserisco l’elemento x5.p.A = B Zsostituisco B a p.A4. B[i] = p.A[i] Zcopio p.A dentro B3. for i = 0 top.A.length-12. ZB nuovo array di 2*p.A.length posizioni1. if p.top == p.A.length-1PUSH(p,x)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#17": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAnalisi ammortizzata\n12 34 8 1 6 3 3 5 9 17 3212\n1° inserimento, costo 1\n2° inserimento, costo 134\n3° inserimento, costo 1\n4° inserimento, costo 1 (array pieno)\n1• Calcoliamo il costo degli inserimenti partendo \nda un array da 4 elementi\n• Diagramma dei costi:",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#18": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAnalisi ammortizzata\n12 344\n81 6 3 35° inserimento, costo 5 \n(4 per la copia, 1 per l’inserimento)\n5 9 17 325° inserimento\n5 1234\nraddoppio la dimensione \ndell’array e ci copio dentro i \nprimi 4 elementi• Calcoliamo il costo degli inserimenti partendo \nda un array da 4 elementi\n• Diagramma dei costi:",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#19": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.it5Analisi ammortizzata\n• Calcoliamo il costo degli inserimenti partendo \nda un array da 4 elementi\n• Diagramma dei costi:\n12 344\n81 6 3 3 5 9 17 321234678\n6° inserimento, costo 1\n7° inserimento, costo 1\n8° inserimento, costo 1",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#2": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itSommario\n• Tipi astratti di dato\n• Strutture di dati elementari\n– le pile\n– le code\n• Realizzazione con array di queste strutture\n• La complessità ammortizzata",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#20": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.it• Calcoliamo il costo degli inserimenti partendo \nda un array da 4 elementi\n• Diagramma dei costi:5Analisi ammortizzata\n8\n12 344\n81 6 3 3 5 9 17 321234678 9° inserimento9\nraddoppio la dimensione \ndell’array e ci copio dentro i \nprimi 8 elementi\n9° inserimento, costo 9 \n(8 per la copia, 1 per l’inserimento)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#21": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAnalisi ammortizzata\n81632\n12 344\n81 6 33 59 1 7 32• Calcoliamo il costo degli inserimenti partendo \nda un array da 4 elementi\n• Diagramma dei costi dopo molti inserimenti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#22": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAnalisi ammortizzata\n81632\n12 344\n81 6 33 59 1 7 32• Calcoliamo il costo degli inserimenti partendo \nda un array da 4 elementi\n• Diagramma dei costi dopo molti inserimenti\n• Distribuiamo i costi sugli inserimenti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#23": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itComplessità ammortizzata\n• Alcuni inserimenti costano effettivamente \nΘ(n), dove n è il numero di elementi già\ncontenuti nella pila\n– è corretto dire che il costo di un inserimento nel \ncaso peggiore è Θ(n)\n• Una sequenza di n inserimenti costa Θ(n)\n– si dice che la complessità “ammortizzata” di un \ninserimento è Θ(1)\n4\n12 34 8 1 6 59 1 7 3332",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#24": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itUso delle realizzazioni di un ADT\n• Dopo aver realizzato un ADT possiamo mettere \nl’implementazione a disposizione di altri programmatori specificando\n– le eventuali limitazioni della realizzazione\n– l’elenco delle operazioni supportate e la loro complessità\nasintotica o ammortizzata\n•E s e m p i o\n– si dispone di una realizzazione di una pila con le seguenti \nfunzioni e complessità nel caso peggiore\n• NEW_STACK(maxsize) Θ(1)\n• IS_EMPTY(p) Θ(1)\n• PUSH(p,x) Θ(1)\n• POP(p) Θ(1)\n• EMPTY(p) Θ(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#25": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itIl tipo astratto di dato coda\n• Le code (o queue) realizzano una strategia \nFIFO (first-in first-out) \n• Tipo astratto: coda di interi\n–d o m i n i\n• il dominio di interesse è l’ins ieme delle code Q di interi\n• dominio di supporto: gli interi Z = {0, 1, -1, 2, -2, …}• dominio di supporto: i booleani B = {true, false}\n– costanti\n• la coda vuota",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#26": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itIl tipo astratto di dato coda\n• Tipo astratto: coda di interi\n– operazioni\n• verifica se una coda è vuota\nIS_EMPTY: Q →B\n• inserimento di un elemento nella coda\nENQUEUE: Q ×Z →Q\n• rimozione e restituzione dell’elemento più vecchio della coda\nDEQUEUE: Q →Q ×Z\n– operazioni aggiuntive\n• lettura dell’elemento più vecchio della coda\nFRONT: Q →Z\n• svuotamento della coda\nEMPTY: Q →Q\n• numero degli elementi nella coda\nSIZE: Q →Z",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#27": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itRealizzazione dell’ADT coda\n• NEW_QUEUE(maxsize)\n– ritorna un riferimento ad una coda vuota che può contenere \nal massimo maxsize interi\n• IS_EMPTY(c)\n– ritorna true se la coda è vuota, altrimenti ritorna false\n• ENQUEUE(c,x)\n– inserisce un elemento xnella coda\n• può dare un errore di “overflow” se l’implementazione prevede un \nnumero massimo di elementi \n• DEQUEUE(c)\n– rimuove l’elemento più vecchio della coda e lo restituisce\n• dà un errore di “underflow” se la coda è vuota",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#28": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAltre operazioni sulle code\n•FRONT(c)\n– ritorna l’elemento più vecchio senza rimuoverlo\n• può dare errore se la coda è vuota\n•EMPTY(c)\n– svuota la coda\n•SIZE(c)\n– ritorna il numero degli elementi in coda",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#29": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itRealizzazione di una coda tramite array\n• Una coda cpuò essere realizzata con un array c.A\narricchito da due attributi c.head e c.tail che \ncontengono gli indici dell’el emento più vecchio e della \nprima posizione utile\n4712\n• Un nuovo elemento viene aggiunto da ENQUEUE nella \nposizione c.tail (che viene incrementato) c.tail4c.head\n0\n47127\nc.tail5c.head\n0c.A\nc.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#3": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itTipo astratto di dato\n•U n  tipo astratto di dato (o ADT, abstract data \ntype) è una descrizione di un tipo di dato \nindipendente dalla sua realizzazione in un \nlinguaggio di programmazione\n• Un tipo astratto di dato è costituito da:\n– i domini interessati\n• tra cui il dominio di intere sse ed eventualmente altri \ndomini di supporto\n– un insieme di costanti\n– una collezione di operazioni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#30": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itRealizzazione di una coda tramite array\n• L’elemento ritornato da DEQUEUE è quello in \nposizione c.head (che viene incrementato)\n4712\n• L’array è gestito come una lista circolarec.tail4c.head\n0\n57124362\nc.tail1c.head\n44712\nc.tail4c.head\n1c.A\nc.A\nc.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#31": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itRealizzazione di una coda tramite array\n• La coda è vuota quando c.head e c.tail puntano \nalla stessa casella\n• La coda non può avere più di n-1 elementi\n– se avesse nelementi, che valore potremmo dare a c.tail\nsenza creare equivoco con la coda vuota?c.tailc.head\n4617362\nc.tailc.headc.A\nc.A5 5\n4 3",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#32": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itOperazioni ENQUEUE e DEQUEUE\n6. else c.tail = c.tail + 15. c.tail = 04. if c.tail == c.A.length-13. else c.A[c.tail] = x2. error (“overflow”)1. if c.head==c.tail+1 or(c.tail==c.A.length-1 andc.head==0)ENQUEUE(c,x)\n7. return x6. else c.head = c.head + 15. c.head = 04. if c.head == c.A.length-13. else x = c.A[c.head]2. error (“underflow”)1. if c.head == c.tailDEQUEUE(c)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#33": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAltre operazioni sulle code\n1. return c.head == c.tailIS_EMPTY(c)\n1.c.head = c.tail = 0EMPTY(c)\n4. return c.A[c.head]3. else2. error (“empty queue”)1. if c.head == c.tailFRONT(c)\n• Il tempo di esecuzione di ogni operazione è Θ(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#34": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itEsercizi sulle code\n1. Scrivi lo pseudocodice della procedura \nNEW_QUEUE () che restituisce il riferimento ad \nuna coda vuota \n2. Scrivi lo pseudocodice della procedura \nSIZE (c) che restituisce il numero di elementi \nin una coda\n3. Scrivi lo pseudocodice della procedura \nENQUEUE(q,x) che abbia complessità\nammortizzata Θ(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#35": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itEsercizi su pile e code\n4. Supponi di disporre di una realizzazione di una pila con le \nseguenti caratteristiche\n– funzioni NEW_STACK , IS_EMPTY e POP di complessità Θ(1)\n– funzione PUSH di complesità nel caso peggiore Θ(n) e complessità\nammortizzata Θ(1)\nDescrivi come sia possibile implementare un coda utilizzando \nesclusivamente pile\n– ti occorreranno due pile p1e p2da usare simultaneamente\n– l’operazione NEW_QUEUE creerà le due pile p1e p2\n– le operazioni ENQUEUE e DEQUEUE si tradurranno in opportune \noperazioni di PUSH e POP sulle due pile p1e p2\nDiscuti la complessità delle operazioni ENQUEUE e DEQUEUE",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#36": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itEsercizi su pile e code\n5. Supponi di disporre di una realizzazione di una coda con le \nseguenti caratteristiche\n– funzioni NEW_QUEUE , IS_EMPTY e DEQUEUE di complessità Θ(1)\n– funzione ENQUEUE di complesità nel caso peggiore Θ(n) e \ncomplessità ammortizzata Θ(1)\nDescrivi come sia possibile implementare un pila utilizzando \nesclusivamente code\n– ti occorreranno due code q1e q2da usare simultaneamente\n– le operazioni PUSH e POP si tradurranno in opportune operazioni di \nENQUEUE e DEQUEUE sulle due code q1e q2\nDiscuti la complessità delle operazioni PUSH e POP",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#37": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itEsercizi su pile e code\n6. Scrivi lo pseudocodice della procedura \nCONTAINS(p,x) che ritorna true se l’elemento \nxè contenuto nella pila pe ritorna false se xnon \nè contenuto (lasciando la pila invariata)\n7. Scrivi lo pseudocodice di una struttura dati in cui si \npossa inserire/rimuovere elem enti sia in testa che in \ncoda\n– deve avere contemporaneamente PUSH , POP, DEQUEUE , \nENQUEUE (dove evidentemente ENQUEUE è uguale a \nPUSH ) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#4": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itIl tipo astratto di dato pila\n• Le pile (o stack) realizzano una strategia LIFO \n(last-in first-out) \n• Tipo astratto: pila di interi\n–d o m i n i\n• il dominio di interesse è l’insieme delle pile P di interi\n• dominio di supporto: gli interi Z = {0, 1, -1, 2, -2, …}• dominio di supporto: i booleani B = {true, false}\n– costanti\n• la pila vuota ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#5": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itIl tipo astratto di dato pila\n• Tipo astratto: pila di interi\n– operazioni\n• verifica se una pila è vuota\nIS_EMPTY: P →B\n• inserimento di un elemento nella pila\nPUSH: P ×Z →P\n• rimozione e restituzione dell’elemento affiorante della pila\nPOP: P →P ×Z\n– operazioni aggiuntive\n• lettura dell’elemento affiorante della pila\nTOP: P →Z\n• svuotamento della pila\nEMPTY: P →P\n• numero degli elementi nella pila\nSIZE: P →Z",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#6": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itRealizzazione di un tipo astratto di dato\n• Un tipo astratto di dato può essere realizzato (o \nimplementato ) in uno specifico linguaggio di \nprogrammazione tramite la definizione di\n– tipi concreti o strutture dati nello specifico \nlinguaggio di programmazione corrispondenti ai domini necessari\n– costrutti che consentono di codificare le costanti\n– funzioni che realizzano le operazioni previste\n• Ovviamente uno stesso tipo astratto di dato può \navere diverse realizzazioni con diverse proprietà",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#7": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itRealizzazione di un ADT: osservazioni\n• La definizione dei tipi concreti o delle strutture che \ncorrispondono ai domini può comportare delle limitazioni\n– esempi di limitazioni \n• il dominio P delle pile viene ristrett o alle pile di dimensione massima \nmaxsize , nota a priori\n• il dominio Z degli interi viene re alizzato tramite il tipo concreto int\nche ha un valore minimo MININT e massimo MAXINT\n• Le costanti vengono spesso codificate tramite delle \nfunzioni che ritornano la  struttura corrispondente\n– esempio di costante: \n• la pila vuota viene realizzata tramite\nCREATE-STACK(maxsize)\nche ritorna il riferimento ad una pila vuota che potrà contenere al \nmassimo maxsize elementi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#8": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itFunzioni che operano su pile\n• NEW_STACK(maxsize)\n– ritorna il riferimento ad una pila vuota che potrà contenere al \nmassimo maxsize elementi \n• IS_EMPTY(p)\n– ritorna true se la pila è vuota, false altrimenti\n• PUSH(p,x)\n– inserimento di un elemento nella pila\n• può dare un errore di “overflow” se l’implementazione prevede un \nnumero massimo di elementi nella pila\n• POP(p)\n– rimozione e restituzione dell’elemento affiorante della pila\n• dà un errore di “underflow” se la pila è vuota",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\080-tipi-astratti-di-dato-04.pdf#9": "080-tipi-astratti-di-dato-04         copyright ©2019 maurizio.patrignani@uniroma3.itAltre funzioni su pile\n•TOP(p)\n– ritorna l’elemento affiorante senza rimuoverlo\n• può dare errore se la pila è vuota\n•EMPTY(p)\n– svuota la pila\n•SIZE(p)\n– ritorna il numero degli elementi in pila",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#0": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itAlgoritmi e Strutture di Dati\nListe implementate tramite array\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#1": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#10": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itLista doppiamente concatenata\n• La struttura di dati supporta il passaggio diretto da un \niteratore al successivo e al precedente\n– si vogliono le operazioni FIRST, NEXT e PREV in Θ(1)\nI\nINFOFIRST\nI\nINFONEXTI\nINFOiteratori\nelementi E E EL\nPREVlista\nNEXT\nPREV",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#11": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itListe con accesso agli estremi\n• La struttura di dati supporta l’accesso diretto al \nprimo e all’ultimo iteratore della lista\n– si vogliono le operazioni FIRST e LAST in Θ(1)\nI\nINFOFIRST\nI\nINFONEXTI\nINFOiteratori\nelementi E E EL\nPREVlista\nNEXT\nPREVLAST",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#12": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itRealizzazione di una lista con array\n• Supponiamo di mettere gli elementi della lista nelle \ncelle successive di un array l.info\n– come facciamo a gestire la cancellazione di un elemento \nintermedio della lista?\n– come facciamo a sapere quali celle dell’array sono \nutilizzate?\n• non c’è nessun valore intero che possiamo associare ad una cella \nvuota2385l.info\n285l.info",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#13": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itRealizzazione di una lista con array\n• Gli elementi della \nlista sono memorizzati in un \narray l.info\n• L’array l.next\ncontiene l’indice \ndell’elemento che segue\n• L’array l.prev\ncontiene l’indice \ndell’elemento che \nsegue235\n31l.head6\nl.next\n352l.info\n61l.prevsequenza:         ,         ,",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#14": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itRealizzazione di una lista con array\n• L’iteratore della \nlista è un intero\n– è l’indice della \nposizione dell’elemento \ncorrispondente\n• L’iteratore non \nvalido èrappresentato dal valore -1235\n3-11l.head6\nl.next\n352l.info\n61-1l.prevsequenza:         ,         ,",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#15": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itPiù liste con gli stessi array?\n• Gli array \nl.next , \nl.info ed \nl.prev possono \nessere condivisi \nda due o più liste\n– le liste non \ninterferiscono, perché utilizzano \nposizioni diverse \ndegli array 3-1-121l2.head5\nl1.next = l2.next\n37542l1.info = l2.info\n651-1-1l1.prev = l2.prevl1.head6235 sequenza 1:         ,         ,\n47 sequenza 2:         ,         ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#16": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itUso della lista libera\n• Per inserire un nuovo elemento \nin lista occorre sapere quali posizioni degli array sono \nancora libere\n• Tutte le posizioni libere \npossono essere memorizzate in \nuna seconda lista l.free\n– un inserimento di un elemento in \nlè un trasferimento di una \nposizione dalla lista l.free\nalla lista l.head\n– una cancellazione da lè un \ntrasferimento di una posizione \ndalla lista l.head alla lista \nl.free430-1721-1l.free5\nl.next\n352l.info\n26510-1-14l.prevl.head6",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#17": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itConfigurazione iniziale: lista vuota\n• Quando la lista lè vuota tutte le posizioni sono \nassegnate alla lista l.free\n1234567-1l.free0\nl.next\nl.info\n-10123456l.prevl.head-1\n-1234567-1l.free1\nl.next\n4l.info\n-1-1123456l.prevl.head0inserimento \ndel primo elemento",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#18": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itCreazione di una lista vuota di interi\n• Procedura per inizializzare una lista vuota di \ninteri\n10.return l9.l.free = 08.l.head = -17.l.next[maxsize-1] = -16.l.prev[i] = i-15.l.next[i] = i+14. for i = 0 to maxsize-13.Zl.head, l.free interi2.Zl.next, l.info, l.prev array di maxsize interi1.Zcreo un nuovo oggetto l con:NEW_LIST(maxsize) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#19": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itGestione della lista l.free\n• Procedura di servizio per ottenere una posizione \nlibera dalla lista l.free\n8. return i 7. l.prev[l.free] = -16. if l.free!=- 15.l.free = l.next[l.free]4.i = l.free Zi è l’indice della nuova posizione3. else2. error (“overflow”)1. if l.free == -1ALLOCATE-COLUMN(l) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#2": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itSommario\n• Il tipo astratto di dato lista\n– tipologie di liste e strategie di realizzazione \n• Realizzazione delle liste con array\n– uso di tre array\n– uso di un solo array\n– liste disomogenee",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#20": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itGestione della lista l.free\n• Procedura di servizio per restituire una \nposizione alla lista l.free\n5.l.free = i4. l.prev[l.free] = i3. if l.free != -12.l.next[i] = l.free1.l.prev[i] = -1FREE-COLUMN(l,i) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#21": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itCodice di INSERT (l,x)\n7.l.head = i6.l.prev[l.head] = i5. ifl.head != -14.l.next[i] = l.head Zil resto della lista segue i3.l.prev[i] = -1 Zi diventa primo elemento2.l.info[i] = x1.i =ALLOCATE-COLUMN(l) Zindice di una nuova colonna liberaINSERT(l,x) Zx è il valore da aggiungere in lista\n3-11l.head6\nl.next\n352l.info\n61-1l.previ\n3-116l.head7\nl.next\n352xl.info\n617-1l.prev7",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#22": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itEsercizi: liste implementate con array\n1. Scrivi lo pseudocodice della procedura SIZE (l) che \nconta gli elementi della lista l\n2. Scrivi lo pseudocodice della procedura \nSEARCH (l,k) che restituisce la posizione del primo \nelemento di lcon valore della chiave k\n3. Scrivi lo pseudocodice della procedura \nDELETE (l,i) che rimuove da ll’elemento in \nposizione i\n4. Scrivi lo pseudocodice della procedura \nDELETE (l,x) che rimuove da lil primo elemento \nche ha valore x",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#23": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itRappresentazione con un solo array\n3-11l.head6\nl.next\n352l.info\n61-1l.prev\n39 -1l.head18\n518 3 23-1• Un solo array è sufficiente a rappresentare le \ninformazioni degli \narray l.next , l.info\ned l.prev\nl.all\nnextprev• Ovviamente è anche \nnecessaria una lista \nl.free (non \nrappresentata in figura)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#24": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itEsercizi: liste su un solo array\nNegli esercizi seguenti supponi che la lista lsia \ndoppiamente concatenata ed implementata tramite \nun solo array\n5. Scrivi lo pseudocodice della procedura \nALLOCATE_OBJECT (l) \n6. Scrivi lo pseudocodice della procedura  \nFREE_OBJECT (l,i)\n7. Scrivi lo pseudocodice della procedura \nLOWER_FREE_POSITION (l) che trova la \nposizione con indice più basso tra gli elementi della \nlista libera l.free",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#25": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itListe con elementi eterogenei\n• L’uso di un singolo array consente la gestione di liste \ndi elementi eterogenei\n– un campo aggiuntivo specifica la dimensione di ogni \nelemento\n– la lista l.free (non rappresentata in figura) viene gestita \ncon criteri analoghi\n1613 5l.head22\n22 22 -14194-1\nsizel.all",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#26": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itEsercizi: liste eterogenee\nnegli esercizi seguenti supponi che la lista lsia eterogenea, \ndoppiamente concatenata, ed implementata tramite un \nsingolo array \n8. Scrivi lo pseudocodice della procedura ALLOCATE-\nOBJECT-WITH-SIZE (l,x) che trova nella lista libera \nl.free una posizione adatta ad ospitare un elemento di \ndimensione x\n9. Scrivi lo pseudocodice della procedura FREE-OBJECT (l,i) \nche inserisce nella lista libera l.free l’oggetto in \nposizione i\n10. Scrivi lo pseudocodice della procedura INSERT (l,A) che \ninserisce nella lista lun elemento di dimensione A.length\nche contiene tutti i valori dell’array A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#27": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itEsercizio: cancellazione lazysu liste eterogenee\nNegli esercizi seguenti supponi che una lista eterogenea doppiamente \nconcatenata ed implementata con un singolo array preveda, per ogni \nelemento, un valore che specifica se  l’elemento è da considerarsi \n“rimosso” oppure no\n11. Scrivi lo pseudocodice della procedura DELETE (l,i) dove iè l’indice \ndella posizione di un elemento  da marcare come “rimosso”\n12. Scrivi lo pseudocodice della procedura GARBAGE-COLLECTION (l) che \ntrasferisce nella lista l.free tutti gli elementi marcati come “rimossi”\n106 2 -14\n21 13 5-14\n0 = elemento rimosso 1 = elemento non rimossol.head\nl.all",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#3": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itListe\n• Le liste sono strutture di dati in cui gli oggetti \nsono disposti in una sequenza lineare\n– si assume che l’utente voglia scorrere gli elementi \ntramite un iteratore\n• Tipo astratto: lista di interi\n–d o m i n i\n• il dominio di interesse è l’ins ieme delle liste L di interi\n• dominio di supporto: gli iteratori I che identificano le \nposizioni\n• dominio di supporto: gli interi Z = {0, 1, -1, 2, -2, …}\n• dominio di supporto: i booleani B = {true, false}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#4": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itRealizzazione di una lista\n• Costanti\n– la lista vuota viene reali zzata tramite la funzione NEW_LIST (maxsize )\n• ritorna il riferimento ad una lista vuota che può contenere al massimo \nmaxsize elementi\n– l’iteratore non valido è solitamente uno specifico valore dell’iteratore\n• Operazioni di aggiornamento\n– l’inserimento in testa alla lista INSERT: L ×Z →L viene realizzato \ntramite la funzione INSERT (l,x)\n– l’inserimento in coda ADD: L ×Z→L viene realizzato tramite la \nfunzione ADD(l,x)\n– l’eliminazione di un elemento a partire dal suo iteratore DELETE: L ×I\n→L viene realizzata tramite la funzione DELETE (l,i)\n– la ricerca e l’eliminazione  di un elemento DELETE: L ×Z→L viene \nrealizzata tramite la funzione DELETE (l,x)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#5": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itOperazioni possibili sulle liste\n• Altre operazioni di aggiornamento\n– l’inserimento prima di una posizione specifica \nINSERT_BEFORE: L ×I ×Z →L viene realizzato \ntramite la funzione INSERT_BEFORE (l,x,i)\n– l’inserimento dopo una posizione specifica \nADD_AFTER: L ×I  ×Z →L viene realizzato \ntramite la funzione ADD_AFTER (l,x,i)\n– lo svuotamento della lista EMPTY: L →L viene \nrealizzato tramite la funzione EMPTY (l)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#6": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itOperazioni possibili sulle liste\n• Operazioni di consultazione\n– l’iteratore del 1° elemento della lista HEAD: L →I si ottiene \ntramite la funzione HEAD (l)\n• ritorna l’iteratore non valido se la lista è vuota\n– l’iteratore successivo all’iteratore corrente NEXT: L ×I→I \nsi ottiene tramite la funzione NEXT (l,i)\n• ritorna l’iteratore non valido se iè l’iteratore dell’ultimo elemento\n– l’iteratore precedente all’iteratore corrente PREV: L ×I→I \nsi ottiene tramite la funzione PREV( l,i)\n• ritorna l’iteratore non valido se iè l’iteratore del primo elemento\n– l’intero associato alla posizion e specificata da un iteratore \nINFO: L ×I→Z si ottiene con la funzione INFO( l,i)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#7": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itOperazioni possibili sulle liste\n• Altre operazioni di consultazione\n– la ricerca della posizione di un elemento SEARCH: L ×Z→\nI si ottiene tramite la funzione SEARCH (l,k)\n• ritorna l’iteratore dell’elemento con chiave knella lista loppure \nl’iteratore non valido\n– l’iteratore associato all’ultimo  elemento della lista LAST: L \n→I si ottiene tramite la funzione LAST (l)\n• ritorna l’iteratore non valido se la lista è vuota\n– la verifica se una lista è vuota IS_EMPTY: L →B è\nrealizzata dalla funzione IS_EMPTY (l)\n– il numero degli elementi in lista SIZE: L →Z si ottiene \ntramite la funzione SIZE (l)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#8": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itStrategie di realizzazione delle liste\n• Dipendentemente dal tipo di operazioni che è\nnecessario compiere in maniera efficiente sulla lista esistono diverse strategie implementative:\n– lista concatenata\n• consente lo scorrimento effici ente della lista in avanti ma \nnon consente un efficiente scorrimento all’indietro\n– lista doppiamente concatenata\n• supporta in maniera efficiente lo scorrimento in avanti e \nindietro\n– accesso agli estremi\n• consente un veloce accesso sia al primo che all’ultimo \nelemento della lista",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\090-liste-tramite-array-03.pdf#9": "090-liste-tramite-array-03         copyright ©2021 patrignani@dia.uniroma3.itLista semplicemente concatenata\n• La struttura di dati supporta il passaggio diretto \nda un iteratore all’iteratore successivo\n– si vogliono le operazioni FIRST e NEXT in Θ(1)\nI\nINFOFIRST\nI\nINFONEXT I\nINFONEXT iteratori\nelementi E E EL lista",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#0": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nImplementazioni di liste con\noggetti e riferimenti\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#1": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#10": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi: liste singolarmente concatenate\nEsercizi sullo scorrimento delle liste\n1. Scrivi lo pseudocodice della procedura \nSOMMA (l) che ritorna la somma degli \nelementi contenuti in una lista singolarmente \nconcatenata di interi\n2. Scrivi lo pseudocodice della procedura \nMASSIMO (l) che ritorna il valore del \nmassimo elemento contenuto in una lista \nsingolarmente concatenata di interi\n• assumi che la lista non sia mai vuota",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#11": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itLista concatenata: cancellazione \n• DELETE (l,i): cancellazione del nodo i\n• La cancellazione di un nodo diverso dal primo è poco \nefficiente in una lista singolarmente concatenata\n• Occorre infatti modificare l’attributo next del nodo \nche lo precedel.head\nl.headi\nl\nl",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#12": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi: liste singolarmente concatenate\n3. Scrivi lo pseudocodice della procedura SEARCH (l,u) \nche ritorna il riferimento all’elemento iche contiene \nil valore intero uin una lista singolarmente \nconcatenata di interi (oppure NULL se unon è nella \nlista)\n– discuti la complessità dell’algoritmo in funzione del \nnumero n degli elementi in lista\n4. Scrivi lo pseudocodice della procedura PREV (l,i) \nche ritorna il riferimento all’elemento che precede \nl’elemento identificato dall’iteratore iin una lista \nsingolarmente concatenata di interi (oppure NULL se \nicorrisponde al primo elemento della lista)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#13": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi: liste singolarmente concatenate\n5. Scrivi lo pseudocodice dell’operazione \nDELETE (l,i) che cancella il nodo idi una \nlista singolarmente concatenata\n– discuti della complessità dell’algoritmo\n6. Scrivi lo pseudocodice dell’operazione \nDELETE (l,u) che cancella il nodo che \ncontiene il valore intero uin una lista \nsingolarmente concatenata di interi\n– discuti della complessità dell’algoritmo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#14": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi: liste singolarmente concatenate\n7. Implementa una pila di interi utilizzando \noggetti e riferimenti\n– devi realizzare le funzioni  NEW_STACK (),\nIS_EMPTY (p), PUSH (p,u), e POP(p) facendo \nuso di oggetti e riferimenti\n8. Implementa una coda di interi utilizzando \noggetti e riferimenti\n– devi realizzare le funzioni NEW_QUEUE (),\nIS_EMPTY (c), ENQUEUE (c,u), e DEQUEUE (c) \nfacendo uso di oggetti e riferimenti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#15": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi: liste singolarmente concatenate\n9. Scrivi lo pseudocodice della procedura \nCOMUNI (l1,l2) che ritorna il numero di elementi \ndella lista l1che sono anche contenuti nella lista l2\n– discuti la complessità dell’algoritmo proposto\n10. Scrivi lo pseudocodice della procedura non ricorsiva \nINVERSA (l) che ritorna una nuova lista \nsingolarmente concatenata in cui gli elementi sono in \nordine inverso\n11. Scrivi lo pseudocodice della precedura \nACCODA (l1,l2) che accoda gli elementi della lista l2alla lista l1mantenendo l’ordine relativo che gli \nelementi avevano nelle liste originarie\n– puoi supporre di poter modificare le liste in input",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#16": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itLista doppiamente concatenata\n• Oltre all’attributo next i nodi dispongono \nanche dell’attributo prev\nl.head\n• Talvolta la lista ldispone anche di un attributo \nl.tail\nl.head\nl.tailll",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#17": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itInserimento nella lista\n•INSERT (l,n): inserimento in testa alla lista\nl.headl\nl.headnx\nl\nnx",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#18": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itInserimento nella lista\n•INSERT (l,n): inserimento in testa alla lista\n9.l.head = x8.l.head.prev = x7. ifl.head != NULL6.x.prev = NULL5.x.next = l.head4.x.info = n3.Zx.prev, x.next (riferimenti  ad oggetti analoghi)2.Zx.info (intero)1.Zx è un nuovo oggetto con tre campi:INSERT(l,n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#19": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itCancellazione di un elemento\n6.i.next.prev = i.prev5. if i.next != NULL4.l.head = i.next3. else 2.i.prev.next = i.next1. if i.prev != NULLDELETE(l,i)l.headi\nl.headl\nl",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#2": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itStrutture di dati con oggetti e riferimenti\n• alcuni linguaggi supportano oggetti e riferimenti\n– lo pseudocodice è uno di questi\n• con oggetti e riferimenti si possono realizzare strutture \ndi dati elementari in modo più naturale \n• vedremo la realizzazione del tipo astratto di dato lista\n– pile e code possono essere rivisti come casi particolari di \nliste\n– due principali varianti implementative:\n• lista singolarmente concatenata\n• lista doppiamente concatenata",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#20": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi su liste doppiamente concatenate\n12. Scrivi lo pseudocodice dell’operazione \nINSERT_BEFORE (l,n,i) che riceva come \nparametri una lista doppiamente concatenata l, un \nintero ned un iteratore i, e inserisca nnella lista \nprima dell’elemento riferito da i\n• discuti la complessità della procedura\n13. Scrivi lo pseudocodice dell’operazione \nADD_AFTER (l,n,i) che riceva come parametri una \nlista doppiamente concatenata l, un intero ned un \niteratore i, e inserisca nnella lista dopo l’elemento \nriferito da i\n• discuti la complessità della procedura",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#21": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi su liste doppiamente concatenate\n14. Implementa una coda utilizzando una lista \ndoppiamente concatenata \n• è possibile che le operazioni ENQUEUE e \nDEQUEUE abbiano entrambe complessità Θ(1)? \n• come si potrebbe fare per ottenere questo \nrisultato?\n15. Scrivi lo pseudocodice della procedura \nDELETE (l,u) che rimuova l’elemento che ha \nvalore uda una lista doppiamente concatenata \ndi interi\n• discuti la complessità dell’algoritmo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#22": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi su liste doppiamente concatenate\n16. Scrivi lo pseudocodice della procedura \nINSERT_ORDERED (l,u) che inserisca nella lista l\n(che si suppone ordinata in senso crescente) un \nintero umantenendo l’ordinamento crescente della \nlista\n17. Scrivi lo pseudocodice della procedura MERGE (l1, \nl2) che accetti come parametri due liste doppiamente \nconcatenate di interi ordinate in senso crescente e \nrestituisca una lista ordinata in senso crescente con \ngli elementi di entrambe\n– puoi supporre che tutti gli elem enti delle liste siano diversi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#23": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi sulle liste ordinate\n18. Scrivi lo pseudocodice della procedura \nDOPPIONI (l) che verifichi che una lista \n(non ordinata) doppiamente concatenata di \ninteri non abbia doppioni\n19. Scrivi lo pseudocodice della procedura \nDOPPIONI_SORTED (l) che verifichi che \nuna lista doppiamente concatenata di interi \nordinata in senso non-decrescente non abbia \ndoppioni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#24": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itListe con sentinelle\n• Le liste realizzate con oggetti e puntatori \noffrono l’opportunità di introdurre speciali \niteratori chiamati “sentinelle”\n• Il primo iteratore della lista (la “sentinella”) è\nsempre presente e non ha nessun elemento \nassociato\n• L’interatore non valido coincide con \nl’interatore che identifica la sentinella\n• La struttura dati è circolare",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#25": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itListe con sentinelle\n• Dalla lista si accede direttamente (ed \nesclusivamente) all’iteratore non-valido, cioè\nalla sentinella\nSNULL\nI\nINFONEXTI\nINFO\nelementi E EL\nPREVlista\nNEXT\nPREVNEXT\nPREV",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#26": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itUso della sentinella\n• Concatenando NULL+NEXT si ottiene FIRST\n• Concatenando NULL+PREV si ottiene LAST\n• Questa strategia comporta diversi altri vantaggi\n– molte procedure risultaranno semplificate\nSFIRST\nI\nINFONEXTI\nINFO\nelementi E EL\nPREVlista\nNEXT\nPREVNEXT\nPREVLAST",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#27": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itRealizzazione della sentinella\n• La sentinella è un nodo fittizio introdotto in \ntesta alla lista\nl.null\n• La lista vuota contiene solamente la sentinella\nl.nulll\nl",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#28": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itProcedure nelle liste con sentinelle\n• Esempi di procedura semplificata dall’uso di sentinelle\n– lista doppiamente concatenata (senza sentinella)\n– lista doppiamente concatenata con sentinella\n2.i.next.prev = i.prev1.i.prev.next = i.nextDELETE(l,i) Zversione con sentinella (i != l.null)6.i.next.prev = i.prev5. if i.next != NULL4.l.head = i.next3. else 2.i.prev.next = i.next1. if i.prev != NULLDELETE(l,i) Zversione senza sentinella (i != NULL)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#29": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itEsercizi sulle liste con sentinelle\n20. Scrivi lo pseudocodice della procedura \nINSERT (l,n) che inserisce in testa ad una \nlista con sentinella lun intero n\n21. Scrivi lo pseudocodice della procedura \nSEARCH (l,n) che ritorna un iteratore \nall’elemento della lista con sentinella lche ha \nvalore n\n• SEARCH (l,n) ritorna l.null se nnon è\npresente nella lista l",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#3": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itOperazioni su una lista di interi\nNEW_LIST () ritorna il riferimento ad una lista vuota\nHEAD (l) ritorna l’iteratore del primo elemento della lista\nLAST (l) ritorna l’iteratore dell’ultimo elemento della lista \nNEXT (l,i) ritorna l’iteratore dell’elemento che segue inella lista\nritorna un iteratore invalido se iè l’ultimo elemento\nPREV (l,i) ritorna l’interatore dell’elemento che precede inella lista\nritorna un iteratore invalido se iè il primo elemento\nINSERT (l,n) inserisce l’elemento nin testa alla lista l\nINSERT_BEFORE (l,n,i) inserisce l’elemento nprima della posizione i\nADD(l,n) aggiunge nin coda alla lista l\nADD_AFTER (l,n,i) aggiunge l’elemento ndopo la  posizione i\nDELETE (l,i) rimuove l’elemento in posizione i dalla lista l\nDELETE (l,n) rimuove l’elemento ndalla lista l\nEMPTY (l) vuota la lista\nSEARCH (l,n) ritorna l’iteratore dell’elemento n nella lista l\nIS_EMPTY (l) ritorna true se la lista è vuota, altrimenti ritorna false",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#4": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itLista concatenata (singly linked list)\n• l’iteratore iè un riferimento ad un nodo della lista, \nche è un oggetto composto dai seguenti attributi\n– i.info\n• elemento in lista  del tipo opportuno\n• può essere un riferimento ad un oggetto \nesterno con dati satellite \n– i.next\n• riferimento al nodo seguente o NULL\n• una lista lha un solo attributo\n– l.head\n• riferimento al primo nodo \nl.head\ni.infoi.next\nl",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#5": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itLista concatenata: lista vuota\n• Quando la lista lè vuota l.head èNULL\n1. return l.head == NULLIS_EMPTY(l)\n1.l.head = NULL Zlo pseudocodice non dealloca memoriaEMPTY(l)• Pseudocodice delle procedure IS_EMPTY e \nEMPTYl.headl",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#6": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itLista concatenata: first e next\n•FIRST : iteratore dell’elemento affiorante\n•NEXT : prossimo elemento\n1. return i.next Zil parametro l non è utilizzatoNEXT(l,i)1. return l.head Zpotrebbe essere NULLFIRST(l)l.head\nl.headi\nll",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#7": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itRealizzazione di funzioni elementari\n• La semplicità di alcune funzioni (come IS_EMPTY , \nEMPTY , FIRST , NEXT , ecc) induce a sostituirle con le \nistruzioni opportune direttamente nello pseudocodice\n– questo ovviamente fa perdere di generalità al codice scritto\n•E s e m p i o\n...x = NEXT(l,x)...then...if !IS-EMPTY (l) ...\n...x = x.next...then...if l.head != NULL ...",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#8": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itLista concatenata: inserimento in testa\n•INSERT : inserimento di nin testa alla lista\nnl.head x\nnl.headl\nl\n5.l.head = x4.x.next = l.head3. Zx.next (rif. ad analogo oggetto)2. Zx.info (intero)1.x.info = n Zx è un nuovo oggetto con due campi:INSERT(l,n) Zn è un intero",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\100-liste-tramite-oggetti-08.pdf#9": "100-liste-tramite-oggetti-08         copyright©2022 maurizio.patrignani@uniroma3.itLista concatenata: cancellazione \n• DELETE_FIRST : rimozione del primo nodo\n5.l.head = l.head.next4. else 3. error (“lista vuota”)2. if l.head == NULL1.ZNOTA: lo pseudocodice non dealloca l’elementoDELETE_FIRST(l)l.head\nl.headl\nl",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#0": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nAlberi radicati\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#1": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica\ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istitutipubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente\nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il\ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve\nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#10": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.it\nAlberi: definizioni\n• Qualunque nodo xsul cammino (unico) dalla yalla radice è un \nantenato diy, mentre yè un discendente dix;\n• L’insieme costituito da un nodo ze da tutti i suoi discendenti è\nilsottoalbero radicato a zx\nyzantenato \ndel nodo y \ndiscendente del \nnodo x zsottoalbero \nradicato a z",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#11": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itAlberi: definizioni\n•U n a l b e r o ordinato è un albero per il quale l’ordine dei figli di ogni nodo è\nsignificativo (non possono essere permutati)\n•U n  a l b e r o  binario è un albero ordinato in cui i nodi hanno grado al più due\n• Un albero binario è completo se ogni livello presenta tutti i nodi possibilialbero binario albero binario completo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#12": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itAlberi: definizioni\n• Un albero binario completo di altezza h\n–h a  2hfoglie, dunque h= log2(numero foglie)\n–h a  2h-1 nodi interni\n–h a  2h+1-1 nodi albero binario completoprofondità\nnodi\n0 1\n1 2\n2 4\n3 8\nnodi totali\n1\n3\n7\n15",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#13": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itIl tipo astratto albero\n• Tipo astratto albero di interi\n–d o m i n i\n• il dominio di interesse è l’insieme degli alberi di interi\n• dominio di supporto: i riferimenti R che identificano le \nposizioni nell’albero\n• dominio di supporto: gli interi Z = {0, 1, -1, 2, -2, …}• dominio di supporto: i booleani B = {true, false}\n– costanti\n• l’albero vuoto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#14": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itOperazioni del tipo astratto albero\n• Operazioni sugli alberi di interi\n– ritorna il riferimento alla radice: ROOT: T →R\n– ritorna il riferimento al figlio sinistro: LEFT: T ×R→R\n– ritorna il riferimento al figlio destro: RIGHT: T ×R→R\n– ritorna l’intero nel nodo specificato: INFO: T ×R→Z\n– verifica se un albero è vuoto: IS_EMPTY: T →B\n– aggiunge un nodo come radice: ADD_ROOT: T ×Z →T\n– aggiunge un nodo come figlio sinistro: ADD_LEFT: T ×R ×Z →T\n– aggiunge un nodo come figlio destro: ADD_RIGHT: T ×R ×Z →T\n– elimina una foglia: DELETE_LEAF: L ×R→L\n– cerca un nodo: SEARCH: T ×Z→R\n– svuota l’albero: EMPTY: T →T\n– conta i nodi dell’albero: SIZE: T →Z\n–…",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#15": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itRappresentazione di alberi binari\n• Analogamente alle liste, gli alberi binari possono \nessere rappresentati mediante oggetti e riferimenti\nt.roott",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#16": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itRappresentazione di alberi binari\n• Un nodo dell’albero binario è un oggetto con i \nquattro campi\n– parent : riferimento al nodo genitore\n–l e f t : riferimento al figlio sinistro\n– right : riferimento al figlio destro\n–i n f o : dati satellite\n• Un albero binario è un oggetto con un solo \ncampo root che è un riferimento al nodo \nradice",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#17": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itOperazioni sugli alberi binari\n• NEW_TREE ()\n– restituisce una struttura rappresentante l’albero vuoto\n– questa funzione rappresenta la costante\n• IS_EMPTY (t)\n– restituisce TRUE se l’albero è vuoto\n• ROOT (t)\n– restituisce il riferimento alla radice dell’albero ( NULL se t è vuoto)\n• LEFT (t,n)\n– restituisce il riferimento (può essere NULL ) al figlio sinistro del nodo n\n• RIGHT (t,n) \n– restituisce il riferimento (può essere NULL ) al figlio destro del nodo n\n• INFO (t,n) \n– restituisce le informazioni (dat i satellite) memorizzate nel nodo n\n•…",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#18": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n1. Scrivi lo pseudocodice delle funzioni\nNEW_TREE () \nIS_EMPTY (t) \nROOT (t) \nLEFT (t,n) \nRIGHT (t,n)\nINFO (t,n) \ndescritte nella slide precedente\n2. Scrivi lo pseudocodice della funzione \nTWO_CHILDREN (n) che ritorna TRUE se il \nnodo nha due figli, FALSE altrimenti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#19": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n3. Scrivi lo pseudocodice della procedura\nADD_ROOT (t,z) che aggiunga il nodo radice con \nvalore zall’albero binario t\n– assumi che tsia vuoto\n4. Scrivi lo pseudocodice delle procedure \nADD_LEFT (t,n,z) e ADD_RIGHT (t,n,z) che \naggiungono il figlio sinistro e destro al nodo n, \ncontenente il valore z\n5. Scrivi lo pseudocodice della funzione \nONLY_LEFT (t) che restituisce TRUE se tutti i nodi \ndell’albero binario thanno solamente il figlio \nsinistro (o nessun figlio), FALSE altrimenti\n• se l’albero è vuoto restituisci TRUE",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#2": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itSommario\n• Alberi radicati\n– definizione e uso\n• Strutture di dati per rappresentare alberi\n– alberi binari, alberi di grado arbitrario\n• Esercizi sugli alberi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#20": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itRappresentazione di alberi di grado arbitrario\n• Per rappresentare alberi di grado arbitrario si \npossono utilizzare diverse strategie\n– uso di una lista per i figli di ogni nodo\n• poco usato perché molto prolisso\n– uso di una struttura detta “figlio-sinistro-fratello-\ndestro”\n• più sintetico",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#21": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itUso di una lista per i figli di ogni nodo\nt.root\nhead\nhead headt",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#22": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itStruttura “figlio-sinistro-fratello-destro”\n• I nodi hanno gli usuali campi parent , left , \nright e info\n– i campi parent e info hanno il significato usuale\n–i l  c a m p o  left è un riferimento al figlio di sinistra \n(cioè al primo figlio)\n–i l  c a m p o  right , invece di essere un riferimento al \nfiglio destro, è un riferimento al prossimo fratello\nparent\n1° figliofratello destro",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#23": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itFiglio-sinistro-fratello-destro\nt.root\nradice\nfigli della radice\nfigli del primo figliot",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#24": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itOperazioni sugli alberi qualsiasi\n• NEW_TREE ()\n– restituisce una struttura rappresentante l’albero vuoto\n• IS_EMPTY (t)\n– restituisce TRUE se l’albero è vuoto\n• ROOT (t)\n– restituisce il riferimento alla radice dell’albero ( NULL se tè vuoto)\n• FIRST_CHILD (t,n)\n– restituisce il riferimento (può essere NULL ) al figlio sinistro del nodo n\n• NEXT_SIBLING (t,n) \n– restituisce il riferimento (può essere NULL ) al fratello destro del nodo n\n• INFO (t,n) \n– restituisce l’intero memorizzato nel nodo n\n•…",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#25": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itEsercizi sugli alberi qualsiasi\n6. Scrivi lo pseudocodice della procedura \nADD_ROOT (t,z) che aggiunga un nodo \nradice con valore zall’albero t\n– supponi che l’albero tsia vuoto\n7. Scrivi lo pseudocodice della procedura \nADD_SIBLING (t,n,z) che aggiunge al nodo \nnun figlio che contiene il valore z",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#3": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itDefinizione di albero radicato (rooted tree)\n•U n  albero radicato è\nun insieme di nodi, su cui è definita una relazione binaria “ xè\nfiglio di y” (oppure “ y\nè genitore di x”) tale \nche:\n1. ogni nodo ha un solo \ngenitore, con \nl’eccezione della radice \nche non ha genitori\n2. c’è un cammino diretto \nda ogni nodo alla radice\n• l’albero, cioè, è\nconnessoradice",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#4": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itEsempio di albero radicato\n• Un albero può essere costruito a partire dalla radice \naggiungendo ogni volta un nodo xcome figlio di un \nnodo ygià esistente\n– ciò giustifica il fatto che, se l’albero ha nnodi, allora ci sono \nn-1 relazioni genitore/figlioradice",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#5": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itNumerose applicazioni usano alberi\n• I rapporti di ereditarietà determinano alberi\n– alberi genealogici o filogenetici\n– ereditarietà di classi nella programmazione ad oggetti\n• I rapporti gerarchici sono alberi\n– gerarchie organizzative, di controllo, di responsabilità\n• I rapporti di contenimento formano alberi\n– la classificazione scientifica degli organismi (tassonomie)– le directory del filesystem– i cammini minimi da una sorgente  a tutti i nodi di una rete \n• La struttura sintattica di una frase è un’albero\n– alberi sintattici \n•…",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#6": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itAlberi: definizioni\n• Due nodi che hanno lo stesso genitore si dicono fratelli\n• Il numero di figli di un nodo è il suo grado\n• I nodi di grado zero sono foglie\n• Un nodo non foglia è detto nodo internofratelli\nnodo interno\nfoglianodo di grado due",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#7": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itTipi di alberi\n• Alberi binari\n– ogni nodo può avere solamente un figlio sinistro e \nun figlio destro\n• l’ordine dei figli è generalmente significativo\n• si distingue tra avere il solo fi glio sinistro e avere il solo \nfiglio destro\n• Alberi di grado arbitrario\n– non è noto a priori il numero massimo dei figli di un \nnodo\n• l’ordine dei figli generalmente non è significativo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#8": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itAlberi: definizioni\n• Una sequenza di nodi tali che uno è il genitore del successivo è\ndetta cammino\n– il cammino percorre gli archi alla r ovescia rispetto alla figura qui sopra\n• Il numero degli archi di un cammino è la sua lunghezzail cammino blu ha \nlunghezza due",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\110-alberi-radicati-08.pdf#9": "110-alberi-radicati-08 copyright ©2019 maurizio.patrignani@uniroma3.itAlberi: definizioni\n•L a  profondità di un nodo è la lunghezza del cammino dal nodo \nalla radice\n• La profondità del nodo più profondo è l’altezza dell’alberoprofondità zero\nprofondità uno\nprofondità due\nprofondità trel’altezza \ndell’albero\nèt r e",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#0": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nVisite di alberi\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#1": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#10": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n• Visita in simmetrica\n– processo il nodo dopo aver processato il figlio sinistro e \nprima di aver processato il figlio destro\n– ordine di visita: 8, 4, 9, 2, 5, 1, 6, 3, 7 \n– complessità: Θ(n)1\n2\n4\n83\n5 6 7\n9",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#11": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n2. Scrivi lo pseudocodice della procedura \nCERCA (t,n) che ritorna TRUE se il valore nè\npresente nell’albero binario t\n– facendo uso di una visita in preordine\n– facendo uso di una visita in postordine\n– facendo uso di una visita simmetrica\n3. Scrivi lo pseudocodice della procedura \nCONTA_NODI (t) che ritorna il numero di \nnodi dell’albero binario t\n– fai uso di una visita in postordine",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#12": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n4. Scrivi lo pseudocodice della procedura CAMMINO (t) \nche verifica se un albero binario tè un cammino\n– cioè se tutti i nodi hanno grado uno con l’eccezione \ndell’unica foglia\n– assumi che un albero vuoto sia un cammino\n5. Scrivi lo pseudocodice della procedura HEIGHT (t) \nche calcola l’altezza di un albero binario t\n– cioè il numero di archi del cammino che va dalla radice \nalla foglia più profonda\n– ritorna -1 se l’albero è vuoto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#13": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itCERCA (t,v) in preordine\nCERCA(t,v)  // ritorna TRUE se il nodo v e’ nell’albero t\n1. return CERCA_PREORDINE (t.root,v) Zinnesco\nCERCA_PREORDINE( n,v)\n1. ifn == NULL \n2. return FALSE\n3. ifn.info == v\n4. return TRUE\n5.l =CERCA_PREORDINE (n.left,v) Zsottoalbero sinistro\n6.r =CERCA_PREORDINE (n.right,v) Zsottoalbero destro\n7. return lor r",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#14": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itCERCA (t,v) in postordine\nCERCA(t,v)   // ritorna TRUE se il nodo v è nell’albero t\n1. return CERCA_POSTORDINE (t.root,v) Zinnesco\nCERCA_POSTORDINE( n,v)\n1. ifn == NULL \n2. return FALSE\n3. if CERCA_POSTORDINE (n.left,v)\n4. return TRUE\n5. if CERCA_POSTORDINE (n.right,v)\n6. return TRUE\n7. return n.info == v",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#15": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itCERCA (t,v) con visita simmetrica\nCERCA(t,v)\n1. return RICERCA_SIMMETRICA (t.root,v) Zinnesco\nRICERCA_SIMMETRICA( n,v)\n1. ifn == NULL \n2. return FALSE\n3. if RICERCA_SIMMETRICA (n.left,v)\n4. return TRUE\n5. ifn.info == v\n6. return TRUE\n7. return RICERCA_SIMMETRICA (n.right,v)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#16": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itAltri esercizi sugli alberi binari\n6. Scrivi lo pseudocodice della procedura \nAVERAGE (t) che calcoli la media dei valori \ncontenuti in un albero binario t\n– puoi far uso o meno di CONTA_NODI (t)\n– se l’albero è vuoto produci un errore \n7. Scrivi lo pseudocodice della procedura \nCOMPLETO (t) che verifichi se un albero \nbinario tè completo\n– puoi far uso o meno della procedura HEIGHT (t)\n– se l’albero è vuoto ritorna TRUE",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#17": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itAltri esercizi sugli alberi binari\n8. Scrivi lo pseudocodice della procedura \nDEALLOCA (t) che rimuova (deallocandoli) tutti i \nnodi di un albero t\n9. Scrivi lo pseudocodice della procedura POTA (t,x) \nche elimini da un albero binario il sottoalbero \nradicato ad un nodo xspecificato tramite riferimento\n– puoi omettere di deallocare i nodi potati\n10. Scrivi lo pseudocodice della procedura POTA (t,h) \nche poti un albero binario lasciando solamente i nodi \na profondità minore di h\n– puoi fare uso o meno di POTA (t,x)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#18": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itRappresentazioni testuali di alberi binari\n11. Scrivi lo pseudocodice della procedura \nPARENTETICA_SIMMETRICA (t) che stampi un \nalbero binario tnella rappresentazione parentetica \nsimmetrica\n–c i o è n e l  f o r m a t o :  \n“(“ <sottoalbero-sx>  <val-radice> <sottoalbero-dx> “)”\n– esempio: ((()2())1(()3()))\n12. Scrivi lo pseudocodice della procedura \nPARENTETICA_PREORDINE (t) che stampi un \nalbero binario tnella rappresentazione parentetica in \npreordine \n–c i o è n e l  f o r m a t o :\n“(“ <val-radice> <sottoalbero-sx> <sottoalbero-dx> “)”\n– esempio: (1(2()())(3()()))",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#19": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itAncora sugli alberi binari\n13. Scrivi lo pseudocodice della procedura \nDUE_FIGLI (t) che calcoli il numero di nodi \nnell’albero binario tche hanno esattamente due figli\n14. Scrivi lo pseudocodice della procedura \nVALORE_NONNO (t) che calcoli il numero di nodi \ndell’albero binario tche hanno lo stesso valore del \ngenitore del genitore (cioè del nonno)\n15. Scrivi lo pseudocodice della procedura \nQUATTRO_NIPOTI (t) che calcoli il numero di nodi \ndell’albero binario tche hanno quattro nipotini",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#2": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itSommario\n• Visite di un albero\n– visita in postordine (postorder traversal)\n– visita in preordine (preorder traversal)– visita simmetrica di alberi binari (inorder traversal)\n• Esercizi sulle visite di alberi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#20": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itAncora sugli alberi binari\n16. Scrivi la procedura CAMMINO (t,n) che ritorni una \nlista con gli identificatori dei nodi del cammino dalla \nradice fino al nodo il cui riferimento è n\n– puoi supporre che nappartenga all’albero\n17. Scrivi la procedura PARENTELA (t, n1, n2) che \ncalcoli il grado di parentela di due nodi  con \nriferimenti n1ed n2\n– il grado di parentela è definito come la lunghezza del \ncammino che unisce i due nodi\n– puoi supporre di avere a disposizione la procedura \nCAMMINO (t,n)\n• come potresti utilizzarla?",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#21": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi di grado arbitrario\n18. Scrivi lo pseudocodice della procedura \nCONTA_NODI (t) che ritorni il numero dei \nnodi di un albero trealizzato tramite una \nstruttura di dati “figlio-sinistro-fratello-\ndestro”\n19. Scrivi la procedura CERCA (t,k) che ritorni il \nriferimento al nodo che contiene il valore kin \nun albero trealizzato tramite una struttura di \ndati “figlio-sinistro-fratello-destro”",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#22": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi di grado arbitrario\n20. Scrivi la procedura BINARIO (t) che verifica \nse un albero trealizzato tramite una struttura \ndi dati “figlio-sinistro-fratello-destro” sia in \nrealtà un albero binario (in cui cioè i nodi hanno grado massimo due)\n21. Scrivi la procedura GRADO_MASSIMO (t) che \nritorni il numero massimo dei figli dei nodi di \nun albero trealizzato tramite una struttura di \ndati “figlio-sinistro-fratello-destro”",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#23": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sulla copia di alberi\n22. Scrivi lo pseudocodice della funzione \nCOPIA_ALBERO (t) che accetti in input un \nalbero binario te restituisca in output una sua \ncopia (senza modificare l’albero t)\n23. Scrivi lo pseudocodice della funzione analoga \nper alberi di grado arbitrario",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#24": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itSoluzioni: COPIA_ALBERO (1)\nCOPIA_ALBERO(t)\n/* t2 è un nuovo albero con il solo campo t.root */if ( t.root == NULL)\nt2.root = NULL\nelse\n/* temp è un nuovo nodo con i campi parent, left, right \n(riferimenti) e info (intero) */\nt2.root = temptemp.parent = NULLCOPIA_RIC(t.root, temp)\nreturn t2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#25": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itSoluzioni: COPIA_ALBERO (2)\nCOPIA_RIC(n, n2)  /* lanciato sempre su due riferimenti non NULL */\nn2.info = n.infoif (n.left == NULL) \nn2.left = NULL\nelse\n/* temp nuovo nodo con i campi parent, le ft, right (rif) e info (intero) */\nn2.left = temptemp.parent = n2COPIA_RIC(n.left, temp)\nif (n.right == NULL) \nn2.right = temp\nelse\n/* temp nuovo nodo con i campi parent, le ft, right (rif) e info (intero) */\nn2.right = temptemp.parent = n2COPIA_RIC(n.right, temp)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#3": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itVisite di alberi\n• Un albero può essere visitato ricorsivamente con due \nopposte discipline\n– visita in preordine (preorder traversal)\n• dopo aver processato un nodo si pr ocede a processare i suoi figli\n• le operazioni sui nodi vengono effettuate top-down\n– visita in postordine (postorder traversal)\n• un nodo può essere processato solo  quando i suoi figli sono stati \nprocessati\n• le operazioni sui nodi vengono effettuate bottom-up\n• Se l’albero è binario è possibile anche una strategia \nintermedia\n– visita simmetrica (inorder traversal)\n• si processa prima il figlio sinist ro, poi il nodo stesso, poi il figlio \ndestro",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#4": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itVisita in preordine\nI. entro nel generico nodo n\n• ricevo dei parametri dalla \nprocedura eseguita sul genitore\nII. eseguo la computazione su n\n• mi avvalgo dei valori già\ncomputati sul genitore\nIII.e IV. lancio la procedura sul \nfiglio sinistro e destro \n• passo dei parametri alle \nprocedure eseguite sui figli \nV. esco dal nodo nnV I\nII\nIII IV",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#5": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itVisita in postordine\nI. entro nel generico nodo n\nII. e III: lancio la procedura sul   \nfiglio sinistro e destro \n• raccolgo gli output dalle \nprocedure lanciate sui figli\nIV.eseguo la computazione su n\n• mi avvalgo dei valori computati \nsui figli\nV. esco dal nodo n\n• restituisco un output alla \nprocedura lanciata sul genitorenV I\nIV\nII III",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#6": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itVisita simmetrica\nI. entro nel generico nodo n\n• ricevo parametri dalla procedura eseguita \nsul genitore\nII. lancio la procedura sul figlio sinistro\n• posso passare dei parametri e ricevere un \noutput\nIII. eseguo la computazione su n\n• posso avvalermi dei parametri passati dal \ngenitore\n• posso avvalermi del valore computato sul \nsolo figlio sinistro\nIV. lancio la procedura sul figlio destro\n• posso passare dei parametri e ricevere un \noutput\nV. esco dal nodo n\n• posso resitituire un output alla procedura \nlanciata sul genitorenV I\nIIIII IV",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#7": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n1. Scrivi la sequenza con cui i nodi vengono processati da una \nvisita in preordine/postordine/simmetrica di questo albero binario\n– qual è la complessità asintotica delle tre visite?\n– nota: la sequenza dei nodi visitati è sempre la stessa. Ciò che cambia è\nil momento in cui avvengono le computazioni sul nodo  1\n2\n4\n83\n5 6 7\n9",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#8": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n• Visita in preordine\n– appena arrivo su un nodo lo processo\n– ordine di visita: 1, 2, 4, 8, 9, 5, 3, 6, 7 \n– complessità: Θ(n)1\n2\n4\n83\n5 6 7\n9",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\120-visite-di-alberi-05.pdf#9": "120-visite-di-alberi-05         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli alberi binari\n• Visita in postordine\n– processo un nodo prima di lasciarlo definitivamente\n– ordine di visita: 8, 9, 4, 5, 2, 6, 7, 3, 1 \n– complessità: Θ(n)1\n2\n4\n83\n5 6 7\n9",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#0": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nCode di priorità\n(Heap e HEAP_SORT )\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#1": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#10": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itUn heap codifica un albero\n•L ’ h e a p  \nconsiste di un \narray h.A che \ncodifica, \nlivello per \nlivello, un albero binario \nquasi \ncompleto0\n1234\n5\n67\n8h.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#11": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itUn heap codifica un albero\n• h.A[0] èl a  \nradice \ndell’albero \n• Dato il nodo \nassociato alla \nposizione i:\n– i nodi figli si \ntrovano in posizione 2 i+1 \ne 2i+2\n– il nodo genitore \n(se i≠0) si \ntrova in \nposizione  \n(i-1)/2 0\n1234\n5\n67\n8\n−\n21i\ni\n12+i\n22+ih.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#12": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itSemplificazione dello pseudocodice\n• Per rendere più leggibile lo pseudocodice \ndefiniamo le seguenti funzioni\nPARENT(i)  /* ritorna l’indice del parent del nodo i */\n1. return (i-1)/2 \nLEFT(i)    /* ritorna l’indice del figlio sinistro di i */\n1. return 2i + 1\nRIGHT(i)   /* ritorna l’indice del figlio destro di i */\n1. return 2i + 2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#13": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itDettagli implementativi\n• Per maggiore flessibilità, anche se l’array è\nlungo h.A.length , supponiamo che solo i \nvalori compresi tra 0e h.size-1 siano \nsignificativi\n– dove ovviamente h.size ≤h.A.length\nh.Ahh.size\nh.A.length\nh.A[0]\nh.A[1]",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#14": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itValori contenuti in un max-heap\n• In un max-heap\nl’elemento memorizzato nel \nnodo iha valore \nmaggiore o uguale degli elementi memorizzati nei \nsuoi figli\n– la radice contiene il \nvalore più alto \ndell’array\n–p e r  j> 0, \nh.A[PARENT (j)] ≥\nh.A[j] 16\n14\n8\n210\n7\n9\n3\n416\n14\n10\n8\n7\n9\n3\n2\n40\n1234\n5\n67\n8h.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#15": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itMin-heap\n• Esiste anche il \nmin-heap che \nha la proprietà\nsimmetrica\n–l a  r a d i c e  \ncontiene il valore minore \ndell’array2\n6\n8\n204\n7\n5\n11\n102\n6\n4\n8\n7\n5\n11\n20\n100\n1234\n5\n67\n8h.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#16": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itProprietà degli heap\n•S e  hè un heap che codifica un albero quasi-\ncompleto con nelementi, gli n/2elementi da \n0 a n/2-1 sono nodi interni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#17": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itDimostrazione della proprietà\nDimostriamo per induzione che i nodi interni sono n/2\n• Passo base\n– dimostriamo l’asserto per gli alberi completi\n• un albero completo è un particolare albero quasi completo\n• Passo induttivo\n– dimostriamo che se vale per un albero quasi completo con n\nnodi, vale anche per un albero quasi completo con n-1 nodi\n• distinguiamo due casi: rimozione de l figlio destro e rimozione del \nfiglio sinistro",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#18": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itPasso base: albero binario completo\n• Sappiamo che un albero binario completo di altezza hha 2h\nfoglie e 2h-1 nodi interni e dunque n=2h+1-1 nodi totali\n• Verifichiamo la formula: \nnodi interni = n/2= (2h+1-1)/2 = 2h-1/2= 2h-1",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#19": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itPasso induttivo: rimuovo un figlio destro\n• Prima della rimozione avevo nnodi e n/2nodi interni (con ndispari)\n– dalla disparità di nsegue che n/2= (n-1)/2 \n• Dopo la rimozione ho n’ = n-1 nodi e il numero dei nodi interni non è\ncambiato \n– ne segue che i nodi interni sono n/2= (n-1)/2 = n’/2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#2": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itSommario\n• Il tipo astratto di dato coda di priorità\n• La struttura di dati heap\n– procedura MAX_HEAPIFY\n– procedura BUILD_MAX_HEAP\n• Coda di priorità realizzata con un heap\n• Algoritmo di ordinamento HEAP_SORT\n– analisi della sua complessità",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#20": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itPasso induttivo: rimuovo un figlio sinistro\n• Prima della rimozione avevo nnodi e n/2nodi interni (con npari)\n– dalla parità di nsegue che n/2= (n-1)/2 + 1\n• Dopo la rimozione ho n ’= n-1 nodi e i nodi interni sono diminuiti di uno\n– dunque i nodi interni sono (n-1)/2 = n’/2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#21": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itProcedura MAX_HEAPIFY\nMAX_HEAPIFY( h,i)\n1.l = LEFT(i) Zindice del figlio sinistro \n2.r = RIGHT(i) Zindice del figlio destro\n3. if(l ≤h.size-1 andh.A[l] > h.A[i]) massimo = l\n4. else massimo = i\n5. if (r ≤h.size-1 andh.A[r] > h.A[massimo] massimo = r\n6./* ora massimo è il massimo  tra h.A[l], h.A[r] ed h.A[i]\n7. ifmassimo ≠i\n8. SCAMBIA_CASELLE (h.A,i,massimo)\n9. MAX_HEAPIFY (h,massimo)• Se i due sottalberi radicati a LEFT (i) e a RIGHT (i) sono dei \nmax-heap, allora la procedura MAX-HEAPIFY (h,i) trasforma il \nsottoalbero radicato ad iin un max-heap",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#22": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.it\nEsecuzione di MAX_HEAPIFY sul nodo i\n16\n4\n14\n210\n7 9 3\n8 116\n14\n4\n210\n7 9 3\n8 1\n16\n14\n8\n210\n7 9 3\n4 1i\nl r\nil sottoalbero \nradicato ad iè\ndiventato un \nmax-heapi sottoalberi \nradicati ad l\ned rsono \nmax-heap",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#23": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itAnalisi di MAX_HEAPIFY\n• Il tempo di esecuzione di MAX_HEAPIFY (h,i) si ottiene sommando\n– il tempo di calcolo di massimo (linee 1-8), che è evidentemente Θ(1)\n– il tempo di calcolo MAX_HEAPIFY (h, massimo ) dove il sottoalbero radicato a \nmassimo ha dimensione ridotta rispetto a quello radicato ad iMAX_HEAPIFY( h,i)\n1.l = LEFT(i) Zindice del figlio sinistro \n2.r = RIGHT(i) Zindice del figlio destro\n3. if(l ≤h.size-1 andh.A[l] > h.A[i]) massimo = l\n4. else massimo = i\n5. if (r ≤h.size-1 andh.A[r] > h.A[massimo] massimo = r\n6./* ora massimo è il massimo  tra h.A[l], h.A[r] ed h.A[i]\n7. ifmassimo ≠i\n8. SCAMBIA_CASELLE (h.A,i,massimo)\n9. MAX_HEAPIFY (h,massimo)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#24": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.it\nAnalisi di MAX_HEAPIFY\n• Il caso peggiore si presenta quando occorre ricorrere su un \nsottoalbero di profondità h-1, mentre il sottoalbero radicato al \nnodo fratello ha profondità h-2\n– ricorda che l’albero è quasi-completo\n• In questo caso, se i nodi dell’albero sono n, i nodi del \nsottoalbero più pesante sono n·2 / 33\n16\n8\n210\n7 9 6\n4 1 516\n3\n8\n210\n7 9 6\n4 1 5h-1hh-2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#25": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itAnalisi di MAX_HEAPIFY\n• Il tempo di calcolo di MAX_HEAPIFY su un \nsottoalbero con nnodi è\nT(n) ≤T(2n/3) + c\n• Questa disequazione di ricorrenza può essere risolta \ncon il master theorem \nT(n) = a·T (n/b) + p( nk) \nnello speciale caso in cui\na=1 b=3/2 k=0\nche per a= bksi risolve in \nT(n) = Θ(nklog n) = Θ(log n)\n• Dunque la complessità di MAX_HEAPIFY èΘ(log n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#26": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itAnalisi alternativa di MAX_HEAPIFY\n• Il tempo di calcolo di MAX_HEAPIFY su un \nsottoalbero con nnodi è chiaramente pari a Θ(1)\nmoltiplicato per il numero di lanci ricorsivi di \nMAX_HEAPIFY\n–Θ(1) è dovuto alle linee 1-8 dello speudocodice\n• Poiché un albero binario quasi-completo ha altezza \nΘ(log n), il numero di lanci ricorsivi nel caso peggiore \nèΘ(log n)\n• La complessità di MAX_HEAPIFY nel caso peggiore è\ndunque: \nΘ(1) · Θ(log n) = Θ(log n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#27": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itProcedura BUILD_MAX_HEAP\n• BUILD_MAX_HEAP trasforma un array A in un heap\n• Se n = h.A.length, gli elementi con indice ≥ n/2sono \ntutte foglie\n– ognuna è un heap con un solo elemento\n• BUILD_MAX_HEAP esegue MAX_HEAPIFY sui nodi \nche non sono foglie, dal basso verso l’alto\nBUILD_MAX_HEAP( h)\n1.h.size = h.A.length\n2. fori = h.A.length/2 -1 downto0  // i nodi interni \n3. MAX_HEAPIFY (h,i)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#28": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di BUILD_MAX_HEAP (1/2)\n5\n9\n23\n6\n7\n15\n9\n2\n6\n7\n1\n5\n9\n27\n6\n3\n15\n7\n2\n6\n3\n13\n95\n9\n27\n6\n3\n17\n2\n6\n3\n195\n9\n23\n6\n7\n15\n3\n2\n6\n7\n19\n5i = 2\ni = 1i = 0",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#29": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di BUILD_MAX_HEAP (2/2)\n5\n9\n27\n6\n3\n17\n2\n6\n3\n195 9\n5\n27\n6\n3\n17\n2\n6\n3\n19\n9\n6\n27\n5\n3\n17\n2\n3\n169 9\n6\n27\n5\n3\n17\n2\n5\n3\n169\n55i = 0",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#3": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itCode di priorità\n• Una coda di priorità (priority queue) è una \ncollezione di elementi\n– ad ogni elemento è associato un valore di priorità\n– i valori di priorità definiscono un ordinamento\n• Operazioni sulle code di priorità\n– l’utente vuole inserire efficientemente nuovi \nelementi con valori arbitrari di priorità\n– l’utente vuole estrarre efficientemente l’elemento a \npiù alta priorità",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#30": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itAnalisi di BUILD_MAX_HEAP\n• BUILD_MAX_HEAP lancia MAX_HEAPIFY un numero Θ(n) di \nvolte\n– il tempo di esecuzione di MAX_HEAPIFY nel caso peggiore è Θ(log n’)\n• dove n’è il numero dei nodi del sottoalbero radicato al nodo sul quale è\nlanciato MAX_HEAPIFY\n• Siccome Θ(log n’) ⊆O(log n) possiamo dire che la complesità\ndi BUILD_MAX_HEAP nel caso peggiore è O( nlog n) \n–O ( nlog n) non è un limite asintoticamente stretto\n– con un’analisi più rigorosa dimostreremo che la complessità di \nBUILD_MAX_HEAP nel caso peggiore è Θ(n)BUILD_MAX_HEAP( h)\n1.h.size = h.A.length\n2. fori = h.A.length/2 -1 downto0  // i nodi interni \n3. MAX_HEAPIFY (h,i)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#31": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itComplessità di BUILD_MAX_HEAP\n• La complessità di BUILD_MAX_HEAP coincide con la \nsomma delle altezze di tutti i sottoalberi radicati ai \nnodi dell’albero\n• Dimostriamo che tale somma sia Θ(n) per un albero \ncompleto con nnodi\n•S i a  S(n) la somma delle altezze di tutti i sottoalberi di \nun albero binario completo con nnodi\n– ricorda che la sua altezza è\n• Dimostriamo per induzione che\n)( )1(log 1 )(2 n n n hnnS Θ∈+ −=−−=1)1(log2 −+ = n h",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#32": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itComplessità di BUILD_MAX_HEAP\n• Caso base\n– per un albero con la sola radice abbiamo\n– infatti un albero con la sola radice ha un solo \nsottoalbero (l’albero stesso) che ha altezza zero1 )( −−= hnnS\n0101)1( =−−=Sn= 1 h= 0",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#33": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itComplessità di BUILD_MAX_HEAP\n• Caso induttivo\n– supponiamo che la formula sia vera per tutti gli \nalberi con un numero di nodi minore di n\nn\n21−n\n21−nh\nh-1\nformula già dimostrata",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#34": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itComplessità di BUILD_MAX_HEAP\n• Caso induttivo\n=+\n\n−− −−= h hn1)1 (212\nhh n + −−= 21=+\n\n−= hnS nS212)(ipotesi induttiva\n1−−= hn=+\n\n−+−−= h hn112121 )( −−= hnnS",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#35": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itRealizzazione di una coda di priorità\n• Le code di priorità possono essere gestite tramite un \nheap\n• Supponiamo di gestire le dimensioni dell’array h.A \ntramite una crescita telescopica\n• La complessità di questa funzione è costante Θ(1)NEW_QUEUE()\n1./* h è un nuovo oggetto con i campi size (intero) ed A\n2.(array di 100 interi) */\n3.h.size = 0\n4.return h",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#36": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itRealizzazione di una coda di priorità\n• Le due funzioni qui sopra hanno evidentemente una \ncomplessità costante Θ(1)IS_EMPTY( h)\n1. return h.size == 0\nMAXIMUM( h)\n1. return h.A[0]",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#37": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itProcedura EXTRACT_MAX\nEXTRACT_MAX( h)\n1. if IS_EMPTY (h)\n2.error(“heap underflow”)\n3.max = h.A[0]\n4.h.A[0] = h.A[h.size - 1]\n5.h.size = h.size - 1\n6. MAX_HEAPIFY (h,0)\n7. return max \n• Viene eliminato il primo elemento dalla coda\n• L’ultimo elemento viene messo al suo posto\n• Viene decrementato h.size\n• La complessità totale è quella di MAX_HEAPIFY , cioè Θ(log n) \nnel caso peggiore",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#38": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itProcedura INSERT\n9.h.A[i] = key 8.i =PARENT(i)INSERT(h,key)\n1. ifh.size == h.A.length\n2.error(“overflow”)\n3.h.size = h.size + 1\n4.i = h.size - 1\n5. while i>0 andh.A[PARENT(i)] < key\n6.h.A[i] = h.A[ PARENT(i)]\n6./* il genitore di i è stato spostato in basso */\n• h.size viene incrementato di 1\n• Il nuovo elemento viene “spinto in a lto” fino a trovare la posizione giusta\n• La complessità nel caso peggiore è data dall’altezza dell’albero, \ncioè Θ(log n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#39": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di INSERT (1)\n9\n6\n27\n5\n87\n2\n5\n869INSERT(h,8)\n9\n6\n28\n5\n78\n2\n5\n7699\n6\n27\n57\n2\n569",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#4": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itDue applicazioni delle code di priorità\n• Allocazione ai processi delle risorse condivise\n– gli elementi della coda sono le ri chieste da parte dei processi di una \nspecifica risorsa\n• per esempio l’accesso all’hard disk o ad una periferica\n– i processi in esecuzione generano n uove richieste con priorità dipendenti \ndall’utente o dal tipo di  operazione richiesta\n– la risorsa è assegnata al processo con più alta priorità\n• Simulazione di un sistema complesso guidata dagli eventi\n– gli elementi della coda sono eventi , con associato il tempo in cui si \ndevono verificare\n– gli eventi vengono simulati in ordine temporale\n– la simulazione di un evento può pr ovocare l’inserimento nella coda di \naltri eventi a distanza di tempo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#40": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di INSERT (2)\n9\n6\n28\n5\n78\n2\n5\n7\n1069INSERT(h,10)\n9\n6\n28\n5\n78\n2\n5\n769\n10\n9\n6\n210\n5\n710\n2\n5\n7\n869\n810\n6\n29\n5\n79\n2\n5\n7\n8610\n8",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#41": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itConclusioni sulle strutture di dati heap\n• Consentono di realizzare delle code di priorità\nin cui\n– la creazione della coda di priorità ha complessità\nΘ(n) \n• procedura BUILD_MAX_HEAP (h)\n– l’inserimento di un elemento con priorità arbitraria \nha complessità Θ(log n)\n• procedura INSERT (h,key)\n– l’estrazione dell’elemento con chiave maggiore ha \ncomplessità Θ(log n)\n• procedura EXTRACT_MAX (h)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#42": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sugli heap\n1. Illustra le operazioni di INSERT (h,10) sullo \nheap \nh.A = <15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1> \n2. Illustra le operazioni di EXTRACT_MAX (h) \nsullo heap \nh.A = <15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1>",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#43": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itProcedura HEAP_SORT\n• A viene trasformato in un heap ( Θ(n))\n•P e r  iche va da 0 ad A.length -1 (cioè Θ(n) volte)\n– viene estratto il primo elemento  di A e viene posto in coda \nall’array ( Θ(1))\n– viene lanciato MAX_HEAPIFY per ripristinare le proprietà\ndell’heap (tempo Θ(log n) se gli elementi sono tutti distinti)HEAP_SORT( A)\n1.h.A = A /* h è un nuovo heap */\n2.h.size = A.length\n3. BUILD_MAX_HEAP (h)\n4. fori = h.A.length-1 downto1 \n5. SCAMBIA_CASELLE (A,0,i)\n6.h.size = h.size – 1\n7. MAX_HEAPIFY (h,0)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#44": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di HEAP_SORT (1/5)\n1\n6\n27\n5\n37\n2\n5\n3\n965\n9\n23\n6\n7\n15\n3\n2\n6\n7\n19\n19\n6\n27\n5\n3\n17\n2\n5\n3\n169BUILD_MAX_HEAP\n1\n6\n27\n5\n37\n2\n5\n3\n961MAX_HEAPIFY\ni = 6",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#45": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di HEAP_SORT (2/5)\n1\n6\n23\n53\n2\n5\n7\n961MAX_HEAPIFY7\n6\n2\n5\n12\n5\n1\n967\n3 31\n6\n2\n52\n5\n7\n961\n3 3\n6\n5\n23\n13\n2\n1\n7\n956i = 5",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#46": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di HEAP_SORT (3/5)\nMAX_HEAPIFY\n1\n5\n2 2\n6\n7\n951\n3 31\n5\n23 3\n2\n6\n7\n951\n5\n2\n1 1\n6\n7\n925\n3 31\n2\n5\n6\n7\n921\n3 3i = 4\ni = 3",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#47": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di HEAP_SORT (4/5)\nMAX_HEAPIFY\n1\n2\n5\n6\n7\n921\n3 33\n2\n5\n6\n7\n923\n1 1\n1\n2\n5\n6\n7\n921\n31\n2\n5\n6\n7\n921\n3MAX_HEAPIFY\ni = 2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#48": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsecuzione di HEAP_SORT (5/5)\n2\n1\n5\n6\n7\n912\n31\n5\n6\n7\n921\n3\n5\n6\n7\n921\n31\n5\n6\n7\n921\n3MAX_HEAPIFYi = 1",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#49": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itHEAP_SORT non è stabile\n• Lo dimostriamo con un controesempio\n• Ora la posizione dei due elementi è invertita5”\n5’5’\n5”5’\n5”\n5”5’\n5” 5”5’BUILD_MAX_HEAP\nMAX_HEAPIFY\n5”\n5’5”\ni = 1 5’5”",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#5": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itCoda di priorità di interi\n• Domini\n– il dominio di interesse Q di tutt e le code di priorità di interi\n– dominio di supporto: l’insieme degli interi Z\n– dominio di supporto: l’insieme dei booleani { true , false }\n• Costanti\n– la coda di priorità vuota\n• NEW_QUEUE (): inizializza e ritorna una coda di priorità vuota\n• Operazioni\nINSERT (Q,x): inserisce l’elemento x nella coda Q\nMAXIMUM (Q): restituisce l’elemento di Q con chiave più grande\nEXTRACT_MAX (Q): restituisce l’elemento di Q con chiave più grande e lo \nrimuove da Q\nIS_EMPTY (Q): riporta true se la coda Q è vuota, false altrimenti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#50": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itAlgoritmi di ordinamento visti finoracaso \nmigliorecaso \nmediocaso \npeggiorein locostabile\nSELECTION_SORT Θ(n2) si si\nINSERTION_SORT Θ(n) Θ(n2) Θ(n2) si si\nMERGE_SORT Θ(n logn) no si\nHEAP_SORT Θ(n logn) si no\nNota: nel caso migliore HEAP_SORT ha complessità  Θ(nlog n) se gli elementi \nsono tutti distinti e complessità Θ(n) se gli elementi sono tutti uguali ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#51": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itDomande sugli heap\n3. Quali sono il numero mi nimo ed il numero massimo \ndi elementi in uno heap di altezza h? \n4. In un max-heap, dove potrebbe risiedere l’elemento \npiù piccolo, assumendo che siano tutti distinti? \n5. Un heap in cui l’array è ordinato in ordine inverso è\nun max-heap? \n6. La sequenza <23, 17, 14, 6, 13, 10, 1, 5, 7, 12> è un \nmax-heap? \n7. Qual è l’effetto di MAX_HEAPIFY (h,i) se l’elemento \nh.A[i] è più grande dei suoi figli?\n8. Qual è l’effetto di MAX_HEAPIFY (h,i) se \ni > h.size/2-1 ? ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#52": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi sulle code di priorità\n9. Illustra le operazioni di MAX_HEAPIFY (h,2) sullo \nheap \nh.A = <27, 17, 3, 16, 13, 10, 1, 5, 7, 12, 4, 8, 9, 0> \n10. Illustra le operazioni di BUILD_MAX_HEAP (h) sullo \nheap \nh.A = <5, 3, 17, 10, 84, 19, 6, 22, 9>\n11. Illustra le operazioni di HEAP_SORT sull’array \nA = <5, 13, 2, 25, 7, 17, 20, 8, 4> ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#6": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itRealizzazioni inefficienti di code di priorità\n• Si potrebbe realizzare una coda di priorità tramite una \nlista ordinata\n– l’inserimento nella lista di un nuovo elemento avrebbe \ncomplessità Θ(n)\n– la rimozione dell’elemento a più alta priorità (il primo della \nlista) avrebbe complessità Θ(1)\n• Si potrebbe realizzare una coda di priorità tramite una \nlista non ordinata\n– l’inserimento (in testa) di un nuovo elemento avrebbe \ncomplessità Θ(1)\n– la ricerca e la rimozione dell’elemento a più alta priorità\navrebbe complessità Θ(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#7": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itSono possibili realizzazioni più efficienti?\n• L’obiettivo è quello di bilanciare i due costi\nRimozione dell’elemento a più alta prioritàInserimento di un nuovo elementoO(n)\nO(log n)\nO(1)\nO(1) O(log n) O(n)?Realizzazione \ntramite una lista \nordinata\nRealizzazione \ntramite una lista \nnon ordinata",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#8": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itLa struttura dati heap\n• Un heap\n– è una struttura dati che può essere utilizzata per \nrealizzare una coda di priorità\n– è uno speciale array i cui valori sono in rapporto \ncon la loro posizione nell’array\n– può essere un max-heap o un min-heap\n• noi vedremo in dettaglio il max-heap",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\130-heap-11.pdf#9": "130-heap-11         copyright ©2022 maurizio.patrignani@uniroma3.itAlberi binari “quasi completi”\n• Gli heap rappresentano alberi binari quasi completi\n• Un albero binario è quasi completo se l’ultimo livello può \nessere incompleto nella sua parte destra",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#0": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Algoritmi e Strutture di Dati\nQuick-sort\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#1": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Nota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#10": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Tempo di esecuzione di PARTITION\n• Le assegnazioni iniziali  e finali richiedono tempo costante\n• Nel caso peggiore, come nel ca so migliore, il sottoarray A[p...r] \nviene scorso per intero da sinistra verso destra\n• Il tempo di esecuzione TPARTITION (n) ∈Θ(n)PARTITION( A,p,r)    /* si assume p < r */\n1.i = p /* i è il primo elemento > A[r] = pivot */\n2. for j = p tor – 1  /* scorro l’array (non il pivot)*/\n3. if A[j] ≤A[r]   /* A[r] è il pivot */\n4. SCAMBIA (A,i,j)\n5. i = i + 1\n6. SCAMBIA (A,i,r)     /* metto il pivot al centro */\n7. return i /* ritorno la posizione del pivot */ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#11": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Esercizi\n1. Che cosa succederebbe nel QUICK_SORT se \nPARTITION (A,p,r) restituisse un valore quguale \na r?\n2. Illustrare le operazioni di PARTITION sull’array \nA = <13, 19, 9, 5, 12, 8, 7, 4, 11, 2, 6, 21>\n3. Illustrare le operazioni di PARTITION su un array\n– già ordinato in senso decrescente\n– già ordinato in senso crescente\n4. Quale valore restituisce PARTITION se tutti gli \nelementi dell’array A[ p...r] hanno lo stesso valore? ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#12": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Esecuzione di PARTITION su 87654321\n87654321j\nir\n87654321j\nir\n1765432887654321j\nir\n87654321j\nir\n87654321j\nir87654321j\nir\n87654321j\nir\n87654321\nir\n17654328\nir",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#13": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Esecuzione di PARTITION su 12345678\n12345678j\nir\n12345678j\nir\n12345678j\nir\n12345678j\nir\n12345678j\nir12345678j\nir\n12345678j\nir\n12345678j\nir\n12345678j\nir\n12345678j\nir12345678j\nir\n12345678j\nir\n12345678j\nir\n12345678j\nir\n12345678\nir",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#14": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Caso peggiore e migliore per QUICK_SORT\n• Il caso peggiore per QUICK_SORT è quando \nPARTITION elegge a pivot il valore massimo \no minimo dell’array\n– in questo caso QUICK_SORT non ricorre su due \nsottoarray bilanciati, ma ricorre su un sottoarray più\ncorto di una casella ed un sottoarray degenere\n• Il caso migliore per QUICK_SORT è invece \nquando PARTITION elegge a pivot il valore \nmediano dell’array\n– in questo caso QUICK_SORT ricorre su due \nsottoarray bilanciati",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#15": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Analisi del caso migliore per QUICK_SORT\n• Nel caso migliore il tempo di calcolo di QUICK_SORT\nsu un array con nposizioni è\nT(n) = 2 · T( n/2) + Θ(n)\n• Questa equazione di ricorrenza può essere risolta con il \nteorema dell’esperto \nT(n) = a·T (n/b) + p( nk) \n• Nello speciale caso in cui\na=2 b=2 k=1\n•C h e  p e r  a= bksi risolve in \nT(n) = Θ(nklog n) = Θ(nlog n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#16": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Analisi del caso peggiore per QUICK_SORT\n•S i  h a\nT(0) = a\nT(n) = T( n-1) + Θ(n) \n• Sappiamo che la soluzione di questa equazione \ndi riccorrenza è\nT(n) = a+ \n• E dunquen\n∑g(k)\nk=1\nn\nΘ(k) = Θ(n\nk)= Θ(n2) T(n) =∑∑\nk=1 k=1",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#17": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Analisi del caso medio per QUICK_SORT\n• Si può dimostrare formalmente che nel caso medio \nQUICK_SORT ha una complessità Θ(n logn)\n– l’analisi, però, è molto più complessa del caso migliore e \ndel caso peggiore\n• Nel seguito vedremo solamente due considerazioni \nintuitive che ci aiutano a giustificare questo risultato\n1. qual è la complessità nel caso in cui lo sbilanciamento della \nricorsione non supera mai una determinata soglia\n2. qual è la complessità nel caso in cui ricorsioni sbilanciate si \nalternano a ricorsioni più bilanciate",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#18": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Caso bilanciato 9-a-1\n• Supponiamo che PARTITION divida il \nsottoarray in due parti che hanno una \nproporzione fissa\n– supponiamo che la proporzione sia 9-a-1\n• Abbiamo\nT(n) ≤T(9n/10) + T( n/10) + cn \ndove cn esplicita Θ(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#19": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Ricorsione con proporzione 9-a-1\ncn\ncn\ncn\ncn\n≤cnn\n1n109n10\n1n1009n1009n100\n181n100\n81n1000729n1000\n1≤cnlog 10 nlog 10/9 n\nO(n lgn)• Ciò fa presumere che il costo nel caso medio sia \nmolto vicino al caso migliore",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#2": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Quick-sort\n• Algoritmo di ordinamento in loco ma non stabile\n• Tempo di esecuzione \n– nel caso peggiore Θ(n2)\n– nel caso migliore e medio Θ(nlog n)\n• i fattori costanti na scosti nella notazione Θsono abbastanza piccoli\n• Introdotto da Hoare nel 1962\n– la versione che vedremo è una variante dovuta a Lomuto\n• Basato sul paradigma divide et impera",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#20": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Alternanza di ricorsioni bilanciate e sbilanciate\n• Supponiamo che nel 20% dei casi PARTITION produca una \npartizione meno bilanciata di 9-a-1\n• Supponiamo che nell’albero delle chiamate ricorsive una \nripartizione sbilanciata sia se mpre seguita da una bilanciata\n• Il costo di una ripartizione sbilanciata può essere assorbito dal \ncosto della ripartizione bilanciata\nΘ(n)\nΘ(n)n\nn-2\n2n-2\n20 n-1Θ(n)\nΘ(n)n\nn-2\n2n-2\n2",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#21": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Versione randomizzata di QUICK_SORT\n• E’ possibile modificare QUICK_SORT in \nmaniera che i casi peggiori non coincidano con \ndisposizioni notevoli degli elementi\nRANDOMIZED_PARTITION( A,p,r)\n1.i = RANDOM(p,r) \n2. SCAMBIA (A,r,i) \n3. return PARTITION (A,p,r)\nRANDOMIZED_QUICK_SORT( A,p,r)\n1. ifp < r then\n2.q = RANDOMIZED_PARTITION (A,p,r) \n3. RANDOMIZED_QUICK_SORT (A,p,q-1)\n4. RANDOMIZED_QUICK_SORT (A,q+1,r)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#22": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Stabilità di QUICK_SORT\n•QUICK_SORT non è stabile:\n56514j\nir\n56514j\nir\n56514j\nir\n56514j\nir56514j\nir\n16554j\nir\n16554\nir\n14556\nir",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#23": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Algoritmi di ordinamento per confrontocaso \nmigliorecaso \nmediocaso \npeggiorein locostabile\nSELECTION-SORT Θ(n2) si si\nINSERTION-SORT Θ(n) Θ(n2) Θ(n2) si si\nMERGE-SORT Θ(n logn) no si\nHEAP-SORT Θ(n logn) si no\nQUICK-SORT Θ(nlog n)Θ(n log n) Θ(n2) si no",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#3": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Divide et impera nel quick-sort\nPer ordinare un sottoarray A[p...r]\n•D i v i d e\n– A[p...r] viene ripartito (e risistemato) in due sottoarray non \nvuoti A[p...q–1] e A[q+1...r], in modo che ogni elemento del \nprimo sia minore o uguale ad A[q] e ogni elemento del secondo sia maggiore ad A[q]\n– l’indice q viene calcolato dalla  procedura di partizionamento\n•I m p e r a\n– i due sottoarray A[p...q–1] e A[q+1...r] sono ordinati, \nricorsivamente\n• Combina\n– non c’è niente da fare: A[p...r] è ordinato",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#4": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Procedura QUICK_SORT\n• La procedura QUICK_SORT ordina in loco l’intervallo A[ p..r]\n–s e  p = r, allora l’intervallo contiene una  sola casella ed è già ordinato: \nl’invocazione di QUICK_SORT non ha effetto\n–s e  p > r, allora l’intervallo è un interval lo degenere e l’invocazione di \nQUICK_SORT non ha effetto \n• Il valore qritornato da PARTITION è tale che p≤q≤r\n• Per ordinare l’intero array viene invocata la procedura:\nQUICK_SORT (A,0,A.length-1)QUICK_SORT( A,p,r)\n1. ifp < r \n2.q = PARTITION (A,p,r) \n3. QUICK_SORT (A,p,q-1)\n4. QUICK_SORT (A,q+1,r)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#5": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Esecuzione di QUICK_SORT su\n16732584pr\n1324pr13247586\n47586pr\n1234 45687\n1pr\n45pr\n23pr\n4 687pr\n678\n78pr6 2\n123456 7816732584\npr\n76PARTITION\nPARTITION\nPARTITIONPARTITION",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#6": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Procedura PARTITION\n• La procedura PARTITION viene invocata su \nun intervallo di almeno due elementi ( p< r)\n– due casi base\n• i due elementi sono ordinati\n• i due elementi non sono ordinatiPARTITION( A,p,r)    /* si assume p < r */\n1.i = p /* i è il primo elemento > A[r] = pivot */\n2. for j = p tor – 1  /* scorro l’array (non il pivot)*/\n3. if A[j] ≤A[r]   /* A[r] è il pivot */\n4. SCAMBIA (A,i,j)\n5. i = i + 1\n6. SCAMBIA (A,i,r)     /* metto il pivot al centro */\n7. return i /* ritorno la posizione del pivot */ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#7": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Esecuzione di PARTITION su 16732584\n16732584j\nir\n16732584j\nir\n16732584j\nir16732584j\nir\n16732584j\nir\n13762584j\nir\n13762584j\nir13267584j\nir\n13267584j\nir\n13267584\nir\n1324758616732584j\nir",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#8": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Esecuzione di PARTITION su\n• Caso base 1: PARTITION su una coppia ordinata16\n16j\nir\n16j\nir\n16\nir\n16PARTITION( A,p,r)    /* si assume p < r */\n1.i = p /* i è il primo elemento > A[r] = pivot */\n2. for j = p tor – 1  /* scorro l’array (non il pivot)*/\n3. if A[j] ≤A[r]   /* A[r] è il pivot */\n4. SCAMBIA (A,i,j)\n5. i = i + 1\n6. SCAMBIA (A,i,r)     /* metto il pivot al centro */\n7. return i /* ritorno la posizione del pivot */ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\140-quick-sort-06.pdf#9": "140-quick-sort-06         copyright ©2019 maurizio.patrignani@uniroma3.it Esecuzione di PARTITION su\n• Caso base 2: PARTITION su una coppia non ordinata87\n87j\nir\n87\nir\n87\nir\n78PARTITION( A,p,r)    /* si assume p < r */\n1.i = p /* i è il primo elemento > A[r] = pivot */\n2. for j = p tor – 1  /* scorro l’array (non il pivot)*/\n3. if A[j] ≤A[r]   /* A[r] è il pivot */\n4. SCAMBIA (A,i,j)\n5. i = i + 1\n6. SCAMBIA (A,i,r)     /* metto il pivot al centro */\n7. return i /* ritorno la posizione del pivot */ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#0": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nAlberi rosso-neri\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#1": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itContenuto\n• Definizione di alberi rosso-neri\n• Proprietà degli alberi rosso-neri\n• Complessità delle operazioni elementari\n• Rotazioni• Inserimenti e cancellazioni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#10": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itAlberi rosso-neri e numero dei nodi\n•Abbiamo appena dimostrato che in un albero \nrosso-nero h∈O(log n)\n•Sappiamo però che in un albero binario h è\nalmeno l’altezza di un albero completo con n\nnodi, cioè h∈Ω(log n) \n•Dunque in un albero rosso-nero h∈Θ(log n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#11": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itOperazioni sugli alberi rosso-neri\n• L’altezza dell’albero è logaritmica nel numero \ndei nodi ( h∈Θ(log n))\n• Tutte le operazioni di consultazione eseguibili \nin tempo Θ(h) su un albero binario di ricerca \nsono eseguibili in tempo Θ(log n) su un albero \nrosso-nero:\n–SEARCH\n–MINIMUM–MAXIMUM",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#12": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itOperazioni INSERT e DELETE\n• Le operazioni INSERT e DELETE possono \nugualmente essere eseguite in Θ(log n)\n•TREE_INSERT e TREE_DELETE , però, non \ngarantiscono la conservazione delle proprietà\ndegli alberi rosso-neri\n– a valle delle operazioni di inserimento e \ncancellazione devono essere lanciate delle \nprocedure che ripristinano tali proprietà in Θ(log n)\n• Nel seguito vedremo a titolo di esempio la sola \nprocedura RB_INSERT per l’inserimento di un \nnodo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#13": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itProcedura RB_INSERT\n16. RB_INSERT_FIXUP( t,new)// ripristina le proprietà15.new.color = RED // i nuovi nodi sono sempre rossi14.new.left = new.right = t.null13. else y.right = new12.y.left = new11. else if new.key < y.key10.t.root = new // …aggiorno t.root9. ify == t.null // se new de ve diventare la radice…8.new.p = y // aggiorno il genitore di new7. else x=x . r i g h t6. x = x.left5. if new.key < x.key4.y = x // cerco il padre y a cui appendere new3. while x != t.null // finché non sono arrivato a t.null2.x = t.root1.y = t.nullRB_INSERT( t,new)/* inserisco il nodo new nell’albero t */",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#14": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itRotazioni\n• L’operazione base che viene utilizzata per ripristinare le proprietà\ndell’albero rosso-nero è la rotazione \n– le rotazioni non alterano i colori dei nodi\n– l’albero rimane un albero binario di ricerca\n– l’operazione può essere eseguita in tempo Θ(1)\nx\ny α\nβγy\nαβγxLEFT-ROTATE (t,x)\nRIGHT-ROTATE (t,y)\nα xβy γ α xβy γ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#15": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itEsempio di rotazione a sinistra\n7\n4\n3 6 911\n218\n14 19\n12 17 22\n12 7\n4\n3 618\n211\n914\n12 1719\n22\n12",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#16": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itProcedura LEFT_ROTATE\n12.x.p = y11.y.left = x10. else x.p.right = y9.x.p.left = y8. else if x == x.p.left7.t.root = y6. if x.p == t.null5.y.p = x.p4.y.left.p = x3. ify.left != t.null2.x.right = y.left // sposto β1.y = x.right // trovo yLEFT_ROTATE( t,x)x\nyα\nβγ\ny\nαβγ x",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#17": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itRipristino dell’albero rosso-nero\n• Il nuovo nodo aggiunto è una foglia e ha colore rosso\n• Ricordiamo i vincoli di un albero rosso-nero\n1. ogni nodo è rosso o nero\n2. la radice e la sentinella t.null sono nere\n3. se un nodo è rosso entrambi i suoi figli sono neri\n4. tutti i cammini che vanno dalla radice a t.null\ncontengono lo stesso numero di nodi neri\n• Se l’albero era vuoto la proprietà 2 è violata\n– in questo caso è sufficiente colorare la radice di nero\n• Altrimenti solo la proprietà 3 potrebbe essere violata\n– situazione più complicata",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#18": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itViolazione: nodo rosso con un figlio rosso \n•S e  RB_INSERT ha appeso il nuovo nodo new (che è\nsempre rosso) ad un genitore rosso \n– chiamiamo “zio di new” il nodo fratello del genitore di new\n• lo zio di new esiste sempre, \neventualmente è t.null\n– sono possibili tre casi\ncaso 1: new è un figlio sinistro e \nlo zio è nero e figlio destro\ncaso 1’: new è un figlio destro e \nlo zio è nero e figlio sinistro\ncaso 2: new è un figlio destro e lo zio è nero e figlio destro\ncaso 2’: new è un figlio sinistro e lo zio è nero e figlio sinistro\ncaso 3: lo zio di new è rosso7\n5 8\n4newzio di new new.p",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#19": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itViolazione: caso 1\ncaso 1:\nnew è un figlio sinistro e \nlo zio è nero e figlio destro\nricolorazione\ndi new.p e di new.p.p5811\n1514\n4zio di new\nnew\n127\n58\n4new\n127\n151411rotazione destra su \nnew.p.p\nora l’albero è rosso-neronew.pnew.p.p",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#2": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itMotivazioni\n• Un dizionario realizzato con un albero binario \ndi ricerca consente operazioni efficienti quando l’albero è bilanciato\n• Ha senso investire delle risorse per mantenere \nl’albero bilanciatoalberi binari di ricerca (complessità nel caso peggiore)\nΘ(log n) Θ(n) cancellazioneΘ(log n) Θ(n) inserimentoΘ(log n) Θ(n) ricercabilanciati sbilanciati operazione",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#20": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itViolazione: caso 2\n1 7\n5 8211\n1514\n4caso 2:\nnew è un figlio destro e lo \nzio è nero e figlio destro\nrotazione sinistra su \nnew.pzio di new\nnew\n5811\n1514\n4zio di new\n127i due nodi violano \nancora la regola 3 \n(ma questa volta new è\nun figlio sinistro e posso \napplicare la procedura \ndel caso 1)new.p\nnewnew.p",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#21": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itViolazione: caso 3\n1 7\n5 8211\n1514\n4\n1 7\n5 8211\n1514\n4caso 3:\nlo zio di new è rosso\nricolorazione\ndi new.p , dello zio di\nnew e di new.p.pnewzio di new\nnewzio di newiterazione:\nnew = new.p.p\nora new e new.p\npotrebbero ancora \nviolare la regola 3 \n(ma new èp i ù\nvicino alla radice)new.pnew.p.p\nnew.p",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#22": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itViolazione: caso 3\n• Nel caso 3 new è più vicino alla radice ma potrebbe \nviolare la regola 3 con new.p\n• Occorre rilanciare la procedura con il nuovo new\n• Il caso peggiore è quando si ha una sequenza di casi 3 \nfino a che non si risale alla radice\n• Quando arriviamo alla radice questa diventa rossa\n– in questo caso è sufficiente ricolorare la radice di nero\n– questo equivale ad incrementa re di uno il numero dei nodi in \nogni cammino dalla radice al nodo t.null",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#23": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itComplessità di RB_INSERT_FIXUP\n• Le violazioni nel caso 1 e 2 vengono risolte in \ntempo Θ(1)\n• Poiché l’albero è alto Θ(log n), la procedura per \nrisolvere una violazione nel caso 3  può essere \nrilanciata al massimo Θ(log n) volte\n• La complessità di RB_INSERT_FIXUP , e \ndunque di RB_INSERT , èΘ(log n) ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#24": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itCancellazioni in un albero rosso-nero\n• Analogamente ad RB_INSERT , la procedura \nRB_DELETE\n– prima cancella un nodo con la stessa strategia di \nTREE_DELETE degli alberi binari di ricerca\n– poi ripristina le proprietà degli alberi rosso-neri \nchiamando una opportuna procedura \nRB_DELETE_FIXUP\n•RB_DELETE_FIXUP utilizza rotazioni e ricolorazioni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#25": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itConclusioni\n• Complessivamente gli alberi rosso-neri offrono \nuna realizzazione di alberi binari di ricerca con \nle seguenti complessità nel caso peggiore\n– inserimento in Θ(log n)\n– cancellazione in Θ(log n)\n– ricerca in Θ(log n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#26": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itEsercizi\n1. Qual è la complessità dell’algoritmo TREE_SORT , \nche utilizza un albero binario di ricerca per ordinare \nun array, nel caso in cui l’albero sia un albero rosso-nero?\n2. Data una realizzazione del tipo astratto di dato \n“insieme” tramite un albero rosso-nero con le \nseguenti funzioni\n– INSERT(t,k) in Θ(log n)\n– REMOVE(t,k) in Θ(log n)\n– SEARCH(t,k) in Θ(log n)\nrealizza la funzione UNIONE (t1,t2) che calcola \nl’unione di due insiemi t1e t2e discutine la \ncomplessità",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#27": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itEsercizi\n3. Mostra come un albero rosso-nero possa \nessere utilizzato per costruire una coda di \npriorità\n– come si può fare per accedere all’elemento \nminimo/massimo della coda?\n– qual è il costo delle operazioni di accesso, di \ncancellazione e di inserimento?",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#3": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itAlbero con sentinelle\n• Gestire il bilanciamento di un albero è un obiettivo complesso\n• Per semplicità vorremmo che non ci siano nodi con un solo \nfiglio destro o un solo figlio sinistro\n– questo può essere realizzato aggiungendo all’albero tun nodo \n“sentinella” t.null e sostituendo con un puntatore a t.null ogni \nvalore NULL del puntatore x.left o x.right di un nodo x\n14\n10\n7\n316\n12 15 NULL\nNULL NULL NULL\nNULL NULLNULL NULL14\n10\n7 12 1516\n3\nt.null",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#4": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itDefinizione di alberi rosso-neri\n• Un albero rosso-nero è un albero binario di ricerca \nnel quale \n1. ogni nodo è rosso o nero\n2. la radice t.root e la sentinella t.null sono nere\n3. se un nodo è rosso entrambi i suoi figli sono neri\n4. tutti i cammini che vanno dalla radice a t.null\ncontengono lo stesso numero di nodi neri\n• Convenzionalmente chiamiamo “altezza” dell’albero \nrosso-nero la lunghezza del cammino più lungo tra la \nradice e t.null\n– corrisponde in realtà all’altezza + 1",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#5": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itt.nullEsempio di albero rosso-nero\n14\n10\n7\n316\n12 1521\n19 23\n2017\n30\n2826\n38\n39 354741• Attenzione\n– l’albero deve essere un albero binario di ricerca\n– non tutti gli alberi binari di ricerca possono essere colorati in maniera da \ndiventare alberi rosso-neri\n……",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#6": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itAlberi rosso-neri e bilanciamento\n14\n10\n7\n316\n12 1521\n19 23\n2017\n30\n2826\n38\n39 354741• Tutti i cammini dalla radice a t.null hanno knodi neri (nell’esempio k=4)\n– ogni cammino ha almeno k-1 archi (nell’esempio: 3 archi)\n– il cammino più lungo alterna nodi neri e rossi e ha 2( k-1) archi (nell’esempio: 6 archi)\ncammino \npiù corto \n(3 archi)cammino più lungo \n(6 archi)esempio con \nk=4\nh=6\nt.null",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#7": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itAlberi rosso-neri e bilanciamento\n14\n10\n7\n316\n12 1521\n19 23\n2017\n30\n2826\n38\n39 354741• Tutti i cammini dalla radice a t.null hanno knodi neri (nell’esempio k=4)\n– la lunghezza del cammino più lungo (2( k-1)) è al massimo due volte la lunghezza \ndel cammino più corto ( k-1)\ncammino \npiù corto \n(3 archi)cammino più lungo \n(6 archi)esempio con \nk=4\nh=6\nt.null",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#8": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itAlberi rosso-neri e profondità\n14\n10\n7\n316\n12 1521\n19 23\n2017\n30\n2826\n38\n39 354741• Tutti i cammini dalla radice a t.null hanno knodi neri (nell’esempio k=4)\n– l’albero contiene un sottoalbero completo di profondità h’=  h/2 – 1 \nesempio con \nk=4\nh=6\nh’=2\ncammino \npiù corto \n(3 archi)cammino più lungo \n(6 archi)sottoalbero completo\nt.null",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\160-alberi-rosso-neri-11.pdf#9": "160-alberi-rosso-neri-11 copyright ©2020 maurizio.patrignani@uniroma3.itAlberi rosso-neri e numero dei nodi\n• Tutti i cammini dalla radice a t.null hanno knodi neri (nell’esempio k=4)\n– l’albero ha profondità massima h= 2(k-1) \n– l’albero contiene un sottoalbero completo di profondità h’=  h/2 – 1\n• I nodi dell’albero sono almeno quelli del sottoalbero completo\n– ricorda che un albero completo di altezza xha 2x+1-1 nodi \n121 21 22112 1'− =− =− ≥+\n\n−+h h\nhn\n)1 log(2 + ≤ n h221h\nn ≥+\n• Dunque h∈O(log n))1 log(2+ ≤ nh",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#0": "170-complessita-problemi-08\n1170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nComplessità dei problemi\nm.patrignani\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono prote tte dalle leggi sul copyright \n• il titolo ed il copyright relati vi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica\ne testo) sono di proprietà degli a utori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istitutipubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente\nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il\ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve\nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#1": "170-complessita-problemi-08\n2170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itContenuto\n• Definizioni\n– complessità O( f(n)), Ω(f(n)) e Θ(f(n)) di un \nproblema \n• Problemi e complessità\n– esempi di problemi di complessità ignota\n– lower bound per gli algoritmi di ricerca basati su \nconfronti\n– lower bound per gli algoritmi di ordinamento per \nconfronto\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itProblemi e complessità\n• Sappiamo che\n– un algoritmo corretto per un problema computazionale è una \n“ricetta” per la sua soluzione\n• termina sempre\n• produce un output che, nella definizione del problema, corrisponde \nall’istanza in input\n• Un problema ammette infiniti algoritmi corretti\n– di ogni algoritmo possiamo calcolare la complessità\nasintotica\n• Alcuni problemi ammettono algoritmi più efficienti di \naltri problemi\n– i problemi hanno una comple ssità asintotica intrinseca?",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#10": "170-complessita-problemi-08\n11170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itAlbero di decisione \n• Possiamo definire un albero i cui nodi interni \nsono i vari confronti es eguiti dall’algoritmo e le \ncui foglie sono le possibili risposte\n• Questo albero è un albero binario con n foglie\n– l’altezza dell’albero è Ω(log n)\n– il numero dei confronti n ecessari per individuare \nuna foglia è Ω(log n)\n• Ne consegue che nel cas o peggiore una ricerca \nimplica Ω(log n) confronti\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itAlgoritmi di ordinamento\n• Con considerazioni an aloghe dimostreremo un \nlower bound sugli algoritmi di ordinamento per confronto\n• Gli algoritmi più veloci che conosciamo, come \nil  MERGE_SORT , hanno una complessità\ntemporale Θ(nlog n)\n• Dimostreremo che tutti gli algoritmi di \nordinamento per confronto hanno una \ncomplessità nel caso peggiore Ω(nlog n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#11": "170-complessita-problemi-08\n12170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itAlgoritmi di ordinamento per confronto\n• Un algoritmo di ordinamento è detto “ algoritmo \ndi ordinamento per confronto ” se il flusso delle \noperazioni dipende dal confronto tra due \nelementi della sequenza\n•E s e m p i o\n–n e l  MERGE_SORT l’operazione MERGE confronta i \nvalori delle due sotto-seque nze ordinate per ottenere \nun’unica sequenza ordinata\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itEsecuzione di un algoritmo per confronto\n• Immaginiamo di lanciare un algoritmo di ordinamento per \nconfronto con una generica sequenza di input (a,b,c)\na ≤b\nc ≤b\na ≤c\nb, c, anonono• L’algoritmo \neseguirà un certo \nnumero di confronti per poi produrre un output\n– l’output è un’opportuna  \npermutazione dei valori di input\n• Se lo lanciamo con una \nsequenza con valori diversi \nalcuni confronti avranno esito \ndiverso\n– l’output prodotto è una diversa \npermutazione dei valori di inputb, a, csi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#12": "170-complessita-problemi-08\n13170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itAlbero di decisione\n• L’esecuzione di un algoritmo di ordinamento per \nconfronto equivale alla discesa in un immaginario albero di decisione\nb ≤c\na, b, ca ≤b\na ≤c\na, c, b c, a, bc ≤b\nc, b, a a ≤c\nb, a, c b, c, asi\nsi\nsisi\nsi nonono\nno\nno\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itNumero di confronti necessari\n• Tutte le permutazioni degli elementi da ordinare \ndevono essere foglie dell’albero di decisione\n– ogni possibile permutazione dei valori di input deve essere \nraggiungibile\n–s e  nsono gli elementi da ordinare  le possibili permutazioni \nsono n!\n• Il numero di confronti eseguiti nel caso peggiore \nequivale al cammino più lungo tra la radice ed una \nfoglia\n– l’altezza di un al bero binario con n! foglie è almeno log2 n!\n– il problema dell’ordin amento per confronto è Ω(log2 n!)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#13": "170-complessita-problemi-08\n14170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itApprossimazione di Stirling\n• Consideriamo la funzione ln n!\n– nel calcolo asintotico la base  del logaritmo è indifferente\n00.511.522.5\n123456789 1 0∑∫\n=≈=+++=n\nknxdx k n n\n11ln ln ln 2ln1ln!ln L\n∑\n=n\nkk\n1ln∫nxdx\n1ln\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itCalcolo di\n• Integrazione per parti:\n• Nel nostro caso\nu=ln xv =x\n•dv/dx= 1; du/dx= 1/x∫∫−= dxdxduv uvdxdxdvu\n∫ ∫−=−=⋅ xxx dxxx xxdxx ln1ln 1ln\n1 ln1) ln( ln ln\n11+−=−= ≈∫∑\n=nnnnxxx xdx knn\nk∫nxdx\n1ln",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#14": "170-complessita-problemi-08\n15170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itOrdinamento per confronto: lower bound\n• L’esecuzione di un al goritmo di ordinamento \nper confronto corrisponde alla discesa in un albero di decisione con n! foglie\n–nè il numero di elementi da ordinare\n• Nel caso peggiore il numero di confronti (nodi \ninterni nel cammino radice-foglia) è Ω(nln n)\n–MERGE_SORT è un algoritmo di ordinamento per \nconfronto asintoticamente ottimo\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itDomande sulla complessità dei problemi\n1. Supponiamo che il problema P abbia \ncomplessità O(n). E’ possibile che esista un \nalgoritmo A che risolve P che abbia una \ncomplessità Ω(n2)?\n2. Supponiamo che un problema P abbia \ncomplessità Θ(n2). Può esistere un algoritmo \nA che risolve P e ha compessità Ω(n)?\n3. Supponiamo che un problema P abbia \ncomplessità Θ(n). Può esistere  un algoritmo A \nche risolve P e ha complessità Θ(n2)?",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#15": "170-complessita-problemi-08\n16170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itSoluzioni\n1. P ∈O(n). Può esistere A ∈Ω(n2)?\n•S ì ,  s e  P  ∈O(n) vuol dire che esiste un (opportuno) \nalgoritmo A’ ∈O(n). Gli altri algoritmi, tra cui A, \nche risolvono P possono avere complessità\narbitrariamente elevata \n2. P ∈Θ(n2). Può esistere A ∈Ω(n)?\n• Sì. Non solo, tutti gli algoritmi che risolvono P \nhanno complessità Ω(n2) e dunque anche Ω(n)\n3. P ∈Θ(n). Può esistere A ∈Θ(n2)?\n• Sì, ciò non contraddice P ∈O(n) né P ∈Ω(n). ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#2": "170-complessita-problemi-08\n3170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itAnalisi della complessità dei problemi\n• Obiettivo\n– classificare i problemi in ba se alla loro difficoltà di \nsoluzione intrinseca\n• determinare la quantità di risorse che comunque è necessario \nspendere per risolverli\n• Strumento\n– associare al problema la co mplessità dell’algoritmo più\nefficiente che lo risolve\n• Inconveniente\n– dato un problema non è possibile considerare tutti gli infiniti \nalgoritmi che lo risolvono\n• non possiamo determinare direttamente la complessità dell’algoritmo \npiù efficiente\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itComplessità O( f(n)) di un problema\n• Un problema ha complessità temporale O(f(n)) se \nesiste un algoritmo che lo riso lve che ha complessità\ntemporale O( f(n))\n• In forma stenografica:\nP ∈O(f(n))⇔∃ A ∈O(f(n))\n•O ( f(n)) sono le risorse sufficienti a risolvere il \nproblema",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#3": "170-complessita-problemi-08\n4170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itComplessità O( f(n)) di un problema\n• Se un problema ha complessità temporale O( f(n))\n– è garantito che il problema possa essere risolto spendendo \nO(f(n)) risorse\n– è possibile che il problema possa essere risolto spendendo \nmeno di O( f(n)) risorse\n• potrebbe esistere un algoritmo più efficiente che non conosciamo\n–f(n) è un limite superiore (upper bound) alle risorse \nsufficienti a risolvere il problema \n• Per dimostrare che un problema ha complessità O( f(n))\n– occorre produrre un algoritmo che lo risolva e che abbia \ncomplessità O( f(n))\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itEsempio: problema O( f(n))\n• La complessità temporale dell’algoritmo SOMMA è\nO(n), dove nè il numero degli elementi dell’array A\n• Il problema della somma di ninteri\n– ha complessità temporale O( n)\n– è limitato superiormente da f(n) = n\n–“ è O ( n)”4. return somma3.somma = somma + A[i]2. fori = 1 toA.length-11.somma = A[0]SOMMA(A) Zrestituisce la somma degli elementi dell’array A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#4": "170-complessita-problemi-08\n5170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itComplessità Ω(f(n)) di un problema\n• Un problema ha complessità temporale Ω(f(n)) se \nogni algoritmo che lo ri solve ha complessità\ntemporale Ω(f(n))\n• In forma stenografica:\nP ∈Ω(f(n))⇔∀ A ∈Ω(f(n))\n•Ω(f(n)) sono le risorse necessarie a risolvere il \nproblema\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itComplessità Ω(f(n)) di un problema\n• Se un problema ha complessità temporale Ω(f(n))\n– non è possibile che il prob lema possa essere risolto \nspendendo meno di Ω(f(n))\n– non è detto che il problema  sia risolvibile spendendo O( f(n))\n–f(n) è un limite inferiore (lower bound) alle risorse \nnecessarie per risolvere il problema \n• Per dimostrare che un problema ha complessità Ω(f(n))\n– non possiamo considerare tutti gli algoritmi ch e lo risolvono\n– non esiste un metodo preciso per determinare Ω(f(n))\n• generalmente si ragiona sulla natura delle istanze e delle relative \nsoluzioni",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#5": "170-complessita-problemi-08\n6170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itEsempio: problema Ω(f(n))\n• Consideriamo il problema del calcolo della somma di \nninteri\n• Tutti gli algoritmi che riso lvono il problema devono \nnecessariamente prendere in considerazione gli n\ninteri in input\n– altrimenti cambiando un valore di input l’algoritmo darebbe \nlo stesso output, e questo è assurdo  \n• Il problema della somma di ninteri\n– ha complessità temporale Ω(n) \n– è limitato inferiormente da f(n) = n\n–“ èΩ(n)”\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itComplessità Θ(f(n)) di un problema\n• Un problema ha complessità temporale Θ(f(n)) se \nse ha contemporaneament e complessità temporale \nO(f(n)) e Ω(f(n)) \n– non è possibile che il prob lema possa essere risolto \nspendendo meno di O( f(n))\n– esiste almeno un algoritmo ch e risolve il problema in Θ(f(n))\n• Limite inferiore e limite superiore coincidono\n–f(n) è la complessità intrinseca del problema\n• Non sempre è possibile determinare Θ(f(n))\n– di molti problemi la complessità intrinseca è ignota",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#6": "170-complessita-problemi-08\n7170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itEsempio: problema Θ(f(n))\n• Per quanto detto sopra il problema della somma \ndi ninteri ha complessità Θ(n)\n– l’algoritmo proposto per dimostrare che il problema \nèO (n) è un algoritmo asintoticamente ottimo\n• possiamo desistere dalla ricer ca di algoritmi più efficienti\n• è anche vero che questo algoritmo ha complessità\ntemporale Θ(n)\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itProblemi dalla complessità ignota\n• Problema del commesso viaggiatore\n– trovare il circuito più breve che tocca ncittà\n• Upper-bound\n– esiste un algoritmo che ha complessità O( n22n)\n• Lower-bound\n– siccome occorre leggere l’input, il problema è Ω(n) \n– non è mai stato dimostrato che il problema non \npossa essere risolto in tempo polinomiale\n• in realtà non è mai stato dimo strato che il problema non \npossa essere risolto in tempo lineare!",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#7": "170-complessita-problemi-08\n8170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itProblemi NP-completi\n• Il problema del commesso viaggiatore \nappartiene ad una classe  di problemi noti come \nproblemi NP-completi\n• I problemi NP-completi sono tutti equivalenti\n– se si trovasse un algoritmo polinomiale in grado di \nrisolvere un qualunque problema NP-completo si potrebbero risolvere in tempo polinomiale tutti i problemi NP-completi\n• Si ritiene (ma non è sta to mai dimostrato) che \nun algoritmo polinomiale per un problema NP-\ncompleto non possa esistere\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itLower bound di problemi comuni\n• E’ molto difficile dimo strare un lower bound \nper un problema\n• Nel seguito dimostre remo dei lower bound \nlimitati al caso in cui gli algoritmi utilizzati \nsiano basati su confronti\n• In particolare dimostreremo\n– lower bound Ω(log n) per algoritmi di ricerca basati \nsu confronti\n– lower bound Ω(n log n) per algoritmi di \nordinamento basati su confronti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#8": "170-complessita-problemi-08\n9170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itIl problema della ricerca\n• Il problema della ricerca può essere descritto \ncome segue\n– è nota una collezione di coppie <chiave,valore>\n– un’istanza del problema è il valore di una chiave\n– la soluzione del problema è il relativo valore\n• oppure l’informazione che una coppia con tale chiave è\nassente nella collezione\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itCosa sappiamo del problema della ricerca \n• Dipendentemente dal tipo di struttura dati che \nadottiamo per la co llezione di coppie \n<chiave,valore> il problema della ricerca ha \ndiverse complessità nel caso peggiore\n• Esiste un algoritmo più veloce di O(log n)?O(log n) Alberi rosso-neriO(log n) Array ordinatiO(n) Array non ordinatiO(n) Liste",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\170-complessita-problemi-08.pdf#9": "170-complessita-problemi-08\n10170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itAlgoritmi basati su confronti\n• Un algoritmo di ricerca è detto “ algoritmo di \nricerca basato su confronti ” se il flusso delle \noperazioni dipende esclusivamente dal \nconfronto tra la chiave cercata ed una chiave \ndella collezione\n•E s e m p i o\n– nella ricerca binaria si accede all’elemento \nintermedio dell’intervallo di  ricerca e si ricorre su \nuno dei due sottointervalli generati in base al \nconfronto della chiave cercata con il valore della chiave dell’elemento intermedio\n170-complessita-problemi-08 copyright ©2018 maurizio.patrignani@uniroma3.itEsecuzione di una ricerca per confronto\n• Immaginiamo di lanciare un algoritmo di ricerca \nbasato su confronti\nx ≤y\nz ≤w\nu ≤v\nsoluzione 2nosìno• L’algoritmo \neseguirà un certo numero di confronti per \npoi produrre un output\n– l’output è un’opportuna  \ncella di memoria\n• Uno qualsiasi dei valori \ndella collezione potrebbe essere l’output giusto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#0": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nTabelle Hash\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#1": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica\ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istitutipubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente\nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il\ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve\nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#10": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itFunzione hash\n• La funzione hash h\n– definisce una corrispondenza tra l’universo K delle \nchiavi e gli indici della tabella hash T[0…m-1]\nh: K  →{0, 1, . . . , m-1}\n– deve essere deterministica\n• altrimenti dopo aver messo i valori nell’array non riesco \npiù a ritrovare la loro posizione\n– si richiede che sia calcolabile in tempo costante\n• per contenere i tempi di calcolo\n• L’elemento con chiave k∈K si troverà nella \nposizione h(k) nella tabella T",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#11": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itIl problema della collisione\n• La funzione hha un codominio (gli mindici di T) \nmolto più piccolo del dominio (tutti gli elementi di K)\n– è inevitabile che si generino collisioni\n0\nh(k3)=1\nh(k2)=2\n3\nh(k1)=4k3\nk2\nk1k4h(k4)=2universo \ndelle chiavi K\nchiavi \nutilizzate \na runtime?\nk1T\nk3\n?\nv1v3",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#12": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itGestione delle collisioni con liste di trabocco\n• Ogni posizione di Tè un riferimento al primo \nelemento di una lista detta “di trabocco”\n0\nh(k7)=1\nh(k2)=h(k4)=2\n3\nh(k1)=h(k3)=h(k6)=4k7v7\nk2v2\nk1v1k4v4\nk3v3k6v6T",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#13": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itListe di trabocco\n• La lista di trabocco è una lista semplicemente \nconcatenata\n• Ogni nodo della lista è un oggetto con tre campi\n–key : valore della chiave\n• può essere un riferimento ad oggetto\n–info : valore associato alla chiave\n• può essere un riferimento ad oggetto\n–next : riferimento al prossimo nodo\n•èNULL per l’ultimo nodo della lista",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#14": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itRicerca di un elemento in base alla chiave\n• Per ricercare un elemento devo scorrere la lista \ndi trabocco opportuna\n7. return NULL // non l’ho trovato6.x = x.next5. return x.info // l’ho trovato!4. if EQUAL (k,x.key) 3. while x != NULL2.x = T[i] // iteratore per  elementi della lista T[i]1.i = HASH(k) // devo guardare la lista i-esimaGET(T,k) // ritorna il valore associa to alla chiave (o NULL)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#15": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itInserimento di una coppia 〈chiave, valore 〉\n• Anche l’inserimento prevede una ricerca\n8.y.key = k // y nuovo elemento della lista\n9.y.info = v\n10.y.next = T[i]1.i = HASH(k) // devo cercare nella lista i-esima di T\n2.x = T[i] // iteratore per  elementi della lista T[i]\n3. while x != NULL\n4. if EQUAL (x.key,k) \n5. x.info = v // sovrascrivo il vecchio valore\n6. return // ho finito ed esco\n11.T[i] = y // inserimento in testa7.x = x.nextPUT(T,k,v) // inserisce <k,v> (e ventualmente sovrascrive)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#16": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itCancellazione di un elemento\n• L’elemento viene rimosso in base alla chiave\n5. if EQUAL (x.key,k) // l’ho trovato\n6. if prev == NULL // x è il primo della lista\n7. T[i] = x.next\n8. else // non è il primo della lista\n9. prev.next = x.next // lo saltiamo\n10. return // ho finito ed esco\n11.prev = x // non trovato, provo il prossimo \n12.x = x.next4. while x != NULL1.i = HASH(k) // devo cercare nella lista i-esima di T\n2.x = T[i] // iteratore per  elementi della lista T[i]\n3.prev = NULL // punterà all’elemento che precede x DELETE(T,k) // rimuove l’elemento (se esistente)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#17": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itFunzione EQUAL e funzione HASH\n• Una precondizione perché si possa realizzare un \narray associativo con hashtable è che siano definite\n– una funzione EQUAL\n– una funzione HASH\n• Entrambe le funzioni devono essere definite in \nbase al contesto applicativo \n• Per motivi di efficienza si richiede generalmene \nche entrambe le funzioni siano calcolabili in tempo costante\n– rispetto al numero ndegli elementi nell’hashtable",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#18": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itUna funzione EQUAL errata\n• Supponi che le chiavi siano stringhe\n– per esempio realizzate tramite array di caratteri\n• La funzione EQUAL seguente è errata:\n– in questo modo non vengono confrontati i valori \ncontenuti negli array A e B, ma i loro riferimenti\n• cioè gli indirizzi, che sono necessariamente diversi anche \nquando le due stringhe sono uguali1. return A == B EQUAL_WRONG (A,B) // A e B sono due array di caratteri",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#19": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itUna funzione EQUAL corretta\n• La funzione seguente è una funzione corretta per \nquesto contesto applicativo\n– questa volta vengono confrontat i tutti i caratteri delle due \nstringhe\n• la complessità della procedura è ancora Θ(1) se le stringhe hanno una \ndimensione massima nota e indipendente da n5. return FALSE // almeno un carattere diverso\n6. return TRUE4. if A[i] != B[i]1. ifA.length != B.length\n2. return FALSE // lunghezza diversa\n3. fori=0 toA.length-1EQUAL(A,B) // A e B sono due array di caratteri",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#2": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itSommario\n• Richiami sui tipi astratti di dato\n– array associativo\n– insieme\n• Tabelle hash\n– collisioni e liste di trabocco– uso per la realizzazione di tipi astratti di dato \n• Funzioni hash\n– per interi, per stringhe, per oggetti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#20": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itHASH: requisiti\n• Requisiti funzionali\n– è deterministica\n• data una chiave k, dà sempre lo stesso risultato HASH(k)\n• Requisiti prestazionali\n– è calcolabile in tempo costante\n– distribuisce le chiavi utilizza te in esecuzione in maniera \npseudocasuale nell’intervallo [0… m-1]\n• questo requisito potrà solo essere soddisfatto solo in modo \nprobabilistico \n– le chiavi che saranno utilizzate dall’utente in esecuzione non sono note \na priori\n– comunque si scelga la funzione HASH esisterà sempre un insieme di \nchiavi che corrispondono alla stessa casella di T",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#21": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itHASH: ipotesi di distribuzione uniforme\n• E’ soddisfatta dalla funzione HASH quando, data una \nchiave k∈K, la probabilità che HASH (k)=csia la stessa \nper ogni casella c∈[0…m-1] di T\n– indipendentemente da quali al tre chiavi siano state già\ninserite in T\n• Implicazioni dell’ipotesi di distribuzione uniforme\n– per chiavi simili vengono generati hash diversi\n• spesso le chiavi utilizzate sono molto simili\n– quali che siano le chiavi utilizzate, queste vengono con alta \nprobabilità distribuite uniformemente nell’intervallo [0…m-1]",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#22": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itFattore di carico\n• Si definisce fattore di carico il rapporto αtra il \nnumero ndi elementi memorizzati e il numero mdi \nposizioni disponibili\nmnα=\n•αè il numero medio di elementi memorizzati in ogni \nlista concatenata\n• A seconda del valore di αabbiamo\nα< 1 molte posizioni disponibili rispetto agli elementi \nmemorizzati\nα= 1 il numero di elementi corrisponde al numero delle \nposizioni disponibili\nα> 1 molti elementi da memorizza re rispetto al numero delle \nposizioni disponibili",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#23": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itComplessità delle operazioni\n• Caso migliore\n– l’operazione è eseguita su una lista di trabocco vuota o con \nun solo elemento\n– la complessità dell’operazione è data dalla complessità di \nHASH oppure di HASH + EQUAL\n• complessità Θ(1)\n• Caso peggiore\n– tutte le chiavi utilizzate corrispondono alla stessa posizione\n– la complessità coincide con quella che si ha per il calcolo di \nHASH (k) + la ricerca in una lista con n posizioni + il calcolo \ndi EQUAL per n volte \n• complessità Θ(1) + Θ(n) + Θ(n) = Θ(n)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#24": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itComplessità nel caso medio\n• Caso medio\n– se la funzione HASH distribuisce le chiavi in modo \nuniforme nell’intervallo [0… m-1] \n• la lunghezza attesa delle liste di trabocco coincide con la \nlunghezza media α\n• le operazioni hanno complessità Θ(α)\n•s e  αnon supera mai una soglia fissata αmaxla \ncomplessità di ogni operazione è Θ(1)\n– se la funzione HASH non dà garanzie rispetto alla \ndistribuzione delle chiavi\n• la complessità è la stessa del caso peggiore",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#25": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itNote sulla complessità nel caso medio\n• Abbiamo stabilito che nel caso medio la complessità\ndelle operazioni sulle tabelle hash è Θ(α)\n– dove αè per definizione α= n/m\n• Dunque sembrerebbe che α∈Θ (n)\n• Questo vorrebbe dire che, anche nel caso medio, la \ncomplessità delle operazioni è Θ(α) = Θ(n)\n– cioè lineare come nel caso delle liste\n– non si avrebbe nessun vantaggi o dall’adozione delle tabelle \nhash\n• In che cosa sono errate le considerazioni qui sopra?",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#26": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itNote sulla complessità nel caso medio\n• E’ errato considerare α∈Θ (n) a partire dalla \ndefinizione α= n/m\n•A n c h e  m, infatti, cambia al crescere di n\n– quando il fattore di carico αsupera una determinata soglia la \ndimensione mdella tabella viene raddoppiata\n• il fattore di carico αviene dimezzato\n– il costo medio delle operazioni è ancora costante in virtù del \nfatto che il costo (lineare) del raddoppio della tabella viene \nassorbito dai precedenti inserimenti eseguiti in tempo costante\n• vedi slides sulla gestione telesc opica delle pile e sulla complessità\nammortizzata\n• Una realizzazione delle tabelle hash che non preveda \nla gestione telescopica della tabella non può garantire \ntempi costanti di accesso",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#27": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itFunzioni hash",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#28": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itFunzioni hash\n• Considereremo delle funzioni hash per le \nseguenti tipologie di chiavi\n– funzioni hash per interi\n• metodo della divisione\n– veloce ma raramente adottato\n• metodo della moltiplicazione\n– funzioni hash per stringhe\n– funzioni hash per oggetti arbitrari",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#29": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itMetodo della divisione: MOD_HASH\n• Utilizza il resto di una divisione intera\nh(k) = kmod m\n• In pseudocodice:\n• E’ un metodo molto veloce\n• Se le chiavi sono già degli interi pseudocasuali \nla funzione MOD_HASH viene utilizzata per \nriportare le chiavi nell’intervallo [0… m-1]1. return k mod mMOD_HASH( k,m) // k ed m sono interi ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#3": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itArray associativi\n• Gli array associativi sono tipi astratti di dato costituiti \nda coppie <chiave,valore>\n– le operazioni che vogliamo fare sono l’inserimento di una \nnuova coppia, la cancellazione di una specifica chiave e la ricerca del valore corrispondente ad una chiave\n• Possono essere realizzati con coppie di array, liste, \nalberi binari di ricerca, alberi rosso-neri\n• Le realizzazioni di array associativi si prestano anche a \nrealizzare insiemi generici\n– basta omettere il valore  e tenere solo la chiave\n– le operazioni supportate sono inserisci elemento, cancella \nelemento e verifica esistenza elemento",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#30": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itMetodo della divisione: MOD_HASH\n• Se le chiavi non sono pseudocasuali\n– la praticità del metodo è compromessa \n–MOD_HASH ha delle forti proprietà di località\n• con altissima probabilità MOD_HASH (k+1) = \nMOD_HASH (k)+1\n•s e  mè una potenza di 2 o di 10, MOD_HASH (k) produce \nla parte meno significativa del numero kespresso in \nquella base\n– se si vuole usare questo metodo, è raccomandabile \nadottare come mun numero primo lontano da una \npotenza di due per limitare le collisioni\n• per esempio 701",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#31": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itMetodo della moltiplicazione: osservazione 1\n• Supponiamo che le chiavi siano numeri reali \npseudocasuali nell’intervallo (0,1) \n• La funzione\nh(k) =  m · k \nè una buona funzione hash\n–h(k) è deterministica\n–h(k) può essere calcolata in Θ(1)\n–h(k) distribuisce uniformemente le chiavi \nnell’intervallo [0… m-1]\n• in quanto le chiavi erano già uniformemente distribuite!",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#32": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itMetodo della moltiplicazione: osservazione 2\n•S i a  irrun numero irrazionale\n–irrha infinite cifre dopo la virgola, ma non è periodico\n• Date delle chiavi intere qualsiasi, le cifre decimali \ndopo la virgola del prodotto k ·irrsi possono assumere \nuniformemente distribuite nell’intervallo (0,1)\n– questo valore coincide con  k·irr − k·irr\n– Knuth propone\n• è la parte dopo la virgola della sezione aurea \n• è un numero irrazionale... 6180339.0215=−=irr\n... 6180339.1215=+=\nϕ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#33": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itMetodo della moltiplicazione\n• Si utilizza come hash la parte intera di un \nprodotto\nh(k) =m ·(k·irr−k·irr) \n• Dove \n–irrè un numero irrazionale in (0,1) \n• per esempio la parte dopo la virgola della sezione aurea ϕ\ndefinita nella slide precedente\n–m può essere scelto arbitrariamente\n• di solito si usa una potenza di due: m=2p, dove pèu n  \nintero",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#34": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itMetodo della moltiplicazione: MUL_HASH\n• La costante irrviene calcolata una volta sola\n• La funzione MUL_HASH riceve in input l’array \nassociativo a (per avere med irr) e la chiave k\n– la funzione INT tronca un reale all’intero inferiore3.prod = m * (prod – INT(prod)) // reale in (0,m)\n4.out = INT(prod) // intero in [0,m-1]1.m = a.T.length // m è un numero intero\n2.prod = k * a.irr // prod è un numero reale\n5. return out MUL_HASH( a,k) // a array associativo, k intero1.a.irr = ( SQRT(5)-1)*0.5 // a.irr costante irrazionale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#35": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itHASH per stringhe: SIMPLE_HASH\n• Ritorna un numero intero che poi deve essere ridotto \nnell’intervallo [0… m-1] con la funzione MOD_HASH\n• Introdotta nella prima edizione del Kernigham-Ritchie\n• Veloce ma generalmente considerata poco efficace nel \ndistribuire i valori in modo pseudocausuale \n– permutazioni di caratteri hanno lo stesso hash!3.hash = hash + ASCII(S[i])\n4. return hash2. fori = 0 toS.length-11.hash = 0 SIMPLE_HASH( S) // S è una stringa (array di caratteri)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#36": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itHASH per stringhe: DJB2_HASH\n• Introdotta da Daniel J. Bernstein (donde il nome)\n• Il numero 5381 è un numero primo\n• Il “magic numer” 33 non è giustificato teoricamente\n– ma dà ottimi risultati nella pratica\n– corrisponde al prodotto * 32 (traslazione di cinque caselle \ndella rappresentazione) + un incremento di uno\n• può essere realizzato velocemente 3.hash = hash*33 + ASCII(S[i])\n4. return hash2. fori = 0 toS.length-11.hash = 5381 DJB2_HASH( S) // S è una stringa (array di caratteri)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#37": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itFunzioni hash per oggetti\n• Supponiamo che \n– l’oggetto abbia come chiave hcampi c1, c2, …, ch\n– siano già definite opportune funzioni hash \nHASH1(c1), HASH2(c2), …HASHh(ch)\n• Una funzione hash si può ottenere facilmente \ncon la loro somma\nHASH (o) = HASH1(o.c1)+HASH2(o.c2)+…+HASHh(o.ch) \n• Il risultato può essere riportato nell’intervallo \nopportuno tramite MOD_HASH",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#38": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itInsiemi\n• Un insieme è una collezione di elementi \nomogenei\n• Esempi di insieme\n– l’insieme degli studenti\n– l’insieme degli oggetti creati da un programma– l’insieme delle variabili utilizzate da un programma",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#39": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itIl tipo astratto di dato insieme\n•D o m i n i\n– il dominio di interesse è l’insieme I degli insiemi\n– dominio di supporto: gli elementi E dell’insieme– dominio di supporto: i booleani B = {true, false}\n• Costanti\n– l’insieme vuoto\n• Operazioni\n– aggiunge un elemento: ADD: I ×E →I\n– elimina un elemento dall’insieme: REMOVE: I ×E→I\n– verifica l’appartenenza: CONTAINS: I ×E→B\n–…",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#4": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itRealizzazioni efficienti di array associativi\n• La realizzazione più efficiente che conosciamo \ndegli array associativi utilizza alberi rosso-neri\n– garantisce un tempo Θ(log n) per la ricerca, \nl’inserimento e la cancellazione nel caso peggiore e \nnel caso medio \n– occorre che sulle chiavi siano definite le funzioni \nMINORE e UGUALE\n• Tutti gli algoritmi di ricerca basati su confronti \nhanno complessità Ω(log n) nel caso peggiore\n– la ricerca è l’operazione più frequente e critica",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#40": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itRealizzazione di un insieme\n• Si può usare una hashtable in cui lo stesso \nelemento funge da valore e da chiave\ni\n0\nh(k7)=1\nh(k2)=h(k4)=2\n3\nh(k1)=h(k3)=h(k6)=4T\nk7\nk2\nk1k4\nk3k6",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#41": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itProblemi\n1. Illustra l’inserimento in una tabella hash di \ndimensione m=10 gestita con liste di trabocco \ndelle chiavi 32, 17, 19, 31, 33, 15, 38, 46, \nutilizzando la funzione hash MOD-H\n2. Scrivi lo pseudocodice delle funzioni \nADD(I,e), REMOVE (I,e) e CONTAINS (I,e) \ndove Iè un insieme realizzato tramite una \nhashtable ed eè un elemento dell’insieme\n– assumi che siano definite opportune funzioni \nHASH (e) e EQUAL (e1,e2)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#42": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itProblemi\n1. Quali sono le prestazioni delle tabelle hash se \nal posto di una lista semplicemente \nconcatenata usiamo come trabocco un albero \nrosso-nero?",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#5": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itTabelle hash\n• Le tabelle hash realizzano array associativi con le \nseguenti caratteristiche\n– gli algoritmi di ricerca, inse rimento e cancellazione non sono \nbasati su confronti\n– le ricerche, gli inserimenti e le cancellazioni avvengono con \nun tempo Θ(n) nel caso peggiore\n• peggiorativo rispetto agli alberi rosso-neri\n– le ricerche, gli inserimenti e le cancellazioni avvengono con \nun tempo Θ(1) nel caso medio\n• migliorativo rispetto ag li alberi rosso-neri\n– la struttura di dati utilizzata è effettivamente un singolo array",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#6": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itDifficoltà da superare nella realizzazione\n• Due difficoltà principali\n1. la tipologia delle chiavi\n• le chiavi non sono necessariamente degli interi\n• non possiamo confidare nelle chiavi per indicizzare \ndirettamente un array\n2. la numerosità delle possibili chiavi\n• anche se le chiavi fossero degli interi, un array in \ngrado di contenere tutte le chiavi sarebbe troppo \ngrande e troppo sparso\n– per esempio se la chiave fosse un numero di matricola di \nsei cifre dovrei allocare un array con un milione di \nposizioni anche se gli studenti del corso che voglio \nconsiderare sono solo qualche centinaio",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#7": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itArray associativi: osservazione\n• Il numero di chiavi effettivamente utilizzate dal \nprogramma in esecuzione è molto minore del numero delle chiavi possibili\n– quest’ultimo è chiamato “universo” delle chiavi K\nk3\nk2\nk1universo \ndelle chiavi K\nchiavi \nutilizzate \na runtime\ndati associati",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#8": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itRealizzazione tramite tabelle hash\n1. Utilizzo un array Tper memorizzare i dati \nassociati\n– la dimensione mdell’array T\n• è molto minore della dimensione dell’universo K\n• è molto vicina al numero delle chiavi effettivamente \nutilizzate dal programma in esecuzione\n– l’array T, come tutti gli array, può essere \nindicizzato solo da un intero \n2. Definisco una funzione hash h che trasforma \nle chiavi di K negli interi nel range [0… m-1]",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\190-tabelle-hash-08.pdf#9": "190-tabelle-hash-08 copyright ©2021 maruzio.patrignani@uniroma3.itTabella hash\n• L’array Tindicizzato tramite la funzione hash h\nè chiamato tabella hash (oppure hashtable )\n0\nh(k3)=1\nh(k2)=2\n3\nh(k1)=4k3\nk2\nk1universo \ndelle chiavi K\nchiavi \nutilizzate \na runtime k2\nk1T\nk3\nv2\nv1v3",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#0": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nI grafi\nrappresentati con matrici e liste di adiacenza\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#1": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica\ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istitutipubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente\nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il\ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve\nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#10": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itIl tipo astratto di dato grafo\n•D o m i n i\n– il dominio di interesse è l’insieme G dei grafi (diretti o non diretti)\n– dominio di supporto: l’insieme degli iteratori NI per i nodi\n– dominio di supporto: l’insieme degli iteratori AI per gli archi\n– dominio di supporto: i booleani B = {true, false}\n•C o s t a n t i\n– il grafo vuoto\n– gli iteratori non validi per nodi e archi\n• Operazioni\n– trova il primo nodo del grafo: FIRST_NODE: G →NI\n– trova il prossimo nodo:                                         NEXT_NODE: G ×NI→NI\n– trova il primo arco di un nodo:                                 FIRST_EDGE: G ×NI→AI\n– trova il nodo adiacente tramite l’arco:                ADJ_NODE: G ×N1 ×AI→N1\n– trova il prossimo arco: NEXT_EDGE: G ×N1 ×AI→AI\n– determina se due nodi sono adiacenti                      ARE_ADJ: G ×N1 ×NI→B\n– aggiunge un nodo al grafo: ADD_NODE: G →NI\n– aggiunge un arco tra due nodi: ADD_EDGE: G ×N1 ×NI→AI",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#11": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itRealizzazione mediante matrice di adiacenza\n• Rappresentazione preferita per \ngrafi densi\n– cioè per i quali il numero degli \narchi mè prossimo ad n2\n• Consente di sapere rapidamente \nse c’è un arco tra due nodi\n• Usa una matrice (o un array di \narray) in cui l’elemento in \nposizione ( i,j) segnala se esiste \nl’arco ( i,j)\n•O c c u p a  Θ(n2) spazio0 100001 1000 100000000000 10 10 100000000012345\n0\n1\n2\n3\n4\n50\n23\n4\n1\n5g\ng.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#12": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itRealizzazione tramite liste di adiacenza\n• Si fa uso di un array Adi liste doppiamente concatenate\n– generalmente si mette nell’array di rettamente il riferimento al primo \nelemento della lista\n0\n1\n2\n3\n42\n54\n0\n5 0 4prevkey\nnext\n40\n23\n4\n1\n5g g.A",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#13": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itRappresentazione dei grafi: liste di adiacenza\n• Questa rappresentazione occupa spazio O( n) + O( m)\n– nel caso peggiore, siccome m= O(n2) utilizza uno spazio \nO(n2) come le rappresentazioni con matrici di adiacenza\n– in numerose applicazioni, però, m∈O(n)\n• in questo caso le rappresentazio ni con liste di adiacenza sono \npreferibili\n• La lista di adiacenza di un nodo può essere lunga O( n)\n• Percorrendo tutte le liste di adiacenza di tutti i nodi si \nimpiega un tempo O( n) + O( m) \n– che diventa O( n2) se il grafo è denso",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#14": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itRealizzazioni del tipo astratto di dato grafo\n• Matrice di adiacenza\n– l’iteratore per un nodo è un intero\n• è valido se è nell’intervallo legittimo per l’array g.A\n– l’iteratore per un arco uscente da un nodo è ancora un intero\n• è valido se è nell’intervallo legittimo e se la cella corrispondente dell’array \ng.A contiene un uno\n– si riuncia a realizzare efficienteme nte l’operazione di aggiunta di un \nnodo\n• Liste di adiacenza\n– l’iteratore per un nodo è un intero\n• è valido se è nell’intervallo legittimo per l’array g.A\n– l’iteratore per un arco è un riferi mento ad un elemento di una lista\n• l’iteratore non valido è NULL\n– anche in questo caso l’operazione di  aggiunta di un nodo richiede una \ngestione non semplice e non sempre efficiente",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#15": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itRappresentazione di grafi non orientati\n• Per ogni arco non orientato ( u,v) vengono \nrappresentati i due archi orientati ( u,v) e (v,u)\n• Nel caso di matrice di adiacenza\n– la matrice è simmetrica\n• Nel caso di liste di adiacenza\n– se la lista del nodo icontiene il nodo j, allora la lista \ndel nodo jcontiene il nodo i",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#16": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itGrafi pesati sugli archi\n• Sono grafi in cui ad ogni arco eè associato un peso we\n• Nella rapprentazione tramite matrici di adiacenza si \nusano i valori dei pesi al posto degli uni:\n– si assume che non esistano archi con peso zero e si usa lo \nzero per rappresentare l’assenza dell’arco\n–s i  u s a  weper rappresentare un arco di peso we\n• Nella rappresentazione tramite liste di adiacenza\n– ogni elemento della lista ha, oltre all’indice del nodo \nadiacente, anche un attributo weight con valore we",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#17": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itRealizzazioni in linguaggio C\n• La rappresentazione dei grafi in linguaggio C \nsegue gli stessi paradigmi della \nrappresentazione in pseudocodifica\n– tuttavia, poiché gli array in linguaggio C non hanno \nun campo “length” che ne riveli la lunghezza \noccorre aggiungere un campo “numero_nodi” ad entrambe le rappresentazioni\ng.Ag.numero_nodi\ng\n.....",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#18": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi su matrici di adiacenza\n• Dato un grafo grappresentato tramite una matrice di \nadiacenza g.A(un array di array)\n1. scrivi una procedura LISTE (g) che ne restituisca la sua \nrappresentazione mediante un array di liste di adiacenza \ndoppiamente concatenate\n2. scrivi una procedura GRADO_USCITA (g,u) che calcoli il \ngrado di uscita del nodo con indice u\n3. scrivi una procedura GRADO_INGRESSO (g,u) per il calcolo \ndel grado di ingresso del nodo con indice u\n4. scrivi una procedura GRADO_USCITA_MEDIO (g) per il \ncalcolo del grado di uscita medio dei nodi del grafo\n5. scrivi una procedura GRAFO_SEMPLICE (g) che verifica se \nil grafo è semplice (privo di cappi)\n• Discuti la complessità degli algoritmi che hai proposto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#19": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi su liste di adiacenza 1/3\n• Dato un grafo diretto grappresentato tramite un array \ng.Adi liste di adiacenza doppiamente concatenate\n6. scrivi una procedura MATRICE (g) che ne restituisca la sua \nrappresentazione mediante una matrice di adiacenza\n7. scrivi una procedura GRADO_USCITA (g,u) che calcoli il \ngrado di uscita del nodo con indice u\n8. scrivi una procedura GRADO_INGRESSO (g,u) per il \ncalcolo del grado di ingresso del nodo con indice u\n9. scrivi una procedura GRADO_USCITA_MEDIO (g) per il \ncalcolo del grado di uscita medio dei nodi del grafo\n10. scrivi una procedura GRAFO_SEMPLICE (g) che verifica \nse il grafo è semplice (privo di cappi)\n• Discuti la complessità degli algoritmi che hai \nproposto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#2": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itContenuto\n• Definizione di grafi diretti e indiretti\n• Rappresentazione di grafi tramite:\n– matrici di adiacenza\n– liste di adiacenza\n• Esercizi su grafi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#20": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi su liste di adiacenza 2/3\n• Dato un grafo diretto grappresentato tramite un array g.Adi \nliste di adiacenza doppiamente concatenate\n11. scrivi lo pseudocodice della funzione VERIFICA_ARCO (g,u,v) che \nrestituisce true se esiste l’arco che va dal nodo identificato dall’indice \nual nodo indentificato dall’indice ve false altrimenti\n12. scrivi lo pseudocodice della funzione \nVERIFICA_NON_ORIENTATO (g) che  restituisce true se il grafo \npresenta un arco ( u,v) per ogni arco ( v,u) e false altrimenti\n• puoi utilizzare la funzione VERIFICA_ARCO (g,u,v)\n13. scrivi lo pseudocodice della funzione VERIFICA_POZZO (g,u) che \nrestituisce true se il nodo identificato dall’indice unon ha archi \nuscenti, false altrimenti\n14. scrivi lo pseudocodice della funzione VERIFICA_SORGENTE (g,u) \nche restituisce true se il nodo identificato dall’indice unon ha archi \nentranti, false altrimenti\n• Discuti la complessità degli algoritmi che hai proposto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#21": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itEsercizi su liste di adiacenza 3/3\n• Dati due grafi g1e g2rappresentati tramite array di liste di \nadiacenza doppiamente concatenate\n15. scrivi lo pseudocodice della funzione VERIFICA_UNIONE (g1,g2) \nche verifica che tra ogni possibile co ppia di nodi ci sia un arco in g1o \nin g2(o in entrambi)\n• puoi supporre che g1e g2abbiano lo stesso numero di nodi \n(g1.A.length =g2.A.length )\n16. scrivi lo pseudocodice della funzione \nVERIFICA_POZZI_E_SORGENTI (g1,g2) che restituisce true se \ntutti i pozzi di g1sono sorgenti di g2e tutte le sorgenti di g1sono \npozzi di g2e restituisce false altrimenti\n• puoi suppore che g1e g2abbiano lo stesso numero di nodi\n• puoi utilizzare le funzioni VERIFICA_POZZO (g,u) e \nVERIFICA_SORGENTE (g,u)\n• Discuti la complessità degli algoritmi che hai proposto",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#3": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itGrafi nelle applicazioni\n• Molti diagrammi utilizzati in ingegneria sono dei grafi\nimpianti industriali\ndata flow\ntopologie di rete\ncircuiti integrati\nschemi circuitali\ndiagrammi ER",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#4": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itGrafi diretti\n•U n  grafo orientato (o diretto ) G=(V,E) è costituito da \nun insieme di nodi Ve un insieme di archi E\n– ogni arco è una coppia ordinata di nodi ( u,v)\n• Denotiamo con nil numero dei nodi ( n= |V|) e con m\nil numero degli archi ( m = |E|)\n– si ha sempre m∈O(n2)\n•E s e m p i o :\nV= {0, 1, 2, 3, 4, 5}\nE= {(2,0) (1,2) (4,0) (4,4) \n(4,5) (5,4) (1,4)}0\n23\n4\n1\n5",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#5": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itGrafi diretti e relazioni\n• La versatilità dei grafi diretti deriva dal fatto \nche essi corrispondono a relazioni binarie\n• Per esempio\n– contatti tra utenti di una rete di telefonia\n– dipendenze tra invocazioni di metodi in un software– partecipazioni di aziende nel capitale di altre\n– rapporti di eredità\n– rapporti di precedenza tra attività– reti sociali\n–…",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#6": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itArchi uscenti ed entranti\n• Dato un nodo u\n– un suo arco uscente è un arco ( u,v) ∈E\n– un suo arco entrante è un arco ( v,u) ∈E\n– un nodo adiacente è un nodo vper cui esiste ( u,v) ∈E\n– il suo grado di uscita è il numero dei suoi archi uscenti\n– il suo grado di ingresso è il numero dei suoi archi entranti\n• Un nodo uèd e t t o …\n–sorgente se non ha archi entranti\n–pozzo se non ha archi uscenti0\n23\n4\n1\n5",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#7": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itCammini\n•U n  cammino (è sottinteso che sia diretto) è una \nsequenza di nodi u1, u2, …, uktali che per i=1,2, …, k-1 \nesistono gli archi ( ui,ui+1)\n• Il cammino è detto semplice se tutti i suoi nodi sono \ndistinti\n• Il numero di archi è la lunghezza del cammino\n0\n23\n4\n1\n5",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#8": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itCicli\n•U n  ciclo è un cammino (non semplice) in cui il primo e l’ultimo \nnodo coincidono\n• Un ciclo è detto semplice se il primo e l’ultimo nodo sono gli \nunici nodi che coincidono\n•U n  cappio (o loop) è un ciclo di un solo arco (e un solo nodo)\n•U n  grafo semplice è un grafo senza cappi\n• Un grafo diretto è aciclico se non ha cicli (diretti)\n0\n23\n4\n1\n5",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\210-grafi-matrici-e-liste-03.pdf#9": "210-grafi-matrici-e-liste--03 copyright ©2022 maurizio.patrignani@uniroma3.itGrafi non orientati\n• In un grafo non orientato (o non diretto ) \nG=(V,E) l’insieme degli archi E è un insieme di \ncoppie non ordinate ( u,v)\n–(u,v) e (v,u) rappresentano lo stesso arco\n• Graficamente si conviene di rappresentare una \nsola linea tra i nodi ue v\nV= {0, 1, 2, 3, 4, 5}\nE= {(0,2) (0,4) (1,2) (1,4) \n(4,4) (4,5)}0\n23\n4\n1\n5",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#0": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nVisita in ampiezza di un grafo\nm.patrignani\n",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#1": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica e testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non a fini\ndi lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente autorizzata\nper iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il contenuto delle\nslides, che sono comunque soggette a cambiamento\n• questa nota di copyright non deve essere mai rimossa e deve essere\nriportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#10": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itProcedura BFS (liste di adiacenza)\n14. x = x.next13. ENQUEUE (q,k)12. color[k] = 1 /* raggiunto e messo in coda */11. if (color[k] == 0)  /* se k non è stato già raggiunto */ 10. k = x.info /* k è l’indice d el nodo adiacente a u */9. while x != NULL /* finché c’è un nodo adiacente */8. x = g.A[u] /* mi preparo ad espor are gli adiacenti di u */7. u=DEQUEUE(q) /* estraggo un indice dalla coda */6. while not QUEUE-VOID (q) /* finché la coda q non è vuota */ 5. ENQUEUE (q,v) /* metto in coda l’indice v */ 4.color[v] = 1 /* uno = raggiunt o e messo in coda */3.q = QUEUE-EMPTY () /* creo una coda vuota */2. color[i] = 0 /* zero = non raggiunto */1. for i = 0 tog.A.length-1BFS(g,v) // g.A è un array di liste di adiacenza, v è un indice",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#11": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itComplessità della visita BFS\n• In una visita in ampiezza\n– ogni nodo è inserito ed estratto dalla coda una sola volta \n– ogni arco (adiacenza) è considerata sia dal nodo di \npartenza che dal nodo di arrivo\n• Dunque la complessità nel caso peggiore è Θ(n+m )",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#12": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itEsercizi sulle visite in ampiezza\n1. Scrivi lo pseudocodice della procedura BFS(g,v) nel \ncaso in cui il grafo non diretto gsia rappresentato \ntramite una matrice di adiacenza \n2. Scrivi lo pseudocodice della procedura BFS(g,v) nel \ncaso in cui il grafo non diretto gsia rappresentato \ntramite oggetti e riferimenti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#13": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itGrafi non orientati e connettività\n• Dato un grafo non orientato G=(V,E) \n– un nodo vèraggiungibile da un nodo use esiste un cammino da ua v\n– se per ogni coppia di nodi ue vdi Vesiste un cammino da ua vil grafo è detto \nconnesso\n– la proprietà raggiungibilità tra nodi di un grafo non orientato è una proprietà di \nequivalenza le cui classi di equivalenza sono chiamate componenti connesse\n0\n23\n4\n1\n58\n6\n9\n7",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#14": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itEsercizi sulle visite in ampiezza\n3. Scrivi lo pseudocodice della procedura \nIS_CONNECTED (g) che restituisce TRUE se il \ngrafo è connesso\n– possibile strategia: \n• eseguo una visita a partire da un nodo qualunque\n• se alcuni nodi rimangono non marcati il grafo non è connesso ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#15": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itVisite di un grafo non connesso\n• Per eseguire una BFS di un grafo non necessariamente \nconnesso è sufficiente lanciare diverse visite con lo stesso array color\n– per esempio nel caso di grafo rappresentato con liste/matrice di\nadiacenza il codice potrebbe essere il seguente\n5. BFS (g,i,color) /* …comincia una BFS da qui */ 4. if color[i] == 0 /* se i non ancora visitato… */3. for i = 0 tog.A.length-12. color[i] = 0 /* zero = non raggiunto */1. for i = 0 tog.A.length-1BFS_non_connesso( g)/* g è rappresentato tramite liste di adiacenza */",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#16": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itCalcolo delle componenti connesse\n4. Scrivi lo pseudocodice della procedura \nCOMPONENTI_CONNESSE (g) \n– input: un grafo non diretto g\n– output: il numero delle componenti connesse del grafo g\n– possibile strategia: \n• pongo il contatore delle componenti connesse a zero\n• finché c’è un nodo non visitato\n– incremento il contatore delle componenti connesse\n– eseguo una visita a partire dal nodo non visitato marcando tutti i nodi \nvisitati",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#17": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itEsercizi sulle visite in ampiezza\n5. Supponi di disporre di un’implementazione di una tabella hash dai \nnodi agli interi\n• new_table ()\n• ritorna una nuova tabella hash vuota\n• add_pair (h,n,i)\n• aggiunge alla tabella huna coppia formata da un nodo ned un intero i\n• get_value (h,n) \n• ritorna il valore associato al nodo n\nScrivi lo pseudocodice della procedura BFS_order (g,v) che \nrestituisce in output una tabella hash order dove \nget_value (order , n) è il numero d’ordine con cui il nodo nè\nstato visitato\n• nel caso in cui il grafo sia rappresentato  come una matrice o un array di liste di \nadiacenza al posto della tabella hash si potrebbe ritornare un semplice array",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#18": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itEsercizi sulle visite in ampiezza\n6. Scrivi lo pseudocodice della procedura DISTANZE (g,v) che \nrestituisce una tabella hash delle distanze di tutti i nodi dal \nnodo v\n– i nodi non raggiunti devono avere distanza -1\n– possibile strategia\n• eseguo un visita in ampiezza a partire da v\n• quando un nodo viene marcato, la sua distanza da vè pari alla distanza del \nnodo da cui è raggiunto più uno \n– nel caso in cui il grafo si rappresentato tramite un array di liste di \nadiacenza o una matrice di adiacenza la funzione potrebbe restituire un semplice array di interi",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#19": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itEsercizi sulle visite in ampiezza\n7. Scrivi lo pseudocodice della funzione \nCAMMINO_MINIMO (g,u,v) che prende in input un grafo g\ne gli identificatori di due nodi ue ve produce in output la \nlista dei nodi del cammino più corto da ua v\n– possibile strategia\n• eseguo una visita in ampiezza a partire da ve memorizzo per ogni nodo u\nil parent di u, cioè il nodo dal quale è stato raggiunto\n• la catena di parent che conduce da ua vè il cammino minimo cercato",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#2": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itContenuto\n• Visita in ampiezza di un grafo indiretto\n• Grafi e connettività\n• Esercizi sulle visite in ampiezza",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#3": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itAlgoritmi di visita di un grafo\n• Lo scopo di questi algoritmi è quello di visitare tutti i nodi \nraggiungibili a partire da un nodo di partenza\n• Caratteristiche:\n– i nodi non sono raggiunti in ordine casuale, ma in un ordine \ndeterminato dalla forma del grafo\n• diversi algoritmi su grafi sono modifiche di algoritmi di visita\n– non tutti i nodi vengono raggiunti\n• perché il grafo potrebbe avere più componenti connesse \n– alcuni nodi possono essere raggiunti da più nodi adiacenti\n• occorre marcare i nodi per non rischiare di ciclare ad infinito ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#4": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itUso dei marcatori\n• Gli algoritmi di visita dei grafi fanno tutti uso di \nmarcatori \n• Un marcatore è un valore associato ad ogni nodo di \nun grafo\n– per esempio un booleano (TRUE o FALSE) oppure un \nintero (generalmente 1 o 0)\n• Nel caso più generale si può associare ad un nodo un \ngenerico intero che viene spesso chiamato “colore”\n(color) del nodo",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#5": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itRealizzazioni di marcatori\n• Se un nodo è identificato da un intero è sufficiente affiancare \n(o aggiungere) alla struttura del grafo un array di interi con n \nposizioni, dove nè il numero dei nodi\n4.color[6] = 1 // coloro con 1 il nodo con indice 63.color[i] = 0 // inizializzo l’array con zero2. for i = 0 tocolor.length-11.// “color” è un array di interi con n posizioni\n• Per verificare se il nodo iè marcato eseguiremo:\n1. if( color[i] == 1 ) // nodo i marcato",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#6": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itRealizzazioni di marcatori\n• Se un nodo è identificato da un riferimento ad un oggetto è\nsufficiente aggiungere alla struttura del nodo un campo intero “color”\n4.x = x.next 3.x.info.color = 0 // inizializzo il colore con zero2.  while x != NULL // scorro la lista dei nodi1.x = g.nodi\n• Per verificare se il nodo nè marcato eseguiremo:\n1. if( n.color == 1 ) // nodo n marcato",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#7": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itVisita in ampiezza (Breadth-First Search)\n• A partire da un nodo vsi visitano i nodi raggiungibili \nda vnell’ordine imposto dalla loro distanza\n– prima i più vicini, poi i più lontani\n0\n23\n4\n1\n58\n69\n7",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#8": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itStrategia per una visita in ampiezza\n• Facciamo uso di una coda sulla quale è possibile eseguire le operazioni \nENQUEUE e DEQUEUE\n• La coda viene inizializzata inserendoci il nodo di partenza\n• I nodi raggiunti vengono marcati e messi in coda\n– per essere sicuri di non metterli in coda  due volte li marchiamo appena li mettiamo \nin coda\n• Finché la coda non è vuota \n– estraiamo un nodo dalla coda\n– consideriamo tutti i suoi adiacenti e se sono raggiunti per la prima volta (non sono \nmarcati) li marchiamo e li mettiamo in coda \n• L’ordine con cui i nodi sono estratti dalla coda corrisponde ad una visita \nin ampiezza\n– è lo stesso ordine con cui i nodi sono messi nella coda e marcati",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\230-visita-in-ampiezza-02.pdf#9": "230-visita-in-ampiezza-02 copyright ©2023 maurizio.patrignani@uniroma3.itEsempio di visita in ampiezza\n1,5,6,8 0,2,4,33,1,5,6 0,2,44,3,1 0,22,4,3 00coda esplorati\n0,2,4,3,1,5,6,8,7,99 0,2,4,3,1,5,6,8,77,9 0,2,4,3,1,5,6,88,7 0,2,4,3,1,5,66,8 0,2,4,3,1,55,6,8 0,2,4,3,10\n23\n4\n1\n58\n69\n7",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#0": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nEsercitazioni in linguaggio C\nCompilazione in linguaggio C\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#1": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#10": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCreazione del file eseguibile\n• Per generare un file eseguibile è necessario\n1. che la compilazione vada a boun fine e generi un \nfile oggetto per ogni singolo file compilato\n2. che il linkaggio vada a buon fine\n– ogni funzione che viene utilizzata in un file oggetto \ndeve essere contenuta in un altro file oggetto o in una \nlibreria specificata nella fase di linking ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#11": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itEsempio di errore di compilazione\nlocalhost:~$ cc file.c -o eseguimi\nfile.c: In function ‘main’:file.c:3:13: warning: incompatible implicit declaration of built-in function ‘sin’\nfloat b = sin(a);\n^\n/tmp/cctqIk3Y.o: In function `main':file.c:(.text+0x1e): undefined reference to `sin'collect2: error: ld returned 1 exit statuslocalhost:~$localhost:~$ cc file.c -o eseguimi\nfile.c: In function ‘main’:file.c:3:13: warning: incompatible implicit declaration of built-in function ‘sin’\nfloat b = sin(a);\n^\n/tmp/cctqIk3Y.o: In function `main':file.c:(.text+0x1e): undefined reference to `sin'collect2: error: ld returned 1 exit statuslocalhost:~$intmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#12": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itEsempio di errore di compilazione\n• Anche includendo il file math.h l’eseguibile \nnon viene generatolocalhost:~$ cc file.c -o eseguimi \n/tmp/cc6y9GHF.o: In function `main':file.c:(.text+0x1e): undefined reference to `sin'collect2: error: ld returned 1 exit statuslocalhost:~$ localhost:~$ cc file.c -o eseguimi \n/tmp/cc6y9GHF.o: In function `main':file.c:(.text+0x1e): undefined reference to `sin'collect2: error: ld returned 1 exit statuslocalhost:~$ #include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}\n#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#13": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itEsempio di errore di compilazione\n• La compilazione, in realtà, va a buon finelocalhost:~$ cc -c file.c \nlocalhost:~$ ls\nfile.c file.olocalhost:~$ lslocalhost:~$ cc -c file.c \nlocalhost:~$ ls\nfile.c file.olocalhost:~$ ls#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}\n#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#14": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itEsempio di errore di compilazione\n• L’errore è nella fase di linkaggiolocalhost:~$ cc file.o -o eseguimi \nfile.o: In function `main':file.c:(.text+0x1e): undefined reference to `sin'collect2: error: ld returned 1 exit statuslocalhost:~$localhost:~$ cc file.o -o eseguimi \nfile.o: In function `main':file.c:(.text+0x1e): undefined reference to `sin'collect2: error: ld returned 1 exit statuslocalhost:~$#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}\n#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#15": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCorrezione dell’errore\n• Il parametro -lmrichiede di linkare la libreria \n“m” (si tratta della libreria “ libm.a ”)localhost:~$ cc file.o -lm -o eseguimi \nlocalhost:~$ ls\neseguimi file.c file.olocalhost:~$localhost:~$ cc file.o -lm -o eseguimi \nlocalhost:~$ ls\neseguimi file.c file.olocalhost:~$#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}\n#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#16": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione e linkaggio corretti\nlocalhost:~$ ls\nfile.clocalhost:~$ cc file.c -lm -o eseguimi \nlocalhost:~$ ls\neseguimi file.clocalhost:~$localhost:~$ ls\nfile.clocalhost:~$ cc file.c -lm -o eseguimi \nlocalhost:~$ ls\neseguimi file.clocalhost:~$#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}\n#include <math.h> /* contiene sin */\nintmain(intargc, char** argv) {\nfloata = 0.5;\nfloatb = sin(a);\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#17": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione ad una sola passata\n• La sintassi del linguaggio C prevede che il codice \npossa essere compilato con una sola passata\n• tipi, variabili e funzioni, devono essere dichiarati \nprima di essere usati\n– attenzione: devono essere dichiarati , non necessariamente \ndefiniti\n– le funzioni che riportano un intero possono essere usate \nsenza essere dichiarate \n• il loro prototipo viene desunto dai parametri\n– molte volte ciò può essere garantito scegliendo un opportuno \nordine per le definizioni ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#18": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itDichiarazione e definizione\n• Dichiarazione di variabile o funzione\n– è un costrutto che annuncia al co mpilatore che la variabile o \nla funzione potrà essere utilizzata\n– non introduce codice nel file oggetto e può essere iterata\n• purché sia coerente con le precedenti\n• la compilazione di un file con sole dichiarazion i produce un file \noggetto vuoto (con la sola intestazione)\n• Definizione di variabile o funzione\n– è un costrutto che descrive in dettaglio come è composta la \nvariabile o la funzione che viene definita\n– una definizione è anche implicitamente una dichiarazione– introduce codice nel file oggetto e non può essere iterata",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#19": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itEsempi di dichiarazioni\n• Dichiarazione di una funzione\nanaloga a\n• Dichiarazione di una variabile\n– in questo caso la parola chiave extern è\nnecessaria, altrimenti il co mpilatore equivoca la \ndichiarazione per una definizionefloat funzione(float parametro);\nfloat funzione(float parametro);\nextern float funzione(float parametro);\nextern float funzione(float parametro);\nextern int variabile_globale;\nextern int variabile_globale;",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#2": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itRichiami di linguaggio C\n• Compilazione\n• Linking\n• Suddivisione del codice in più file\n• Dichiarazioni e definizioni• Compilazione secondo lo standard ANSI",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#20": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itDichiarazione e definizione di tipi\n• Dichiarazioni di tipi\n– sono costrutti che annunciano al compilatore ciò che potrà\nessere utilizzato\n– non introducono codice nel file oggetto e possono essere \niterate\n• purché siano coerenti con le precedenti\n• Definizioni di tipi\n– sono costrutti che descrivono in dettaglio come sono \ncomposti i tipi che vengono definiti\n– le definizioni di tipi non introducono codice nel file oggetto \ne possono essere iterate\n• purché siano coerenti con le precedenti",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#21": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itEsempio di ordine di definizione errato\nintmain(intargc, char** argv) {\nfloata = doppio(2.5);\n}floatdoppio(floatv) {\nreturnv * 2.0; \n}\nintmain(intargc, char** argv) {\nfloata = doppio( 2.5);\n}floatdoppio(floatv) {\nreturnv * 2.0; \n}\nlocalhost:~$ cc -c file.c\nfile.c:4:7: error: conflicting types for ‘doppio’\nfloat doppio(float v) {\n^\nfile.c:2:13: note: previous implicit declaration of ‘doppio’ was here\nfloat a = doppio(2.5);\n^\nlocalhost:~$localhost:~$ cc -c file.c\nfile.c:4:7: error: conflicting types for ‘doppio’\nfloat doppio(float v) {\n^\nfile.c:2:13: note: previous implicit declaration of ‘doppio’ was here\nfloat a = doppio(2.5);\n^\nlocalhost:~$funzione ancora \nnon dichiarata",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#22": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itOrdine di definizione corretto\nfloatdoppio(floatv) {\nreturnv * 2.0; \n}\nintmain(intargc, char** argv) {\nfloata = doppio(2.5);\n}\nfloatdoppio(floatv) {\nreturnv * 2.0; \n}\nintmain(intargc, char** argv) {\nfloata = doppio( 2.5);\n}\nlocalhost:~$ ls\nfile.clocalhost:~$ cc -c file.c\nlocalhost:~$ ls\nfile.c file.olocalhost:~$ localhost:~$ ls\nfile.clocalhost:~$ cc -c file.c\nlocalhost:~$ ls\nfile.c file.olocalhost:~$ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#23": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itAlternativamente: uso di una dichiarazione\nfloatdoppio(floatv);\nintmain(intargc, char** argv) {\nfloata = doppio(2.5);\n}floatdoppio(floatv) {\nreturnv * 2.0; \n}\nfloatdoppio(floatv);\nintmain(intargc, char** argv) {\nfloata = doppio( 2.5);\n}floatdoppio(floatv) {\nreturnv * 2.0; \n}dichiarazione\ndefinizione\nlocalhost:~$ ls\nfile.clocalhost:~$ cc -c file.c\nlocalhost:~$ ls\nfile.c file.olocalhost:~$ localhost:~$ ls\nfile.clocalhost:~$ cc -c file.c\nlocalhost:~$ ls\nfile.c file.olocalhost:~$ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#24": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itUso obbligatorio di una dichiarazione\n• Non esiste un ordine di definizione delle due \nfunzioni che metta al riparo da un errore di \ncompilazionefloatfunzione1( floatv) {\nif( v > 10 ) returnfunzione2(v)-1;\nreturnv; \n}\nfloatfunzione2( floatv) {\nif( v > 10 ) return0.5*funzione1(v);\nreturnv;\n}\nfloatfunzione1( floatv) {\nif( v > 10 ) returnfunzione2(v)- 1;\nreturnv; \n}\nfloatfunzione2( floatv) {\nif( v > 10 ) return0.5*funzione1(v);\nreturnv;\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#25": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itUso obbligatorio di una dichiarazione\nfloatfunzione2( floatv);\nfloatfunzione1( floatv) {\nif( v > 10 ) returnfunzione2(v)-1;\nreturnv; \n}\nfloatfunzione2( floatv) {\nif( v > 10 ) return0.5*funzione1(v);\nreturnv;\n}\nfloatfunzione2( floatv);\nfloatfunzione1( floatv) {\nif( v > 10 ) returnfunzione2(v)- 1;\nreturnv; \n}\nfloatfunzione2( floatv) {\nif( v > 10 ) return0.5*funzione1(v);\nreturnv;\n}dichiarazione",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#26": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itDichiarazione di un tipo\n• Poiché la definizione può essere iterata è più\nraro trovarsi nella necessità di dichiarare un tipo\n• In alcuni casi, però, ciò è indispensabile\ntypedef struct str1 struttura1;   \ntypedef struct str2 struttura2;   \ntypedef struct str1 {\nstruttura2* puntatore;\n} struttura1;\ntypedef struct str2 {\nstruttura1* puntatore;\n} struttura2;\ntypedef struct str1 struttura1;   \ntypedef struct str2 struttura2;   \ntypedef struct str1 {\nstruttura2* puntatore;\n} struttura1;\ntypedef struct str2 {\nstruttura1* puntatore;\n} struttura2;forward\ndeclarations",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#27": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itLocalizzazione delle dichiarazioni\n• Generalmente, tutte le dichiarazioni vengono messe in \nun file .h che può essere importato da altri file .c\n• Le variabili e le funzioni dichiarate nel file .h devono \nessere contenute in un file .o (quindi definite nel corrispondente file .c) perché la fase di linking vada a buon fine#ifndef _NOME_DEL_FILE\n#define _NOME_DEL_FILE\n/* dichiarazioni varie */#endif \n#ifndef _NOME_DEL_FILE\n#define _NOME_DEL_FILE\n/* dichiarazioni varie */#endif ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#28": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itDistribuizione del codice nei file\n• Ogni file .c contiene il codice relativo alle \ndichiarazioni del corrispondente .h \nfile1.cint \nmain(int \nargc, \nchar ** argv), \n{} \nint \nmain(int \nargc, \nchar ** argv), \n{} \nfile2.cint \nfunct2 \n(int a) \n{ bla bla bla \nbla bla} \nint \nfunct2 \n(int a) \n{ bla bla bla \nbla bla} \nfileN.cint \nfunctN \n(int a) \n{bla bla bla bla \nbla bla} \nint \nfunctN \n(int a) \n{bla bla bla bla \nbla bla} file1.hint \nmain(int \nargc, \nchar ** argv), \n{} \nint \nmain(int \nargc, \nchar ** argv), \n{} \nfile2.hint \nfunct2 \n(int a); \n……\n…\nint \nfunct2 \n(int a); \n……\n…\nfileN.hint \nfunctN \n(int a);\n……\n…\nint \nfunctN \n(int a);\n……\n…\nrelazioni #include",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#29": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itVariabili globali\n• Le variabili globali \n– sono dichiarate e definite in un solo file .c \n(tipicamente il file principale)\n– sono dichiarate come “extern” in tutti i file .c che le \nutilizzano\nfile1.cint status = 0;\nint main(int argc, \nchar ** argv){………\n} \nint status = 0;\nint main(int argc, \nchar ** argv){………\n} \nfileK.cextern int status; \nint functK (int a) \n{ ………\n} \nextern int status; \nint functK (int a) \n{ ………\n} ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#3": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione in linguaggio C\n• La compilazione traduce il codice sorgente nel \ncodice oggetto (cioè compilato)\n– usualmente il codice sorgente è in uno o più file con \nestensione .c\n– il codice oggetto viene messo in file con \nestensione .o\nfile.cint \nmain(int \nargc, \nchar ** \nargv), {} \nint \nmain(int \nargc, \nchar ** \nargv), {} \nfile.o0100 111 \n101010 0 \n10101 11 \n01010111 \n101 1001 10101000\n0100 111 \n101010 0 \n10101 11 \n01010111 \n101 1001 10101000compilazione",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#30": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione secondo lo standard ANSI\n• Lo standard ANSI C originale (X3.159-1989) \nfu ratificato nel 1989 e pubblicato nel 1990\n– per questo motivo è spesso chiamato C89 o C90\n• Se si vuole che il compilatore si attenga \nstrettamente allo standard ANSI originale \noccorre compilare con l’opzione -pedantic-\nerrors\nlocalhost:~$ cc file.c -pedantic-errors -o filelocalhost:~$ cc file.c -pedantic-errors -o file",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#31": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione secondo lo standard ANSI\nintmain(intargc, char** argv) {\ninta = 10;  // commento \nintb = a;\n}\nintmain(intargc, char** argv) {\ninta = 10;  // commento \nintb = a;\n}\nlocalhost:~$ cc file.c -pedantic-errors -o file\nfile.c: In function ‘main’:file.c:3:16: error: C++ style comments are not allowed in ISO C90\nint a = 10;  // commento\n^\nfile.c:3:16: error: (this will be reported only once per input file)localhost:~$localhost:~$ cc file.c -pedantic-errors -o file\nfile.c: In function ‘main’:file.c:3:16: error: C++ style comments are not allowed in ISO C90\nint a = 10;  // commento\n^\nfile.c:3:16: error: (this will be reported only once per input file)localhost:~$",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#32": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione secondo lo standard ANSI\nintmain(intargc, char** argv) {\ninta = 10, b = 5, c;\nc = a + b;charch;\n}\nintmain(intargc, char** argv) {\ninta = 10, b = 5, c;\nc = a + b;charch;\n}\nlocalhost:~$ cc file.c -pedantic-errors -o file\nfile.c: In function ‘main’:file.c:5:3: error: ISO C90 forbids mixed declarations and code [-Wpedantic]\nchar ch;^\nlocalhost:~$localhost:~$ cc file.c -pedantic-errors -o file\nfile.c: In function ‘main’:file.c:5:3: error: ISO C90 forbids mixed declarations and code [-Wpedantic]\nchar ch;^\nlocalhost:~$",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#33": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itEsercizi\n1. Scrivi una funzione in linguaggio C che \nverifichi se in un array di interi ci sia almeno \nun intero ripetuto due volte\n2. Scrivi una funzione in linguaggio C che \nverifichi se in un array di interi ci sia almeno \nun intero ripetuto trevolte",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#4": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione da linea di comando\n• Avviene chiamando un compilatore (cc, gcc, \necc) specificando che si vuole compilare un file \nsorgente\n– l’opzione -csignifica “voglio solo la compilazione”localhost:~$ ls\nfile.clocalhost:~$ cc -c file.c\nlocalhost:~$ ls\nfile.c file.olocalhost:~$ ls\nfile.clocalhost:~$ cc -c file.c\nlocalhost:~$ ls\nfile.c file.o",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#5": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itLinking in linguaggio C\n• La fase di linking trasforma il codice oggetto in \nun file eseguibile\n– vengono anche utilizzate (linkate, appunto) le \nlibrerie standard del linguaggio C\nfile.o a.out0100 111 \n101010 0 10101 11 \n01010111 \n101 1001 \n10101000\n0100 111 \n101010 0 10101 11 \n01010111 \n101 1001 \n10101000linking\n0100 111 \n101010 0 10101 11 \n01010111 \n101 1001 \n10101000\n0100 111 \n101010 0 10101 11 \n01010111 \n101 1001 \n10101000",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#6": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itLinking da linea di comando\n• Avviene chiamando un compilatore (cc, gcc, \necc) e fornendo come parametro il file oggetto\n• Se non viene specificato il nome del file di \noutput (-o eseguimi ) allora viene generato un \nfile eseguibile di nome a.outlocalhost:~$ ls\nfile.c file.olocalhost:~$ cc file.o -o eseguimi\nlocalhost:~$ ls\neseguimi file.c file.olocalhost:~$ ls\nfile.c file.olocalhost:~$ cc file.o -o eseguimi\nlocalhost:~$ ls\neseguimi file.c file.o",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#7": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itUn esempio di errore di linking\n• Il file linkato deve contenere la funzione main\n– in realtà il messaggio d’errore dice che la funzione \nstandard _start non ha trovato la funzione mainlocalhost:~$ cc file.o -o eseguimi\n/usr/lib/gcc/x86_64-linux-gnu/4.9/../../../x86_64-linux-gnu/crt1.o: In function `_start':\n(.text+0x20): undefined reference to \n`main'collect2: error: ld returned 1 exit statuslocalhost:~$localhost:~$ cc file.o -o eseguimi\n/usr/lib/gcc/x86_64-linux-gnu/4.9/../../../x86_64-linux-gnu/crt1.o: In function `_start':\n(.text+0x20): undefined reference to \n`main'collect2: error: ld returned 1 exit statuslocalhost:~$",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#8": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itCompilazione e linking\n• Si può compilare e linkare con un solo \ncomando\n• Il file prova.o non viene nemmeno conservato\n• In un Integrated Development Environment \n(IDE) la compilazione e il linking avvengono automaticamente al “build” del progettolocalhost:~$ ls\nprova.clocalhost:~$ cc prova.c -o prova\nlocalhost:~$ ls\nprova prova.clocalhost:~$ ls\nprova.clocalhost:~$ cc prova.c -o prova\nlocalhost:~$ ls\nprova prova.c",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c010-compilazione-in-c-03.pdf#9": "c010-compilazione-in-c-03         copyright ©2021 maurizio.patrignani@uniroma3.itint \nmain(int \nargc, \nchar ** argv), \n{} \nint \nmain(int \nargc, \nchar ** argv), \n{} Codice distribuito in file diversi\n• Quando il codice viene distribuito in diversi file \n…\n… la compilazione e il linking generano un \nsingolo file eseguibile\n– uno e uno solo dei file oggetto deve contenere la \nfunzione mainfile1.c\nint \nmain(int \nargc, \nchar ** \nargv), {} \nint \nmain(int \nargc, \nchar ** \nargv), {} \neseguimi0100 111 \n101010 0 \n10101 11 \n01010111 \n101 1001 10101000\n0100 111 \n101010 0 \n10101 11 \n01010111 \n101 1001 10101000compilazione\ne linking\nfile2.c",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#0": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itAlgoritmi e Strutture di Dati\nEsercitazioni in linguaggio C\nArray e puntatori\nm.patrignani",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#1": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itNota di copyright\n• queste slides sono protette dalle leggi sul copyright \n• il titolo ed il copyright relativi alle slides (inclusi, ma non \nlimitatamente, immagini, foto, animazioni, video, audio, musica \ne testo) sono di proprietà degli autori indicati sulla prima pagina\n• le slides possono essere riprodotte ed utilizzate liberamente, non \na fini di lucro, da università e scuole pubbliche e da istituti pubblici di ricerca\n• ogni altro uso o riproduzione è vietata, se non esplicitamente \nautorizzata per iscritto, a priori, da parte degli autori\n• gli autori non si assumono nessuna responsabilità per il \ncontenuto delle slides, che sono comunque soggette a \ncambiamento\n• questa nota di copyright non deve essere mai rimossa e deve \nessere riportata anche in casi di uso parziale",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#10": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itAlgebra degli operatori *e &\n• Regola mnemonica\n– ogniqualvolta si incontra l’operatore *lo si può \nsostituire con “il puntato da” oppure “il contenuto \ndi”\n– ogniqualvolta si incontra l’operatore &lo si può \nsostituire con “l’indirizzo di”\nint a;\nint* b;b = &a;    /* “l’indirizzo di a” */*b = 3;    /* “il puntato da b” */\nint a;int* b;b = &a;    /* “l’indirizzo di a” */*b = 3;    /* “il puntato da b” */",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#11": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itAlgebra degli operatori *e &\n• I due operatori possono essere combinati \ninsieme in modo spesso complesso\n• Osservazione\n–*x identifica un right value (cioè un valore)\n–&y identifica un left value (cioè una variabile)int a = 3;\nint* b = &a;int** c = &b;*b = *b+1;         /* ora a vale 4 */\n**c = **c+1        /* ora a vale 5 */ \nint a = 3;\nint* b = &a;int** c = &b;*b = *b+1;         /* ora a vale 4 */\n**c = **c+1        /* ora a vale 5 */ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#12": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itPassaggio di parametri\n• Tutti i passaggi di parametri in linguaggio C \nsono per valore\n– esempio\n…\nint a = 3;mia_funzione(a);  /* viene passato 3 */\n/* qui “a” vale sempre 3 */\n…\n…\nint a = 3;mia_funzione(a);  /* viene passato 3 */\n/* qui “a” vale sempre 3 */\n…\nvoid mia_funzione(int a){\na = a + 1;  /* non ha side effects */\n}\nvoid mia_funzione(int a){\na = a + 1;  /* non ha side effects */\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#13": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itPassaggio di parametri\n• Passaggio di parametri “per riferimento”\n– in realtà il passaggio è sempre per valore\n– esempio\n…\nint a = 3;mia_funzione(&a);   /* l’indirizzo */        \n/* qui “a” vale 4 */\n…\n…\nint a = 3;mia_funzione(&a);   /* l’indirizzo */        \n/* qui “a” vale 4 */\n…\nvoid mia_funzione(int* a){\n(*a) = (*a) + 1;  /* side effects */\n}\nvoid mia_funzione(int* a){\n(*a) = (*a) + 1;  /* side effects */\n}",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#14": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itGli array\n• Un array è una sequenza di variabili omogenee, tutte \nmemorizzate in celle contigue  di memoria e indicizzate \ncon un indice intero\n• In linguaggio C un array si dichiara premettendo il tipo \ndegli elementi e posponendo il numero degli elementi tra parentesi quadre\n– in ansi C il numero di elementi  deve essere una costante, non \npuò essere una variabile o un’espressione genericaint a[10];   /* a array di 10 interi */\nfloat b[5];  /* b array di 5 float */\nint a[10];   /* a array di 10 interi */float b[5];  /* b array di 5 float */\nint n=10;    /* numero di celle */\nint a[n];    /* errore! */\nint n=10;    /* numero di celle */int a[n];    /* errore! */",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#15": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itArray e allocazione della memoria\n• Cosa succede in memoria quando dichiariamo un \narray?\nint a[5]; /* a array di 5 interi */ \nint a[5]; /* a array di 5 interi */ \na\n10001001 a[0] 1001\na[1] 1002\na[2] 1003\na[3] 1004\na[4] 1005\n• Quante celle di memoria vengono allocate quando \ndichiaro un array di 5 interi?",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#16": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itArray e puntatori\n• Un array è a tutti gli effetti un puntatore\n– è l’indirizzo della prima cella di memoria dell’array\n• In linguaggio C l’identità tra array e puntatori è\nesplicita\n• Tuttavia un array è un puntatore costante \n– non è legittimo sovrascrivere o modificare \nl’indirizzo di memoria associato ad un arrayint a[5];     /* a è di tipo int* */\nint* b = a;   /* b ha lo stesso valore\ndi a */\nint a[5];     /* a è di tipo int* */int* b = a;   /* b ha lo stesso valore\ndi a */",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#17": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itIncremento di un puntatore\n• Incrementando un puntatore si salta una \nporzione di memoria pari alla dimensione \ndell’oggetto puntato\nint a[5];    /* a array di 5 interi */\nint* b = a;  /* b punta alla prima \ncella di a */\nb = b + 1;   /* b punta alla seconda\ncella di a */\nb++;         /* b punta alla terza \ncella di a */\nint a[5];    /* a array di 5 interi */int* b = a;  /* b punta alla prima \ncella di a */\nb = b + 1;   /* b punta alla seconda\ncella di a */\nb++;         /* b punta alla terza \ncella di a */",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#18": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itInterpretazione delle parentesi quadre\n• Date due espressioni xe y, il costrutto x[y] è\nequivalente all’operazione *(x+y)\n• Quindi queste espressioni sono equivalenti\na[0] = 100; \na[0] = 100; *(a+0) = 100; \n*(a+0) = 100; \na[1] = 200; \na[1] = 200; *(a+1) = 200; \n*(a+1) = 200; \n• Si può verificare l’equivalenza persino delle seguenti \nespressioni\n– infatti *(a+1) è uguale a *(1+a)a[1] = 200; \na[1] = 200; 1[a] = 200; \n1[a] = 200; ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#19": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itTempi di accesso agli array\n• Consideriamo il tempo di esecuzione di\n– è evidente che le due operazioni sulla destra si \npossono fare in tempo costante\n– il tempo di accesso è costante anche se si afferisce \nad una cella lontana dalla prima\n• Il tempo di accesso ad un array è sempre θ(1)a[0] = 100; \na[0] = 100; *(a+0) = 100; \n*(a+0) = 100; \na[9999] = 200; \na[9999] = 200; *(a+9999) = 200; \n*(a+9999) = 200; ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#2": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itRichiami di linguaggio C\n• Puntatori in linguaggio C\n• Gli operatori *e &\n• Algebra dei puntatori\n• Array• Allocazione statica e dinamica di array",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#20": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itAllocazione statica degli array\n• La dichiarazione di array che abbiamo visto \nalloca le celle dell’array direttamente sullo \nstack\n• Questo comporta due vincoli\n– il numero delle celle non può essere modificato\n– il valore di anon può essere modificato int a[5]; /* a è nello stack.\nAnche a[0],a[1],… ,a[4]\nsono nello stack */ \nint a[5]; /* a è nello stack.\nAnche a[0],a[1],… ,a[4]\nsono nello stack */ ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#21": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itAllocazione dinamica degli array\n• Nell’allocazione dinamica l’array viene posto \nnello heap tramite la funzione calloc\nint* a;      /* a è nello stack */\na = (int *)calloc(5,sizeof(int)); a[0] = 5;\nint* a;      /* a è nello stack */a = (int *)calloc(5,sizeof(int)); a[0] = 5;\na\n10002001a[0] 2001\na[1] 2002\na[2] 2003\na[3] 2004\na[4] 2005Stack Heap\n5",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#22": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itVantaggi della dichiarazione dinamica\n• La dichiarazione dinamica degli array comporta \ndiversi vantaggi\n– il numero di celle dell’array può essere specificato tramite \nuna variabile o un’espressione generica\n– l’indirizzo dell’array può essere modificato– il numero delle celle dell ’array può essere modificato\nint* a; \na = (int *)calloc(5,sizeof(int));a[0] = 100; \n…\na = realloc(a,10*sizeof(int));a[8] = a[0];          /* cioè 100 */\nint* a; a = (int *)calloc(5,sizeof(int));a[0] = 100; \n…\na = realloc(a,10*sizeof(int));a[8] = a[0];          /* cioè 100 */",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#23": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itEsercizi\n1. Implementa in linguaggio C uno stack di interi con le \nfunzioni NEW_STACK , IS_EMPTY , PUSH , e POP e \ncon la gestione telescopica della memoria\n• operazione PUSH con complessità ammortizzata O(1)\n2. Implementa in linguaggio C una coda di interi con le \nfunzioni NEW_QUEUE , IS_EMPTY , ENQUEUE , e \nDEQUEUE e con la gestione telescopica della \nmemoria\n• operazione ENQUEUE con complessità ammortizzata O(1)",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#3": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itPuntatori in linguaggio C\n• Molti linguaggi supportano i riferimenti\n– un riferimento è una variabile che viene utilizzata per \nidentificare un’altra variabile (o un oggetto)\n• Nel linguaggio C i riferimenti sono i puntatori\n– un puntatore è una variabile (cioè una cella di memoria con \nun tipo e un nome) che contiene l’indirizzo di un’altra cella di memoria\n• per esempio l’indirizzo di un’altra variabilea\n10002000b\n2000a b",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#4": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itL’operatore *\n• Nel linguaggio C il tipo di una variabile puntatore si \nottiene dal tipo della variabile puntata seguito dal \nsimbolo *(leggi “star”)\n•E s e m p i\n– un puntatore ad intero ha tipo int*\n• leggi “int star”\n• è indifferente la presenza di spazi o meno tra la stringa “ int”e  i l  \nsimbolo “*”\n– un puntatore a carattere ha tipo char*\n• leggi “char star”\n– un puntatore a puntatore ad intero ha tipo int**\n• leggi “int star star”",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#5": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itEsempi di dichiarazioni di puntatori\n• La variabile aè un puntatore ad intero\n• La variabile bè un puntatore a puntatore ad intero\n• La variabile cè un puntatore ad un float\n• La variabile dè un puntatore ad una strutturaint* a;\nint* a;\nint** b;\nint** b;\nstruct {\nint minimo;int massimo;\n}* d; \nstruct {\nint minimo;int massimo;\n}* d; float* c;\nfloat* c;",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#6": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itLa costante NULL\n• Il valore costante NULL è un valore \nconvenzionale che si suppone assegnato ad un \npuntatore che non contiene alcun indirizzo \nsignificativo\n– nella rappresentazione interna NULL corrisponde al \nvalore 0(zero), che non è legittimo per un indirizzo \ndi memoria a disposizione dell’utente",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#7": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itLa costante NULL\n• Quando viene dichiarato un puntatore senza \ninizializzarlo, si suppone che il suo valore sia \nNULL\n• Questo può essere anche esplicitato da \nun’assegnazioneint* a;  /* a è ancora uguale a NULL */\nint* a;  /* a è ancora uguale a NULL */\nint* a = NULL;  \nint* a = NULL;  ",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#8": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itDue diverse interpretazioni dell’operatore *\n• L’espressione “ int* a ”p u ò  e s s e r e  \ninterpretata in due modi equivalenti\n1. la variabile “ a”èd i  t i p o  “ int* ”\n2. “*a”, cioè “l’oggetto puntato da a” è di \ntipo “int”int* a\nint*a\nint*a\n• Infatti, l’operatore *anteposto ad un indirizzo di \nmemoria identifica il contenuto della memoria \nstessa\n–“*(a+b) ” è il contenuto della cella di memoria che si \ntrova in posizione a+b",
    "data_test\\rootfolder\\università\\AlgoritmiStruttureDati\\c020-array-e-puntatori-06.pdf#9": "c020-array-e-puntatori-06         copyright ©2021 maurizio.patrignani@uniroma3.itL’operatore &\n• L’operatore &(e commerciale) anteposto ad una \nvariabile ne estrae l’indirizzo\n•E s e m p i o\n– l’indirizzo della variabile intera “ a”è“&a”\n•“&a” è un’espressione di tipo “ int* ”\n• Non ha senso utilizzare l’operatore &con \nespressioni generiche\n–l a  s c r i t t u r a  “ &(a+b) ” non ha senso perché “ a+b”\nnon è una variabile, ma solo un valore\n• tecnicamente: non ha left-value ma solo right-value",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#0": "Prof. Riccardo TorloneUniversità di Roma TreCalcolatori ElettroniciParte I: Evoluzione dei calcolatori e tipologie di Calcolatori ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#1": "Riccardo Torlone -Corso di Calcolatori Elettronici2Architetture..\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#10": "Il Boom del MercatonFinora gli elaboratori sono limitati all’ambito scientifico, a quello militare e istituzionale (censimento)nDiventa ormai chiara l’occasione di mercatonNel 1950 Mauchleye Eckertescono dal progetto EDVAC (~1950 USA, successore dell’ENIAC, mai giunto a termine) e fondano la UNIVAC, la prima grossa società del settore\nRiccardo Torlone -Corso di Calcolatori Elettronici11\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#11": "La Macchina di Von Neumann\nIAS(~ 1950, Princeton USA)nProgramma memorizzatonAritmetica binarianMemoria: 4096 x 40 bitnFormato istruzioni a 20 bit:\nRiccardo Torlone -Corso di Calcolatori Elettronici12OPCODEINDIRIZZO128\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#12": "Riccardo Torlone -Corso di Calcolatori Elettronici13Sistemi CommercialiInizialmente il mercato è dominato dalla UNIVACL’IBM entra nel mercato nel 1953, e assume una posizione dominante che manterrà fino agli anni ‘80:nIBM 701 (1953):nMemoria: 2K word di 36 bitn2 istruzioni per wordnIBM 704 (1956):nMemoria: 4K word di 36 bitnIstruzioni a 36 bitnFloating-point hardwarenIBM 709 (1958)nPraticamente un 704 potenziatonUltima macchine IBM a valvole\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#13": "II  Generazione  (1955-1965)(Tecnologia a transistor)TXO e TX2 macchine sperimentali costruite al MITUno dei progettisti del TX2 fonda una propria società la Digital EquipmentCorporation (DEC)La DEC produce il PDP-1 (1961):nMemoria: 4Kword di 18 bitnTempo di ciclo di 5 µsecnPrestazioni simili all’IBM 7090nPrezzo meno di un decimonSchermo grafico 512 ´512 pixel nComincia la produzione di massa\nRiccardo Torlone -Corso di Calcolatori Elettronici14\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#14": "Il Minicomputer \nnDEC PDP-8 (1965) nSuccessore diretto del PDP-1nInterconnessione a bus, molto flessibilenArchitettura incentrata sull’I/OnPossibilità di connettere qualsiasi perifericanProdotto in oltre 50.000 esemplari\nRiccardo Torlone -Corso di Calcolatori Elettronici15\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#15": "Sistemi CommercialiMainframe: grossi calcolatori per applicazioni scientifiche, militari e PubblicaAmministrazioneIBM 7090nVersione transistorizzata del 709nMemoria 32Kword da 36 bitnTempo di ciclo 2 µsecnDomina il mercato fino agli anni ‘70nPochi esemplari, costano milioni di dollariPiccoli sistemi: per medie aziende o di appoggio ai mainframeIBM 1401nStessa capacità di I/O del 7090nMemoria 4K word 8bit (1byte)nOrientata a caratterinIstruzioni per la manipolazione di stringheRiccardo Torlone -Corso di Calcolatori Elettronici16\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#16": "III  Generazione (1965-1980)(Tecnologia LSI e VLSI) Evoluzione dell’architettura HWnMicroprogrammazionenUnità veloci floating-pointnProcessori ausiliari dedicati alla gestione dell’I/OEvoluzione dei Sistemi OperativinVirtualizzazione delle risorsenMultiprogrammazione: esecuzione concorrente di più programminMemoria Virtuale: rimuove le limitazioni dovute alle dimensioni della memoria fisica \nRiccardo Torlone -Corso di Calcolatori Elettronici18",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#17": "Serie IBM System/360\nnL’IBMintroduce una famigliadi elaboratori (passo decisivo)nSerie IBM System/360nMacchine con lo stesso linguaggio nRangedi prestazioni (e prezzo) 1-20nCompleta compatibilitànPortabilità totale delle applicazioninSistema Operativo comune OS/360Riccardo Torlone -Corso di Calcolatori Elettronici19\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#18": "Serie DEC PDP-11 e UNIXnEvoluzione diretta del PDP-8 nParole di memoria e istruzioni a 16 bitnArchitettura a bus (Unibus)nGrande flessibilità nella gestione e nell’interfacciamento di periferiche e strumentazione al busnDomina il mercato fino alla fine degli anni ’70nProdotto in milioni di esemplarinDiffusissimo nelle universitànSupporta il sistema operativo UNIX, indipendente dalla piattaformanInfluenzerà un’intera generazione di progettisti e di utentiRiccardo Torlone -Corso di Calcolatori Elettronici20\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#19": "Riccardo Torlone -Corso di Calcolatori Elettronici21IV Generazione: PCDiretto discendente del minicomputer:nArchitettura a busnParole e istruzioni a 16 bit Boom negli anni 80 con i PC prodotti da IBM Esplosione del mercato dei “cloni”Macintosh introduce le interfacce graficheOsborneintroduce i portatiliCrollo dei costi ed enorme espansione dell’utenzaDai grandi Centri di Elaborazione a un contesto di Informatica DistribuitaL’espansione del PC è trainata da tre fattori:nAumento della capacità della CPUnDiscesa dei costi della memoria principalenDiscesa dei costi delle memorie secondarie\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#2": "Riccardo Torlone -Corso di Calcolatori Elettronici3Come si arriva ad una architettura complessa?\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#20": "Evoluzioni modernen1986: PDA (Personal Digital Assistants)nOrganizer II -Psionn1994: SmartphonesnSimon –IBMn2002: TabletnMicrosoft Tablet PCn2000: Architetture multi-corenPOWER4 -IBM\nRiccardo Torlone -Corso di Calcolatori Elettronici22\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#21": "V Generazione: i computer invisibiliLa Apple introduce il primo computer palmare (PDA)Successivamente, si sono diffusi i computer embeddednElettrodomesticinAutomobilinCellularinOrologinBancomatn…Architetture non nuove ma diversa prospettivaModello del ubiquitous(o pervasive) computingo dell’Internet of ThingsRiccardo Torlone -Corso di Calcolatori Elettronici23\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#22": "La legge di Moore (1965)\nIl numero di transistor su di un chip raddoppia ogni 18 mesiCirca un aumento del 60% all’annoConseguenze:nAumento della capacità dei chip di memorianAumento della capacità delle CPU Riccardo Torlone -Corso di Calcolatori Elettronici24\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#23": "Legge di Moore per le CPU\nPiù transistor in una CPU significano:nEseguire direttamente istruzioni più complessenMaggiore memoria sul chip (cache)nMaggiore parallelismo interno Riccardo Torlone -Corso di Calcolatori Elettronici25\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#24": "Andamento corrente della legge di Moore\nRiccardo Torlone -Corso di Calcolatori Elettronici26\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#25": "Recentiavanzamentinellascaladi integrazione(2nm)\nRiccardo Torlone -Corso di Calcolatori Elettronici27\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#26": "Evoluzionedellatecnologiarispetto allaleggedi Moore\nRiccardo Torlone -Corso di Calcolatori Elettronici28",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#27": "Futuro della legge di Moore\nRiccardo Torlone-Corso di Calcolatori Elettronici29\nOppureicomputer “analitici”: https://www.youtube.com/watch?v=GVsUOuSjvcg",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#28": "Legge di NathanIl software è un gas: riempie sempre completamente qualsiasi contenitore in cui lo si mettaAl calare dei costi e all’aumentare della memoria disponibile, le dimensioni del software sono sempre cresciute in proporzioneIl Circolo VirtuosonSpinta tecnologica (Moore law)nCosti più bassi e prodotti migliorinNuove applicazioni software nNuovi mercati e maggiore competizionenEsigenza di migliori prestazioni hardware nSpinta tecnologica …...\nRiccardo Torlone -Corso di Calcolatori Elettronici30",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#29": "Riccardo Torlone -Corso di Calcolatori Elettronici31Tipologie di Computer\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#3": "Una architettura opera di molti artisti..nLeon Battista AlbertinBernardo RossellinonBramante nRaffaello SanzionAntonio da SangallonMichelangelo BuonarrotinVignolanPirro LigorionGiacomo Della Porta nDomenico Fontana nCarlo MadernonGian Lorenzo Bernini Riccardo Torlone -Corso di Calcolatori Elettronici4\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#30": "Riccardo Torlone -Corso di Calcolatori Elettronici32RFID (Radio FrequencyIDentification)Appartengono alla categoria usa-e-gettanSu chipnTipicamente senza batteria (passivi)n0.5 mm di diametronDotati di un piccolo transponder radionMemorizzano un numero di 128 bitnEsistono anche RFID attiviQuando ricevono un segnale radio trasmettono il proprio numeroVengono usati in molte applicazioninMagazzini e punti venditanTrasportinControllo presenze ed accessinIdentificazione degli animalinBiblioteche -movimento librinAntitaccheggionCarte di pagamento\nvideo 1video 2\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#31": "Riccardo Torlone -Corso di Calcolatori Elettronici33MicrocontrolloriPiccoli computer inclusi in vari dispositivi, tipicamente connessi in rete:nElettrodomesticinTelefoninAutomobilinPeriferichenGiochinMacchine fotografichenDispositivi medicin…Dotati dinUna CPUnUna piccola memorianQualche dispositivo di I/O\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#32": "Riccardo Torlone -Corso di Calcolatori Elettronici34Game computersComputer “normali”nEffetti grafici specialinSoftware di base limitatonNon estendibiliPlay Station 5nCPU: AMD a 8 core da 3.5 GHzn16GB di RAM (+VRAM)nGPU: AMD a 2.2 Ghz, > 10 TFLOPSXbox OneX/SnCPU: AMD a 8 core da 3.8 GHzn16GB di RAM (+VRAM)nGPU: AMD a 1,8 Ghz, > 10 TFLOPSSono sistemi specializzati e “chiusi”\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#33": "SmartphoneTelefoni cellulari dotati di CPU nCon sistema operativo (Android, iOS, Windows)nTelecameranFunzionalità estendibilinCPU relativamente potenti (ARM, 1.6 Ghz, octa-core)\nRiccardo Torlone -Corso di Calcolatori Elettronici35\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#34": "Tablet (Phablet, 2-in-1)Quasi dei computer \"normali\" con schermi ridotti (9.7inc.)nDotati di interfacce grafiche basate su touch-screennTastiere virtualinCPU potenti (>1.5Ghz, quad/octa-core)nProcessore graficonMemorie ridotte (512MB-2GB RAM, 128-256GB SSD)\nRiccardo Torlone -Corso di Calcolatori Elettronici36\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#35": "Riccardo Torlone -Corso di Calcolatori Elettronici37Tipologie di Computer \"tradizionali\"Personal ComputernSappiamo cosa è (desktop, laptop)Server -WorkstationnSu rete locale o Web servernMemorie fino a diversi GBnDiversi TBdi disconGestione di rete efficiente COW (Cluster of workstations)nSistema multiprocessore ad accoppiamento lasconHardware di tipo standard: costi contenutinStrutture di connessione velocinElevata affidabilità e capacità di elaborazione complessivanDetti COTS (Commodity Off The Shelf) o Server Farm\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#36": "Riccardo Torlone -Corso di Calcolatori Elettronici38Tipologie di Computer \"tradizionali\"MainframenDiretti discendenti della serie 360nGestione efficiente dell’I/OnPeriferie a dischi di molti TbytenCentinaia di terminali connessinCosti di parecchi milioni di EuroVersione moderna (in cluster)nServer farms+ client intelligentinData centersnOffrono soluzioni di \"cloudcomputing\"video",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#37": "Riccardo Torlone -Corso di Calcolatori Elettronici39La famiglia Intel\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#38": "Riccardo Torlone -Corso di Calcolatori Elettronici40CPU attualmente sul mercato13th Generation Intel Core: nome commerciale di una serie di microprocessori Intel (fascia desktop) di nuova generazione a 64 bit (x86-64) –Architettura: nomi come \"Coffee Lake\"nRaggruppa processori destinati a diversi settori di mercato nIntel Core i3nIntel Core i5nIntel Core i7nIntel Core i9nXeonnSono tutte architetture multi core ibride (p-core vs e-core)nEsistono versioni per portatilinTecnologia di integrazione fino a 7 nmnFinoa 25 MB cache L3 condivisenPiù di 8 miliardi di transistors!nFino a 24 stadi di pipeline\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#39": "Come produrre tanti modelli?nCon un’unica catena di produzione!! (o poche)nOgni CPU prodotta viene venduta come modello diverso sulla base della qualità di produzione\nRiccardo Torlone -Corso di Calcolatori Elettronici41\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#4": "Riccardo Torlone -Corso di Calcolatori Elettronici5Evoluzione degli Elaboratori (opera di molti artisti)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#40": "Intel Core i7 (x86)nArchitettura Sandy Bridgen8 core di cui 6 abilitati nVersione full: xeonnOltre 1,4 miliardi di transistor in un chipnNuove istruzione SSE per applicazioni multimedialinTecnologia di integrazione a 22nmnDue cache locali per ogni core (64KB-256KB)nCacheglobaleL3(2MB-8MB)n2.6–3.5GHz di frequenza di clocknDissipazione: <60 WattnHyper-ThereadingPipeline\nRiccardo Torlone -Corso di Calcolatori Elettronici42\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#41": "Architetture ARMnNasce negli anni 80 da un progetto AcornnBasato su principi RISC (AcornRISC Machine)nPrima versione su PC \"Archimedes\" (1985)nUsato nel progetto Newton di ApplenTarget: applicazioni embedded/mobile a basso consumo di energianArchitettura \"aperta\"ndiversi produttorinEsempio di uso: Nvidia Tegra2n2 CPU ARM Cortex-A9 a 1.2Ghzn1 GPU GeForce333-Mhzn1 CPU AM7 per la configurazionenL2 condivisa di 1MB Riccardo Torlone -Corso di Calcolatori Elettronici44\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#42": "Architettura AVRnTarget: sistemi embeddeda bassissimo consumo di energianNasce da un progetto del NIT nel 1996n(A)lfand (V)ergardRISC processornStesso pinoutdell’8051 IntelnPeriferiche disponibili nel AVR XMEGA:n3 timer –Orologio interno -Trasmettitori di impulso –Interfaccia per sensori -Convertitori analogico/digitali -Transponder -Comparatore di tensioni  \nRiccardo Torlone -Corso di Calcolatori Elettronici45\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#5": "Riccardo Torlone -Corso di Calcolatori Elettronici6Quasi tutta l’evoluzione ha avuto luogo negli ultimi 70 anni\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#6": "Generazione 0  (1600-1945)Pascal (1623-1662)naddizioni e sottrazioniLeibniz (1646-1716)nanche moltiplicazioni e divisioni\nRiccardo Torlone -Corso di Calcolatori Elettronici7\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#7": "La macchina di BabbageCharles Babbage(1792-1871)nA) Macchina DifferenzialenCalcolo funzioni polinomialinAlgoritmo fisso (differenze finite)nOutput su piastra di ramenB) Macchina AnaliticanPrima macchina programmabilenMemoria: 1000 x 50 cifre decimalinMulino (CPU)nI/O su schede perforatenLimite: tecnologia meccanicanPrimo programmatore: nAda LovelaceRiccardo Torlone -Corso di Calcolatori Elettronici8\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#8": "Generazione 0  (continua)(Macchine elettromeccaniche)Konrad Zuse(~1930 Germania)nMacchina a relènDistrutta nella guerraJohn Atanasoffe George Stibbitz(~1940 USA)nAritmetica binarianMemoria a condensatoriHoward Aiken (~1940 USA)nMARK 1: versione a relè della macchina di BabbagenMemoria: 72 x 23 cifre decimalintempo di ciclo: 6 sec.nI/O su nastro perforatoRiccardo Torlone -Corso di Calcolatori Elettronici9\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\02 Evoluzione dei calcolatori e tipologie di Calcolatori.pdf#9": "I  Generazione (1945-1955)(Tecnologia a valvole) COLOSSUS  (~1940 GB)nGruppo di Alan TuringnDecifrazione del codice EnigmanProgetto mantenuto segretoENIAC  (~1946 USA)nJ. Mauchley, J. Eckertn18.000 valvolen30 tonnellate di peson140KWconsumo energianProgrammabile tramite 6000 interruttori e pannelli cablatin20 registri da 10 cifreRiccardo Torlone -Corso di Calcolatori Elettronici10\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#0": "CalcolatoriElettroniciParte II:SistemidiNumerazioneBinariaProf. Riccardo TorloneUniversità di Roma Tre",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#1": "Riccardo Torlone -Corso di Calcolatori Elettronici2Unità di misura\nAttenzioneperò, se stiamo parlando di memoria:n1Byte = 8 bitn1K (KiB: KibiByte) = 210= 1.024                            ~ 103n1M (MeB: MebiByte) = 220= 210210=1.048.576          ~ 106n1G (GiB: GibiByte) = 230= 210210210=1.073.741.824~ 109n1T (TiB: TebiByte) = 240= ... =1.099.511.627.770~ 10121 Mb = 1 Mega bit = 106bit (misura di velocità)4 GB = 4 Giga bytes= 232bytes(misura di memoria)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#10": "Addizioni tra numeri naturalinLe addizioni fra numerali si effettuano cifra a cifra (come in decimale) portando il riporto alla cifra successiva\nRiccardo Torlone -Corso di Calcolatori Elettronici120 + 0 = 00 + 1 = 11 + 0 = 11 + 1 = 0 con il riporto di 1ES     3 + 2 = 50011+0010 =0101Se il numero di cifre non permette di rappresentare il risultato si ha un trabocco nella propagazione del riporto",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#11": "Moltiplicazioni fra numeri naturalinLa tabellina delle moltiplicazioni èmolto semplice:0    10  0    01  0    1nL’operazione fra numerali si effettua come in decimale: si incolonnano e si sommano i prodotti parziali scalandoli opportunamente:(11)101011x(5 )100101=101100001011 (55)10110111nNotare che ciascun prodotto parziale èpari a zero o al moltiplicandoRiccardo Torlone -Corso di Calcolatori Elettronici13",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#12": "Riccardo Torlone -Corso di Calcolatori Elettronici14Numeri in virgola fissa senza segnonNaturale estensione della rappresentazione dei numeri naturalinSi stabilisce il numero di bitnViene fissata la posizione della virgolanSi interpreta con il meccanismo posizionale di basenEsempio:n6 cifre di cui due decimalinNumerale: 1010.01nInterpretazione: 1·23+0·22+1·21+0·20+0·2-1+1·2-2=10.25",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#13": "Riccardo Torlone -Corso di Calcolatori Elettronici15Addizioni tra numeri positivi in virgola fissanSi opera come in decimaleES     3,5 + 2,75 = 6,250011.10+0010.11 =0110.01",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#14": "Riccardo Torlone -Corso di Calcolatori Elettronici16Moltiplicazioni tra numeri positivi in virgola fissanSi opera come in decimale, tenendo conto del numero di cifre frazionarie e riposizionando il punto:(2.75)1010.11x(1.25)1001.01=10 110 00010 11 (3.4375)1011.0111nNotare che:nmoltiplicareper 2nequivale a spostare il punto di n posti a destranmoltiplicareper 2-nequivale a spostare il punto di n posti a sinistra(2.75)10010.11x(2)1010=00 001 01 1(5.5)101 01.10",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#15": "Riccardo Torlone -Corso di Calcolatori Elettronici17Moltiplicazione per potenze di duenMoltiplicareper 2nequivale a spostare il punto di n posti a destra(3.75)100011.11  x22= (4)100100.00  =0000000000000000000001111(15)1001111.0000",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#16": "Riccardo Torlone -Corso di Calcolatori Elettronici18Moltiplicazione per potenze di duenMoltiplicareper 2-nequivale a spostare il punto di n posti a sinistra(3.75)1011.11  x2-2= (0.25)1000.01  =1111000000000000(0,9375)10000.1111",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#17": "Riccardo Torlone -Corso di Calcolatori Elettronici19Interi con segnonPer rappresentare gli interi relativi, a parità di cifre si dimezza l’intervallo  dei valori assolutinSi utilizzano varie rappresentazioniModulo e segnonun bit per il segno   0: +   1: –nn-1 bit per il modulonintervallo    [–2n–1+1, +2n–1–1]ESn=4 bit       intervallo  [–7,+7]5 = 0101 –5 = 1101NBnintervallo simmetricondoppia rappresentazione dello zero",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#18": "Complemento a 1nSi aggiunge uno 0 a sinistranI numeri positivi si rappresentano con il sistema posizionale nPer cambiare di segno si complementail numerale bit a bitnI numerali positivi iniziano per 0, i negativi per 1nCon n bit:   [–2n-1+1, +2n-1–1]ESn=4 bit     intervallo [–7, +7]5 = 0101            –5 =1010nComplementare = cambiare segnonDoppia rappresentazione dello 0\nRiccardo Torlone -Corso di Calcolatori Elettronici20",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#19": "Complemento a 2nI positivi hanno la stessa rappresentazione che in complemento a 1nI negativi si ottengono sommando 1 alla loro rappresentazione in complemento a 1nIntervallo con n bit:   [–2n–1, +2n–1–1]nRegola pratica per complementare (cambiare segno al numerale): nPartendo da destra si lasciano invariati tutti i bit fino al primo uno compreso, e poi si complementabit a bitESn=4  bit    intervallo [–8, +7]5 = 0101            –5 = 1011   nIntervallo più estesonUna sola rappresentazione dello 0nComplementare (a 2) = cambiare segnoRiccardo Torlone -Corso di Calcolatori Elettronici21",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#2": "Riccardo Torlone -Corso di Calcolatori Elettronici3Ordini di grandezzaLe potenze di 2:§20... 29= 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, ..§210= 1.024                            ~ 1031K§220= 210210=1.048.576          ~ 1061M§230= 210210210=1.073.741.824~ 1091G §240= ... =1.099.511.627.770~ 1012  1T§250= ... =1.125.899.906.842.624~ 10151PES    226=  26×220= 64 MIl numero n di bit di un indirizzo binario determina le dimensioni della memoria (disposizioni con ripetizione di 0/1 su n posizioni):CPUbit indirizzoMemoria808016 bit64 K8086 20 bit1 Mega8028624 bit16 Mega8048632 bit4 GigaPentium32 bit4 Giga",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#20": "Rappresentazioni in CP1 e CP2\nRiccardo Torlone -Corso di Calcolatori Elettronici22ES  rappresentare (-347)10in CP2§28= 256 < 347 < 512 = 29§intervallo con n bit:  [-2n-1,+2n-1-1]§pertanto   nmin=10§+347 in notazione a 10 bit:512256128643216842101     0     1    0     1     1    0    1    1§complementandoa 2:-51225612864321684211     0     1     0    1    0    0    1    0    1§Se il numero è positivo:a)determinare il numero di bit nb)rappresentare il numero con il sistema posizionale su n bit§Se il numero è negativo:a)determinare il numero di bit nb)rappresentare il numero positivo con il sistema posizionale su nbitc)complementare (a 1 o a 2) il numerale così ottenuto",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#21": "Interpretazionein CP1 e in CP2nDato un numerale:nEs. 00110 oppure10101nSi determinail segnonSe èpositivosiinterpretacon il sistemaposizionalen00110 →22+21= 6nSe ènegativosicomplementa(a 1 o a 2) e poi siinterpretacon il sistemaposizionale(aggiungendoil -)n10101 (CP2) →-(01011) →-(23+21+20) = -11\nRiccardo Torlone-Corso di Calcolatori Elettronici23",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#22": "Riccardo Torlone -Corso di Calcolatori Elettronici24Addizioni in complemento a duenIn CP2 somme e sottrazioni tra numerali sono gestite nello stesso modo, ma si deve ignorare il trabocco:4+0100+                            2=0010=60110nSe i due operandi hanno segno diverso il risultato è sempre corretto:4+0100+-1=1111=310011nSe i due operandi hanno lo stesso segno e il risultato segno diverso c’è errore6+0110+3=0011=91001( 9 non è compreso nell’intervallo )×",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#23": "Riccardo Torlone -Corso di Calcolatori Elettronici25Altre operazioni su numeri con segnonPer fare la differenza si complementail sottraendo e si somma:6−0110+                            2=                   0010 1110=40100nLe moltiplicazioni si fanno tra i valori assoluti e alla fine, se necessario, si complementa:(11)10x01011x(-5)1000101=010110000001011 0000000000000110111(-55) 10111001001",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#24": "Eccesso 2n-1nI numeri vengono rappresentati come somma fra il numero dato e una potenza di 2, detta eccessonCon n bit l’eccesso è tipicamente 2n–1nIntervallo come CP2:  [–2n–1,+2n–1–1]nI numerali positivi iniziano per 1, i negativi per 0nRegola pratica: nI numerali si ottengono da quelli in CP2 complementandoil bit più significativo ESn=4 bit: eccesso 8,  intervallo [-8,+7]–3       –3+8=5     :  0101+4      +4+8=12   :  1100nIntervallo asimmetriconRappresentazione unica dello 0Riccardo Torlone -Corso di Calcolatori Elettronici26",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#25": "Rappresentazioni in eccesso 2n-1nDato un numero m (positivo o negativo) determinare il numero minimo di cifre nminnecessarie nDeterminare l’eccesso corrispondentenSommare m all’eccesso e rappresentare il numero ottenutoESrappresentare (-347)10in eccesso 2n-1n28= 256 < 347 < 512 = 29nintervallo con n bit:  [–2n-1,+2n-1–1]npertanto   nmin=10, eccesso 29= 512n–347 + 512 = 165n165 = 128+32+4+1n(–347)10in eccesso 29è:512 25612864321684210     0    1   0   1    0    0   1   0   1  Riccardo Torlone -Corso di Calcolatori Elettronici27",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#26": "Interpretazionein eccessonDato un numerale:nEs. 00110 oppure10101 in ecc. a 16nIndipendentementedal segno siinterpretacon il sistemaposizionalee poi sitogliel’eccesson00110 →22+21-16 = -10n10101 →24+22+20-16 = 5nOppure:nSe èpositivosiinterpretacon il sistemaposizionaleignorandoil primo bitn10101 →22+20= 5nSe ènegativosiinverteil primo bit, sicomplementaa 2 e poi siinterpretacon il sistemaposizionale(aggiungendoil -)n00110 →10110 →01010 = -11Riccardo Torlone -Corso di Calcolatori Elettronici28",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#27": "Riccardo Torlone -Corso di Calcolatori Elettronici29Rappresentazioni a confrontoDecimaleM&SCP1CP2Ecc 8+ 70111011101111111 + 60110011001101110 + 50101010101011101+ 40100010001001100+ 30011001100111011+ 20010001000101010 + 10001000100011001+ 00000000000001000–010001111–––––––11001111011110111–21010110111100110–31011110011010101–41100101111000100–51101101010110011–61110100110100010–71111100010010001–8––––––10000000",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#28": "Notazione in base 16nPer i numerali esadecimali occorrono 16 cifre {0,1,…,9,A,B,C,D,E,F}nConversione esadecimale-binario:nSi fa corrispondere a ciascuna cifra esadecimale il gruppo di 4 bit che ne rappresenta il valorenConversione binario-esadecimale:nPartendo da destra si fa corrispondere a ciascun gruppo di 4 o meno cifre binarie la cifra esadecimale che ne rappresenta il valoreESF57A31111101010111101000110001nSi usano spesso stringhe esadecimali per rappresentare stringhe binariein forma compattaRiccardo Torlone -Corso di Calcolatori Elettronici30",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#29": "Riccardo Torlone -Corso di Calcolatori Elettronici31Numerali e numerinUn numerale è solo una stringa di cifrenUn numerale rappresenta un numero solo se si specifica un sistema di numerazionenLo stesso numerale rappresenta diversi numeri in diverse notazioniESla stringa   110100  rappresenta:nCentodiecimilacentoin base 10n(+52)10in binario naturalen(-11)10in complemento a 1n(-12)10in complemento a 2n(+20)10in eccesso 32nIn esadecimale un numero grandissimo",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#3": "Riccardo Torlone -Corso di Calcolatori Elettronici4Un sistema di riferimento impreciso..\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#30": "Riccardo Torlone -Corso di Calcolatori Elettronici32Notazione in virgola mobilenEstende l’intervallo di numeri rappresentati a parità di cifre, rispetto alla notazione in virgola fissanNumeri reali rappresentati tramite una coppia di numeri  <m,e>n = m ×benm : mantissa(normalizzata tra due potenze successive della base)bi-1 £| m | <bine : esponenteintero con segnonSia m che e hanno un numero finitodi cifre:nIntervalli limitatinErrori di arrotondamento",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#31": "Esempio in base 10nNumerali a 5 cifre   + .XXX + EEnMantissa: 3 cifre con segno     0.1 £|m| <1nEsponente: 2 cifre con segno -99 £e £+99\nnNotare che con lo stesso numero di cifre in notazione a virgola fissa  +XXX.YY   nL’intervallo scende [-999.99,+999.99]nMa si hanno 5 cifre significative invece di 3Riccardo Torlone -Corso di Calcolatori Elettronici330.999*10+9900.1*10-99-0.1*10-99Underflownegativo-0.999*10+99OverflowpositivoOverflownegativo-10+99Underflowpositivo-10-10010-10010+9910-105",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#32": "Standard IEEE 754 (1985)nFormato non proprietario cioè non dipendente dall’architetturanSemplice precisione a 32 bit: \nnDoppia precisione a 64 bit\nnNotazioni con mantissa normalizzata e nonAlcune configurazioni dell’esponente sono riservateRiccardo Torlone-Corso di Calcolatori Elettronici341espmantissa823+/-1espmantissa1152+/-",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#33": "Standard IEEE 754 a 32 bit: numeri normalizzati§Esponente: eccesso 127  [–127, +128]non si usano gli estremi, quindi:–126 £e £127§Mantissa: rappresentata solo la parte decimale con la notazione posizionale:1 £m<2§Intervallo numeri normalizzati   [ 2–126, ~2128]§Uso delle configurazioni riservate:§med etutti 0: rappresenta lo 0§mtutti 0 ed etutti 1: overflow§m¹0 ed etutti 1: NotA Number§m¹0 ed etutti 0: numero denormalizzatoRiccardo Torlone -Corso di Calcolatori Elettronici351espmantissa823+/-",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#34": "Riccardo Torlone -Corso di Calcolatori Elettronici36Standard IEEE normalizzati: estremi intervallonPiù grande normalizzato~2128:X1111111011111111111111111111111+/–2127(1.11...1)2»~2nPiù piccolo normalizzato2-126:X0000000100000000000000000000000+/–2–126(1.00...0)2= 1",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#35": "Standard IEEE 754 a 32 bit: numeri denormalizzati§Esponente§Uguale a 00000000§evale convenzionalmente 2-126 §Mantissa: §diversa da 0§0 < m < 1 §Intervallo di rappresentazione§[2-1262-23= 2-149 , ~2-126]\nRiccardo Torlone -Corso di Calcolatori Elettronici371espmantissa823+/-",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#36": "Riccardo Torlone -Corso di Calcolatori Elettronici38Standard IEEE denormalizzati: estremi intervallonPiù grande denormalizzato~2-126:X0000000011111111111111111111111+/–2-126(0.11...1)2»~1nPiù piccolo denormalizzato2-149:X0000000000000000000000000000001+/–2–126(0.00...1)2= 2 -23",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#37": "Riccardo Torlone -Corso di Calcolatori Elettronici39Addizioni in virgola mobilenPer addizione e sottrazione occorre scalare le mantisse per eguagliare gli esponentiES     nn1+ n2n1:01001100100010111011100101100111n2:01010101011001100111000111000100ne1= (26)10, e2= (43)10:occorre scalare n1di 17 postin'1:01010101000000000000000001000101 +n2:0101010101100110011100011100010001010101011001100111001000001001 nNotare che l’addendo più piccolo perde cifre significative",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#38": "Riccardo Torlone -Corso di Calcolatori Elettronici40Moltiplicazioni in virgola mobile§Si moltiplicano le mantisse e si sommano algebricamente gli esponenti§Se necessario si scala la mantissa per normalizzarla e si riaggiusta l’esponenteESn3 = n1x  n2n1:   0 10011001 10010111011100101100111n2:   1 10101010 10000000000000000000000§e1= (26 )10, e2= (43 )10§e1+ e2 = (69)10= 11000100§m1x m2 = 10.011000110010101110110101§si scala la mantissa di un posto §si aumenta di  1  l’esponenten3:    1 11000101  00110001100101011101101 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#39": "Riccardo Torlone -Corso di Calcolatori Elettronici41Errore assoluto e relativonRappresentando un numero reale nin una notazione floating-point si commette un errore di approssimazionenIn realtà viene rappresentato un numero razionale n´con un numero limitato di cifre significativenErrore assoluto:  eA= n–n´nErrore relativo:eR=eA/n = (n–n´)/nnSe la mantissa è normalizzata l’errorerelativo massimo ècostante su tutto l’intervallo rappresentato ed è pari ad un’unitàsull’ultima cifra rappresentaES     10cifre frazionarie  eR = 2 -10nNelle notazioni non normalizzate l’errore relativo massimo non è costante",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#4": "Numeri e numeralinNumero: entità astratta nNumerale: stringa di caratteri che rappresenta un numero in un dato sistema di numerazionenLo stesso numero è rappresentato da numerali diversi in sistemi di numerazione diversi n156 nel sistema decimale -CLVI in numeri romaninLo stesso numerale rappresenta numeri diversi in sistemi di numerazione diversin11 vale undici nel sistema decimale tre nel sistema binarionIl numero di caratteri del numerale determina l’intervallo di numeri rappresentabilininteri a 3 cifre con segno nel sistema decimale:[-999,+999] Riccardo Torlone -Corso di Calcolatori Elettronici5",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#40": "Riccardo Torlone -Corso di Calcolatori Elettronici42Codifica di caratteri: codice ASCII (Hex0-1F)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#41": "Riccardo Torlone -Corso di Calcolatori Elettronici43Codice ASCII (Hex 20-7F)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#42": "Codifica di caratteri: Codice UNICODEnCodice UNICODE a 16 bit, nuova proposta di standard:n65.536 code pointsnSemplifica la scrittura del softwaren336 code points: alfabeti latinin112 accenti e simboli diacriticinGreco, cirillico, ebraico, ecc.n21.000 ideogrammi cinesi……nUn consorzio assegna quello che restanUTF-8: codice a lunghezza variabile basato su Unicoden0ddddddd: ASCIInAltri prefissi che iniziano per 1: codifiche più lunghe\nRiccardo Torlone -Corso di Calcolatori Elettronici44",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#43": "Esempio 1: virgola mobilenRappresentazione binaria in virgola mobile a 16 bit:n1 bit per il segno (0=positivo)n8 bit per l'esponente, in eccesso 128n7 bit per la parte frazionaria della mantissa normalizzata tra 1 e 2nCalcolare gli estremi degli intervalli rappresentati, i numerali corrispondenti, e l’ordine di grandezza decimale assumendo che le configurazioni con tutti 0 e con tutti 1 siano riservate.nRappresentare in tale notazione:nil numero m rappresentato in compl. a 2 dai tre byte FF5AB9nil numero n rappresentato in compl. a 1 dai tre byte 13B472 nCalcolare l’errore relativo ed assoluto che si commette rappresentando i numero m ed n nella notazione data\nRiccardo Torlone -Corso di Calcolatori Elettronici45",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#44": "Esempio 2: virgola mobilenRappresentazione binaria in virgola mobile a 16 bit:n1 bit per il segno (0=positivo)n8 bit per l'esponente, in eccesso 128 (configurazioni con tutti 0 e con tutti 1 riservate)n7 bit per la parte frazionaria della mantissa normalizzata tra 1 e 2nDato il numero m rappresentato in tale notazionedai due byte C3A5, calcolare l’intero n che approssima m per difetto, e rappresentarlo in complemento a 2 con 16 bit.\nRiccardo Torlone -Corso di Calcolatori Elettronici46",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#45": "Esempio 3: virgola mobilenRappresentazione binaria in virgola mobile a 16 bit:n1 bit per il segno (0=positivo)ne bit per l'esponente, in eccesso 2e-1n15–e bit per la parte decimale della mantissa normalizzata tra 1 e 2nconfigurazioni dell’esponente con tutti 0 e con tutti 1 riservatenCalcolare il valore minimo emindi bit per l’esponente che consenta di rappresentare il numero n rappresentato in complementoa 2 dai tre byte FF5AB9nRappresentare nnel sistema trovato\nRiccardo Torlone -Corso di Calcolatori Elettronici47",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#46": "Esempio 4: virgola mobilenRappresentazione binaria in virgola mobile a 16 bit:n1 bit per il segno (0=positivo)n7 bit per l'esponente, in eccesso 64n8 bit per la parte decimale della mantissanormalizzata tra 1 e 2nconfigurazioni dell’esponente con tutti 0 e con tutti 1 riservatenDati m e n rappresentati in tale notazione dalle stringhe esadecimali FC53 e F8F2 nCalcolare la somma di m e n e fornire la stringa esadecimale che la rappresenta nella notazione suddettanIndicare l’eventuale errore assoluto che si commettenProvare anche con FC53 e 78F2nProvare anche con 7C53 e F8F2\nRiccardo Torlone -Corso di Calcolatori Elettronici48",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#47": "Esercizio 5: virgola mobileSi considerino i numeri m ed nche, nel sistema di rappresentazione in eccesso a 27, sono rappresentati rispettivamente dalle le stringhe esadecimali 63 e 93.A.Calcolare il valore di s= m + ne rappresentare snel sistema di rappresentazione in complemento a due su 12 bit.B.Individuare una rappresentazione in virgola mobile che consenta di rappresentare il suddetto numero scon il numero minimo possibile di bit ed indicare l’intervallo di rappresentazione della rappresentazione individuata tenendo conto del fatto che le configurazioni dell’esponente composte da tutti 0 e da tutti 1 sono riservate;C.Rappresentare, nella notazione in virgola mobile definita al punto B, i numeri decimali 0, -2 e 1,25 indicando gli eventuali errori di rappresentazione commessi;D.Individuare il numero e di bit dell’esponente e il numero m di bit della mantissa di una notazione in virgola mobile a 16 bit che sia in grado di rappresentare tutti i numeri rappresentabili nella definita al punto A e che abbia l’intervallo di rappresentazione più grande possibile.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#48": "Esercizio 6: virgola mobileSi consideri una rappresentazione binaria in virgola mobile a 12 bit, di cui (nell’ordine da sinistra a destra) 1 bit per il segno (0=positivo), e per l’esponente, che è rappresentato in eccesso a 2e−1, e i rimanenti bit per la parte frazionaria della mantissa m che è normalizzata tra 1 e 2.A.Calcolare il valore di e che consente di rappresentare, con la massima precisione possibile, numeri compresi in valore assoluto tra 1000 e 0,001;B.tenendo conto del fatto che le configurazioni dell’esponente composte da tutti 0 e da tutti 1 sono riservate, indicare il più piccolo e il più grande numero che è possibile rappresentare nella notazione in virgola mobile definita al punto A specificando i numerali corrispondenti;C.rappresentare nella notazione individuata al punto A il numero 512 e il numero -1, indicando gli eventuali errori assoluti che si commettono;D.dato il numero nrappresentato nella notazione definita al punto A dalla stringa esadecimale D85, rappresentarlo: (a) in complemento a 2 su 8 bit e (b) in eccesso 29su 10 bit.Riccardo Torlone -Corso di Calcolatori Elettronici50",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#49": "Esercizio 7: virgola mobileSi consideri una rappresentazione binaria in virgola mobile a 20 bit, di cui si usa: 1 per il segno (0=positivo), 7 per l'esponente, che è rappresentato in eccesso a 64, e 12 per la parte decimale della mantissa. Con valori dell'esponente diversi da 0000000 la mantissa è normalizzata tra 1 e 2 (1 ≤ man < 2). Con esponente pari a 0000000 si rappresentano invece numeri denormalizzati, con esponente uguale a -63 e mantissa compresa tra 0 e 1 (0 < man < 1). A.Calcolare l'ordine di grandezza decimale del più piccolo numero positivo normalizzato e del più grande numero positivo denormalizzato, rappresentabili nella notazione suddetta.B.Dato il numero n rappresentato in complemento a 2 dai tre byte FF323B, ricavare il numerale che approssima meglio nella notazione suddetta il numero m = n×2-85, esprimendolo come stringa esadecimale.C.Calcolare gli ordini di grandezza sia binari che decimali dell'errore assoluto che si commette rappresentando m nella notazione suddetta.Riccardo Torlone -Corso di Calcolatori Elettronici51",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#5": "Numeri a precisione finitanNumero finito di cifre nSi perdono alcune proprietà:nchiusura operatori ( + , -, ´)nproprietà associativa, distributiva,..nEsempio:n2 cifre decimali e segno [–99,+99]n78+36=114  (chiusura)n60+(50–40) ¹(60+50)–40 (associatività)nErrori di arrotondamenton\"Buchi\" nella rappresentazione dei realinEsempio:nnumerali decimali con due sole cifre frazionarie\nRiccardo Torlone -Corso di Calcolatori Elettronici60?0.010.02",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#6": "Meccanismo di base: sistema posizionaleCiascuna cifra rappresenta il coefficiente di una potenza della base L’esponente è dato dalla posizione della cifra\nSe la base è b occorrono b simboli:nb = 10  {0,1,…,9}nb =  2   {0,1}nb =  8   {0,1,…,7}nb = 16  {0,1,…,9,A,B,C,D,E,F}Riccardo Torlone -Corso di Calcolatori Elettronici7125.4210010-110-2101102am.... a1a0. a-1 a-2... a-kb = base 0 £ai £b-1ESN=aibii=-kmS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#7": "Riccardo Torlone -Corso di Calcolatori Elettronici8Esempio in base binaria (virgola fissa)\nNumero rappresentato in formato decimale:1·23+ 0·22+ 1·21+ 0·20+ 0·2-1+ 1·2-2 = 10.251010.01202-12-2212223",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#8": "Riccardo Torlone -Corso di Calcolatori Elettronici9Esempio in base ottale (virgola fissa)\nNumero rappresentato in formato decimale:2·83+ 1·82+ 0·81+ 7·80+ 4·8-1+ 5·8-2 = 1095.5781252107.45808-18-2818283",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\03 Sistemi di Numerazione Binaria.pdf#9": "Numeri naturalinRappresentando gli interi positivi in notazione binaria con nbit si copre l’intervallo [0 , 2n–1]nSi sfruttano tutte le 2ndisposizioniES      n=3    [0,7]00001001201030114100510161107111N.B.  Anche gli 0 non significativi devono essere rappresentatiRiccardo Torlone -Corso di Calcolatori Elettronici11",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#0": "Calcolatori ElettroniciParte III:L’organizzazione generale del calcolatoreProf. Riccardo TorloneUniversità Roma Tre",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#1": "Riccardo Torlone -Corso di Calcolatori Elettronici2Terminologia di basenCalcolatore elettronico: macchina fatta di dispositivi elettronici che può risolvere problemi eseguendo istruzioni forniteglinProgramma: sequenza di istruzioni in un linguaggionLinguaggio macchina: eseguibile direttamente da un calcolatore (binario)Con dispositivi elettronici si possono eseguire direttamente solo un numero limitato di istruzioni semplici (costi)I linguaggi macchina non sono adatti per le persone",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#10": "Vari Tipi di ParallelismoIl parallelismo è ormai l’unica strada per aumentare le prestazioni Limite di un’esecuzione sequenziale: velocità della luce (30 cm in 1 nsec)Due tipi di parallelismo:A)alivello di istruzioninDiverse istruzioni eseguite insiemenDiverse fasi della stessa istruzione eseguite insiemeB)alivello di processorinMolti processori lavorano insieme allo stesso problemanFattori di parallelismo molto elevatinDiversi tipi di interconnessione e di cooperazione (più o meno stretta)\nRiccardo Torlone -Corso di Calcolatori Elettronici13",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#11": "Riccardo Torlone -Corso di Calcolatori Elettronici14Parallelismo a livello di istruzioni: Pipelining\nnCiascuna istruzione è divisa in fasinL’esecuzione avviene in una pipelinea più stadinPiù istruzioni in esecuzione contemporaneanUna istruzione completata per ogni cicloN.B.Si guadagna un fattore pari al numero di stadi della pipeline\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#12": "Riccardo Torlone -Corso di Calcolatori Elettronici15Caratteristiche di una pipelineUna pipeline consente un compromesso tra:nLatenza: tempo per eseguire una istruzionenAmpiezza di banda: numero di istruzioni completate per unità di tempo misurata in MIPS (milioni di istruzioni al secondo) -oggi in GFLOPS o TFLOPS (109o 1012istruzioni in virgola mobile al secondo)Con:nVelocità di clock = Tnsec(periodo del segnale di clock)nNumero di stadi = nAbbiamo:nLatenza= nTnAmpiezza di banda= 1 istr. ogni T nsec, ovvero: 109/T istr. ogni sec., ovvero: 1000/TMIPS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#13": "Architetture SuperscalariArchitetture nelle quali si avviano più istruzioni (4-6) insiemeSi aumenta il parallelismo avendo più di una pipeline nel microprocessoreLe pipeline possono essere specializzate:nUna versione dell’i7 ha diverse pipeline a più stadinPuò eseguire fino a 6 micro-istruzioni in paralleloProblema: compatibilità dell’esecuzione parallelanIndipendenza tra le istruzioninCiascuna istruzione non deve utilizzare i risultati dell’altraRiccardo Torlone -Corso di Calcolatori Elettronici16\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#14": "Unità Funzionali Multiple \nnVariante: solo lo stadio più lento della pipeline (che condiziona la velocità) viene parallelizzatonLa CPU contiene al suo interno diverse unità funzionali indipendentinArchitettura adottata nei processori Intel CoreRiccardo Torlone -Corso di Calcolatori Elettronici17\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#15": "Parallelismo a livello di processorinMiglioramento delle prestazioni con parallelismo a livello di istruzioni: 5-10 voltenPer migliorare ancora: CPU multiplenApprocci:nData parallelism(SIMD)nProcessori matricialinProcessori vettorialinGPUnTask parallelism(MIMD)nMultiprocessorinMulticorenMulticomputer\nRiccardo Torlone -Corso di Calcolatori Elettronici18[  || (inti : 100) array[i]++;  ][   a++;  ||  b+c;   ]  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#16": "GPUnOperazioni comuni su pixel, vertici, archi, figurenEs.: Nvidia Fermi GPU (2009)n16 processori streamSIMDnOgni processore ha 32 corenFino a 512 operazioni per ciclo di clock\nRiccardo Torlone -Corso di Calcolatori Elettronici21\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#17": "Multiprocessori\nnLe CPUlavorano indipendentementenSharedmemory: il bus può divenire collo di bottiglianPrivate memory: contiene il codice e parte dei datinScambio dati tramite la sharedmemory\nRiccardo Torlone -Corso di Calcolatori Elettronici22\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#18": "ArchitetturemulticorenLa CPU è composta da più core, ovvero da più nuclei di processori fisici montati sullo stesso packagenOgni core:nè un processore indipendentenpuò essere dotato di cache autonomanArchitetture omogenee (core identici) o eterogenee (p-core vs e-core)nOgni corepuò essere multiscalarenAccoppiamento dei core:nstretto: sharedcachenlasco: private cachenNascono a partire dal 2003:nIBM: PowerPCnIntel: Pentium D, Core 2, Core I3-i5-i7 nAMD: Athlon, Opteron, Phenom, RyzenRiccardo Torlone-Corso di Calcolatori Elettronici23\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#19": "MulticomputernI singoli elementi sono normali Workstationo PCnComunicazionetramitescambiodi messaggi(shared nothing)\nRiccardo Torlone -Corso di Calcolatori Elettronici24\nMIMD (Multiple InstructionMultiple Data)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#2": "Riccardo Torlone -Corso di Calcolatori Elettronici5Struttura del computer\nnLa memoria contiene sia i dati che le istruzioninIl contenuto dei registri può essere scambiato con la memoria e l’I/OnLe istruzioni trasferiscono i dati e modificano il contenuto dei registrinRegistri particolari:nPC: indirizza la prossima istruzionenIR: contiene l’istruzione corrente\nPC:Program CounterIR:  Instruction Register",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#20": "Le varie forme di parallelismo\nRiccardo Torlone -Corso di Calcolatori Elettronici25\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#21": "La Memoria CentralenContiene sia i programmi che i datinMemorizzazione binaria (bit)nCella (o locazione): unità indirizzabilenbyte: 8 bit (minimo indirizzabile)nword: insieme di Kbyte (Kdipende dall’architettura)nIndirizzo (della cella): tramite il quale la CPU accede al dato nella cellanIndirizzi binari a m bit: spazio di indirizzamento  2mcelleESPentium IVnArchitettura a 32 bitnRegistri e ALU a 32 bit nWord di 4 byte 32 bitnIndirizzi a 32 bitnSpazio indirizzabile 232= 4 GB(64GB con opportuniaccorgimenti)Riccardo Torlone -Corso di Calcolatori Elettronici26",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#22": "Riccardo Torlone -Corso di Calcolatori Elettronici27Organizzazione della memoriaDiverse possibilitànEsempio con 96 bit totali:\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#23": "Riccardo Torlone -Corso di Calcolatori Elettronici28Dimensione locazioni di memoriaDiverse soluzioni possibili\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#24": "Riccardo Torlone -Corso di Calcolatori Elettronici30Codici a correzione di erroreTecniche per garantire maggiore affidabilità nella registrazione / trasmissione di informazioni binarieRecupero degli errori hardware tramite codifiche ridondantiCodifiche con n= m + rbitnnbit complessivi codificanmbit datinrcheckbit (ridondanti)Si utilizza solo un sottoinsieme (2m) delle codifiche (dette valide)ESCodicecon n=10, m=2, r=800000000000000011111111110000011111111112m= 4 codifiche valide (su 210)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#25": "Riccardo Torlone -Corso di Calcolatori Elettronici31Distanza di HammingDistanza di Hammingtra due codifiche:numero di bit diversi:  0101 e 1001 sono a distanza 2Distanza di Hammingdi un codice:h = distanza di Hammingminima tra due codifiche valide del codice\nnPer rilevareerrori su k bit occorre che sia: nalmeno h = k + 1 ovvero k ≤ h -1 nPer correggereerrori su k bit occorre che sia: nalmeno h = 2k + 1 ovvero k ≤ (h –1)/2 0000000000000001111111111000001111111111Distanza di Hamming del codice h=5ES",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#26": "Riccardo Torlone -Corso di Calcolatori Elettronici32Codici a correzione di errore (Esempio)ESCodice con n=10, m=2, r=80000000000000001111111111000001111111111Distanza di Hamming = 5h=5=k+1 ÞE’ possibile rilevareerrori quadrupli0000011111 →11110111111111011111 viene riconosciuto come erratoh=5=2k+1 ÞE’ possibile correggereerrori doppi0000011111 → 11000111111100011111 viene corretto in 0000011111",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#27": "Riccardo Torlone -Corso di Calcolatori Elettronici33Rilevazione di errore singolo (controllo di parità)nNel caso più semplice si vogliono solo rilevare errori singolinBasta aggiungere un solo check bit r=1, n=m+1nBit di parità: scelto in modo che il numero complessivo di 1 nella codifica sia sempre pari (o dispari)nQuesto codice ha distanza h=2nErrore rilevato da circuiti molto semplicinLe memorie segnalano parity errorquando un errore si manifestaES. 11011010 bit di parità:1 →110110101OK01100101 bit di parità:0 →011011010Error",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#28": "Correzione di errore singolonm data bit, rcheckbit, nbit totalin2mcodifiche validenncodifiche errate a distanza 1 da ciascuna delle validenOgni codifica valida ne richiede in tutto n+1ESLa codifica:0000Richiede le codifiche errate:1000010000100001Riccardo Torlone -Corso di Calcolatori Elettronici34",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#29": "Riccardo Torlone -Corso di Calcolatori Elettronici35Correzione di errore singolonSe ogni codifica valida ne richiede n+1 deve essere:(n+1) 2m£2ncioè(m+r+1) £2r\nN.B.Al crescere di m l’overheadscende\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#3": "Struttura della CPU\nnEsecuzione di operazioni aritmetiche e logiche sui dati contenuti nei registrinSpostamento di dati fra registri e fra registri e memorianCiclo elementare: due operandi sono inviati alla ALU e il risultato e messo in un registroRiccardo Torlone -Corso di Calcolatori Elettronici6\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#30": "Esercizio 1Riferendosi all'organizzazione generale di un calcolatore, indicare se le seguenti affermazioni sono vere o false.nNelle architetture RISC le istruzioni macchina vengono tradotte in microistruzioni che vengono poi eseguite dall'hardware.nLe tecnica del pipeline non è compatibile con una architettura superscalare.nUna architettura con indirizzi a 16 bit con indirizzamento al byte non può gestire una memoria più grande di 64KB.nIn processore con pipeline a 4 stadi e un clock con periodo di 2 nsecuna istruzione macchina richiede 2 nsecper essere eseguita.nUn processore con pipeline a 5 stadi e un clock con periodo di 5 nsecha un'ampiezza di banda di 200 MIPS.nL'ampiezza di banda (numero di istruzioni eseguite al secondo a regime) di un processore con pipeline non dipende dal numero di stadi della pipeline.nIn una architettura con pipeline sono necessari più cicli di clock per completare una istruzione macchina.nIn linea di principio, se si raddoppia la frequenza del clock si dimezza la latenza e si raddoppia l’ampiezza di banda.FALSO\nVEROVEROFALSOVEROFALSOVEROVERO",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#31": "Esercizio 2Con riferimento ai codici a rilevazione e correzione di errore indicare se le seguenti affermazioni sono vere o false.nLa distanza di Hammingtra una codifica e il suo complemento a uno e pari alla lunghezza della codifica.nCon distanza di Hammingh=3 è possibile correggere 2 errori.nIl numero di bit di controllo necessari per rilevare un errore singolo su un codice a 8 bit è minore rispetto al numero bit di controllo necessari per un codice a 16 bit.nLa distanza di Hammingnel codice composto solo dalle parole 1100, 0011 e 1111 è 4.nLa percentuale di bit di controllo rispetto alla lunghezza complessiva di un codice a correzione di errore singolo diminuisce all'aumentare della lunghezza del codice.nPer rilevare r errori è necessario che un codice abbia una distanza di Hammingpari a 2r+1.nUn bit di parità permette solo di rilevare errori singoli.nSe in una parola si commette un errore singolo ma si conosce la sua posizione, il bit di parità è sufficiente a correggerlo.VERO\nVEROVEROFALSOFALSOFALSOVEROFALSO",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#32": "Riccardo Torlone -Corso di Calcolatori Elettronici40Caching…\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#33": "Memorie Cache\nnLa memoria è sempre più lenta della CPU e tende a rallentarlanMemorie veloci sono disponibili ma solo per piccole dimensioninLa cache (da cacher) funziona alla velocità del processore, e quindi nasconde la “lentezza” della memorianContiene le ultime porzioni di memoria acceduta: se la CPU vuole leggere una di esse evita un accesso a memorianFunziona bene a causa della località degli accessiRiccardo Torlone -Corso di Calcolatori Elettronici41\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#34": "Riccardo Torlone -Corso di Calcolatori Elettronici42Cache Hit RatioSeuna parola viene letta kvolte di seguito, k –1volte sarà trovata in cachenCache hit ratio:H = (k –1) /knTempo medio di accesso a memoria:nm: tempo di accesso della memorianc:  tempo di accesso della cacheA = c + (1 –H)mLa memoria è organizzata in blocchiPer ogni cache missun intero blocco è spostato in cache",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#35": "Tipologie schede memoria\nnSIMM (Single InlineMemory Module)n72 piedini, 32 bit, 8-16 chip, 128 MBytenA coppie nel Pentium (bus dati 64 bit) nDIMM (Double InlineMemory Module)n120/240 piedini, 64 bit, 8 chip, 256 MBytenSO-DIMM (Small OutlineDIMM)nPer notebook di dimensioni più piccolenDDR, DDR2, DDR3, (M)DDR4, DDR5 (Double Data Rate): introducono un meccanismo di pipeline nella lettura/scrittura, fino a 288 pin.nAlcune hanno bit di parità altre noRiccardo Torlone -Corso di Calcolatori Elettronici43\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#36": "DDR a confronto\nRiccardo Torlone -Corso di Calcolatori Elettronici44\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#37": "Gerarchie di memoria\nScendendo nella gerarchia:nCresce il tempo di accessonAumenta lacapacitànDiminuisce ilcosto per bitSolo il livello più alto della gerarchia è a contatto con la CPUMigrazione dei dati fra livelli della gerarchiaRiccardo Torlone -Corso di Calcolatori Elettronici45≤10-9~2 ·10-9~ 10 ·10-9~ 10-6/10-3100 ·10-3Access time (sec)24219230240>240Capacity(byte)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#38": "Dischi magnetici\nnDimensione: <10cm, Densità: 25Gb/cmnRegistrazione seriale su tracceconcentrichen50.000 tracce/cm (larghe ~200nm)nDischi ad alta densità con bit registrati perpendicolarmentenTracce divise in settoricontenenti i dati, un preamboloe un ECC(Error-CorrectingCode) (la capacità formattatascende del 15%)nVelocità di rotazione costante (5.400-10.800 RPM)nVelocità di trasferimento di 150 MB/sec (1 settore in 3.5 µsec)nBurstrate: velocità da quando la testina è sopra il primo bitnSustainedrate: velocità di trasferimento in un certo intervalloRiccardo Torlone -Corso di Calcolatori Elettronici46\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#39": "Riccardo Torlone -Corso di Calcolatori Elettronici47Dischi magnetici (2)\nnCilindro: insieme di tracce sulla stessa verticalenTempo di seektseek: spostamento delle testine sul cilindro desiderato, dipende in parte dalla distanza (~5-10ms)nTempo di latencytlat: spostamento sul settore desiderato (~3-6ms)nTempo di accesso:tacc= tsee+ tlat\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#4": "Riccardo Torlone -Corso di Calcolatori Elettronici7Il ciclo Fetch-Decode-ExecuteL’esecuzione di ciascuna istruzione nella CPU richiede i seguenti passi:1.Carica l’istruzione da memoria in IR(Instruction Register) (Fetch)2.Incrementa PC(Program Counter)3.Decodifica l’istruzione (Decode)4.Se l’istruzione usa un dato in memoria calcolane l’indirizzo5.Carica l’operando in un registro6.Esegui l’istruzione (Execute)7. Torna al passo 1.Per l’esecuzione dell’istruzione successivaAccessi alla  memoria sono effettuati sempreal passo 1, e non sempreai passi 4 e 5",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#40": "Riccardo Torlone -Corso di Calcolatori Elettronici48Organizzazione dei dati su discoDensità di registrazione variabile con il raggio della traccia (~ 25 Gbit/cm)\nLa gestione è fatta da controllori di disco(CPU specializzate)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#41": "Riccardo Torlone -Corso di Calcolatori Elettronici49Un hard disk\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#42": "Riccardo Torlone -Corso di Calcolatori Elettronici50Dischi IDE e EIDEnIDE: standard nato con il PC XT IBMnLimite di 16 testine, 63 settori e 1024 cilindri: in tutto 504 MB, transfer rate: ~4MB/secnEIDE estende lo standard mediante lo schema LBA (LogicalBlock Addressing) che prevede 228settorinTotale di 228×29B = 128GBn2 controllori -4 dischi per controllore ntransfer rate più alta ~17MB/secnATA-3 (AT Attachment) a 33MB/secnATAPI-5 (ATA PAcketInterface) a 66MB/sec nATAPI-6 a 100MB/sec nLBA a 48 bit –Massimo: 248×29B=128PBnATAPI-8 e successivi: basato su SATA (Serial ATA)nconnettori a meno bit (da 80 a 7), tensioni più basse (0.5V), velocità maggiori (>500MB/sec)nSCSI: Controller e interfaccia più intelligente, Bus con connessione daisychain, versione moderna: Serial attachedSCSI (>10Gb/sec)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#43": "Riccardo Torlone -Corso di Calcolatori Elettronici52Dischi RAIDProblema: miglioramento lento delle prestazioni dei dischi (1970: tseek=50ms; 2018: tseek=5-10ms)Soluzione: RAID(RedundantArray of  InexpensiveDisks)nDividere i dati su più dischinParallelizzare l’accessonAumentare il data ratenIntrodurre una resistenza ai guasti Contrapposti a SLED (Single Large ExpensiveDisk)Data Striping: dati consecutivi nello stesso file vengono \"affettati\" e disposti  su dischi diversi, dai quali possono essere letti (e scritti) in parallelo",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#44": "Riccardo Torlone -Corso di Calcolatori Elettronici53RAID Level 0 e Level 1\nnSu n dischi si può guadagnare un fattore n sia in lettura che in scritturanLo MTBF (Mean Time Between Failures) peggioranNon c’è ridondanza: non è un vero RAID\n§Ciascun disco è duplicato: shadowing§Ottime prestazioni soprattutto in lettura: molte possibilità di bilanciare carico§Eccellente resistenza ai guasti§Supportato anche da vari SO (Es. Windows)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#45": "Riccardo Torlone -Corso di Calcolatori Elettronici54RAID Level 2\nnStripinga livello di word o di bytenEsempio: un nibble(mezzo byte) più 3 bit: codice di Hamming a 7 bitnRegistrazione ad 1 bit per ogni disconRotazione dei dischi sincronizzatanResiste a guasti semplicinGuadagna un fattore 4 in read e writenForte overhead(nell’esempio 75%)nHa senso con molti dischi:n32 bit+(6+1) parità Þ39 dischinOverhead del 19%nGuadagna un fattore 32 in read e write\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#46": "Riccardo Torlone -Corso di Calcolatori Elettronici55RAID Level 3\nnVersione semplificata di RAID 2nResiste a guasti semplici! Il bit di parità, sapendo quale drive è rotto, consente la correzionenOverheadabbastanza contenutoRAID 2e 3offrono un’eccellente data rate ma permettono di gestire solo una operazione su disco per volta perché ciascuna operazione coinvolge tutti i dischi\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#47": "Riccardo Torlone -Corso di Calcolatori Elettronici56RAID 4 e RAID 5\nnStripinga livello di blocco: drive non sincronizzatinRAID 4: la stripnell’ultimo disco contiene i bit di parita dell’insieme di bit omologhi di tutte le altre stripnResiste a guasti singoli (vedi RAID 3)nSe una sola stripè scritta occorre leggere tutte le altre per calcolare la parità nIl disco di parità è il collo di bottiglianRAID 5distribuisce le stripdi parità \n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#48": "Unità a stato solido (SSD)nBasata sul fenomeno \"Hot-carrierinjection\" dei transistornCelle di memoria flash a stato solidonMontate sopra un normale transistornApplicando una tensione al CG:nIl FG si carica (no alimentazione)nAumenta la tensione di commutazionenTest di commutazione a basso voltaggionTempi di trasferimento: >200MB/secnAdatto a dispositivi mobilinCosti più alti: ~1c/GB ®~1€/GBnMaggiore \"failurerate\": ~ 100.000 WritenWearleveling: distribuzione uniforme delle scritture sulle celle dell’unitànAumento di capacità con celle multilivellonVersione moderna: 3D XPointRiccardo Torlone -Corso di Calcolatori Elettronici57\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#49": "Riccardo Torlone -Corso di Calcolatori Elettronici64Dispositivi di I/O\nnI dispositivi di I/O sono connessi al bus tramite controllernI controller gestiscono autonomamente i trasferimenti da e per la memoria: DMA (Direct Memory Access)nPossono comunicare con la CPU tramite le interruzioninIl bus è condiviso da CPU e controller, e gli accessi sono regolati da un arbitro\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#5": "Esecuzione e InterpretazioneEsecuzione direttanLe istruzioni possono venire eseguite direttamente dai circuiti hardware nApproccio molto complesso: nRepertorio di istruzioni limitatonProgettazione dell’HW complessanEsecuzione molto efficienteInterpretazionenL’hardware può eseguire solo alcune operazioni elementari molto semplici dette microistruzioninCiascuna istruzione è scomposta in una successione di microistruzioni poi eseguite dall’hardwarenVantaggi:nRepertorio di istruzioni estesonHW più compattonFlessibilità di progettoRiccardo Torlone -Corso di Calcolatori Elettronici8",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#50": "Struttura fisica del PC\nnLa base della struttura è costituita dalla Scheda Madre (MotherBoard) nSulla scheda madre sono la CPU, il Chipset, il bus e vari connettori per la memoria e i dispositivi di I/OnIl bus è costituito da una serie di piste sul circuito stampatonSpesso sono presenti più bus, secondo diversi standard nLe schede di I/O vengono inserite nei connettoriRiccardo Torlone -Corso di Calcolatori Elettronici65\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#51": "Riccardo Torlone -Corso di Calcolatori Elettronici66Una schema madre\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#52": "Scheda madre “moderna”\nRiccardo Torlone -Corso di Calcolatori Elettronici67\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#53": "Esercizio 3Si consideri una CPU con pipeline a 6 stadi che lavora a una frequenza di 400 Mhze in cui ogni stadio viene eseguito in un ciclo di clock; indicare se le seguenti affermazioni sono vere o false.nA regime e in condizioni ideali la CPU completa un'istruzione ogni 2.5 nsec.nUna istruzione richiede 10 nsecper essere eseguita.nL'ampiezza di banda della CPU è di 500 MIPS.nLa latenza della CPU è di 15 nsec.nIn linea di principio, se la frequenza del clock aumenta a 800 Mhzsi raddoppia l'ampiezza di banda.nIn linea di principio, se la frequenza del clock scende a 200 Mhzsi raddoppia la latenza.nIl tempo di esecuzione di un programma di 3 istruzioni è di 20 nsec.nIn linea di principio, togliendo uno stadio si aumenta la latenza e si diminuisce l'ampiezza di banda.\nn.VERO\nFALSOVEROFALSOVEROFALSOVEROVERO",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#54": "Esercizio 4Si consideri un programma che confronta il contenuto di una variabile X con tutti gli elementi di un vettore di interi A. Il vettore è composto da 5 elementi di 4 byte memorizzati in locazioni contigue della memoria principale mentre X è memorizzato in un’altra zona della memoria principale. L’esecuzione del programma avviene su un microprocessore che dispone di una cache con tempo di accesso di 2 nsece di una memoria con tempo di accesso di 20 nsec. Si assuma che i trasferimenti tra memoria e cache avvengano per blocchi di 16B. nIndicare la percentuale di successo nell'accesso alla cache (cache hit ratio) per la variabile XnIndicare il tempo necessario per il primo accesso alla variabile X, espresso in nanosecondi. nIndicare il tempo medio di accesso alla variabile X, espresso in nanosecondi. nIndicare il cache hit ratio complessivo (percentuale globale di successo nell’accesso alla cache) e il tempo medio di accesso alla memoria del programma;nAssumendo che il confronto di due elementi sia eseguito dal microprocessore in 1 nsec, indicare il tempo complessivo necessario all’esecuzione del programma, espresso in nanosecondi. Riccardo Torlone -Corso di Calcolatori Elettronici86",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#55": "Esercizio 4Si consideri un programma che confronta il contenuto di una variabile X con tutti gli elementi di un vettore di interi A. Il vettore è composto da 5 elementi di 4 byte memorizzati in locazioni contigue della memoria principale mentre X è memorizzato in un’altra zona della memoria principale. L’esecuzione del programma avviene su un microprocessore che dispone di una cache con tempo di accesso di 2 nsece di una memoria con tempo di accesso di 20 nsec. Si assuma che i trasferimenti tra memoria e cache avvengano per blocchi di 16B. nIndicare la percentuale di successo nell'accesso alla cache (cache hit ratio) per la variabile XLa variabile X viene acceduta 5 volte, la prima volta si trova in memoria principale, le altre in cache:Cache hit ratio=4/5=0,8 ®80%nIndicare il tempo necessario per il primo accesso alla variabile X, espresso in nanosecondi. Tempo di accesso alla cache + tempo di accesso alla RAM = 22nsecnIndicare il tempo medio di accesso alla variabile X, espresso in nanosecondi. Tempo medio di accesso a X=2+(20×1/5)=6nsecnIndicare il cache hit ratio complessivo (percentuale globale di successo nell’accesso alla cache) e il tempo medio di accesso alla memoria del programma;Cache hit ratio complessivo=7/10=0,7 ®70%Tempo medio di accesso mem.=2+(20×3/10)=8 nsecnAssumendo che il confronto di due elementi sia eseguito dal microprocessore in 1 nsec, indicare il tempo complessivo necessario all’esecuzione del programma, espresso in nanosecondi. Per eseguire il programma sono necessari: 10 letture di cui 3 richiedono l’accesso a memoria principale e 10 a cache (la cache è comunque sempre acceduta). Inoltre, il calcolo richiede 5 confronti.Tempo compl.=3×20nsec+10×2nsec+5×1nsec=85nsecRiccardo Torlone -Corso di Calcolatori Elettronici92",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#56": "Esercizio 5Illustrare la composizione e funzionamento di un'unità RAID di 200 GB (spazio utilizzabile di memoria fisica) e con blocchi (strip) di 512 KB, con riferimento: (A)ad una configurazione di livello 1 con 4 dischi, (B)ad una configurazione di livello 2,(C)ad una configurazione di livello 4 con 5 dischi e (D)ad una configurazione di livello 5 con 3 dischi.Indicare in entrambi i casi la dimensione effettiva di memoria fisica necessaria per la realizzazione (in numero di byte).",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#57": "Soluzione esercizio 5(A)100G200Kstrip100G100G100GTotale400GB(B)50G400Gbit50G50G50GTotale350GB50G50G50G(C)100Kstrip50G50G50GTotale250GB50G50G(D)200KstripTotale300GB100G100G100G",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#6": "La MicroprogrammazioneL’HW può eseguire microistruzioni:nTrasferimenti tra registrinTrasferimenti da e per la memorianOperazioni della ALU su registriCiascuna istruzione viene scomposta in una sequenza di microistruzioniL’unità di controllo della CPU esegue un microprogramma per effettuare l’interpretazione delle istruzioni macchinaIl microprogramma è contenuto in una memoria ROM sul chip del processoreVantaggi:nDisegno strutturatonSemplice correggere errorinFacile aggiungere nuove istruzioniRiccardo Torlone -Corso di Calcolatori Elettronici9",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#7": "CISC e RISCArchitetture RISC(ReducedInstructionSet Computer):nEsecuzione direttanRepertorio ristretto (alcune decine)nIstruzioni prevalentemente su registrinUna istruzione per ciclo di macchina (del data path)Architetture CISC (ComplexInstructionSet Computer) nInterpretazione tramite microprogrammanRepertorio esteso (alcune centinaia)nIstruzioni anche su memorianMolti cicli di macchina per istruzioneEsempi:nPowerPC, SPARC, MIPS, ARM: RISCnVAX (DEC), Pentium II/III/IV/i7(Intel), AMD: CISCAll’inizio degli anni ’80 i progettisti di sistemi veloci riconsiderano l’approccio dell’esecuzione direttaRiccardo Torlone -Corso di Calcolatori Elettronici10",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#8": "Principi progettuali dei computer moderninFar eseguire le istruzioni macchina dall’hardwarenMassimizzare la velocità con la quale le istruzioni sono eseguite misurata in MIPS (Millionsof Instr. per Second) o XFLOPS (M/G/T floatingpointoper. per Second)nSemplificare la decodifica delle istruzioni: formati molto regolarinLimitare i riferimenti alla memoria (solo LOAD e STORE)nAmpliare il numero di registriN.B. Questi principi sono tipici della filosofia RISC ma anche le architetture CISC vi si adeguano, almeno in parteRiccardo Torlone -Corso di Calcolatori Elettronici11",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\04 Organizzazione di un Calcolatore.pdf#9": "Riccardo Torlone -Corso di Calcolatori Elettronici12Introduzione del parallelismo\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#0": "Calcolatori ElettroniciParte IV: I Circuiti Digitali e le MemorieProf. Riccardo TorloneUniversità di Roma Tre",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#1": "LogisimUno strumento freeware per il progetto e la simulazione di circuiti digitali.\nMolto utile per comprendere quello che vedremo a lezione.Verrà usato negli homework.Riccardo Torlone -Corso di Calcolatori Elettronici2\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#10": "Riccardo Torlone -Corso di Calcolatori Elettronici12Algebra Circuitale (Booleana)nRappresentazione algebrica di funzioni booleaneInsieme:   I= { 0,1}Operatori:  AND , ORComplementazione:  NOTNotazionenSe x e y sono due variabili booleane:nL’AND  di x e y si indica con  x · y (o xy)nL’OR  di x e y si indica con x + ynIl NOT  di x si indica con x",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#11": "Riccardo Torlone -Corso di Calcolatori Elettronici13Espressioni AlgebricheTeorema: ogni funzione booleana è algebrica, cioè rappresentabile con un’espressione dell’algebra§Prima Forma Canonicadi funzione a nvariabili:f = Sj=1..mPi=1..n   xij*§xij*vale  xioppure xi§f  è espressa come OR delle combinazioni per cui la funzione è vera (somma dimintermini)§in base al teorema, qualsiasi funzione booleana può essere espressa in questa forma",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#12": "Riccardo Torlone -Corso di Calcolatori Elettronici14Funzioni Booleane (Esempio)ESnTre variabili booleane A, B, CnFunzione di maggioranza  M: è vera solo se almeno due delle tre variabili sono vere\nABCABCABCABCM = ABC + ABC + ABC + ABC",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#13": "Riccardo Torlone -Corso di Calcolatori Elettronici15Circuiti Logici\nnPorte Logiche: circuiti elementari che realizzano gli operatori dell’algebraQualsiasi funzione booleana può essere calcolata con un circuito realizzato con sole porteAND,OReNOT\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#14": "Riccardo Torlone -Corso di Calcolatori Elettronici16Realizzazione di porte logiche con circuiti elettronici\nNOTNAND                     NOR\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#15": "Riccardo Torlone -Corso di Calcolatori Elettronici17Implementazione di Funzioni Booleane con Circuiti Logici\nM = ABC + ABC + ABC + ABC",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#16": "Riccardo Torlone -Corso di Calcolatori Elettronici18Proprietà dell’Algebra Booleana\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#17": "Riccardo Torlone -Corso di Calcolatori Elettronici19Completezza delle porte NANDe NORÈ possibile simulareAND, OR eNOT, e quindi realizzare qualsiasi circuito, usando soliNAND oppure soliNOR\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#18": "Riccardo Torlone -Corso di Calcolatori Elettronici20Realizzazione della maggioranza con solo NAND\n8",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#19": "Riccardo Torlone -Corso di Calcolatori Elettronici21Ottimizzazione di circuiti logici (esempio)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#2": "Riccardo Torlone -Corso di Calcolatori Elettronici3Astrazione di un calcolatoreMACCHINA VIRTUALE(compilazione o interpretazione)L1L0MACCHINA REALE(esecuzione diretta)Se L0 ed L1 sono troppo diversi il problema si decompone introducendo livelli intermedi",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#20": "Porte XOR\nnCalcola la funzione OR esclusivo: dà uscita 1(vero) quando uno solo degli ingressi (ma non entrambi) vale 1nFacilmente realizzabile con porte AND,ORe NAND\nRiccardo Torlone -Corso di Calcolatori Elettronici22\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#21": "Riccardo Torlone -Corso di Calcolatori Elettronici23Circuiti Integrati\nnMolte porte realizzate sulla stessa piastrina di silicio (chip)nContenitori (schede) da 14 a 68 piedininVari livelli di integrazione:nSSI(Small Scale)         1-10 portenMSI(Medium Scale)   10-100 portenLSI(Large Scale)       102-105portenVLSI(VeryLarge Sc.)  > 105portenTempi di commutazione: 0,1-10 nsec\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#22": "Esempipratici\nRiccardo Torlone -Corso di Calcolatori Elettronici24\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#23": "Circuiti integrati moderninDual InlinePackages(DIPs)nPin GridArrays (PGAs)nLand GridArrays (LGAs)\nRiccardo Torlone -Corso di Calcolatori Elettronici25\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#24": "Riccardo Torlone -Corso di Calcolatori Elettronici26Circuiti CombinatoriCircuiti  in cui l’uscita dipende solo dagli ingressi, e non dallo stato cioè dalla storia passataES……………………………………...MULTIPLEXER2ningressi controllatiningressi dicontrollouna uscita§Gli ingressi di controllo selezionano quale degli ingressi controllati viene mandato in uscita",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#25": "Riccardo Torlone -Corso di Calcolatori Elettronici27Multiplexer (circuito)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#26": "Realizzazione di funzioni booleane tramite multiplexer nCon un multiplexer ad n bit si può calcolare qualsiasi funzione di n variabilinGli ingressi controllati corrispondono ai mintermini nSi cablano a 0 o 1, a seconda che il mintermine compaia o meno nella forma canonica\nRiccardo Torlone -Corso di Calcolatori Elettronici28ES(Funzione di maggioranza)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#27": "Decodificatore\nnCircuito a n ingressi e 2nuscitenUna ed una sola delle 2nuscite assume valore vero in corrispondenza della configurazione di nbit in ingressoRiccardo Torlone -Corso di Calcolatori Elettronici29\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#28": "Comparatore\nnCompara i bit omologhi di due stringhenL'uscita vale 1 se e solo se Ai=Bi  \"inSe  Ai=Bi  allora  Ai XOR Bi = 0nIl NOR da uscita 1 solo quando tutti i suoi ingressi valgono 0Riccardo Torlone -Corso di Calcolatori Elettronici30\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#29": "Shifter\nnIl segnale C determina il verso dello shift(sinistra/destra)Riccardo Torlone -Corso di Calcolatori Elettronici31\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#3": "Architettura a livelli\nnAl livello icorrispondono una macchina virtuale Mied un linguaggio LinIl linguaggioLiètradotto nel linguaggio Li-1o interpretato da un programma che gira sulla macchina Mi-1Riccardo Torlone -Corso di Calcolatori Elettronici5\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#30": "Semiaddizionatore (Half Adder)\nnCircuito a 2 ingressi e 2 uscite: somma e riporto (carry)nNon può essere usato per la somma di numerali a più bit, dove occorre sommare anche il riporto della cifra precedenteRiccardo Torlone -Corso di Calcolatori Elettronici32\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#31": "Addizionatore Completo (Full Adder)nCircuito a 3 ingressi e 2 uscitenRiceve il riporto dalla cifra precedente\nRiccardo Torlone -Corso di Calcolatori Elettronici33\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#32": "Riccardo Torlone -Corso di Calcolatori Elettronici34ALU a 1 bit (bit slice)\nOperazioni00: AND01: OR10: NOT11: SUM",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#33": "ALU ad n bitnRealizzata connettendo nALU ad 1 bit (bit slices)nINC incrementa la somma di 1 (A+1, A+B+1)nProblema: propagazione dei riportinCiascuno stadio deve attendere il riporto dal precedentenTempo di addizione lineare con n\nRiccardo Torlone -Corso di Calcolatori Elettronici35\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#34": "Clock\nnTutti i cambiamenti di stato vengono sincronizzati da un segnale (clock)nDa un clock primario ne vengono ricavati per sfasatura, sottrazione ecc.nLe transizioni di stato del circuito possono avvenire:nIn corrispondenza dei livellinIn corrispondenza dei frontiRiccardo Torlone -Corso di Calcolatori Elettronici36\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#35": "Riccardo Torlone -Corso di Calcolatori Elettronici37Circuiti Sequenziali\nnLe uscite del circuito dipendono dagli ingressi e dalla storia passatanLa storia passata è riassunta nello stato che è codificato nelle variabili di stato booleane s1,…,srnLe variabili di stato sono memorizzate in elementi di memoria binarinCircuiti combinatori calcolano le uscite e il nuovo valore dello statos1…srSTATOi1ino1omUSCITEINGRESSIoi = fi(i1,...,in ,s1,…,sr)i=1,…,ms’i =gi(i1,...,in ,s1,…,sr)j=1,…,r",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#36": "Latch\nnDispositivo di memoria elementarenDue stati stabili Q=0 e Q=1nS(SET): forza  Qa  1nR(RESET): forza  Qa 0nCon S=R=0 il circuito mantiene lo statonIl circuito commuta sui livelli cioè quando So Rvalgono 1 nSed Rnon devono mai andare insieme ad 1Riccardo Torlone -Corso di Calcolatori Elettronici38\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#37": "Latch con Clock, Latch D\nnRed Svengono trasferiti sugli ingressi del latch solo quando il clock è ad 1nQuando il clock è a 0 vengono ignorati\nnIl latchD (Delay) quando il clock va ad 1 registra nello stato Qil valore dell’ingresso  DRiccardo Torlone -Corso di Calcolatori Elettronici39\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#38": "Riccardo Torlone -Corso di Calcolatori Elettronici40Flip-flop\nIl flip-flope’ una variante del latch checommuta sui fronti del clockGeneratore diimpulsiPossibile realizzazione di un flip-flop D:",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#39": "Riccardo Torlone -Corso di Calcolatori Elettronici41Latch e Flip-FlopI Latchcommutano sui livelli del clock ( a) alto, b) basso)I Flip-Flopcommutano sui fronti del clock:a)  Commuta sul fronte di salitab)  Commuta sul fronte di discesa\na)b)\na)b)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#4": "Riccardo Torlone -Corso di Calcolatori Elettronici6Perché la stratificazione?nM0è facilmente realizzabile in hardware, ma difficile da programmarenMnè facile da programmare ma impossibile da realizzabile in hardwarenImplementazione progressiva e modularenTrasparenza per l’utente e le applicazioninIl linguaggio Lnnon dipende dalla piattaforma (hardware) M0:nDiversi linguaggi disponibili sulla stessa piattaformanLo stesso linguaggio disponibile su diverse piattaforme",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#40": "Registri\nnI Flip-Flop sono gli elementi base di memorizzazione nel computernMolti Flip-Flop possono essere messi su un unico chipnOccorrono in genere da 6 a 10 transistor per ogni Flip-Flop Riccardo Torlone -Corso di Calcolatori Elettronici42\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#41": "Riccardo Torlone -Corso di Calcolatori Elettronici43Organizzazione della Memoria\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#42": "Dispositivi a 3 stati\nnIn base ad un segnale di controllo C si comporta:nC=1: come circuito chiuso nC=0: come circuito apertonTempo di commutazione: pochi nsecnConsente di usare gli stessi piedini sia per la lettura che per scritturanUsato anche per la connessione ai bus e a qualsiasi linea bidirezionaleRiccardo Torlone -Corso di Calcolatori Elettronici44\n10i\n0IiOUTIN",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#43": "Chip di Memoria\nnChip da n´m  bit complessivi (nparole da m bit)nm linee dati bidirezionalinlog2n  linee di indirizzonSegnali di controllo:nCS (Chip Select)nOE (Output Enable)nWE (Write Enable)nProblema: numero limitato di piedini del contenitoreRiccardo Torlone -Corso di Calcolatori Elettronici45……………….….….log2n linee indirizzom linee dati in/out CSOEWECHIP DI MEMORIAn ´m",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#44": "Riccardo Torlone -Corso di Calcolatori Elettronici46Matrice di selezione\nnSi risparmia nella complessità della logica di decodificanUn decoder n®2n richiede 2nporte ANDnRAS (RowAddressStrobe), CAS (ColumnAddressStrobe)ESn4M parole da 1 bit ®22 linee di indirizzo n1 decoder a 22 ®4M  porte ANDn2 decoder a 11 ®2×211= 4Kporte ANDDECODERn/2 ®2n/2DECODERn/2 ®2n/2n/2n/2n\n1 BitRASCAS2nparole di 1 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#45": "Riccardo Torlone -Corso di Calcolatori Elettronici47Segnali asseriti e negatiIn alcuni casi (a seconda delle scelte di progetto) un segnale provoca l’azione corrispondente quando la sua tensione è alta, in altri quando è bassaPer evitare confusione si parla di:nSegnale asserito: quando assume il valore che provoca l’azionenSegnale negato: altrimentiSi adotta la seguente notazione:nS: segnale che è asserito altonS: segnale che è asserito bassoUlteriore notazione (usata da Intel):nS: segnale che è asserito altonS#: segnale che è asserito basson(adatta al set di caratteri UNICODE)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#46": "Riccardo Torlone -Corso di Calcolatori Elettronici48Chip di Memoria (Esempi)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#47": "Tassonomia delle RAM e ROMnRAM  (Random Access Memory) nROM  (Read OnlyMemory)nSRAM (StaticRAM): a Flip-Flop, molto veloce (~5nsec)nDRAM (DynamicRAM): basata su capacità parassite; richiede refresh, alta densità, basso costo (~70 nsec)nFPM: selezione a matricenEDO: (Extended Data Output) lettura in pipeline, più bandanSDRAM (SynchronousDRAM)nSincrona, prestazioni migliorinDDR (Double Data Rate)nLettura/scrittura in pipelinenDDR2-3-4-5: 100Mhz/1.6Ghz, fino a 25.6 GBsnPROM (ProgrammableROM)nEPROM (ErasablePROM) raggi UVnEEPROM: cancellabile elettricamentenFlash Memory: tipo di EEPROM, ciclo 50 nsec, max100.000 riscrittureRiccardo Torlone -Corso di Calcolatori Elettronici53",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#48": "Riccardo Torlone -Corso di Calcolatori Elettronici54Tipi di RAM e di ROM e loro impieghi\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#49": "Field ProgrammableGate Array (FPGA)nConsentono di realizzare circuiti logici arbitrarinDue componenti replicatinLookUpTables(LUT): piccola memoria che si usa per implementare una qualunque funzione booleananConnessioni programmabili\nRiccardo Torlone -Corso di Calcolatori Elettronici55\nf1f2f3f40000000100100011",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#5": "Tipica struttura a livelli\nnIl livello 2 è il più basso al quale un utente può programmare la macchina (confine tra software e hardware)nNormalmente si programma a livello 5Riccardo Torlone -Corso di Calcolatori Elettronici7\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#50": "Riccardo Torlone -Corso di Calcolatori Elettronici56EsercizioSi vuole realizzare un circuito combinatorio che effettui un controllo di paritàsu tre linee digitali:nrealizzare il circuito mediante porte logiche;nindicare come bisogna trasformare il circuito ottenuto per ottenere un circuito equivalente contenente solo porte NAND;nrealizzare il circuito con un singolo multiplexer.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#51": "Riccardo Torlone -Corso di Calcolatori Elettronici57SoluzioneABCOUT00010010010001111000101111011110P = ABC + ABC + ABC + ABC",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#52": "Riccardo Torlone -Corso di Calcolatori Elettronici58Soluzione con porte logiche qualsiasi\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#53": "Riccardo Torlone -Corso di Calcolatori Elettronici59Soluzione con solo Porte NAND\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#54": "Riccardo Torlone -Corso di Calcolatori Elettronici60Soluzione con multiplexer\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#55": "Riccardo Torlone -Corso di Calcolatori Elettronici61EsercizioSi vuole realizzare un circuito combinatorio che ha in ingresso tre segnali A, B e C e che si comporta come segue: a)quando C=0 fa un test di uguaglianza ovvero restituisce 1 se A e B sono uguali e 0 altrimenti,  b)quando C=1 fa un test di disuguaglianza ovvero restituisce 1 se A e B sono diversi e 0 altrimenti.nrealizzare il circuito mediante porte logiche qualunque;nrealizzare il circuito con un multiplexer;nrealizzare il circuito utilizzando solo porte NAND e XOR.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#56": "EsercizioSi consideri il seguente circuito combinatorio:\nnDeterminare la tabella di verità corrispondente al circuito.nIndicare la funzione booleana in prima forma canonica corrispondente alla tabella di verità ottenutanSemplificare, se possibile, l'espressione booleana rappresentata dalla prima forma canonica ottenuta e disegnare il circuito corrispondenteRiccardo Torlone -Corso di Calcolatori Elettronici62\nDEFG",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#57": "Riccardo Torlone -Corso di Calcolatori Elettronici63EsercizioFornire lo schema di uno shiftera 4 ingressi e 4 uscite che, sulla base di un segnale di controllo C: a)sposta l'ingresso di un bit a sinistra riempiendo il bit meno significativo con 0 se C=0 e b)sposta l'ingresso di un bit a destra riempiendo il bit piusignificativo con 1 se C=1. Illustrare sinteticamente il funzionamento del circuito.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#58": "Riccardo Torlone -Corso di Calcolatori Elettronici64Possibile soluzione\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#59": "Riccardo Torlone -Corso di Calcolatori Elettronici65EsercizioFornire lo schema di uno shiftera 2 ingressi e 2 uscite che, sulla base di un segnale di controllo C: a)sposta l'ingresso di un bit a sinistra riempiendo il bit meno significativo con 0 se C=0 e b)sposta l'ingresso di un bit a destra riempiendo il bit più significativo con il bit più significativo dell’ingresso se C=1. Illustrare sinteticamente il funzionamento del circuito.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#6": "Riccardo Torlone -Corso di Calcolatori Elettronici8Semplici elementi alla base di sistemi complessi …\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#60": "Riccardo Torlone -Corso di Calcolatori Elettronici66EsercizioFornire lo schema di un sottrattore a 4 bit per notazione in complemento a 2 realizzato con sommatori completi.nIllustrarne concisamente il funzionamento,nspecificare il valore di uscita di ciascuna componente quando in un ingresso (minuendo) c‘è il numerale 0011 e nell'altro (sottraendo) il numerale 0100.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#61": "Riccardo Torlone -Corso di Calcolatori Elettronici67Una possibile soluzione\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#62": "EsercizioFornire lo schema di un circuito combinatorio a 4 bit in grado di calcolare il valore assoluto di un numero, secondo il sistema di rappresentazione in complemento a due. Tale circuito, ricevuto in ingresso un numerale X a 4 bit, deve restituire in uscita: nlo stesso numero in ingresso, se X rappresenta un numero positivo, e nil numero in ingresso con il segno invertito, se X rappresenta un numero negativo (per esempio, se X = 3 allora l’uscita vale 3, se X = −1 allora l’uscita vale 1)E’ possibile utilizzare componenti di base quali half-addere full-adder. Illustrare concisamente il funzionamento del circuito e specificare il valore di uscita di ciascuna componente quando l’ingresso sitrovaa 1011.\nRiccardo Torlone -Corso di Calcolatori Elettronici68",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#63": "Possibile soluzione\nRiccardo Torlone -Corso di Calcolatori Elettronici69\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#64": "EsercizioSi vuole progettare una piccola ALU avente due operandi in ingresso da 4 bit (A e B). Tale ALU deve essere in grado di svolgere, in base al valore di due segnali di controllo, le seguenti operazioni:nla trasmissione di A inalterato (segnali di controllo: 00), nl’inversione bit a bit di B (segnali di controllo: 01)nla somma di A e B (segnali di controllo: 10) e nla differenza di A e B (segnali di controllo: 11)Definire lo schema di una ALU di questo tipo e illustrare sinteticamente il suo funzionamento. E’ possibile utilizzare componenti predefiniti quali decodificatori e full adder.\nRiccardo Torlone -Corso di Calcolatori Elettronici72",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#65": "Possibile soluzione a 1 bit\nRiccardo Torlone -Corso di Calcolatori Elettronici73\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#66": "ALU a 4 bit ottenuta componendo quella a 1 bit\nRiccardo Torlone -Corso di Calcolatori Elettronici74\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#67": "Riccardo Torlone -Corso di Calcolatori Elettronici75EsercizioFornire lo schema e illustrare sinteticamente il funzionamento di una piccola memoria di 4 locazioni da 1 bit ciascuna, realizzata con flip-flop e porte logiche. Tale memoria deve avere: n2 linee per la selezione della locazione, n1 linea condivisa per gli ingressi e le uscite, nuna linea di chip select,nuna linea per indicare se si vuole compiere una operazione di lettura o scrittura. Indicare poi come è possibile utilizzare la memoria concepita per costruire una memoria di 8 locazioni da 4 bit.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#68": "Riccardo Torlone -Corso di Calcolatori Elettronici76Una possibile soluzione\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#69": "EsercizioFornire lo schema di un circuito sequenziale che realizza un registro a 4 bit complementabile, utilizzando half-adder, full-addere flip-flop (come scatole chiuse). Tale circuito deve avere un segnale di set (S), un segnale di controllo (C), 4 linee di ingresso (X3X2X1X0) e 4 linee di uscita (Y3Y2Y1Y0). nQuando S=1 e C=0 il registro memorizza i segnali presenti sugli ingressi. nQuando C=1 e S=0 gli ingressi vengono ignorati e il contenuto del registro viene complementatoa due (es. da 0101 si passa a 1011). nIn ogni istante il contenuto del registro può essere letto sulle uscite. Illustrare concisamente il funzionamento e specificare il valore di uscita di ciascuna componente del circuito quando C=1, S=0 e il registro memorizza 0111.\nRiccardo Torlone -Corso di Calcolatori Elettronici79",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#7": "Circuiti DigitalinCircuiti elettronici i cui ingressi e le cui uscite assumono solo due livellinAl circuito sono associate le funzioni che calcolano le uscite a partire dagli ingressio1= f1( i1,….,in)...om= fm( i1,….,in)\nRiccardo Torlone -Corso di Calcolatori Elettronici9CIRCUITODIGITALEi1ino1omUSCITEINGRESSI",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#70": "Possibile soluzione\nRiccardo Torlone -Corso di Calcolatori Elettronici80\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#71": "Altra soluzione che compone un modulo da 1 bit\nRiccardo Torlone -Corso di Calcolatori Elettronici81\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#8": "Funzioni Logiche (Booleane)ny = f(x1,…,xn)     y, x1,…, xnÎ{ 0,1}{ 0,1}n®{ 0,1}nVariabili che possono assumere due soli valori:{ 0, 1}                  { F, T }nDefinizione tramite tavola di verità:x1x2……xn-1xnf00……00000……011..................         11……110n2ncombinazioni di ingresson22funzioni distinte di nvariabiliRiccardo Torlone-Corso di Calcolatori Elettronici10fFALSOVEROFALSETRUE\nn",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\05 Circuiti Digitali e Memorie.pdf#9": "Riccardo Torlone -Corso di Calcolatori Elettronici11Funzioni Booleane (Esempi)nCon n=1si hanno 4funzioni:x1f0f1f2f30        0     0    1     11        0     1    0     1nLa funzione f2è detta  NOTnCon n=2si hanno 16funzioni, tra cui:x1x2f0f1f2f3f4f5f6f70    0     0     0    0    0    0    0    0     00    1     0     0    0    0    1    1    1     11    0     0     0    1    1    0    0    1     11    1     0     1    0    1    0    1    0     1nLa funzione f1è nota come ANDnLa funzione f7è nota come OR",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#0": "CalcolatoriElettroniciParte V: BusProf. Riccardo TorloneUniversita di Roma Tre",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#1": "Architettura a più  Bus\nnDiversi bus, interni ed esterni al chipnSoddisfano diverse esigenze:nVelocità di trasferimentonNumero di lineenCompatibilità all’indietronNegli attuali PC almeno due bus esterniRiccardo Torlone -Corso di Calcolatori Elettronici2\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#10": "Arbitraggio Decentralizzato\nnQuando nessun dispositivo vuole il Bus, la linea di arbitraggio è asserita con propagazione a tutti i dispositivinQuando un dispositivo vuole il Bus:ninvia una richiesta di busnverifica se il bus è libero nse In è asserito diventa master, nega Out e asserisce Busynse In non è asserito non diventa master e nega Out nNon necessita di arbitro, è più semplice e più veloce Riccardo Torlone -Corso di Calcolatori Elettronici11\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#11": "Esercizio su BUSCon riferimento al funzionamento dei bus di un calcolatore:ntracciare e illustrare il diagramma di temporizzazione di un bus sincrono a 40 Mhzcon linee separate per dati e indirizzie segnali di MREQ, RD e WAIT, per una lettura da una memoria con un tempo di risposta di 40 nsecdal momento in cui gli indirizzi sono disponibilintracciare e illustrare il diagramma di temporizzazione di un bus asincrono con linee separate per dati e indirizzi per una scrittura in una memoria con un tempo di risposta di 50 nsec.Si assuma di lavorare in condizioni ideali (nessun ritardo nell'asserimento dei segnali)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#12": "Soluzione esercizio su BUS sincrono\nADDRDATAMREQRDWAITT125 nsecT2T340 nsecacquisizione dato sul master",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#13": "Soluzione esercizio su BUS asincrono50 nsecADDRESSMREQRDMSYNDATASSYNIndirizzo\nDati",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#14": "Esercizio su BUSCon riferimento al funzionamento dei bus di un calcolatore:ntracciare e illustrare il diagramma di temporizzazione di un bus sincrono con linee separate per dati e indirizzi che lavora alla frequenza di 50 Mhz, per una lettura da una dispositivo I/O con un tempo di risposta di 100 nsecdal momento in cui gli indirizzi sono disponibili;ntracciare e illustrare il diagramma di temporizzazione di un bus asincrono con linee separate per dati e indirizzi per una scrittura in una memoria con un tempo di risposta di 30 nsecdal momento in cui il segnale di master syncronizationè stato asserito.\nRiccardo Torlone -Corso di Calcolatori Elettronici15",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#15": "Esercizio sull’arbitraggio di busRiferendosi agli schemi di arbitraggio dei bus discussi a lezione, indicare le affermazioni esatte tra le seguenti.nNell'arbitraggio decentralizzato non è necessaria la linea che segnala l'occupazione del bus.nNell'arbitraggio centralizzato con livelli di priorità la posizione del dispositivo non influisce sulla concessione del grant.nNell'arbitraggio centralizzato non è possibile che due dispositivi si prenotino contemporaneamente asserendo la linea di request.nNell'arbitraggio decentralizzato ai fini delle attese la posizione fisica dei dispositivi è comunque ininfluente.nNell'arbitraggio centralizzato a più livelli di priorità un dispositivo può dover attendere un tempo indefinitamente lungo.nPer motivi di imparzialità l'arbitro è sempre un dispositivo diverso ed esterno al microprocessore.nLa priorità dei dispositivi di I/O è generalmente più alta della CPU.FALSO\nVEROFALSOFALSOFALSOVEROFALSO",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#16": "Bus realinRequisiti:nVideo a 1024×768 con 3B per pixel (truecolor)n1 frame: 2.25 MBn30 frame al secondo: 67.5 MB/secnHD →RAM →VRAM: 135 MB/secnVideo a 1920×1080 con 3B per pixel n1 frame: ~5.2 MBn30 frame al secondo: 155 MB/secnHD →RAM →VRAM: 310 MB/secnBus legacy:nISA: 8.33Mhz, 2B per ciclo, 16.7MB/secnEISA: 8.33Mhz, 4B per ciclo, 33.3MB/secnBus PCI (1990, Intel): fino a 528 MB/secnBus AGP (fine anni ’90): fino a 2.1 GB/secnBus PCIe(2004): 20GB/sec e oltre nBus USB (1995): 10Gb/sec e oltre Riccardo Torlone -Corso di Calcolatori Elettronici24",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#17": "Riccardo Torlone -Corso di Calcolatori Elettronici26\nSchede e slot PCI e AGP\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#18": "Bus PCI: Transazioni\nnInT1 il master invial’indirizzo su AD e il comando su C/BE#nPoi asserisce FRAME# e poi IRDY#nIn T2  C/BE# specifica quali byte leggerenIn T3 lo slave asserisce DEVSEL# e quando i dati sono su AD asserisce TRDY#nTra due transazioni c’è un ciclo di idlenLa transazione di scrittura è più compattaRiccardo Torlone -Corso di Calcolatori Elettronici33",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#19": "Esercizio su BUSCon riferimento al funzionamento dei bus di un calcolatore:ntracciare e illustrare il diagramma di temporizzazione di un bus sincrono a 40 Mhzcon linee condiviseper dati e indirizzie segnali di MREQ, RD e WAIT, per una lettura da una memoria con un tempo di risposta di 40 nsecdal momento in cui gli indirizzi sono disponibilintracciare e illustrare il diagramma di temporizzazione di un di bus asincrono con linee separate per dati e indirizzi e segnali di MREQ, RD, MSYN e SSYN, per una lettura da una memoria con un tempo di risposta di 50 nsecdal momento in cui gli indirizzi sono disponibiliSi assuma in entrambi i casi di lavorare in condizioni ideali (nessun ritardo nell'asserimento dei segnali)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#2": "Comunicazione sul Bus\nnLa comunicazione sul bus è regolata da un protocollo di busnIn ciascun ciclo comunicano due soli dispositivi il master e lo slavenLo stesso dispositivo può avere ruoli diversi a seconda dei casinI dispositivi sono connessi al bus tramite un bus transceivernLa connessione al bus o avviene tramite dispositivi a tre stati oppure è di tipo open collectorRiccardo Torlone-Corso di Calcolatori Elettronici3\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#20": "Soluzione esercizio su BUS sincrono\nMREQADRDT1T2T3T4T5TurnaroundAddressDataWAIT(25 nsec)40 nsec",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#21": "Soluzione esercizio su BUS asincrono50 nsecADDRESSMREQRDMSYNDATASSYNIndirizzo\nDati",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#22": "PCI expressLinea di tendenza nei bus:nbypassare il bus PCI nel caso di periferiche velocincomunicazione seriale!nusare slot più piccoleSoluzione: PCI expressnConnessione punto-a-punto\nRiccardo Torlone -Corso di Calcolatori Elettronici37\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#23": "Caratteristiche PCI expressnTrasmissione serialenTrasferimenti come in una rete di computer:ndati in pacchetti (header+ payload+ CRC)nmaggiore lunghezza dei cavinplug-and-playnBanda: attualmente >120GB/sec (ver. 6.0, x16) nControllo del flusso in base alle dimensioni dei buffern4 spazi di indirizzamento (ovvero ti tipologie ci comunicazione):nMemorianI/OnConfigurazionenMessaggiRiccardo Torlone -Corso di Calcolatori Elettronici38\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#24": "PCI Express Protocol StacknTrasmissione basata su protocollo multi-layerlungo coppie di corsie (lane)nCodifica: 8b/10b nUn meccanismo di acknowledgmentgarantisce maggiore affidabilitànIl software layergarantisce: nla gestione dei pacchettinla compatibilità con il passato\nRiccardo Torlone -Corso di Calcolatori Elettronici39\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#25": "Bus USB (Universal Serial Bus)nBus economico concordato da varie aziende per la gestione di dispositivi di I/O a bassa velocità (~ 1995)nObiettivi:1) Evitare switch, jumpers2) Installazione di tipo esterno3) Cavo di connessione unificato4) Alimentazione fornita dal cavo5) Fino a 127 dispositivi collegabili6) Supporto di dispositivi real-time7) Installazione a PC acceso8) Rebootnon necessario 9) Bus e dispositivi economicinTutti gli obiettivi sono di fatto rispettatiRiccardo Torlone -Corso di Calcolatori Elettronici40",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#26": "Riccardo Torlone -Corso di Calcolatori Elettronici41USB (Universal Serial Bus): Specifiche di basenBus economico concordato da varie aziende per la gestione di dispositivi di I/O a bassa velocità (~ 1995)nDifferenti connettori (A/B/C)nBanda complessivanUSB 1.X: 1.5 –12 Mb/secnUSB 2.0: 480 Mb/secnUSB 3.X: 5 Gb/sec –20 Gb/sec (<3GB/s)nUSB4: 40Gb/secnUSB42.0: 120Gb/secn\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#27": "Riccardo Torlone -Corso di Calcolatori Elettronici42USB: comunicazionesubus\nnRoot hubdi connessione a bus internonConnessione con dispositivi e con altri hubnStruttura complessiva ad alberonCavo a 4 fili: +5V, GND, 2 di segnalenAlla connessione di un dispositivo:nInterrupt: intervento del SOnRichiesta di bandanAssegnazione di indirizzonLogicamente: connessione dedicata tra roothube ciascun devicenCompetitor: nFireWire IEEE 1394 serial bus",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#28": "USB: Struttura dei Frame\nnFrame emessi ogni 1.00±0.05 msecnIdleframe se non c’è comunicazionenContenuto del frame:nSOF: Start of FramenIN / OUT: richiesta in lettura/scritturanDATA: payloadfino a 64 byte più controllo e codice di errorenACK / NACK: acknowledgeo errorenPolling usato invece delle interruzioni  Riccardo Torlone -Corso di Calcolatori Elettronici43\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#3": "“Larghezza”del Bus\nnLarghezza = numero di lineenLinee indirizzo: dimensione dello spazio (di memoria) indirizzabile, 2nlocazioni con n bit di indirizzonLinee + velocità di trasmissione: banda di trasferimentonCondivisione di più segnali sulla stessa lineaper diminuireicostinProblema: al crescere della velocità del bus aumenta il bus skew(differenza nella velocità di propagazione dei segnali su linee diverse)Riccardo Torlone -Corso di Calcolatori Elettronici4\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#4": "Riccardo Torlone -Corso di Calcolatori Elettronici5Segnali asseriti e negatiIn alcuni casi (a seconda delle scelte di progetto) un segnale provoca l’azione corrispondente quando la sua  tensione  è alta(1), in altri quando è bassa(0).Per evitare confusione si parla di:nSegnale asserito: quando assume il valore (alto o basso) che provoca l’azionenSegnale negato: altrimentiSi adotta la seguente notazione:nS: segnale che è asserito altonS: segnale che è asserito bassoUlteriore notazione (usata da Intel):nS: segnale che è asserito altonS#: segnale che è asserito basson(adatta al set di caratteri ASCII)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#5": "Riccardo Torlone -Corso di Calcolatori Elettronici6\nBus Sincroni: ciclo di letturaT=10nsec\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#6": "Bus Sincrono: TemporizzazioneESFrequenza 100 MHz, periodo 10 nsec.nprimo vincolo: tempoa disposizione della memoria fra:nla comparsa dell’indirizzo sul Busnla disponibilità dei dati sul Bust1= 2.5´T –TAD–TDS= 25–4 –2 = 19 nsec(unamemoria da 10 nsecce la fa di sicuro)nsecondo vincolo: tempoa disposizione della memoria fra:nl’asserzione di MREQ e RDnla disponibilità dei dati sul Bust2= 2 ´T –TM–TDS= 20–3 –2 = 15 nsecSe ilchip di memorianon soddisfaquestirequisitimantiene asserito il segnale di WAIT per introdurre stati di wait, cioè cicli di bus addizionali.Riccardo Torlone -Corso di Calcolatori Elettronici7",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#7": "Bus Asincrono: ciclo di lettura\nnAccoppiamento di dispositivi con velocità diversenGli eventi avvengono in  risposta ad altri eventi nFullhandshake:nMSYN asseritonSSYN asserito in risposta a MSYN quando il dato è prontonMSYN negato in risposta a SSYNnSSYN negato in risposta alla negazione di MSYNRiccardo Torlone -Corso di Calcolatori Elettronici8\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#8": "Arbitraggio del Bus\nnPermette di decidere quale dispositivo sarà il prossimo Bus Master risolvendo eventuali conflittinSpesso l’arbitro è nel chip del microprocessorenLinea di richiesta condivisanIl Bus grantè propagato dall’arbitro prima dell’inizio del ciclonViene intercettato dal futuro masternNB:Favoriti i dispositivi situati vicino all’arbitroRiccardo Torlone -Corso di Calcolatori Elettronici9\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\06 Bus.pdf#9": "Livelli Multipli di priorità\nnDiverse linee di richiesta associate a diversi livelli di priorità nIn caso di conflitto favorite le catene a priorità più altanAll’interno di ciascuna catena vale la posizionenIn genere se c’è un solo bus con anche la memoria,la CPU ha priorità più bassa dei dispositivi di I/O (e.g. dischi) Riccardo Torlone -Corso di Calcolatori Elettronici10\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#0": "CalcolatoriElettroniciParte VI: Microarchitetturadi una CPUProf. Riccardo TorloneUniversita di Roma Tre",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#1": "Riccardo Torlone -Corso di Calcolatori Elettronici2L’approccio di San Clemente..\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#10": "Funzioni della ALU\nnENA e ENB abilitano o inibiscono gli ingressi della ALU nINVA e INC permettono di fare il C2 di A, utile per le sottrazioninPossibile incrementare sia A che B e generare le costanti 0,1 e -1Riccardo Torlone -Corso di Calcolatori Elettronici12\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#11": "Riccardo Torlone -Corso di Calcolatori Elettronici13Il Cammino dei Dati nella JVM\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#12": "Riccardo Torlone -Corso di Calcolatori Elettronici14Temporizzazione del ciclo base\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#13": "Temporizzazione del CicloIn ciascun ciclo di clock viene eseguita una microistruzione, cioè:1)Caricamento di un registro sul bus B2)Assestamento di ALU e shifter3)Caricamento di registri dal bus CTemporizzazione:nFronte di discesa: inizio del ciclonDw: tempo assestamento segnali di controllonDx: tempo propagazione lungo bus BnDy: tempo assestamento ALU e shifternDz: tempo propagazione lungo bus CnFronte di salita: caricamento registri dal bus CI tempi Dw, Dx, Dy, Dz, possono essere pensati come sottocicli(impliciti)Riccardo Torlone -Corso di Calcolatori Elettronici15",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#14": "Riccardo Torlone -Corso di Calcolatori Elettronici16Accesso alla MemoriaAccesso parallelo a due memorie:nMemoria Dati: 32 bit indirizzabili a word  (in lettura e scrittura)nMemoria Istruzioni: 8 bit indirizzabili a byte (solo in lettura)Registri coinvolti:nMAR (Memory Address Register): contiene l’indirizzo della word datinMDR (Memory Data Register): contiene la word datinPC (Program Counter): contiene l’indirizzo del byte di codicenMBR (Memory Buffer Register): riceve il byte di codice (sola lettura)Caricamento di B da parte di MBR:nEstensione a 32 bit con tutti 0nEstensione del bit più significativo (sign extension)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#15": "Riccardo Torlone -Corso di Calcolatori Elettronici17Il Cammino dei Dati nella JVM\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#16": "Riccardo Torlone -Corso di Calcolatori Elettronici18Struttura delle µ-istruzioniUna µ-istruzione da 36 bit deve contenere:nTutti i segnali di controllo da inviare al data pathdurante il ciclonLe informazioni per la scelta della µ-istruzione successivaSegnali di controllo:n9  Selezione registri sul bus Cn9  Selezione registro sul bus Bn8  Funzioni ALU e shiftern2  Lettura e scrittura dati (MAR/MDR)n1  Lettura istruzioni (PC/MBR)Selezione µ-istruzione successiva:n9  Indirizzo µ-istruzione (su 512)n3  Modalità di sceltaDato che si invia su B solo un registro per volta, si codificano 9 segnali con 4",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#17": "Riccardo Torlone -Corso di Calcolatori Elettronici19\nFormato delle µ-istruzioni\n§Addr: Indirizzo prossima µ-istruzione§JAM: Scelta prossima µ-istruzione§ALU: Comandi ALU e shifter§C: Registri da caricare da C§Mem: Controllo memoria§B: Registro da inviare su B",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#18": "Riccardo Torlone -Corso di Calcolatori Elettronici20La Sezione di Controllo\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#19": "Riccardo Torlone -Corso di Calcolatori Elettronici21Temporizzazione del ciclo base\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#2": "Il livello della microarchitettura\nnAl livello della microarchitettura studiamo come la CPU “implementa” le istruzioni macchina mediante i dispositivi digitali (l’hardware) a sua disposizionenLa descrizione considera i componenti di base della CPU (registri, ALU, ecc.) e il flusso dei dati tra di essi trascurandone i dettagli realizzativiRiccardo Torlone -Corso di Calcolatori Elettronici3\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#20": "La Sezione di Controllo (2)nControl Store: è una ROM 512´36 bit che contiene le µ-istruzioninMPC (Micro Program Counter): contiene l’indirizzo della prossima µ-istruzionenMIR (MicroInstructionRegister): contiene la µ-istruzione correntenIl contenuto di MPC diviene stabile sul livello alto del clocknLa microistruzione viene caricata in MIR sul fronte di discesa dell’impulso di clocknTemporizzazione della memoria:nInizio ciclo di memoria subito dopo il caricamento di MAR e di PCnCiclo di memoria durante il successivo ciclo di clocknDati disponibili in MDR e MBR all’inizio del ciclo ancora successivo\nRiccardo Torlone -Corso di Calcolatori Elettronici22",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#21": "Riccardo Torlone -Corso di Calcolatori Elettronici23Scelta della µ-istruzionenCiascuna µ-istruzione indica sempre l’indirizzo della successiva (Addr)nNotare: il default non è un esecuzione sequenzialenIl bit più alto di Addr(Addr[8])è dato da:n(JAMZ AND Z) OR (JAMN AND N) OR Addr[8]nPossibile realizzare salti condizionatiESnAddr = 0 1001 0010 (0x92)nJAM [JAMPC,JAMN,JAMZ] = 001nse Z=0 allora Addr= 0 1001 0010(0x92)nse Z=1 allora Addr = 1 1001 0010(0x192)nSe JMPC = 1 allora gli 8 bit bassi di Addr (tipicamente a 0) vanno in OR con il contenuto di MBR nPossibile realizzare salti in tutto il Control Store",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#22": "Riccardo Torlone -Corso di Calcolatori Elettronici24Salti condizionatiEsempio di salto condizionato basato su Z\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#23": "Come migliorare le prestazioniMigliorare le prestazioni significa massimizzare il rapporto: VelocitàPrezzoDa un punto di vista progettuale esistono tre approcci:nRiduzione del numero di cicli necessari per eseguire una istruzione (introducendo hardware “dedicato”);nAumento della frequenza del clock (semplificando l’organizzazione);nIntroduzione del parallelismo (sovrapponendo l’esecuzione delle istruzioni).\nRiccardo Torlone -Corso di Calcolatori Elettronici25",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#24": "Riccardo Torlone -Corso di Calcolatori Elettronici26Introduzione di componenti “dedicate”\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#25": "Riccardo Torlone -Corso di Calcolatori Elettronici27Il Cammino dei Dati nella JVM base\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#26": "Riccardo Torlone -Corso di Calcolatori Elettronici28Aumento del numero di Bus\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#27": "Instruction Fetch Unit\nnIl carico della ALU può essere alleviato introducendo una unità indipendente che carica le istruzioni da eseguirenUna possibile IFU incrementa autonomamente il PC e anticipa il caricamento delle istruzioniRiccardo Torlone -Corso di Calcolatori Elettronici29\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#28": "Riccardo Torlone -Corso di Calcolatori Elettronici30Partizionamento del data-path\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#29": "Riccardo Torlone -Corso di Calcolatori Elettronici31Introduzione di pipeline\nIldata pathrichiede più cicli di clock ma ad una frequenza maggiore!",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#3": "Microarchitettura generica e data path\nnLa microarchitettura della CPU è tipicamente composta da alcuni registri, una ALU, dei bus interni e alcune linee “di controllo”nLe istruzioni macchina comandano il funzionamento della CPU e il percorso dei dati (data path)Riccardo Torlone -Corso di Calcolatori Elettronici4\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#30": "Memorie Cache\nnScopo della cache: disaccoppiare le velocità di CPU e RAMnLocalità spaziale: alta probabilità di accedere in tempi successivi a indirizzi molto vicininLocalità temporale: alta probabilità di accedere più volte agli stessi indirizzi in tempi molto vicini nGerarchie di cache: a 2 o 3 livellinCache inclusive: ciascuna contiene quella del livello superioreRiccardo Torlone -Corso di Calcolatori Elettronici34\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#31": "Ci accorgiamo della presenza della cache?Matrice 30.000x30.000, Intel Core i7 @ 3.6 GHz, 16GB RAM\nRiccardo Torlone -Corso di Calcolatori Elettronici35intsum1(int** m, intn) {inti, j, sum = 0;for(i=0; i<n; i++)for(j=0; j<n; j++)sum += m[i][j];returnsum;}intsum1(int** m, intn) {inti, j, sum = 0;for(i=0; i<n; i++)for(j=0; j<n; j++)sum += m[j][i];returnsum;}18,63 secondi(circa 10 volte più lento)1,84 secondi",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#32": "Organizzazione della Memoria in presenza di cachenLo spazio di memoria è organizzato in blocchi (da 4 a 64 byte), chiamati anche linee di cachenCiascuna linea contiene più wordnCiascuna word contiene più bytenLe cache sono organizzate in righe (o slot): ciascuna contiene una linea di cache, cioè un blocco di memorianTutti i trasferimenti avvengono a livello di blocconQuando una word non viene trovata in cache, si trasferisce l’intera linea dalla memoria, o dalla cache di livello più basso\nRiccardo Torlone -Corso di Calcolatori Elettronici36",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#33": "Organizzazione della Memoria(esempio)\nnIndirizzi a 32 bit (spazio di indirizzamento di 232byte)nLinee di cache (blocchi) di 32 bytenWord di 4 bytenStruttura dell’indirizzo:nI 27 bit più significativi rappresentano il numero di blocconI successivi 3 bit il numero della word all’interno del blocconGli ultimi due bit il numero del byte all’interno della wordRiccardo Torlone -Corso di Calcolatori Elettronici37BLOCCOWORDBYTE2732Struttura degli indirizzi",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#34": "Riccardo Torlone -Corso di Calcolatori Elettronici38Esempi di indirizzin000000000000000000000000000 000 00 n1°blocco –1°word –1°byte n000000000000000000000000000 000 01 n1°blocco –1°word –2°byte n000000000000000000000000000 001 00 n1°blocco –2°word –1°byte (5°byte del blocco)n000000000000000000000000000 011 10 n1°blocco –4°word –3°byte (15°byte del blocco)n000000000000000000000000001 010 11 n2°blocco –3°word –4°byte n000000000000000000000000101 110 10 n6°blocco –7°word –3°byte n000000000000000000000010110 101 00 n23°blocco –6°word –1°byte ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#35": "Ricerca di un blocco in cachenUna cache contiene un sottoinsieme di blocchi di memoria di indirizzo non contiguonQuando la CPU cerca una word, non sa in quale posizione essa si possa trovare nella cache (se effettivamente c’è)nNon c’è modo di risalire dall’indirizzo di un blocco di memoria alla sua posizione in cache nNon è possibile utilizzare il normale meccanismo di indirizzamento delle RAM:nSi fornisce un indirizzonViene letto il dato che si trova  allo indirizzo specificatonSi usano allora Memorie Associative\nRiccardo Torlone -Corso di Calcolatori Elettronici39",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#36": "Memorie Associative\nnCiascun elemento(riga)è costituito da due parti: la chiave e il dato nL’accesso ad un elemento viene effettuato non solo in base all’indirizzo ma anche in base a parte del suo contenuto (chiave)nL’accesso associativo avviene in un unico ciclonNel caso di una cache:nUn elementovienechiamatoslot di cachenLa chiave vienechiamatatag (etichetta)nL’informazione èunacache line (o blocco)Riccardo Torlone -Corso di Calcolatori Elettronici40CHIAVEINFORMAZIONECHIAVE DELL’INFORMAZIONE\nINFORMAZIONE",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#37": "Cache a Mappatura Diretta\nnSpazio di memoria di 2nbyte, diviso in blocchi da 2rbytenGli n-rbit più significatividell’indirizzospecificanoilblocconIn una cache con 2sslot si associa ad ogni blocco la slot di cache indicata dagli sbit meno significativi del suo indirizzo nSe il blocco è in cache deve essere in quella slot, e lì bisogna cercarlonIl TAG sono gli n-s-rbit più significativi dell’indirizzonIl TAG è contenuto nella slotnIl TAG permette di distinguere tra tutti i blocchi che condividono la stessa slot (collidono)Riccardo Torlone -Corso di Calcolatori Elettronici41Indirizzoa n bitSlot di CacheBLOCCO DI MEMORIATAGVTAGINDIRIZZO DI SLOTBYTEn-s-rsw            b      1   n-r-s bit                          2rbyteWORDrn–r",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#38": "Riccardo Torlone -Corso di Calcolatori Elettronici42EsempionIndirizzi a 8 bit (n = 8)nLinee di cache a 8 byte (r = 3)nWord di 2 bytenCache di 8 slotnStruttura indirizzo: \nnStruttura slot: ind. bloccowordbyteslottagbyte nel bloccotagbitvalidBlocco (8 byte)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#39": "Riccardo Torlone -Corso di Calcolatori Elettronici4300000011000000110000100100001001Esempio –continua (è un’animazione che si può scaricare dal sito)\n0 0 00 0 00 0 00 0 00 0 00 0 0....00001 11100001 11000001 10100001 10000001 01100001 01000001 00100001 00000000 11100000 11000000 10100000 10000000 01100000 01000000 00100000 000rqponmlihgfedcba1111111111111110\nAccessi:MemoriaCachea          b         c         d         e          f          g          h10 0i          l         m         n         o          p         q          r1 0 00 0 00 0 0\n01001110000001010000010101001110w         x         y         z         £          $         %          &10 1111110101100011010001000Bit validTag",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#4": "Possibili implementazioninEsecuzione diretta delle istruzioni (RISC)nLe istruzioni possono venire eseguite direttamente dalla microarchitetturanPro e contro: nRepertorio di istruzioni limitatonProgettazione dell’HW complessanEsecuzione molto efficientenInterpretazione delle istruzioni (CISC)nLa microarchitettura sa eseguire direttamente solo alcune semplici operazioninCiascuna istruzione è scomposta in una successione di operazioni base poi eseguite dalla microarchitetturanPro e contro:nRepertorio di istruzioni estesonHW più compattonFlessibilità di progettoRiccardo Torlone -Corso di Calcolatori Elettronici5",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#40": "Riccardo Torlone -Corso di Calcolatori Elettronici44Cache a Mappa Diretta (esempio)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#41": "Cache Associative ad Insiemi\nnOgni slot è costituita da nelementi, ciascuno composto di bit valid, tag e blocconUn blocco può stare in un elemento qualsiasi della slot che gli corrispondenAttenua il problema della collisione di più blocchi sulla stessa slotRiccardo Torlone -Corso di Calcolatori Elettronici45\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#42": "Gestione della CachenLa CPU deduce numero di slot e TAG del blocco a partire dall’indirizzonSe lo slot è valido, si confronta il TAG nella slot con quello del blocco nCache Hit in lettura: tutto oknCache Hit in scrittura:nwritethrough: scrive anche in memorianwriteback: unica scrittura finale, quando il blocco è rimosso dalla cachenCache Miss in lettura: il blocco viene trasferito in cache e sovrascrive quello presente (questo va copiato in memoria se modificato)nCache Miss in scrittura:nwriteallocation: porta il blocco in cache (conviene per scritture ripetute)nwriteto memory: si effettua la scritturain memoriaRiccardo Torlone -Corso di Calcolatori Elettronici46",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#43": "Esercizio su memorie cache IUna cache a mappa diretta con 16K slot e  cache line di 64 byte, è installata  in un sistema con indirizzi a 32 bit:nspecificare la struttura di ciascuna slot, indicando esplicitamente la dimensione complessiva della slot e quella di ciascun campo;ncalcolare il numero di slot e la posizione nella slot del byte con indirizzo esadecimale 7B80034A;nverificare se i due byte di indirizzo esadecimale  32353793 e 3F5537BC collidono sulla stessa slot.\nRiccardo Torlone -Corso di Calcolatori Elettronici47",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#44": "Riccardo Torlone -Corso di Calcolatori Elettronici48Esercizio su memorie cache II Si consideri una memoria cache associativa a 4 vie composta da 4K slot in un sistema con indirizzi a 24 bit e cache line da 16 byte. Indicando con X la cifra meno significativa non nulla del proprio numero di matricola, specificare:nla struttura dell'indirizzo di memoria, specificando la dimensione dei vari campi in bit;nla struttura della slot di cache, specificando la dimensione dei vari campi in bit;nla dimensione totale della cache (ordine di grandezza decimale);ni passi necessari alla ricerca nella cache del byte di indirizzo BXAXF2.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#45": "Memoria cache IIISi vuole progettare una cache a mappatura diretta per un sistema con indirizzi a 32 bit e linee di cache di 32 byte. Calcolare:nil numero minimo di slot necessario a garantire che non più di 213blocchi collidano sulla stessa slot;nla relativa struttura dell'indirizzo di memoria e della slot di cache, specificando la dimensione dei campi in bit;nquanto varia il numero di slot necessari nel caso di cache associativa a due vie;ni passi necessari alla scrittura del byte di indirizzo 7CA3F37D con riferimento a situazioni di cache hit e cache miss.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#46": "Memoria cache IVSi vuole progettare una cache unificata a mappatura diretta per una CPU con indirizzi a 32 bit e linee di cache di 32 byte. Supponendo di avere a disposizione una memoria di 4MB e 40KB di spazio disponibile massimo sul chip della CPU determinare:nla struttura di una possibile slot di cache che soddisfi questi requisiti e la relativa struttura dell’indirizzo di memoria;nle dimensioni totali della cache progettata;nse e come sia possibile modificare la struttura determinata al punto A per ridurre le collisioni sulle slot di cache;ncosa può succedere se la CPU vuole leggere il byte 260 della memoria principale.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#47": "Domande cacheCon riferimento ad una cache a mappatura diretta con 16K slot e cache line di 64 byte installata in un'architettura a 32 bit, indicare se le seguenti affermazioni sono vere o false.nIl campo TAG della cache è di 14 bit.nI primi 6 bit dell'indirizzo non vengono usati per indirizzare una slot di cache.nIl numero di collisioni su una slot di cache aumenta se aumentiamo le dimensioni della cache fino a 32K.nI byte di indirizzo F4B6A598 e 3CE6A5B3 collidono sulla stessa slot della cache.nI byte di indirizzo 4F3B7318 e 4F3B733A collidono sulla stessa slot della cache.nUna slot della cache è grande 525 bit.nSu una slot della cache collidono 4K cache line di memoria.nL'accesso a un byte di memoria contiguo a un byte presente nella cache non genera mai cache miss.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#48": "Soluzioni esercizio precedenteCon riferimento ad una cache a mappatura diretta con 16K slot e cache line di 64 byte installata in un'architettura a 32 bit, indicare se le seguenti affermazioni sono vere o false.n@NO Il campo TAG della cache è di 14 bit.n@SI I primi 6 bit dell'indirizzo non vengono usati per indirizzare una slot di cache.n@NO Il numero di collisioni su una slot di cache aumenta se aumentiamo le dimensioni della cache fino a 32K.n@SI I byte di indirizzo F4B6A598 e 3CE6A5B3 collidono sulla stessa slot della cache.n@NO I byte di indirizzo 4F3B7318 e 4F3B733A collidono sulla stessa slot della cache.n@SI Una slot della cache è grande 525 bit.n@SI Su una slot della cache collidono 4K cache line di memoria.n@NO L'accesso a un byte di memoria contiguo a un byte presente nella cache non genera mai cache miss.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#49": "CPU Core i7Esternamente:nMacchina CISC tradizionale nOperazioni su interi a 8/16/32 bitnOperazioni FP a 32/64 bit (IEEE 754) nSet di istruzioni esteso e molto disordinatonLunghezza variabile da 1 a 17 bytesn8 registriInternamente:nArchitettura \"Sandy Bridge” (32 nm)nSuccessivi: nIvyBridge e Haswell(22nm)nBroadwell, Skylake, KabyLake e Coffee Lake (14nm)nIce/CometLake (10nm) (10th generation)nTiger/Rocket Lake (10/14nm) (11th generation)nAlderLake (7nm) (12th generation)nRaptor Lake (7nm) (13th generation)nNucleo RISCnLunga pipelinenMulti-core (4/6)Riccardo Torlone -Corso di Calcolatori Elettronici76",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#5": "Riccardo Torlone -Corso di Calcolatori Elettronici7Un esempio di µ-architetturanImplementazione di un JVM (Java Virtual Machine) con sole istruzioni su interinIn questo corso ci limitiamo a:nLa microarchitettura(data path)nLa temporizzazione di esecuzionenL’accesso alla memoria (cache)nIl formato delle micro-istruzioninLa sezione di controllonSul libro l’esempio è sviluppato fino alla definizione di un microprogramma completo per una JVM (con aritmetica intera)nQuesta ultima parte non fa parte del programma",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#50": "Intel Core i7\nnSandy Bridge (2011): 1,16 miliardi di transistor, 32nm, 3.5 GhznArchitettura a 64 bit compatibile con i predecessorinAritmetica Floating-pointIEEE 754nArchitettura multicore(2-6) nHyper-threaded, superscalare (fattore 4), pipelinednBus di memoria sincrono a 64 bit + Bus PCIenQPI (QuickPathInterconnect): comunicazione con altri processorinCache 1olivello 32KB dati +32KB istruzioninCache 2olivello 256 KB per core (snooping)nCache 3olivello condivisa da 4 a 15 MB nScheda con 1155 pin (diversa dai predecessori)nConsuma da 17 a 150W (stati differenti per ridurre il consumo)Riccardo Torlone -Corso di Calcolatori Elettronici77\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#51": "Riccardo Torlone -Corso di Calcolatori Elettronici78Intel Core i7: PinoutLogico\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#52": "Intel Core i7: PinoutLogico (2)n1155 piedinin447 per i segnali (alcuni duplicati, 131 in tutto) n286 connessioni di alimentazionen360 connessioni di massan62 per “uso futuro”nDue gruppiindipendentiper l’interfacciacon unaDRAMn64bit, 666Mhz, 1.333 MTPS, 20 GB/sec complessivinUn gruppoper  l’interfacciacon lineePCIen16 linee(lane), 16GB/sec complessivinUn gruppo per la comunicazione con i chipset(DMI)nP67: SATA, USB, Audio, PCIe, Flash; nICH10: PCI, 8259A, clock, timer, controlloriDMAnGestione delle interruzioni sia come l’8088 che con APIC (Advanced ProgrammableInterrupt Controller)nGestione della tensione: (possibili diversi valori di Voltaggio)nThermal monitoring: sensori di calore per il \"thermalthrottling\"n11 linee di diagnosi secondo lo standard IEEE 1149.1 JTAGRiccardo Torlone -Corso di Calcolatori Elettronici79",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#53": "Struttura di un sistema moderno basato su i7\nRiccardo Torlone -Corso di Calcolatori Elettronici80\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#54": "Intel Core i7: Memory Bus\nnBus gestito con pipelining: è possibile sovrapporre 4 transazioninOgni interfaccia DRAM ha 3 gruppi di lineenFasi di una transazione (usano gruppi di linee indipendenti):nAttivazione e invio indirizzinComando di Read/Write di parole contigue della memoria (bank)nRichiede due passi: comando e trasferimento datinChiusura e preparazione per la prossima transazionenFunziona solo con memorie sincroneRiccardo Torlone -Corso di Calcolatori Elettronici81\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#55": "Riccardo Torlone -Corso di Calcolatori Elettronici82MicroarchitetturaSandy Bridge di un core i7\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#56": "Riccardo Torlone -Corso di Calcolatori Elettronici84MicroarchitetturaSandy Bridge i7Sottosistema di memoria:nContiene una cache L2 unificata n8-way, 256KB, cache line 64B, write-backnInterfaccia verso L3 condivisa che contiene una unità di prefetchingn12-way, da 8 a 20MB, cache line 64B, interfaccia con RAMFront end:nPreleva istruzioni dalla cache L2 e le decodificanIstruzioni in L1 (8-way, 64KB, cache line 64B)nScompone istruzioni in micro-op RISC e le appoggia in una cache L0Controllo dell’esecuzione:nSceglie le microistruzioni che possono andare in esecuzione nRitira in ordine le microistruzioniUnità di esecuzione:nEsegue le microistruzioni su unità funzionali multiplenAccede a dati nei registri e nella cache dati L1nInvia informazioni al perditore di salti",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#57": "Riccardo Torlone -Corso di Calcolatori Elettronici85Pipeline dell’architettura Sandy Bridge\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#58": "Riccardo Torlone -Corso di Calcolatori Elettronici88CPU OMAP4430SOC con 2 microprocessori ARM CortexA9nImplementazione Texas Instruments dell’architettura ARMnA 32 bit, bus di memoria a 32 bitnRISC puran2 livelli di cachenSet di istruzioni ridotto e ordinatonLunghezza fissa (4 bytes)nHardware dedicato per istruzioni multimedialin16 registri generalin32 registri opzionali per operazioni in virgola mobilenOrganizzazione piuttosto semplicenMulticore(fino a 4 core)nPipeline a 11 stadi",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#59": "OMAP4430 della Texas InstrumentsnSoCbasato su ISA ARMnTarget: sistemi mobile o embeddednEquipaggiamento:n2-core CPU ARM RISC Cortex-A9 a 1Ghz, 45nmn1 GPU POWERV SGX540 (rendering3D)n1 ISP (manipolazione immagini)n1 VPU IVA3 (video enc/dec)nInterfacce I/O:nTouchscreen, keypadnDRAM, FlashnUSB, HDMInBasso consumo di potenzan660mW/100µWndynamicvoltagescalingnpowergatingnSuperscalare (2 istruzioni per ciclo)n2 L1: 32KB+32KB, 1 L2: 1MBnInterfaccia DRAM (LPDDR2)nScheda a 547 pinRiccardo Torlone -Corso di Calcolatori Elettronici89\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#6": "Riccardo Torlone -Corso di Calcolatori Elettronici8Il Cammino dei Dati nella JVM\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#60": "Riccardo Torlone -Corso di Calcolatori Elettronici90Architettura OMAP4430\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#61": "Riccardo Torlone -Corso di Calcolatori Elettronici91MicroarchitetturaARM CortexA9\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#62": "MicroarchitetturaARM CortexA9nCache L2:nUnificata da 1MBnI-cache L1: n32KB 4-wayncache linedi 32 byte: 8K istruzioni in tuttonUnità di lancio:nprepara fino a4 istruzioni per ciclo e le mette in un buffernUnità di esecuzionenDecodifica e sequenzia le istruzioni nProduce una coda di istruzioni da eseguirenAlmeno 2 Unità di esecuzione n1 con 2 I-ALU per somme + 1 I-ALU per prodotti con registri dedicatin1 di load/storecon:n4-way D-L1 da 32KB con line di cache a 32BnPrefetchingdi datin1 FP-ALU (VFP) e 1 SIMD vettoriale (NEON) opzionalenInterfaccia con la memoria:nArchitettura a 32 bit, word da 4Bn4GB di memoria indirizzabile su 2 canali indipendenti (8GB totali)Riccardo Torlone -Corso di Calcolatori Elettronici93",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#63": "Riccardo Torlone -Corso di Calcolatori Elettronici94Pipeline ARM CortexA9\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#64": "Riccardo Torlone -Corso di Calcolatori Elettronici97CPU ATmega168Chip semplificato con <1M transistornEconomicità prevale sulle prestazioninMacchina RISC a 8 bit n32 registri eterogeneinIstruzioni eseguite in un ciclonPipeline a due stadi: fetch+esecuzioneInternamente:nOrganizzazione semplicenBasata su un Bus principalen1 SRAM da 1KB per i dati volatilin1 EEPROM da 1KB per dati staticinEsecuzioni e ritiri in ordine",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#65": "Riccardo Torlone -Corso di Calcolatori Elettronici98AtmelATmega168 nMicrocontrollore per applicazioni embedded(~1$)nCPU a 8 bit basata su ISA AVRnScheda a 28 pinn23 porte di I/On8 per porte B e Dn7 per porta C (analogica)n1 Vcc+2GNDn2 per configurare circuiti analogicinMemorie incorporaten16KB Flashn1KB EEPROMn1KB SRAMnNo RAM esternanClock realtimenInterfaccia seriale\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#66": "Riccardo Torlone -Corso di Calcolatori Elettronici100MicroarchitetturaATmega168RAM\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#67": "MicroarchitetturaATmega168nRegistri collegati al Bus principale a 8 bitnRegisterfile: contiene 32 registri a 8 bit per dati temporaneinStatus e control: registro di stato nProgram counter: indirizzo istruzione da eseguirenRegistro delle istruzioni: istruzione correntenCiclo macchina attraverso il mainbusnIndirizzamento a memoria ndati: 2 registri (64KB max)nistruzioni: 3 registri (16MB max)nUnità di controllo delle interruzioninInterfaccia serialenTimernComparatore analogicon3 porte digitali di I/O (fino a 24 dispositivi)Riccardo Torlone -Corso di Calcolatori Elettronici101",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#68": "Riccardo Torlone -Corso di Calcolatori Elettronici102Esecuzione di una istruzione nell’ATmega168nSemplice pipelinenDue stadi1.Fetchdell’istruzione nel registro delle istruzioni2.Esecuzione dell’istruzione:a)Lettura dei registri sorgenteb)Elaborazione della ALUc)Memorizzazione del risultato nel registro targetnTutto in 2 cicli di clock a 10-20Mhz",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#69": "Esercizio sulle architetture di CPU ISi vuole realizzare una semplice CPU con architettura CISC a 8 bit dotata di due registri generalpurpose, due registri per il fetchdelle istruzioni (il ProgramCountere il registro istruzione corrente) e due registri per il trasferimento dei dati da/per la memoria (uno per gli indirizzi e l'altro per i dati). La CPU deve essere in grado di svolgere 8 operazioni aritmetiche a numeri interi. Tutte le altre specifiche possono essere liberamente scelte.nDisegnare l'architettura generale (in particolare il data path) di tale CPU (comprensiva dei segnali di controllo) e illustrare concisamente il suo funzionamento.nDefinire il formato di una microistruzione per tale architettura cercando di minimizzare la sua lunghezza.nIndicare possibili modiche dell'architettura proposta in grado di migliorare le prestazioni.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#7": "Il Cammino dei Dati (2)nRegistri: contraddistinti da nomi simbolici ciascuno con una precisa funzionenBus B: presenta il contenuto di un registro all’ingresso B della ALUnALU: ha come ingressi il bus B e il registro H (holding register)nShifter: consente di effettuare vari tipi di shift sull’uscita della ALUnBus C: permette di caricare l’uscita dello shifter in uno o più registrinSegnali di controllo: nB bus enable: trasferisce il contenuto di un registro sul bus BnWrite C bus: trasferisce il contenuto dello shifter in uno o più registrinControllo della ALU: seleziona una delle funzioni calcolabili dalla ALUnControllo dello shifter: specifica se e come scalare l’uscita della ALU Riccardo Torlone -Corso di Calcolatori Elettronici9",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#70": "Architettura di riferimento\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#71": "Esercizio sulle architetture di CPU IISi vuole realizzare una semplice CPU con architettura RISC a 8 bit dotata di un registro general purpose, un registro accumulatore, due registri per il fetch delle istruzioni (il Program Counter e il Registro Istruzione Corrente) e due registri per il trasferimento dei dati da/per la memoria (uno per gli indirizzi e l'altro per i dati). La CPU deve essere in grado di svolgere 16 operazioni aritmetiche a numeri interi. Tutte le altre specifiche possono essere liberamente scelte.nDisegnare l'architettura generale (in particolare il data path) di tale CPU (comprensiva dei segnali di controllo) e illustrare coincisamente il suo funzionamentonDefinire il formato di una istruzione macchina per tale architettura cercando di minimizzare la sua lunghezza.nIndicare possibili modifiche dell'architettura proposta per trasformarla in un'architettura CISC.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#72": "Architetture di CPU IIISi vuole realizzare una CPU per applicazioni embeddedche non possiede RAM e nella quale tutte le istruzioni macchina da eseguire sono memorizzate in una ROM. Tale CPU è dotata di due registri generalpurpose, un registro accumulatore, una porta di I/O e due registri per il caricamento delle istruzioni dalla ROM. La CPU deve essere in grado di eseguire 8 operazioni aritmetiche a numeri interi. L’esecuzione delle istruzioni macchina è strettamente sequenziale. Tutte le altre specifiche possono essere liberamente scelte.nDisegnare l’architettura generale (in particolare il data path) di tale CPU (comprensiva dei segnali di controllo) secondo i principi RISC e illustrare coincisamenteil suo funzionamento.nDefinire il formato di una istruzione macchina per tale architettura fissando la dimensione dei registri.nIndicare possibili modifiche dell’architettura proposta per poter leggere e scrivere dati memorizzati su una memoria RAM.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#73": "Riccardo Torlone -Corso di Calcolatori Elettronici107Architettura di riferimento\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#74": "Architetture di CPU IVSi vuole realizzare una CPU con architettura CISC dotata di tre registri general purpose, un registro accumulatore e due coppie di registri per il trasferimento di dati e istruzioni da/per la memoria. La CPU deve essere in grado di eseguire 16 operazioni aritmetiche a numeri interi e deve essere dotata di 4 stadi di pipeline, il primo dei quali è costituito da una unità IFU. Tutte le altre specifiche possono essere liberamente scelte.nDisegnare l’architettura generale (in particolare il data path) di tale CPU (comprensiva dei segnali di controllo) e illustrare coincisamenteil suo funzionamento.nDefinire il formato di una istruzione macchina per tale architettura fissando la dimensione dei registri.nIndicare possibili modifiche dell’architettura proposta per diminuire il fenomeno delle collisioni tra istruzioni macchina.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#75": "Riccardo Torlone -Corso di Calcolatori Elettronici109Architettura di riferimento\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#76": "Riccardo Torlone -Corso di Calcolatori Elettronici110Esempio di pipeline\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#8": "Utilizziamo la ALU vista\nnA e B sono bit omologhi degli operandinF0 e F1 selezionano la funzione (00: AND), (01: OR), (10: NOT), (11: SUM)nENA ed ENB sono segnali di enablee INVA permette di negare AnDefault ENA=ENB=1 e INVA=0 Riccardo Torlone -Corso di Calcolatori Elettronici10\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\07 Microarchitettura di una CPU.pdf#9": "L’ALU è a 32 bit\nnRealizzata connettendo 32 ALU ad 1 bit (bit slices)nINC incrementa la somma di 1 (A+1, A+B+1)\nRiccardo Torlone -Corso di Calcolatori Elettronici11\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#0": "Calcolatori ElettroniciParte VII: Il linguaggio assemblativo 8088(basato su materiale di M. Di Felice)Prof. Riccardo TorloneUniversitaRoma Tre",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#1": "Linguaggi assemblativiIl linguaggio assemblativo(assembly)nRappresentazione simbolica dell'insieme di istruzioni macchina di un'architettura.nAssocia ai dati nomi simbolici che identificano le corrispondenti posizioni nei registri della CPU o in memorianNasconde i dettagli relativi alle singole istruzioni (codici operativi, formati, ecc.), non quelli relativi all'architettura della macchinanFornisce un set di istruzioni (direttive) che facilitano la traduzione in linguaggio macchina\nRiccardo Torlone -Corso di Calcolatori Elettronici2",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#10": "I registri di uso generalenAX: registro accumulatore, usato per memorizzare il risultato dell'elaborazione e come destinazione di molte istruzioni (a volte implicitamente)Esempio: ADD AX,20nBX: registro base, usato come accumulatore o come puntatore alla memoriaEsempio: MOV AX,(BX)nCX: registro contatore, usato come contatore dei ciclinDX: registro dati, usato insieme ad AX per contenere le istruzioni lunghe due parole (32 bit)nDX:AX\nRiccardo Torlone -Corso di Calcolatori Elettronici11",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#100": "Esercizio VIIIScrivere un programma in linguaggio assemblativo 8088 che, dato un numero memorizzato in memoria principale, calcola il fattoriale del numero (n! = n×(n−1)×. . .×1) e lo stampa.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#101": "Esercizio VIII!Calcolo del fattoriale_EXIT= 1_PRINTF = 127.SECT .TEXTstart:MOV    AX,(number)CMP     AX,1JG       1fMOV     AX,1JMP     3f1:MOVCX,AXDECCX2:IMULCXLOOP2b3:MOV(result), AXPUSH(result)PUSH(number)PUSH    fmtPUSH    _PRINTFSYSMOVSP,BPPUSH0PUSH_EXITSYS.SECT .DATAnumber: .WORD5result: .WORD   1fmt:.ASCII \"il fattoriale di %d e'%d\\0\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#102": "Esercizio XScrivere un versione ricorsiva del programma del calcolo del fattoriale.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#103": "Esercizio X: Possibile soluzione!Calcolo del fattoriale: versione ricorsiva_EXIT = 1_PRINTF = 127.SECT .TEXTstart:PUSH  (number)CALL   fattPOP    CXMOV   SP,BPPUSH  CXPUSH  (number)PUSH  fmtPUSH  _PRINTFSYSMOV   SP,BPPUSH  0PUSH  _EXITSYSfatt:PUSH  BPMOV   BP,SPMOV   CX,4(BP)CMP    CX,1JG      1fMOV   4(BP),1JMP    2f1:DEC   CXPUSH  CXCALL  fattPOP   CXMOV   AX,4(BP)IMUL  CXMOV   4(BP),AX2:MOV   SP,BPPOP   BPRET.SECT .DATAnumber: .WORD 3fmt:.ASCII \"il fattoriale di %d e'%d\\n\"",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#104": "Esercizio IXScrivere un programma in linguaggio assemblativo 8088 che dato un numero nmemorizzato in memoria principale, verifica se è un numero primo.Consiglio: utilizzare l'istruzione DIV che divide l'argomento per il contenuto di AX mettendo il risultato in AX e il resto in DX",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#105": "Esercizio IX_EXIT= 1_PRINTF= 127.SECT .TEXTstart:MOVBX,(n)MOVCX,BX1:DECCXCMPCX,1JLE3fMOV AX,BXMOVDX,0DIVCXCMPDX,0JE2fJMP1b2:MOVBX, nonprimoJMP4f3:MOVBX, primo4:PUSH(n)PUSH    BXPUSH    _PRINTFSYSMOVSP,BPPUSH0PUSH_EXITSYS.SECT .DATAn:.WORD49primo: .ASCII \"%d e'un numero primo\\0\"nonprimo: .ASCII \"%d non e'un numero primo\\0\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#106": "Esercizio IXbis: altra possibile soluzione_EXIT= 1_PRINTF = 127.SECT .TEXTstart:MOVBX,(n)MOVCX,BXDECCX1:CMPCX,1JLE3fMOV AX,BXMOVDX,0IDIVCXCMPDX,0LOOPNZ  1b2:MOVBX, nonprimoJMP4f3:MOVBX, primo4:PUSH(n)PUSH    BXPUSH    _PRINTFSYSMOVSP,BPPUSH0PUSH_EXITSYS.SECT .DATAn:.WORD49primo: .ASCII \"%d e'un numero primo\\0\"nonprimo: .ASCII \"%d non e'un numero primo\\0\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#107": "Esercizio XIScrivere un programma in linguaggio assemblativo 8088 che calcola il del prodotto scalare di due vettori (somma dei prodotti degli elementi omologhi). Il programma deve essere dotato di una subrountineprodvecavente quattro parametri:nvec1 (indirizzo del primo vettore)nvec2 (indirizzo del secondo vettore)ndimensione (dimensione dei vettori –si assuma che siano della stessa lughezza)nrisultato (parametro di output che al termine dell'esecuzione della subroutine memorizza il risultato del prodotto).",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#108": "Esercizio XI: possibile soluzione_EXIT= 1_PRINTF= 127.SECT .TEXTstart:MOV CX,vec2-vec1 SHR  CX,1PUSH 0       !quarto parametro inz. a zeroPUSH CX     !terzo parametro PUSH vec2  !secondo parametro PUSH vec1  !primo parametro CALL prodvecADD SP,6   !tolgo i primi tre parametriPOP AXMOV  SP,BPPUSH AXPUSH fmtPUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYSprodvec:PUSH BPMOV  BP,SPMOV  CX,8(BP)MOV  SI,0PUSH 0         !variabile locale inz. a zero1:MOV  BX,4(BP)  MOV  AX,(BX)(SI) MOV  BX,6(BP)MUL  (BX)(SI)ADD  -2(BP),AXADD  SI,2LOOP 1bPOP  10(BP)  !salvo la var. nel 4 argomento POP  BPRET.SECT .DATAvec1: .WORD 3,4,7,11,3vec2: .WORD 2,6,3,1,0fmt: .ASCII \"Il prodotto dei due vettori e': %d!\\0\"!.SECT.BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#109": "Esercizio XIIScrivere una subroutine PAL in assembler8088 che, dato una stringa (vettore di caratteri) S memorizzata in memoria principale, stampa restituisce 1 se la stringa S è palindroma (è uguale leggendola nei due versi; per esempio la stringa \"anna\" è palindroma) e 0 altrimenti. La subroutine PAL ha come parametri:nL’indirizzo della stringa da verificare e nla lunghezza della stringa",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#11": "Registri a 8 e a 16 bitTutti i registri possono essere visti come coppie di registri di 8 bit accessibili autonomamente (esempio: AX=AH:AL)\nRiccardo Torlone -Corso di Calcolatori Elettronici120000000000000000AXAHALMOVE AX,2580000000100000010AXAHALADD AH,AL0000001100000010AXAHALAX=770",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#110": "Soluzione Esercizio XII! Verifica di stringhe palindrome_EXIT= 1_PRINTF= 127.SECT .TEXTstart:PUSH ends-str!secondo parametro PUSH str!primo parametro CALL palMOV  SP,BPPUSH AXPUSH fmtPUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYSpal:PUSH BPMOV  BP,SPMOV  BX,4(BP)MOV  CX,6(BP)MOV  SI,CXDEC  SIMOV  DI,str21:MOVB  AL,(BX)(SI)STOSB  DEC  SILOOP 1bMOV  SI,4(BP)MOV  DI,str2MOV  CX,6(BP)REPE CMPSBJE   2fMOV  AX,0JMP  3f2:      MOV  AX,13:POP  BPRET.SECT .DATAstr: .ASCII \"ingegni\"ends: .SPACE 1fmt: .ASCII \"%d\" endf: .SPACE 1str2: .ASCII \".\" .SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#111": "Esercizio XIIIScrivere un programma in linguaggio assemblativo 8088 che trova il più grande degli elementi di un vettore vecmemorizzato in memoria principale. Si assuma che il vettore abbia almeno un elemento.Il risultato deve essere stampato sullo standard output (video). ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#112": "Soluzione Esercizio XIII! Trova il piu'grande tra gli elementi di un vettore di interi_EXIT= 1_PRINTF= 127 .SECT .TEXTstart:MOV  CX,end-vecSHR  CX,1 !in CX va la dimensione di vecMOV  BX,vec!il registro base punta al primo elemento di vecMOV  AX,(vec) !inizializzo AX con il primo elemento di vec1:CMP  AX,(BX)(SI)JGE  2fMOV  AX,(BX)(SI)2:ADD  SI,2LOOP 1bPUSH AXPUSH formatPUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYS.SECT .DATAvec:   .WORD 3,-4,7,11,34,-4,22,0,5end:   .SPACE 1format: .ASCII \"Il piu'grande tra elementi del vettore e'%d\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#113": "Laboratorio di ricerca sui Big DataVari temi di ricerca:nData managementnData analyticsnData integrationnData wranglingStrumenti:nDistributed processingnMachine LearningnLarge Language ModelsCosa offriamo agli studenti:nTirocini aziendali/interninIncubatore di start-up nContratti su progettiRiccardo Torlone -Corso di Calcolatori Elettronici115\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#12": "I registri puntatore ed indicenSP: registro puntatore alla cima dello stack. nViene modificato automaticamente dalle operazioni sullo stack(PUSH, POP).nBP: registro puntatore base dello stack. nPunta alla base del frame (record di attivazione) assegnato alla procedura correntenSI: registro indice sorgentenusato in combinazione con BP per riferirsi a dati sullo stacko con BX per localizzare dati in memoria.nDI: registro indice destinazionenusato come SI\nRiccardo Torlone -Corso di Calcolatori Elettronici13",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#13": "Registro di statonIl registro di stato (flag) e un insieme di registri da 1 bit.nI bit sono impostati da istruzioni aritmetiche:nZ-il risultato e zeronS-il risultato e negativo (bit di segno)nO-il risultato ha causato un overflownC-il risultato ha generato un riportonA-riporto ausiliario (oltre il bit 3)nP-parità del risultatonGli altri bit del registro controllano alcuni aspetti dell'attività del processore nI= attiva gli interruptnT= abilita il tracingnD= operazioni su stringhenNon tutti i bit sono utilizzatiRiccardo Torlone -Corso di Calcolatori Elettronici14",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#14": "Segmenti e registri di segmentonLo spazio di memoria indirizzabile dalla CPU e suddiviso in segmenti logici. Ogni segmento e costituito da 65.536 (64K) byte consecutivi.nQuattro registri di segmento puntano ai quattro segmenti correntemente attivi.\nnCSpunta al segmento contenente le istruzioni da eseguirenDSpunta al segmento contenente le variabili del programmanSSpunta al segmento contenente lo stackcorrentenESpunta al segmento extra, usato tipicamente per i datiRiccardo Torlone -Corso di Calcolatori Elettronici15\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#15": "Segmentazione della memorianLo spazio di memoria viene visto come un gruppo di segmentinOgni segmento:nCostituisce un'unita di memoria indipendentenE' formata da locazioni contigue di memorianHa un limite massimo di 64KBnInizia ad un indirizzo di memoria multiplo di 16nOgni riferimento alla memoria richiede l'intervento di un registro di segmento per la costruzione di un indirizzo fisico:Indirizzo Effettivo = indirizzo nel programma  + indirizzo segmento\nRiccardo Torlone -Corso di Calcolatori Elettronici17",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#16": "Costruzione indirizzo fisico1.Si considera il registro di segmento corrispondente2.Si aggiungono 4 zero a destra (×16)nindirizzo a 20 bit3.Si somma all'indirizzo da 20 bit\nRiccardo Torlone -Corso di Calcolatori Elettronici18\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#17": "Segmento per la gestione dello stacknIl segmento di stacke costituito da parole di 2 bytenLo stackcresce andando dagli indirizzi alti a quelli bassinSS punta all'indirizzo di partenza dello stacknSP punta alla locazione in cima allo stack\nRiccardo Torlone -Corso di Calcolatori Elettronici19\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#18": "Indirizzamento immediato e a registronIndirizzamento a registronL'operando si trova nei registri, e non è necessario accedere alla memoriaEsempio: CX=5→MOV AX,CX →AX=5nIndirizzamento immediatonL'operando e contenuto nell'istruzione. nIl dato può essere una costante di 8 o 16 bitEsempio: MOV AX,5 →AX=5\nRiccardo Torlone -Corso di Calcolatori Elettronici20",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#19": "Indirizzamento direttoL'istruzione contiene l'indirizzo dei dati nell'operando stesso\nRiccardo Torlone -Corso di Calcolatori Elettronici21\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#2": "Perché imparare un linguaggio assemblativo?nPer scrivere routine di sistema operativo? NOnSi scrive in C (o in C++)nPer scrivere codice ottimizzato? Solo in partenImpossibile battere i compilatorinSolo localmente abbiamo qualche chancenRegola 90%-10%nPer conoscere meglio il calcolatore? SI!nPer imparare l’assemberbisogna conoscere l’architetturanIl debugdell’assemblerci fa comprendere come funziona l’architettura \n3Riccardo Torlone -Corso di Calcolatori Elettronici",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#20": "Indirizzamento indiretto a registroL'indirizzo dell'operando e memorizzato in uno dei registri BX, SI o DI\nRiccardo Torlone -Corso di Calcolatori Elettronici22\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#21": "Indirizzamento indiretto a registro con spiazzamentoL'indirizzo si ottiene dalla somma del contenuto di uno dei registri BX, SI o DI e una costante\nRiccardo Torlone -Corso di Calcolatori Elettronici23\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#22": "Indirizzamento a registro indiceL'indirizzo si ottiene dalla somma del contenuto dei dei registri SI o DI e BX\nRiccardo Torlone -Corso di Calcolatori Elettronici24\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#23": "Indirizzamento a registro indice con spiazzamentoL'indirizzo si ottiene dalla somma del contenuto dei registri SI o DI, BX ed una costante\nRiccardo Torlone -Corso di Calcolatori Elettronici25\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#24": "Modalità di indirizzamento\nRiccardo Torlone -Corso di Calcolatori Elettronici26\n#=operando immediato",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#25": "L'assemblatore as88Disponibile presso:nCD-ROM allegato al libro di testo del corsonSito Web del corsoIl toolcomprende:nProgramma assemblatore (as88)nUtilizzo Generale: as88 Nomeprogetto(.s)nEmulatore-Interprete dell'architettura 8088 (s88)nUtilizzo Generale: s88 NomeprogettonProgramma tracerper il debugging(t88)nUtilizzo Generale: t88 Nomeprogetto(.$)\nRiccardo Torlone -Corso di Calcolatori Elettronici27",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#26": "Direttive dell’assemblatorenOgni programma assemblye strutturato in 3 sezioni:1.sezione di TESTO (direttiva: .SECT .TEXT): contiene le istruzioni del programma2.sezione DATI (direttiva: .SECT .DATA): alloca spazio nel segmento DATI per i dati (inizializzati)3.sezione BSS (direttiva: .SECT .BSS): alloca spazio nel segmento DATI per i dati (non inizializzati)nE' possibile definire etichette di due tipi:nglobali: identificatori alfanumerici seguiti dal simbolo “:” (possono occupare una intera riga)nlocali: utilizzabili solo nel segmento TESTO, costituite da una sola cifra seguita dal simbolo “:”.Riccardo Torlone -Corso di Calcolatori Elettronici28",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#27": "Vincoli sulle etichettenLe etichette globali DEVONO essere univochenEs: .SECT .DATA   hw: .ASCII \"Hello\"nLe etichette locali possono occorrere più voltenEs. JMP 1fnSalto verso la prossima etichetta denominata \"1\"nE' possibile attribuire nomi simbolici alle costanti mediante la sintassi: identificatore=espressionenEs. BLOCKSIZE=1024nI valori numerici possono essere:ndecimali,nottali (cominciano per zero), nesadecimali (cominciano per 0x)nI commenti iniziano con il carattere \"!\"Riccardo Torlone -Corso di Calcolatori Elettronici29",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#28": "Direttive del compilatore\nRiccardo Torlone -Corso di Calcolatori Elettronici30\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#29": "The Tracer(debugger)\nIl tracerconsente di effettuare l'esecuzione step-by-stepdel programma e di monitorare lo stato di registri/memoriaRiccardo Torlone -Corso di Calcolatori Elettronici31\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#3": "Assembler \"embedded\"#include<stdio.h>voidmain(void) {staticintx = 3;asm{MOV %EAX, xADD %EAX, xADD %EAX, xNOPMOV x, %EAX}printf(\"%2u\",x);return;}4Riccardo Torlone -Corso di Calcolatori Elettronici",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#30": "Uso dei registri con il tracer\n(a) Parte del programma(b) I registri dopo l’esecuzione di 7 righe(c) I registri dopo l’esecuzione di 6 iterazioni del cicloRiccardo Torlone -Corso di Calcolatori Elettronici32\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#31": "The ACK-BasedAssembler, as88\nValori di escapeconsentiti nell’as88.Riccardo Torlone -Corso di Calcolatori Elettronici33\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#32": "Comandi del tracer(1)\nE' possibile interagire con il tracer: nin modalità batch (fornendo in input un file con i comandi del tracer) nin modalità interattiva (inserendo comandi da tastiera seguiti dal tasto INVIO)Riccardo Torlone -Corso di Calcolatori Elettronici34\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#33": "Comandi del tracer(2)\nRiccardo Torlone -Corso di Calcolatori Elettronici35\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#34": "Chiamate di sistemanLe chiamate di sistema consentono di utilizzare le procedure fornite dal sistema operativo.nLe routine di sistema possono essere attivate con la sequenza di chiamata standard:nSi impilano gli argomenti sullo stacknSi impila il numero di chiamatanSi esegue l'istruzione SYSnI risultati sono restituiti nel registro AX o nella combinazione di registri AX:DX (se il risultato e di tipo long)nGli argomenti sullo stackdevono essere rimossi dalla funzione chiamante\nRiccardo Torlone -Corso di Calcolatori Elettronici36",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#35": "Chiamate di sistema in as88 (1)L'interprete 8088 supporta 12 chiamate di sistema.n_OPEN: Apre il file namein lettura-scritturaIdentificativo chiamata:5Argomenti: *name, 0=lettura/1=scrittura/2=lettura-scrittura;Valore Ritorno: un descrittore di file (fd)n_CREAT: Crea un nuovo file di nome nameIdentificativo chiamata: 8Argomenti: *name, *mode= permessi UNIX;Valore Ritorno: un descrittore di file (fd)n_READ: Legge nbyte da un file con descrittore fdtrasferendoli nel buffer bufIdentificativo chiamata: 3Argomenti: fd, buf, n;Valore Ritorno: numero di byte letti correttamenteRiccardo Torlone -Corso di Calcolatori Elettronici37",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#36": "Chiamate di sistema in as88 (2)n_WRITE: Scrive n byte sul file con descrittore fdprelevandoli dal buffer bufIdentificativo chiamata: 4Argomenti: fd, buf, n;Valore Ritorno: numero di byte scritti correttamenten_CLOSE: Chiude un file precedentemente apertoIdentificativo chiamata: 6Argomenti: fd(descrittore di file)Valore Ritorno: 0 se l'operazione ha successon_LSEEK: Sposta il puntatore del file con descrittore fddioffset bytesIdentificativo chiamata: 19Argomenti: fd, offset, 0/1/2;Valore Ritorno: nuova posizione all'interno del filen_EXIT: Interrompe un processoIdentificativo chiamata: 1;Argomenti: 0=successo/1=errore;Riccardo Torlone -Corso di Calcolatori Elettronici38",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#37": "Chiamate di sistema in as88 (3)n_GETCHAR: Legge un carattere dallo standard inputIdentificativo chiamata: 117Valore ritorno: il carattere letto e posto in ALn_PUTCHAR: Scrive un carattere sullo standard outputIdentificativo chiamata: 122Argomenti: carattere da scriveren_PRINTF: Stampa una stringa formattata sullo standard outputIdentificativo chiamata: 127Argomenti: stringa di formato, argomentin_SSCANF: Legge gli argomenti dal bufferbufIdentificativo chiamata: 125Argomenti: buf, stringa di formato, argomentin_SPRINTF: Stampa una stringa formattata sul buffer bufIdentificativo chiamata: 121Argomenti: buf, stringa di formato, argomentiRiccardo Torlone -Corso di Calcolatori Elettronici39",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#38": "Primo esempio!calcolo di (a+3)*b_EXIT= 1.SECT .TEXTstart:MOVAX,(a)ADDAX,3MUL     (b)PUSH0PUSH_EXITSYS.SECT .DATAa: .WORD 5b: .WORD 3.SECT .BSSRiccardo Torlone -Corso di Calcolatori Elettronici40",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#39": "Istruzione di copia e trasferimentoMOV(B): Trasferisce un byte (MOVB) o una word (MOV) da una sorgente ad una destinazione senza alterare il contenuto della sorgenteIndirizzamento effettivo: un qualunque indirizzamento tra quelli vistinIndirizzamento:nregistro ←indirizzo effettivo (Es. MOV AX,(200))nindirizzo effettivo←registro (Es. MOV (BX), AX)nindirizzo effettivo←dato immediato (Es. MOV AX,100)nVincoli:nNon e possibile caricare un valore immediato in un registro segmentonIl registro CS non e utilizzabile come destinazione di un'istruzione MOV.\nRiccardo Torlone -Corso di Calcolatori Elettronici41",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#4": "Assemblatore e TracerAssembler (assemblatore)nProgramma che riceve in ingresso un programma in linguaggio assemblativoe genera un programma in linguaggio macchina (binario) pronto per essere eseguito dall'hardware.Tracer(interprete)nSimulatore dell’esecuzione di un programma scritto in un linguaggio assemblativonConsente di procedere “passo-passo”nDebuggerper l’Assembler\n5Riccardo Torlone -Corso di Calcolatori Elettronici",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#40": "Operazioni sullo stackPUSH e POP aggiungono/rimuovono un elemento dalla cima dello stackselezionato da SS:SPnLe operazioni sullo stackmodicano il valore di SP:nIndirizzamento a stackimplicitonOperandi validi:nPUSH: operando immediato o indirizzo effettivo (es. PUSH 30 oppure PUSH AX)nPOP: indirizzo effettivo (esPOP AX)Le operazioni PUSHF e POPF trasferiscono il contenuto del registro flagnella cima dello stacke viceversa.\nRiccardo Torlone -Corso di Calcolatori Elettronici42",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#41": "Operazioni di PUSH e POPnL'operazione di PUSH decrementa SP di 2 bytenL'operazione di POP incrementa SP di 2 byte\nRiccardo Torlone -Corso di Calcolatori Elettronici43\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#42": "AddizioneADD(B): somma l'operando sorgente all'operando destinazione e memorizza il risultato nell'operando destinazionenIndirizzamento:nregistro ←indirizzo effettivo (Es. ADD AX,(200))nindirizzo effettivo←registro (Es. ADD (BX), AX)nindirizzo effettivo←dato immediato (Es. ADD AX,100)nL'istruzione ADD modica i bit del registro di flagADC comprende nella somma il flagdel riporto.\nRiccardo Torlone -Corso di Calcolatori Elettronici44",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#43": "SottrazioneSUB(B): sottrae l'operando sorgente all'operando destinazione e memorizza il risultato nell'operando destinazione.nIndirizzamento:nregistro←indirizzo effettivo (Es. SUB AX,(200))nindirizzo effettivo←registro (Es. SUB (BX), AX)nindirizzo effettivo←dato immediato (Es. SUB AX,100)nL'istruzione SUB modica i bit del registro di flagSBB comprende nella sottrazione il flagdel riporto.\nRiccardo Torlone -Corso di Calcolatori Elettronici45",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#44": "Moltiplicazione(I)MUL(B): moltiplica due operandi con/senza segnonE’ un'operazione unaria: MUL sourcenIl primo operando (implicito) è il registro accumulatore (AL per moltiplicazione tra byte, AX per word).nIl secondo operando e specificato da source e può essere un qualsiasi indirizzo effettivo.nIl risultato e posto in AX se si moltiplicano byte, in AX:DX se si moltiplicano wordIMUL effettua la moltiplicazione con segno\nRiccardo Torlone -Corso di Calcolatori Elettronici46",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#45": "Divisione(I)DIV(B): divide due operandi con/senza segno.nE’ un’operazione unaria: DIV sourcenIl divisore e specificato da sourcee può essere un qualsiasi indirizzo effettivo.nSe sourceha dimensioni di 1 byte:nIl dividendo (implicito) e AXnIl risultato della divisione è in AL, il resto in AHnSe sourceha dimensioni di 1 word:nIl dividendo (implicito) e DX:AXnIl risultato della divisione è in AX, il resto in DXIDIV effettua la divisione con segno.Riccardo Torlone -Corso di Calcolatori Elettronici47",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#46": "Riepilogo istruzioni di movimento e aritmetiche\nRiccardo Torlone -Corso di Calcolatori Elettronici48\ne=indirizzoeffettivo, r=registro, #=operando immediato",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#47": "Operazioni logiche, su bit e di scorrimentoPrincipali Operazioni logiche: NEG(B), NOT(B), INC(B), DEC(B)nL'operando è un indirizzo effettivoPrincipali Operazioni su bit: AND(B), OR(B), XOR(B)nregistro ←indirizzo effettivo (Es. AND AX,(200))nindirizzo effettivo←registro (Es. AND (BX), AX)nindirizzo effettivo←dato immediato (Es. AND AX,1)Principali Operazioni di scorrimento: SHR(B), SHL(B), ROL(B), ROR(B)nLa destinazione è un indirizzo effettivonIl secondo argomento quantifica lo spostamento\nRiccardo Torlone -Corso di Calcolatori Elettronici49",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#48": "Operazioni logiche, su bit e scorrimento\nRiccardo Torlone -Corso di Calcolatori Elettronici50\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#49": "Salti incondizionatiJMP: trasferisce il controllo all'istruzione specificata dall'operando in maniera incondizionata.Due tipi di salto:nSalto Corto: la destinazione si trova nel segmento di codice corrente (cui fa riferimento il registro CS)nSalto Lungo: l'istruzione modifica il contenuto del registro CSEsempio:JMP labelADD AX,BXlabel:AND AX,BXN.B. La prossima istruzione ad essere eseguita è la ANDRiccardo Torlone -Corso di Calcolatori Elettronici51",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#5": "Assemblye linguaggio macchina\n6Riccardo Torlone -Corso di Calcolatori Elettroniciassemblaggiocompilazione",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#50": "ConfrontiCMP(B): Effettua una sottrazione fra due operandi senza modificare nessuno dei due operandiEsempio: CMP operando1  operando2nIl risultato della sottrazione viene scartato.nI bit del registro di flagvengono modificati.\nRiccardo Torlone -Corso di Calcolatori Elettronici52",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#51": "Salti condizionatiJxx: istruzioni di salto in base ai valori del registro di flagnLe istruzioni di salto condizionato controllano se una certa condizione e verificata. nLa condizione è specificata dal valore dei registri di flag. nAzioni:nSe la condizione e verificata, il controllo passa all' istruzione il cui indirizzo e specificato come operandonSe la condizione non e verificata, l'esecuzione prosegue con l'istruzione successivanVincoli:nJxxconsente salti di lunghezza massima pari a 128 byte\nRiccardo Torlone -Corso di Calcolatori Elettronici53",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#52": "Salti condizionati\nRiccardo Torlone -Corso di Calcolatori Elettronici54\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#53": "Registro di statonIl registro di stato (flag) e un insieme di registri da 1 bit.nI bit sono impostati da istruzioni aritmetiche:nZ-il risultato e zeronS-il risultato e negativo (bit di segno)nO-il risultato ha causato un overflownC-il risultato ha generato un riportonA-riporto ausiliario (oltre il bit 3)nP-parità del risultatonGli altri bit del registro controllano alcuni aspetti dell'attività del processore nI= attiva gli interruptnT= abilita il tracingnD= operazioni su stringhenNon tutti i bit sono utilizzatiRiccardo Torlone -Corso di Calcolatori Elettronici55",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#54": "Implementazione di istruzioni condizionali e cicliIf(a> b) thenCMP AX,BXa=b; JLE else_labelelse MOV AX,BXb=a; JMP end_labelelse_label:MOV BX,AXend_label:….while(a<1000) whileSum:... CMP AX,1000...JGE end_whileend while; ...JMP whileSumend_while:Riccardo Torlone -Corso di Calcolatori Elettronici56",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#55": "CicliL'istruzione LOOP consente di implementare esplicitamente cicli nIl registro CX deve essere inizializzato con il numero di cicli dell'istruzione LOOPnLOOP statementLabelnIl valore di CXviene decrementatonSe CXvale zero, l'esecuzione continua con l'istruzione successiva all'istruzione LOOPnSe CXe diverso da zero, allora viene eseguito un salto all'etichetta statementLabel\nRiccardo Torlone -Corso di Calcolatori Elettronici57",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#56": "Esempi di uso di loope sue variantifor(i=0; i<5; i++) { MOV AX,(a)a=a+3; MOV CX,5}repeat:ADD AX,3LOOP repeatnLOOPE statementLabelnDecrementa CX e cicla(saltando all'etichetta statementLabel) se CX≠0 e il flagZF=1nLOOPNE statementLabelnDecrementa CX e cicla(saltando all'etichetta statementLabel) se CX≠0 e il flagZF=0.Riccardo Torlone -Corso di Calcolatori Elettronici58",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#57": "Altre istruzioni\nRiccardo Torlone -Corso di Calcolatori Elettronici59\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#58": "Esercizio IIScrivere un programma in linguaggio assemblativo 8088 che, preso un intero n in memoria, calcola la somma dei primi n interi.Il risultato deve essere stampato sullo standard output (video). ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#59": "Soluzione Esercizio!Somma dei primi n numeri_EXIT= 1_PRINTF= 127.SECT .TEXTstart:MOVAX,0            MOVCX,(number)1:ADDAX,CXLOOP1bMOV(result), AXPUSH(result)PUSH(number)PUSH    formatPUSH    _PRINTFSYSMOVSP,BPPUSH0PUSH_EXITSYS.SECT .DATAnumber:.WORD5result:.WORD   1!format: .ASCII \"La somma dei primi %d interi e' %d\"",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#6": "Il Tracer\n(a) Un programma in linguaggio assemblativo(b) Il tracerin esecuzione sul programma Riccardo Torlone -Corso di Calcolatori Elettronici7\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#60": "Esercizio IVScrivere un programma in linguaggio assemblativo 8088 che calcola la somma degli elementi di un vettore vecmemorizzato in memoria principale. Il risultato deve essere stampato sullo standard output (video). ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#61": "Soluzione Esercizio! Stampa la somma di un vettore di interi_EXIT= 1_PRINTF= 127 .SECT .TEXTstart:MOV  CX,end-vecSHR   CX,1! In CXva la lunghezza del vettoreMOV  BX,vecMOV  SI,0MOV  AX,01:ADD  AX,(BX)(SI)ADD  SI,2LOOP 1bPUSH AXPUSH formatPUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYS.SECT .DATAvec:   .WORD 3,4,7,11,3end:   .SPACE 1format: .ASCII \"La somma degli elementi del vettore e'%d\"",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#62": "Chiamate di procedura (subroutine)Un programma in linguaggio assemblativopuò essere suddiviso in sottoprogrammi detti subroutine.nVantaggi:nConsentono di suddividere il codice in blocchi funzionalinConsentono di riutilizzare codicenCaratteristiche:nPossono ricevere parametri in ingressonPossono disporre di variabili localinPossono restituire un valore di ritorno.\nRiccardo Torlone -Corso di Calcolatori Elettronici64",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#63": "Chiamata di procedura e indirizzo di ritorno\nRiccardo Torlone -Corso di Calcolatori Elettronici65\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#64": "Struttura tipica Stacknell’invocazione di subroutine\nRiccardo Torlone -Corso di Calcolatori Elettronici66\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#65": "Invocazione di subroutine in x86L'istruzione CALL trasferisce il controllo dal programma chiamante alla procedura chiamatanSintassi: CALL Nome FunzionenSalva l'indirizzo di ritorno sulla cima dello stacknPassa il controllo alla procedura chiamatanEsistono invocazioni:nravvicinate (all'interno dello stesso segmento di codice) na distanza (tra segmenti diversi, occorre salvare CS ed indirizzo di ritorno)\nRiccardo Torlone -Corso di Calcolatori Elettronici67",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#66": "Ritorno da subroutineL'istruzione RET trasferisce il controllo dalla procedura chiamata alla procedura chiamantenSintassi: RET [No argomenti]nLegge dalla cima dello stackl'indirizzo di ritorno salvato dalla precedente CALL.nRestituisce il controllo alla procedura chiamantenN.B. Al momento dell'esecuzione della RET, la cima dello stackdeve contenere l'indirizzo di ritorno\nRiccardo Torlone -Corso di Calcolatori Elettronici68",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#67": "Invocazione sempliceEsempio di subroutine con CALL e RET che non usa variabili locali e parametriMOV AX,BXCALL esempio ;Chiamo la subroutine....esempio:MOV BX,2....RET ;Restituisco il controllo al chiamante\nRiccardo Torlone -Corso di Calcolatori Elettronici69",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#68": "Invocazione con argomenti (1)\nRiccardo Torlone -Corso di Calcolatori Elettronici70\nBP",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#69": "Invocazione con argomenti (2)\nRiccardo Torlone -Corso di Calcolatori Elettronici71\nBP",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#7": "Assembler 8088 / IA32 / x86nx86: famiglia di ISA della famiglia Intel. Varie estenzioni:n8086 →… →80386 →… →Pentium IV →… →Core i7nx86-32 (IA32): linguaggio macchina dei processori x86 a 32 bitnVersione moderna: x86-64 estensione a 64 bit dell'x86nFaremo riferimento ad una delle prime versioni: 8088nversione semplificata di un microprocessore Intel modernonil relativo codice assemblerpuò essere eseguito anche su i microprocessori correntinCaratteristiche principalinArchitettura a 16 bitnIndirizzi: 20 bit (1 MB di RAM)nData bus: 8 bitnL'unita minima indirizzabile: 1 bytenRangedi indirizzi: [00000:FFFFF]8Riccardo Torlone -Corso di Calcolatori Elettronici",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#70": "Invocazione con argomenti (3)\nRiccardo Torlone -Corso di Calcolatori Elettronici72\nBP",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#71": "Invocazione con argomenti (4)\nRiccardo Torlone -Corso di Calcolatori Elettronici73\nBP",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#72": "Invocazione con argomenti (5)\nRiccardo Torlone -Corso di Calcolatori Elettronici74\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#73": "Invocazione con argomenti (6)\nRiccardo Torlone -Corso di Calcolatori Elettronici75\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#74": "Invocazione con argomenti (7)\nRiccardo Torlone -Corso di Calcolatori Elettronici76\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#75": "Stackdurante l’esecuzione di una subroutine\nRiccardo Torlone -Corso di Calcolatori Elettronici77\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#76": "Invocazione di subrountinecon argomentinLa funzione chiamante deve:nImpilare sullo stackgli argomenti della funzione in ordine inverso (dall'ultimo al primo)nTrasferire il controllo con l'istruzione CALLnLa funzione chiamata deve:nSalvare sullo stackil valore corrente del registro BPnSovrascrivere BP con il contenuto corrente di SP (inizializza il record di attivazione)nAllocare le variabili locali sullo stacknAl termine della funzione occorre:nSovrascrivere SP con il contenuto di BP (svuota il record di attivazione)nEffettuare una POP dallo stacksu BPnEseguire l'istruzione RETnRimuovere gli argomenti dallo stackRiccardo Torlone -Corso di Calcolatori Elettronici78",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#77": "Chiamate di procedura: esempioEsempio di subroutine che calcola a + b + cPUSH (c)PUSH (b)PUSH (a)CALL subroutineADD SP,6....subroutine:PUSH BPMOV BP, SPMOV AX,4(BP)MOV BX,6(BP)MOV DX,8(BP)ADD AX,BXADD AX,DXMOVE SP, BP  !Inutile in questo caso. Serve se ho allocato variabili locali.POP BPRETRiccardo Torlone -Corso di Calcolatori Elettronici79Indirizzo di ritornovecchio BPbacSPBPsituazionestackBP+4BP+6BP+8",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#78": "Chiamate di sistemanLe chiamate di sistema consentono di utilizzare le procedure fornite dal sistema operativonLe routine di sistema possono essere attivate con la sequenza di chiamata standard:nSi copiano gli argomenti sullo stacknSi impila il numero di chiamatanSi esegue l'istruzione SYSnI risultati sono restituiti nel registro AX o nella combinazione di registri AX:DX (se il risultato e di tipo long)nGli argomenti sullo stackdevono essere rimossi dalla funzione chiamante\nRiccardo Torlone -Corso di Calcolatori Elettronici80",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#79": "System Callsand System Subroutines\nRiccardo Torlone -Corso di Calcolatori Elettronici81\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#8": "I registri generali\nDisponibili 14 registri, suddivisi in 4 gruppi funzionaliRiccardo Torlone -Corso di Calcolatori Elettronici9\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#80": "EsercizioScrivere un programma in linguaggio assemblativo 8088 che, preso un numero ain memoria, calcola il quadrato del numero facendo uso di una subroutine “square” che ha come unico argomento il numero a.Il risultato deve essere stampato sullo standard output (video). ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#81": "Soluzione Esercizio III! Calcola la il quadrato di un numero con la subroutine “square”_EXIT= 1_PRINTF= 127.SECT .TEXTstart:PUSH (a)CALL squareMOV  SP,BPPUSH AXPUSH pfmtPUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYSsquare:PUSH BPMOV  BP,SPMOV  AX,4(BP)MOV  BX,AXMUL  BXPOP  BPRET.SECT .DATApfmt: .ASCIZ \"Il quadrato e' %d!\\n\"a:.WORD 3",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#82": "Esercizio VScrivere un programma in linguaggio assemblativo 8088 che calcola la somma degli elementi di un vettore vecmemorizzato in memoria principale, facendo uso di una subroutine \"vecsum\" che ha come argomento la dimensione del vettore e il vettore. Il risultato deve essere stampato sullo standard output (video). ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#83": "Soluzione Esercizio V! Stampa la somma di un arraydi interi mediante una subroutine \"vecsum\"_EXIT= 1_PRINTF= 127 .SECT .TEXTvecpstrt:PUSH vecMOV  CX,end-vecSHR  CX,1PUSH CXCALL vecsumMOV  SP,BPPUSH AXPUSH formatPUSH _PRINTFSYSMOV SP,BPPUSH 0PUSH _EXITSYSvecsum:PUSH BPMOV  BP,SPMOV  CX,4(BP)MOV  BX,6(BP)MOV  SI,0MOV  AX,01:ADD  AX,(BX)(SI)ADD  SI,2LOOP 1bMOV  SP,BPPOP  BPRET.SECT .DATAvec:   .WORD 3,4,7,11,3end:   .SPACE 1format: .ASCII \"La somma della stringa e' %d\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#84": "Operazioni su arraye stringhe: MOVSBL'istruzione MOVSB sposta un byte dalla posizione indirizzata dal registro SI alla posizione indirizzata dal registro DI.nSintassi: MOVSB [No Operandi]nL'indirizzo del byte sorgente deve trovarsi in SInL'indirizzo del byte destinazione deve trovarsi in DInAl termine dell'operazione, i registri SI/DIvengono incrementati o decrementati a seconda del valore corrente del bit di direzione nel registro di flag:nCLD: con questa istruzione, i registri SI e DIvengono incrementati (scorrimento in avanti)nSTD: con questa istruzione, i registri SI e DI vengono decrementati (scorrimento all'indietro)nMOVSB per operazioni su byte (8bit), MOVSW per operazioni su parole (16 bit).Riccardo Torlone -Corso di Calcolatori Elettronici86",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#85": "Operazioni su arraye stringhe: REPL'istruzione REP MOVSB itera l'esecuzione dell'istruzione MOVSB un numero di volte pari al valore corrente del registro CX.nSintassi: REP MOVSBnI registri SI e DIsono inizializzati con l'indirizzo della stringa sorgente e della stringa destinazione.nCX deve essere inizializzato con la lunghezza della stringa.nL'istruzione REP MOVSB ripete l'esecuzione di MOVSB finché CX=0.nIn ogni iterazione, il valore dei registri SI e DI viene incrementato/decrementato.\nRiccardo Torlone -Corso di Calcolatori Elettronici87",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#86": "Esempio (1)\nRiccardo Torlone -Corso di Calcolatori Elettronici88\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#87": "Esempio (2)\nRiccardo Torlone -Corso di Calcolatori Elettronici89\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#88": "Esempio (3)\nRiccardo Torlone -Corso di Calcolatori Elettronici90\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#89": "Esempio (4)\nRiccardo Torlone -Corso di Calcolatori Elettronici91\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#9": "Riccardo Torlone -Corso di Calcolatori Elettronici10Confronto con i registri del Core i7\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#90": "Operazioni su array e stringhe: LODSBL'istruzione LODSB trasferisce il contenuto del byte di memoria indirizzato dal registro DI nel registro AL.nSintassi: LODSB [No Operandi]nL'indirizzo di memoria del dato da trasferire deve trovarsi in DI.nLa destinazione è il registro AL.nAl termine dell'operazione, il registro DI viene incrementato o decrementato a seconda del valore corrente del bit di direzione nel registro di flag.nLODSB per operazioni su byte (8bit), LODSW per operazioni su parole (16 bit, destinazione registro AX).\nRiccardo Torlone -Corso di Calcolatori Elettronici92",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#91": "Operazioni su arraye stringhe: STOSBL'istruzione STOSB trasferisce il contenuto del registro AL nel byte di memoria indirizzato dal registro DI.nSintassi: STOSB [No Operandi]nIl dato da trasferire deve trovarsi in AL.nL'indirizzo di memoria della destinazione deve trovarsi in DI.nAl termine dell'operazione, il registro DI viene incrementato o decrementato a seconda del valore corrente del bit di direzione nel registro di flag.nSTOSB per operazioni su byte (8bit), STOSW per operazioni su parole (16 bit).\nRiccardo Torlone -Corso di Calcolatori Elettronici93",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#92": "Operazioni su arraye stringhe: CMPSBL'istruzione CMPSB confronta due byte, indirizzati rispettivamente dal registro SI e dal registro DI.nSintassi: CMPSB [No Operandi]nL'indirizzo del primo byte deve trovarsi in SI.nL'indirizzo del secondo byte deve trovarsi in DI.nAl termine dell'operazione, i registri SI/DIvengono incrementati o decrementati a seconda del valore corrente del bit di direzione nel registro di flag.nCMPSB per confronto tra byte (8bit), CMPSW per confronto tra parole (16 bit).\nRiccardo Torlone -Corso di Calcolatori Elettronici94",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#93": "Operazioni su arraye stringhe: SCASBL'istruzione SCASB confronta il byte indirizzato dal registro DI con il contenuto del registro AL.nSintassi: SCASB [No Operandi]nL'indirizzo del primo byte deve trovarsi in DI.nIl secondo byte deve trovarsi in AL.nAl termine dell'operazione, il registri DIviene incrementato o decrementato a seconda del valore corrente del bit di direzione nel registro di flag.nIl registro AL non viene alterato, ma i bit del registro di flagvengono modificatinSCASB per operazioni su byte (8bit), SCASW per operazioni su parole (16 bit, confronto con registro AX).Riccardo Torlone -Corso di Calcolatori Elettronici95",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#94": "Operazioni su arraye stringhe: REP, REPE, REPNEnREP {MOVSB|MOVSW|LODSB|LODSW|STOSB|STOSW}nRipete l'istruzione fino a fine stringa (CX=0).nREPE (o REPZ) {SCASB|SCASW|CMPSB|CMPSW}nRipete l'istruzione fino a fine stringa (CX=0), oppure fino a quando il confronto fallisce (se ZF=0 ripete, se ZF=1 si ferma).nREPNE (o REPNZ) {SCASB|SCASW|CMPSB|CMPSW}nRipete l'istruzione fino a fine stringa (CX=0), oppure fino a quando il confronto ha successo (se ZF=1 ripete, se ZF=0 si ferma)\nRiccardo Torlone-Corso di Calcolatori Elettronici96",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#95": "Esercizio VICon riferimento al programma assemblativo 8088 che segue, indicare cosa fa e il valore stampato. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#96": "Esercizio VI_EXIT = 1_PRINTF= 127 .SECT .TEXTstart:MOV  CX,num-vecSHR  CX,1MOV BX,vecMOV  SI,0MOV  AX,(num)1:CMP  AX,(BX)(SI)JE   2fADD  SI,2LOOP 1bMOV  DX,0JMP  3f2:      MOV  DX,13:PUSH DXPUSH formatPUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYS.SECT .DATAvec:   .WORD 3,4,7,11,3num:   .WORD 5format: .ASCII \"%d\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#97": "Esercizio VIbis! Equivalente al VI utilizzando pero'l'istruzione SCASW insieme alla REPNE_EXIT= 1_PRINTF= 127 .SECT .TEXTstart:MOV  CX,num-vecSHR  CX,1MOV  AX,(num)MOV  DI, vecCLDREPNE SCASWJE   1fMOV  DX,0JMP  2f1:   MOV  DX,12:PUSH DXPUSH formatPUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYS.SECT .DATAvec:   .WORD 3,4,7,11,3num:   .WORD 11format: .ASCII \"%d\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#98": "Esercizio VIIScrivere un programma in linguaggio assemblativo 8088 che verifica se due vettori di interi memorizzati in memoria principale sono identici. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici2\\08 Il linguaggio assemblativo 8088.pdf#99": "Esercizio VII_EXIT= 1_PRINTF= 127 .SECT .TEXTinizio:MOV  CX,end1-vec1SHR  CX,1MOV  AX,end2-vec2SHR  AX,1CMP  AX,CXJNE  1fMOV  SI,vec1MOV  DI,vec2CLDREPE CMPSWJNE  1fPUSH ugualiJMP  2f1:PUSH diversi2:PUSH _PRINTFSYSMOV  SP,BPPUSH 0PUSH _EXITSYS.SECT .DATAvec1: .WORD 3,4,7,11,3end1: .SPACE 1vec2: .WORD 3,4,7,11,3end2: .SPACE 1uguali: .ASCII \"Uguali!\\0\"diversi: .ASCII \"Diversi!\\0\".SECT .BSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#0": "Calcolatori  Elettronici T  Complementi ed Esercizi  di Reti Logiche  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#1": "Stefano Mattoccia  Ricevimento : su appuntamento via email    Telefono  : 051 2093860  Email   : stefano.mattoccia@unibo.it  Web   : www.vision.deis.unibo.it/smatt ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#10": "OE=0 0 1 U=? Quale valore logico assume U ? \nChe cosa è necessario garantire nella rete seguente ?  Quando il segnale U assume un valore logico significativo ? 1 U=? OE1 OE2 I1 I2 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#11": "Esercizio 1  Registro a 1 bit con uscita tri-state Utilizzando latch SR progettare una rete che, quando WE=1,memorizza sull\u0001uscita OUT il segnale di ingresso IN. L\u0001ultimo valore trasferito in uscita deve essere mantenuto per tutto il tempo in cui il segnale WE=0. La rete deve essere inoltre dotata di un segnale OE che, se a livello logico 0, pone il segnale di OUT nello stato di alta impedenza. WE IN OUT OE ? WE IN OE OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#12": "S Q Q* R WE IN OE OUT Soluzione \nLa rete tratteggiata (8X) è un latch CD dotato di uscita tri-state ed esiste in forma integrata (‘373). Q \nNOTA - Perché le due reti seguenti NON sono equivalenti ? a b c b a c ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#13": "RSA notevoli: Flip-Flop D FFD D CK Q Q* D CK Q Q* FFD: RSA che assume il valore logico presente sull’ingresso D durante i fronti di salita (positive edge triggered) dell’ingresso CK \nIl FFD è tipicamente utilizzato come cella elementare di memoria  nelle reti sequenziali sincrone. In tal caso, il segnale CK, è un segnale di tipo periodico (clock). CK D Q ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#14": "FFD D CK Q Q* D CK Q Q* A_SET* A_RES* \nA_RES* A_SET* \nCK D Q* Q I FFD sono dotati di due ulteriori ingressi “asincroni” che consentono di settare (A_SET) o resettare (A_RES) Q indipendentemente da CK e D. A_SET* \nA_RES* Tipica realizzazione di  un FFD della famiglia  TTL (‘374) mediante 3  latch SR.  Q=0 se A_RES=1 Q=1 se A_SET=1  A_SET e A_RES sono  prioritari rispetto  a CK e D NOTA: i segnali asincroni di set e reset denominati nella slide (rispettivamente) A_SET e A_RES  sono spesso denominati (rispettivamente) PR e CL oppure S e R. Inoltre, se non indicati nello  schema logico si suppone che tali comandi siano non asseriti (A_SET=0 e A_RES=0). ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#15": "Vincoli di corretto impiego per i FFD   Tempi di Setup (τSU), Hold (τH) e Risposta (τR) FFD D CK Q Q* D CK Q Q* CK D Q τH τSU τR Il corretto funzionamento è garantito solo se τSU≥ τSUmin e τH ≥ τHmin. In caso contrario, metastabilità.   Cosa implicano i parametri τSUmin e τRmin indicati nei datasheet ? ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#16": "Il FFD come elemento fondamentale delle RSS \nD CK Q Se all’ingresso CK viene inviato un segnale periodico (clock):   il FFD ritarda (D = Delay) il segnale di uscita Q, rispetto al  segnale di ingresso D, di un tempo pari al periodo di clock T  Qn+1 = Dn FFD D CK Q Q* D CK Q Q* \nT T T T ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#17": "Vincoli di campionamento e metastabilità Il mancato rispetto dei vincoli sul campionamento dei segnali porta  a metastabilità.  CK D Q τSU τH ???????????? \n0 1 metastabile \nstabile stabile ? ? 1? 0? τ = ??? ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#18": "Sincronizzazione di segnali (non sincroni) FFD D CK Q I metastabile FFD D CK Q Stabile (?) I_sync I_M Normalmente i segnali provenienti dall’esterno (ma non solo) non sono sincroni con il clock della RSS. Questo è un problema molto comune.   Come gestire potenziali situazioni di metastabilità che potrebbero  compromettere il corretto funzionamento della RSS?  \nCK •  La soluzione mostrata garantisce che l’uscita I_sync assume il valore   di I nel momento in cui tale segnale è stato campionato?   •  Sono sufficienti due livelli di FF?  •  Quali sono gli effetti collaterali di questa soluzione? ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#19": "Reti Sequenziali Sincrone (RSS) ? k (k) FFD k k FFD sull’anello di retroazione  Tutti con lo stesso clock di periodo T S S* S S* CK S U S* It t+T t+2·T t-T Nel caso specifico: Moore o Mealy ? Lo stato cambia anche se non cambia l’ingresso ? L’uscita cambia anche se non cambia l’ingresso ? CK ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#2": "Introduzione Reti Logiche: sintesi mediante approccio “formale” \nCalcolatori Elettronici: sintesi mediante approccio “diretto” Grafo degli Stati Tabella di Flusso Tabella delle Transizioni Sintesi (Karnaugh, etc) Specifiche del Problema RL \nGrafo degli Stati Tabella di Flusso Tabella delle Transizioni Sintesi (Karnaugh, etc) Specifiche del Problema RL ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#20": "Alcune considerazioni sulle RSS •  Lo stato della rete cambia solo in corrispondenza dei fronti di   salita del clock che si susseguono con periodo T  •  La rete risponde ogni T ⇒ se si desidera massimizzare la velocità   di risposta della rete è necessario adottare il modello di Mealy   •  La rete è svincolata dai ritardi della rete G! Quindi, nessun   problema di corse critiche (purché T > τSUmin + τRmin !)  •  All’interno di uno stesso progetto sono tipicamente presenti più    RSS e non necessariamente per tutte le RSS il clock è lo stesso   e/o coincide con il clock del processore  •  Le RSS sono (più) facili da progettare delle RSA ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#21": "Clock gating e glitch sul clock Nelle reti sincrone è necessario evitare variazioni spurie (glitch) del segnale di clock che possono provocare commutazioni indesiderate dei FFD.   Ad esempio, per via dei reciproci ritardi tra i t segnali D[t-1..0] e/o le alee introdotte dalla rete combinatoria di decodifica, a causa del “clock gating“, può verificarsi quanto segue FFD D CK Q Q* D CK Q Q* CK P CK_G \nCK_G Glitch sul clock → commutazione spuria del FFD ! NO !! Rete di  Decodifica D[t-1..0] P t ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#22": "Il clock gating, oltre a generare potenziali glitch introduce  “clock-skew”. Ad esempio, consideriamo le due RSS seguenti Clock gating e clock-skew \nCK CK_G τAND FFD D CK Q Q* I1 CK B B* CK_G 1 FFD D CK Q Q* I2 CK A A* τAND \nτAND I clock delle due reti sono sfasati di un tempo pari al ritardo introdotto dall’AND. Tale fenomeno (“clock-skew”) è potenzialmente dannoso. Perchè ? \nIl “clock-skew” non è causato solo dal clock gating ma anche (ad esempio) da percorsi elettrici di lunghezza diversa. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#23": "Esercizio 2  Progettare un registro a 8 bit con uscita tri-state utilizzando FFD positive edge triggered.  La rete, ad ogni fronte di salita del clock, memorizza il byte IN[7..0] in ingresso se WE=1 mentre mantiene il valore precedentemente memorizzato in caso contrario (WE=0). L’uscita OUT[7..0] della rete deve essere posta nello stato di alta impedenza quando il segnale OE=0. Inoltre, la rete deve essere dotata di un ingresso asincrono di RESET (A_RESET) che, se 1, pone al livello logico 0 l’uscita OUT[7..0] indipendentemente dal valore dei segnali WE, IN e CK. Quali condizioni devono essere soddisfatte perché sia garantito il corretto funzionamento della rete ? ? WE A_RESET IN[7..0] CK OUT[7..0] OE WE IN[7..0] OE OUT[7..0] ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#24": "WE OE FFD D Q Q* R IN OUT 0 1 Q A_RESET Soluzione Caso singolo bit \nNOTA  - Per garantire il corretto funzionamento della rete è   necessario rispettare tempi di setup e hold  - Il FFD esiste (8X) in forma integrata (74XX374) ed è dotato   di comando di OE CK ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#25": "NOTA  - La soluzione seguente NON è corretta in quanto:       a) variazioni spurie (glitch), dovute a instabilità del        segnale WE, possono causare commutazioni indesiderate         del flip-flop       b) il gate ritarda il segnale di clock del FFD e potrebbe        causare potenziali sfasamenti (“clock-skew”) tra i clock        dei vari componenti della rete sincrona complessiva WE OE FFD D Q Q* R IN OUT Q A_RESET CK ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#26": "FFD D Q Q* R IN7 WE OE OUT7 0 1 \nFFD D Q Q* R IN1 OUT1 0 1 \nFFD D Q Q* R IN0 OUT0 0 1 Q7 \nQ1 Q0 A_RESET Estensione a 8 bit CK ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#27": "Estensione a 8 bit (meglio) \nWE OE FFD D Q Q* R IN[7..0] OUT[7..0] 0 1 Q[7..0] A_RESET CK 8 8 8 8 8 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#28": "Esercizio 3  Progettare una rete che periodicamente dopo tre periodi di clock setta al livello logico 1 la propria uscita per un periodo clock. \nA_RESET CK OUT \nCK OUT (0) (1) (2) (0) (1) (2) (3) (3) ? OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#29": "COUNTER X4 Una possibile soluzione si basa sull\u0001utilizzo di un contatore modulo 4.  Soluzione 3.1  \nCK u1 u0 OUT A_RESET Progettare un contatore modulo 4…. A_RES Perchè ? u1 u0 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#3": "Modello della Macchina a Stati Finiti (FSM) - Mealy F G k n I? r U S S* U=F(S,I) S*=G(S,I) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#30": "FFD D Q Q* FFD D Q Q* XOR u0 u1 R* R* CK A_RESET* 0  0 0  1 1  0 1  1 u1 u0 Contatore modulo 4 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#31": "Contatore modulo 4 con comando di ENABLE (EN) \nFFD D Q Q* FFD D Q Q* XOR u0 u1 R* R* CK A_RESET* 0 1 EN 0 1 EN ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#32": "0  0 0  1 1  0 1  1 u1 u0 Contatore modulo 4 UP/DOWN (U/D*)  \nFFD D Q Q* FFD D Q Q* XOR u0 u1 R* R* CK A_RESET* 0 1 U/D* ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#33": "Contatore modulo 4 con LOAD (L) FFD D Q Q* FFD D Q Q* XOR u0 u1 R* R* CK A_RESET* 1 0 L 1 0 L i0 i1 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#34": "Esercizi E3-1) Progettare un contatore modulo 4 dotato dei segnali       U/D*, EN e L nei seguenti 2 casi:   a) segnale L prioritario rispetto a EN    b) segnale EN prioritario rispetto a L       In entrambi i casi si supponga che U/D* sia il     segnale meno prioritario tra i tre.  E3-2) Progettare un contatore modulo 8  E3-3) Progettare un contatore modulo 5 utilizzando un       contatore modulo 8 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#35": "Osservando le forme d\u0001onda mostrate sotto si può ottenere una soluzione alternativa alla precedente (3.1) Soluzione 3.2  CK u1 u0 OUT (0) (1) (2) (0) (1) (2) (3) (3) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#36": "FFD D Q Q* R* CK A_RESET* FFD D Q Q* R* OUT NOTA - Questa soluzione non può essere ottenuta con il metodo   della sintesi formale studiato a Reti Logiche ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#37": "ττττNOTA - Non è il caso della rete della pagina precedente, ma la   presenza di alee può creare problemi alle reti che seguono   se queste utilizzano come ingresso di clock un segnale che    presenta oscillazioni spurie (glitches).    Si consideri ad esempio il caso seguente: \nFFD D Q Q* c b a 1 1 IN OUT S u u S τττ\nAlea statica: provoca un campionamento indesiderato del FFD ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#38": "NOTA -  Le alee possono essere eliminate introducendo ulteriori   gates (vedi reti logiche)   -  In alcuni casi le alee possono essere filtrate dagli   stessi gates (ad esempio nel caso di ‘lentezza’ dei   dispositivi rispetto ai tempi del glitch); questa   possibilità deve essere verificata attentamente   analizzando i datasheets dei componenti utilizzati a b c a b c Un impulso troppo breve potrebbe essere filtrato dall\u0001AND ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#39": "Soluzione canonica ottenuta mediante sintesi formale. Soluzione 3.3  \nA,0 B,0 C,0 D,1 Grafo degli stati  \nTabella di flusso  sn sn+1 sn,u u A B 0 B C 0 C D 0 D A 1 Tabella delle  transizioni  y1n y0n u 0 0 0  1 0 0 1 1  0 0 1 0 1  1 0 1 1 0  0 1 y1n+1 y0n+1 Sintesi minima (mappe di Karnaugh,…) u = y1n·y0n y0n+1 = y0n* y1n+1 = y1n XOR y0n ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#4": "F G k n I? r U S S* U=F(S) S*=G(S,I) Modello della Macchina a Stati Finiti (FSM) - Moore ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#40": "FFD D Q Q* FFD D Q Q* XOR y0 y1 R* R* CK u NOTA  - Se si desidera aggiungere un segnale di ENABLE alla rete   precedente mediante il metodo della sintesi formale ?   - E\u0001 necessario ripetere tutti i passi precedenti (grafo,   diagramma stati, …) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#41": "Esercizio 4  Progettare un registro a scorrimento (shift-register) a 3 bit. ? IN A_RESET CK OUT1 OUT2 OUT0 IN A_RESET O1 O2 O0 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#42": "CK IN A_RESET OUT1 Soluzione \nOUT2 OUT0 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#43": "FFD D Q Q* R* A_RESET* FFD D Q Q* R* A_RESET* FFD D Q Q* R* A_RESET* IN OUT2 OUT1 OUT0 \nCK Esercizi E4-1) Progettare uno shift-register dotato di comandi         di enable EN e LOAD (parallelo e prioritario         rispetto all\u0001enable).  E4-2) Utilizzando due shift-register a 4 bit e un        contatore modulo 8: progettare un convertitore         serie parallelo a 8 bit dotato di un segnale (ACK)    che comunica l\u0001avventura ricezione degli 8 bit.  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#44": "Esercizio 5  Progettare una rete sincrona dotata di un ingresso  IN e di un’uscita OUT. L’uscita OUT deve asserirsi esattamente per un periodo di clock se viene rilevata una transizione da 0 a 1 del segnale di ingresso (monoimpulsore). Si noti che il segnale di ingresso potrebbe anche essere non sincrono (purché rispetti tempi di setup  e hold) ? IN CK OUT CK IN OUT IN OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#45": "FFD D Q Q* FFD D Q Q* IN OUT CK Soluzione \nCK IN OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#46": "FFD D Q Q* IN OUT CK Perchè questa soluzione è sbagliata (1) ? \nCK IN OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#47": "Perchè questa soluzione è sbagliata (2) ? \nFFD D Q Q* IN OUT CK CK IN OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#48": "Perchè questa soluzione è sbagliata (3) ? \nFFD D Q Q* IN OUT CK CK IN OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#49": "Esercizio 6  Progettare un rete che controlla se gli ultimi tre caratteri che si sono presentati sull’ingresso IN[7..0] mentre il segnale EN era a livello logico 1 sono stati FFh (primo carattere della sequenza), 27h e 30h. Nel caso sia rilevata la sequenza FF-27-30, nel periodo di clock successivo a quello dell’ultimo carattere ricevuto (30h), deve essere asserita l’uscita OUT e rimanere tale fino a che non viene asserito il segnale (asincrono) di reset A_RESET. In seguito ad un reset deve riprendere immediatamente il controllo della sequenza in ingresso come se non fosse stato ricevuto alcun carattere. ? EN A_RESET IN[7..0] CK OUT EN A_RESET IN[7..0] OUT ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#5": "Reti Sequenziali Asincrone (RSA) ? k τk Retroazione diretta (τ: ritardo intrinseco della RC G) S U S* IS S* S S* \nt t+τ(1) (2) (3) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#50": "CK IN[7…0] A_RESET EN 30h FFh FFh 27h 55h 30h 30h 16h 80h \nOUT (1) (2) (3) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#51": "374 D Q Q* 0 1 EN 8 8 374 D Q Q* 0 1 EN 8 8 30h IN[7…0] 27h FFh 8 \nEN \nFFD D Q Q* 1 0 OUT R* R* R* A_RESET* \nA_RESET* A_RESET* \nIl segnale EN condiziona l\u0001ultimo carattere della sequenza CK CK \nCK DEC_30 DEC_27 DEC_FF OE* OE* 0 0 Soluzione 6.1 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#52": "Soluzione 6.2 \nCK A_RESET* LOAD ENABLE 0 0 COUNTER X4 EN RES* Q1 Q0 L I1    I0 DEC 2:4 I1 I0 O1 O0 O3 O2 EN 1 OUT ATTESO_30 ATTESO_27 ATTESO_FF 30h IN[7…0] 27h FFh 8 DEC_30 DEC_27 DEC_FF LOAD = ATTESO_FF·EN·DEC_FF* + ATTESO_27·EN·DEC_27* + ATTESO_30·EN·DEC_30*  ENABLE = ATTESO_FF·EN·DEC_FF + ATTESO_27·EN·DEC_27 + ATTESO_30·EN·DEC_30  Una soluzione alternativa utilizzando un contatore dotato di  comando di LOAD \nC\u0001è un problema… ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#53": "CK A_RESET* LOAD ENABLE 0 COUNTER X4 EN RES* Q1 Q0 L I1    I0 DEC 2:4 I1 I0 O1 O0 O3 O2 EN 1 OUT ATTESO_30 ATTESO_27 ATTESO_FF 30h IN[7…0] 27h FFh 8 DEC_30 DEC_27 DEC_FF LOAD = ATTESO_FF·EN·DEC_FF* + ATTESO_27·EN·DEC_27* + ATTESO_30·EN·DEC_30*  ENABLE = ATTESO_FF·EN·DEC_FF + ATTESO_27·EN·DEC_27 + ATTESO_30·EN·DEC_30  .. nella soluzione della pagina precedente cosa accade se i  caratteri ricevuti (con EN=1) sono FF-FF-27-30 ? \nDEC_FF ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#54": "Esercizi E5-1) Riprogettare la rete dell\u0001esercizio 6 in modo che     OUT assuma il valore logico 1 in seguito alla     ricezione anche non consecutiva (con EN=1) dei     caratteri FFh, 27h e 30h.          Ad esempio, OUT=1 se i caratteri ricevuti (mentre     EN=1) sono stati: FF-7A-80-9F-27-B2-30-…  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#55": "Esercizio 7  Modificare l\u0001esercizio precedente in modo che, in seguito al rilevamento della sequenza, l\u0001uscita OUT assuma il valore logico 1 per un solo periodo di clock. Appena ricevuta una sequenza completa il controllo dei caratteri in ingresso deve riprendere immediatamente. ? EN A_RESET IN[7..0] CK OUT OUT EN A_RESET IN[7..0] ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#56": "CK IN[7…0] A_RESET EN 30h FFh FFh 27h 55h 30h 30h 16h 80h \nOUT (1) (2) (3) Soluzione 7.1 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#57": "374 D Q Q* 0 1 EN 8 8 374 D Q Q* 0 1 EN 8 8 30h IN[7…0] 27h FFh 8 \nEN \nFFD D Q Q* 1 0 OUT R* R* R* A_RESET* A_RESET* \nA_RESET* CK CK \nCK ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#58": "Soluzione 7.2 \nCK A_RESET LOAD ENABLE 0 COUNTER X4 EN RES* Q1 Q0 L I1    I0 DEC 2:4 I1 I0 O1 O0 O3 O2 EN 1 OUT ATTESO_30 ATTESO_27 ATTESO_FF 30h IN[7…0] 27h FFh 8 DEC_30 DEC_27 DEC_FF LOAD = ATTESO_FF·EN·DEC_FF* + ATTESO_27·EN·DEC_27* + ATTESO_30·EN·DEC_30* + OUT ENABLE = ATTESO_FF·EN·DEC_FF + ATTESO_27·EN·DEC_27 + ATTESO_30·EN·DEC_30  Rispetto all\u0001esercizio 6.2 è sufficiente modificare il comando di LOAD facendo in modo che LOAD=1 quando OUT=1 ?  \nEN·DEC_FF \nCosa accade se (con EN=1) la sequenza è 45-FF-27-30-FF-27-30-… ? ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#59": "Esercizi E6-1) Riprogettare la rete dell\u0001esercizio 6 in modo che    OUT=1 in seguito alla ricezione anche non consecutiva    (con EN=1) dei caratteri FFh, 27h e 30h.         Ad esempio, OUT=1 se i caratteri ricevuti mentre EN=1    sono stati: FF-7A-80-9F-27-B2-30-…   E6-2) Cosa accade alle soluzioni 6.1 e 6.2 se (mentre EN=1)       la sequenza è: 45-FF-27-30-FF-27-30-… ?  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#6": "•  Le reti asincrone rispondono molto rapidamente (appena possibile)   alle variazioni degli ingressi •  Non è necessario un segnale di sincronismo (clock) •  Ridotta dissipazione di potenza Aspetti positivi delle RSA (vs RSS) Aspetti negativi delle RSA (vs RSS) •  Vincoli per il corretto impiego   - l’ingresso può variare solo quando la rete ha raggiunto     una condizione di stabilità   - i segnali di ingresso possono variare uno alla volta •  Esposte a potenziali malfunzionamenti (corse critiche)  •  Difficili da progettare In pratica, sono utilizzate per realizzare latch e flip-flop.  A noi interessano (maggiormente) le reti sincrone (RSS) ! ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#60": "Esercizio 8  Progettare un rete che controlla se gli ultimi tre caratteri che si sono presentati in ingresso IN[7..0] mentre il segnale EN=1 sono stati FFh (primo carattere della sequenza), 27h  e 30h. Nel caso sia rilevata tale sequenza, due periodi di clock successivi a quello dell’ultimo carattere della sequenza ricevuto deve essere asserita l’uscita OUT e rimanere tale fino a che il segnale di reset (asincrono) A_RESET non assume il valore logico 1. In seguito ad un reset (asincrono) la rete deve riprendere immediatamente  il controllo della sequenza in ingresso come se non fosse stato ricevuto alcun carattere.  ? EN A_RESET IN[7..0] CK OUT OUT EN A_RESET IN[7..0] ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#61": "CK IN[7…0] A_RESET EN 30h FFh FFh 27h 55h 30h 18h 16h 80h \nOUT (1) (2) (3) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#62": "374 D Q Q* 0 1 EN 8 8 374 D Q Q* 0 1 EN 8 8 30h IN[7…0] 27h FFh 8 \nEN \nFFD D Q Q* 1 0 OUT R* R* R* A_RESET* \nA_RESET* A_RESET* \nIl segnale EN condiziona l\u0001ultimo carattere della sequenza CK CK \nCK FFD D Q Q* R* A_RESET* CK Soluzione 8.1 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#63": "CK A_RESET* LOAD ENABLE 0 COUNTER X4 EN RES* Q1 Q0 L I1    I0 DEC 2:4 I1 I0 O1 O0 O3 O2 EN 1 OUT ATTESO_30 ATTESO_27 ATTESO_FF 30h IN[7…0] 27h FFh 8 DEC_30 DEC_27 DEC_FF LOAD = (ATTESO_FF·EN·DEC_FF* + ATTESO_27·EN·DEC_27* + ATTESO_30·EN·DEC_30*)·OUT_1*  ENABLE = (ATTESO_FF·EN·DEC_FF + ATTESO_27·EN·DEC_27 + ATTESO_30·EN·DEC_30)·OUT_1*  DEC_FF Soluzione 8.2 FFD D Q Q* R* A_RESET* CK OUT_1 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#64": "Esercizio 9  Progettare una rete dotata di tre ingressi E, A/I*, A_RES e un\u0001uscita OUT. Il segnale di ingresso A/I* influisce sulla rete solo se contemporaneamente E=1. L\u0001uscita della rete deve andare al livello logico 1 per un periodo di clock se viene rilevato per cinque volte, anche non consecutive, il valore 1 del segnale A/I* in presenza del segnale E=1. Ogni volta che il segnale A/I* assume il valore 0 (con E=1) deve essere ridotto di uno il numero di eventi rilevati fino a quel momento. Successivamente a un reset (segnale asincrono) o nel caso nessun evento sia stato ancora rilevato (o che il numero di incrementi sia stato compensato da un numero equivalente di decrementi la rete deve rimanere nello stato 000 anche se A/I*=0 ed E=1. Dopo avere rilevato cinque eventi la rete deve riprendere l’analisi degli ingressi. ? E A/I* A_RESET CLOCK OUT OUT E A/I* A_RES ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#65": "COUNTER X 8 EN U/D# LOAD I2 I1 I0 O2 O1 O0 E A/I* OUT OUT CLOCK 0 0 A/I* \nO2 O1 O0 A/I* RESET A_RESET Soluzione 9.1 E L’OR blocca il conteggio (EN=0), anche con E=1, se il contatore si trova nello stato 000 e il comando DOWN è asserito (A/I*=0). Perché ? \nO1 è strettamente necessario ?  (No, perché ?) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#66": "A,0 B,0 C,0 D,0 E,0 F,1 E A/I* 0 – 1 0 0 – 0 – 0 – 0 – 1 1 1 1 1 1 1 1 1 1 1 1 0 – 1 0 1 0 1 0 1 0 1 0 Soluzione mediante sintesi formale: grafo -> tabella di  flusso -> tabella delle transizioni,... NON SI USA !!!! Soluzione 9.2 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#67": "Esercizio 10  Utilizzando un microprocessore dotato di un bus indirizzi a 16 bit e di un bus dati a 8 bit: mappare nello parte bassa dello spazio di indirizzamento 12k di RAM e nella parte alta 16k di EPROM. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#68": "Soluzione RAM (12K) \nEPROM (16K) 0000h  2FFFh \nC000h FFFFh A15..A12 A11..A8 A7..A4 A3..A0 0000 0000 0000 0000 (0000h)  \n1111 1111 1111 1111 (FFFFh) 0010 1111 1111 1111  (2FFFh) \n1100 0000 0000 0000 (C000h) RAM_1 (8k) RAM_2 (2k) RAM_3 (2k) \nEPROM (16k) 0001 1111 1111 1111 (1FFFh)  0010 0000 0000 0000  (2000h) 0010 0111 1111 1111  (27FFh) 0010 1000 0000 0000  (2800h) CS_RAM_1=A15*·A13* CS_RAM_2=A15*·A13· A11* CS_RAM_3=A15*·A13· A11 CS_EPROM=A15 Segnali di decodifica: ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#69": "- Il segnale CS_EPROM si attiva per ogni indirizzo maggiore   o uguale di 8000h (seconda metà dello spazio di   indirizzamento) 0000h  \nC000h FFFFh 8000h Indirizzi di memoria con A15=1 CS_EPROM=A15 NOTA  - La codifica semplificata implica l\u0001attivazione dei   segnali di selezioni anche per indirizzi diversi da   quelli in cui sono realmente presenti i dispositivi    di memoria.  \nEPROM (16K) EPROM (16K) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#7": "RSA notevoli: Latch SR SR S R Q Q* S R Q Q* S R 0 0 0 1 1 0 1 1 Q Q* Q Q* 0 1 1 0 \nQ = S’ ↑ (q ↑ R’) S’ R’ Q Q*  Q = R ↓ (S ↓ q) S R Q Q*  \nI comandi di set e reset devono avere una durata minima (vedi datasheet) per consentire il raggiungimento della condizione di stabilità ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#70": "- Il segnale CS_RAM_1 si attiva per ogni indirizzo compreso   tra 0000h e 7FFFh (A15=0) per il quale A13=0:  0000h  \nFFFFh 8000h CS_RAM_1=A15*·A13* RAM_1 (8k) A15..A12  A11..A8    A7..A4   A3....A0 0000  0000  0000  0000 (0000h)  0001  1111  1111  1111 (1FFFh)  A15..A12  A11..A8    A7..A4   A3....A0 0100  0000  0000  0000 (4000h)  0101  1111  1111  1111 (5FFFh)  Quindi, CS_RAM_1=1 per entrambi i  seguenti intervalli di memoria: 1FFFh  4000h RAM_1 (8k) 5FFFh ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#71": "- Il segnale CS_RAM_2 si attiva per ogni indirizzo compreso   tra 0000h e 7FFFh (A15=0) per il quale A13=1 e A11=0 :  0000h  \nFFFFh 8000h CS_RAM_2=A15*·A13·A11* A15..A12  A11..A8    A7..A4   A3....A0 0010  0000  0000  0000 (2000h)  0010  0111  1111  1111 (27FFh)  A15..A12  A11..A8    A7..A4   A3....A0 0011  0000  0000  0000 (3000h)  0011  0111  1111  1111 (37FFh)  Quindi, CS_RAM_2=1 per i seguenti quattro intervalli di memoria: 2000h  4000h 6000h A15..A12  A11..A8    A7..A4   A3....A0 0110  0000  0000  0000 (6000h)  0110  0111  1111  1111 (67FFh)  A15..A12  A11..A8    A7..A4   A3....A0 0111  0000  0000  0000 (7000h)  0111  0111  1111  1111 (77FFh)  RAM_2 (2k) RAM_2 (2k) 3000h  RAM_2 (2k) RAM_2 (2k) 7000h ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#72": "- Il segnale CS_RAM_3 si attiva per ogni indirizzo compreso   tra 0000h e 7FFFh (A15=0) per il quale A13=1 e A11=1 :  0000h  \nFFFFh CS_RAM_3=A15*·A13·A11 A15..A12  A11..A8    A7..A4   A3....A0 0010  1000  0000  0000 (2800h)  0010  1111  1111  1111 (2FFFh)  A15..A12  A11..A8    A7..A4   A3....A0 0011  1000  0000  0000 (3800h)  0011  1111  1111  1111 (3FFFh)  Quindi, CS_RAM_3=1 per i seguenti quattro intervalli di memoria: 2800h  6800h A15..A12  A11..A8    A7..A4   A3....A0 0110  1000  0000  0000 (6800h)  0110  1111  1111  1111 (6FFFh)  A15..A12  A11..A8    A7..A4   A3....A0 0111  1000  0000  0000 (7800h)  0111  1111  1111  1111 (7FFFh)  RAM_3 (2k) RAM_3 (2k) 3800h  RAM_3 (2k) RAM_3 (2k) 7800h ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#73": "0000h  \nFFFFh 2800h  RAM_2 (2k) RAM_3 (2k) RAM_1 (8k) 2000h  3800h  RAM_2 (2k) RAM_3 (2k) 3000h  4000h  6800h  RAM_2 (2k) RAM_3 (2k) RAM_1 (8k) 6000h  7800h  RAM_2 (2k) RAM_3 (2k) 7000h  EPROM (16K) EPROM (16K) 8000h  C000h Effetto di replica nella mappatura in memoria dovuto alla  decodifica semplificata. Nella figura seguente sono indicati  solo gli indirizzi iniziali. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#74": "Esercizio 11  Utilizzando un microprocessore dotato di un bus indirizzi a 20 bit e di un bus dati a 8 bit:  - mappare nello parte bassa dello spazio di indirizzamento    32k di RAM e nella parte alta 32k di EPROM   Nel sistema sono presenti anche due dispositivi di I/O denominati D1 (dotato di due registri interni) e D2 (dotato di quattro registri interni):  - mappare in memoria anche i due dispositivi di I/O D1 e    D2 agli indirizzi 2000h e 1000h  Osservando che esiste una sovrapposizione tra gli indirizzi di una memoria e dei due dispositivi di IO, si scrivano i CS, in forma semplificata, di tutti i dispositivi presenti nel sistema riducendo al minimo gli indirizzi “sottratti” dai dispositivi di IO alla memoria. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#75": "Soluzione  RAM: 1 chip da 32KB RAM  (00000h->07FFFh) CS_RAM = BA19*·CS_D1*·CS_D2*   EPROM: 1 chip da 32KB EPROM (F8000h – FFFFFh) CS_EPROM = BA19   D1: Mappato in memoria all\u0001indirizzo 02000h, occupa 2 locazioni (A0)     nello spazio di indirizzamento. CS_D1 = BA19*·BA14*·BA13·BA12*·BA11*·BA10*·BA9*·BA8*·BA7*·BA6*·          BA5*·BA4*·BA3*·BA2*·BA1*   D2: Mappato in memoria all\u0001indirizzo 01000h, occupa 4 locazioni (A1A0) nello spazio di indirizzamento.  CS_D2 = BA19*·BA14*·BA13*·BA12·BA11*·BA10*·BA9*·BA8*·BA7*·BA6*·       BA5*·BA4*·BA3*·BA2*  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#76": "Esercizio 12  Utilizzando un microprocessore dotato di un bus indirizzi a 20 bit e di un bus dati a 8 bit:    - mappare 32k di RAM nella parte bassa dello spazio di    indirizzamento, 32k di RAM a partire dall\u0001indirizzo     1C000h e 64k EPROM nella parte alta dello spazio di    indirizzamento ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#77": "RAM_1 (32k) RAM_2 (32k) \nEPROM (64k) 00000h 10000h 20000h 30000h \nF0000h 1C000h    0001 1100 0000 0000 0000  23FFFh    0010 0011 1111 1111 1111     \nFFFFFh Soluzione 00000h    0000 0000 0000 0000 0000  07FFFh    0000 0111 1111 1111 1111     \nF0000h    1111 0000 0000 0000 0000  FFFFFh    1011 1111 1111 1111 1111     CS_RAM_1=A19*·A17*·A16* CS_RAM_2=A19*·(A17 + A16) CS_EPROM=A19 CS_RAM_2=A19*·CS_RAM_1* oppure ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#8": "RSA notevoli: Latch CD CD C D Q Q* C D Q Q* C D 0 0 0 1 1 0 1 1 Q Q* Q Q* Q Q* 0 1 1 0 SR S R Q Q* C D Q Q* C D Q τSU τH τSU ≥ τSUmin τH≥ τHmin Vincoli: Tempo di risposta: τR > τH Latch CD: il problema/vantaggio delle “uscite trasparenti” ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\00_Complementi_Esercizi_Reti_Logiche.pdf#9": "Driver 3-state \nOE I U OE=0 I U OE=1 I U I OE U Quale è il valore della tensione  ? OE I 1 0 1 1 0 0 0 1 U 0 1 Z Z ? ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#0": "01 IntroduzioneCalcolatori Elettronici TIngegneria Informatica\nStefano Mattoccia",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#1": "DocentiStefanoMattocciaRicevimento:pressostudio,vicinoaula5.7,suappuntamentoconcordatoviaemailEmail:stefano.mattoccia@unibo.itHomepage:http://vision.disi.unibo.it/~smattMatteoPoggi(tutor,AA2020/21)Ricevimento:pressoLaboratoriodiComputerVision,suappuntamentoconcordatoviaemailEmail:m.poggi@unibo.itHomepage:https://mattpoggi.github.io/",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#10": "Struttura di una prova d’esame \nEsercizio 1: progetto di un sistema a microprocessore.Per superare l’esame, è necessario proporre una soluzione ragionevole. L’esercizio 1 determina il superamento/non superamentodella prova.Esercizi 2 e 3: domande di teoria che richiedono qualcheragionamento. Volutamente non è fornita la soluzione.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#11": "Elaborazione delle informazioni\nInputOutputUnessereviventeelaboracontinuamentedelleinformazionieforniscedellerisposteoagisceinundeterminatomodoinbasealleinformazioniiningresso.\nUnsistemadiqualsiasinaturaperl’elaborazionedelleinformazioniisolatodall’esternoservirebbeabenpoco(meglio,nulla).Inputeoutputdebbonoesserecodificatiinmodoappropriato\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#12": "Elaborazione delle informazioniOvviamente,inquestocorso,siamointeressatiasistemidinaturaelettronicaperelaborareleinformazioni.Inparticolare,perleragionievidenziateaRetiLogicheT,siamointeressatiasistemidigitali->RetiLogiche\nInputOutput\nRL01101101011101",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#13": "Elaborazione delle informazioniUnprimoproblema:leretilogicheelaboranoinformazioniditipodigitaleMoltiinputeoutputdiparticolareinteressenonsonodinaturadigitalemaanalogica(quellapreferitadagliesseriumani)•Prossimitàaunsemaforoounostacolo•Monitor•Movimentodelmouse•Voce/Audio•Pressionetasti•Touchscreen•etc",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#14": "InputOutput\nRLA/D\nD/A\nPertanto,è(spesso)necessarioconvertiresegnalidinaturaanalogicainsegnalidigitalieviceversaElaborazione delle informazioni\n‘01001101’ ‘110111’ Inunsistemadielaborazione,iningressoeinuscitatroviamosolosegnalidigitali(binari)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#15": "Elaborazione delle informazioniAltroproblema:laretelogicacheelaboraleinformazionièspessomoltoefficienti/veloce/etcmaallostessotempopocoflessibileperquantoriguardaillinguaggioutilizzabileperfornireleinformazioni(digitali)ininputeinterpretareleinformazioni(digitali)elaborateinoutput.Sesidesiderainteragireconunsistemadielaborazionedigitaleenecessarioadeguarsiallinguaggiochelaretelogicautilizza.Questolinguaggioècodificatodall’evoluzionetemporaledialcunisegnali(ilminimoindispensabile)emessioricevibilidallaretelogica.L’evoluzionetemporalediquestisegnalisidefinisceciclodibus(buscycle).",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#16": "RLElaborazione delle informazioni\nttttttttQuindi,l’unicomodoperinteragireconunsistemadigitaleperl’elaborazionedelleinformazioniconsistenell’adottarequestaconvenzione(ie,iciclidibus,descrittiindettagliosuidatasheetcheilproduttoredelsistemarendesempredisponibili).\nciclo di busciclo di bus",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#17": "Quale rete logica utilizzare?Sonodisponibilidiversearchitetturedielaborazione(i,e,retilogiche).Qualescegliere?Dipendedalcontestoapplicativoedalleprestazioni,consumi,ingombri,peso,etc:-sistemageneralpurpose-sistemaembedded-sistemapervideogiochi-etcUnatipologiadiarchitettura,basatasulmodellodiVonNeumann,èmoltopiùflessibiledellealtreepuòessereutilizzata,anchesenonsempreconrisultatiottimali,inognicontesto.Questomodelloèl’oggettodiquestocorsoeutilizzacomeelementodibaseunaCPU(microprocessore).",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#18": "Esempi di architetture di elaborazione\n19\n•CPU(CentralProcessingUnit)•CPUMulticore•CPUEmbedded•SOC(SystemonaChip)•GPU(GraphicProcessingUnit)•FPGA(FieldProgrammableGateArray)•Sistemiibridi(e.g.FPGA+CPU)•...",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#19": "■Architetturadeltuttogeneralecheportaarealizzazionipocodipendentidalfunzionamentodesiderato■Ilfunzionamentodesideratoèespressointerminidi✸sequenzadiistruzioni(programma)✸memorizzatesuunsupportodimemoria■Percambiarefunzionamentoèsufficientecambiareilprogramma:questodifatto\u0001modifica\u0002laretelogicadicontrolloperogniistruzioneeseguita■L’architetturaèadattaatrattareproblemimoltopiùcomplessidiquellivistinelcorsodiretilogichemaconefficienza(tipicamente)inferiore■L'importanzaeladiffusionedeicalcolatoridipendefortementedallaflessibilitàdiquestomodelloIl modello di riferimento: Von Neumann",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#2": "Obiettivi del corsoApprendere,•i principi di funzionamento•le architetture•la progettazione hardware e softwaredei sistemi per l’elaborazione delle informazioni basati microprocessore(o CPU –Central Processing Unit)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#20": "Rete combinatoriaUscite OjIngressi Xi  Variabilidi statoYk(n+1)Variabili di statoYk(n)Rete Sequenziale(Sincrona)RegistriSistema di elaborazione: rete sequenzialeIl sistema di elaborazione può essere schematizzato in modoastratto come una Rete Sequenziale (Sincrona), RSS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#21": "Come cambiareil funzionamento della RL?\nRegistriUscite OjIngressi Xi Variabilidi statoYk(n+1)Variabilidi statoYk(n)RetecombinatoriaSegnali dicontrolloProgramma(software)RispettoaRetiLogiche,lanovitàèilsoftware(programma)checonsentedivariareilfunzionamentodellareteinbasealleesigenzedesiderate(ie,ilcodicescrittodalprogrammatore).",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#22": "In realtà le cose sono più complesse\nUscite OjIngressi Xi Variabilidi statoYk(n+1)Variabilidi statoYk(n)RCSegnali dicontrolloProgramma(software)Unità dicontrollo(RSS)Istruzioni (dalla memoria)\nL\u0001unitàdicontrollononsologovernalaretecombinatoriamaanchetuttelealtreretilogichepresentinelsistemaEsempio:abilitagliingressieleuscitequandonecessario,etcRegistri\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#23": "•Ilprogrammarisiedeinmemoriaedècostituitodaistruzionicodificateinformabinaria(linguaggiomacchina)•Inmemoriarisiedonoancheglioperandidelleistruzioni,cioèidatielaboratiedaelaborare(formabinaria)•LeistruzionivengonoeseguiteinsequenzadallaCPU•LaCPUèunamacchinasequenzialesincrona(conunclock)Modello di esecuzione del programma\nUscite\nistruzioniIngressiCPUIstr. #1Istr. #2. . .Istr. #NCk",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#24": "Alivellodimassimaastrazione,ilfunzionamentodell’interosistemapuòesseredescrittomedianteduesolistati:–StatoincuilaCPUleggeinmemorialaprossimaistruzionedaeseguire(INSTRUCTIONFETCHoIF)–StatoincuilaCPUeseguel\u0001istruzionelettainIF(EXECUTEoEX)ISTRUCTIONFETCH(IF)EXECUTE(EX)\nCPUIstr. #1Istr. #2. . .Istr. #N",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#25": "RL CPU e istruzioni•Leretilogichevistearetilogicheelaboravanoeproducevanosegnalibinari•SelaCPUèunaRL,comesonocodificateleistruzioni?•Ovviamenteinbinario...•Esempioistruzione#1->00010100000101111101010000010011istruzione#2->10110101100100011001010000011001......istruzione#N->01010110100101010101010110011110•OgniistruzioneindicaallaCPUqualeoperazionedevesvolgere/eseguire•Nonsembraessereunmodomoltocomodo(pergliesseriumani)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#26": "Esempi di istruzioni•QualiistruzionipossonoessereeseguitedaunaCPU?•Somme•Moltiplicazioni•Divisioni•Confronto•Letturedallamemoriaodaaltridispositivi*•Scrittureinmemoriaoversoaltridispositivi*•...EsistonosostanzialmenteduetipologiediCPU:•RISC(ReducedInstructionSetComputer)Pocheesempliciistruzioni,retilogichesempliciemoltoveloci(frequenzadiclockelevata).eg,ARM•CISC(ComplexInstructionSetComputer)Molteistruzioni,alcunemoltocomplesse,retilogichecomplicate.eg,InteleAMD",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#27": "RISC vs CISC•TipicamenteaunasolaistruzioneCISCcorrispondonopiùistruzioniRISC•Tuttavia,ognisingolaistruzioneRISCèeseguitaspessopiùrapidamentediunaistruzioneCISC•Spesso,ilcodiceRISC,anchesepiùdenso,èpiùveloce•LeRLRISCsonotipicamentepiùsemplicidiquelleCISC•Seleretisonopiùsemplicilospazio(silicio)puòessereutilizzatoperaltrefinalità(registri,cache,etc)•IprocessoriRISCsonomoltodiffusi(smartphone,tablete)•AncheiprocessoriCISCsonomoltodiffusi(PC)perviadelsoftwareesistente*•LeCPUCISCmodernesonoinrealtàinternamentedeiRISC",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#28": "•IndipendentementedaltipodiCPU,leistruzioniinformabinarianonsonofacilmenteinterpretabilieperquestononutilizzateinquestaformaquandosiscriveilcodice•Illinguaggiochesiutilizzaèl’assembler:ADDR1,R2,R3;poneinR1lasommatraregistriR2eR3Questaistruzionepotrebbeesserecodificatacon:00010100000101111101010000010011Iltraduttoreassembler->codicemacchinainbinarioèunaLookUpTable(LUT),ovverounatabellaL’assemblersembraessereunpassoavantinotevolema...Perchénonavetemaiutilizzatoillinguaggioassemblernonostantescrivetecodicedalprimoanno?Istruzioni in forma più comprensibile",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#29": "Compilatore e istruzioni binarie•Ilmotivoècheavetescrittocodiceadaltolivello(C)eutilizzatouncompilatore(e.g.,GCC)•Ilcompilatore,converteilcodicescrittoinlinguaggioadaltolivelloinistruzionimacchinabinarie#include<stdio.h>intmain(intargc,char**argv){inta=5;intb=6;intc;c=a+b;printf(“Lasommatra%de%dè%d\\n”,a,b,c);}GCC0001010000010111110101000001001110110101100100011001010000011001. . .01010110100101010101010110011110",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#3": "Orario lezioniOrario ufficiale:https://corsi.unibo.it/laurea/IngegneriaInformatica/orario-lezioni?anno=2&curricula=Lunedì inizio ore 9.00-11onlineGiovedì inizio ore 14.00-17.00Aula 6.1",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#30": "Come si accede alla memoria (e non solo)?•SappiamocheilcodicenelmodellodiVonNeumannrisiedeinmemoria•Comesilegge(escrivedallamemoria)?•Comesileggonoescrivonoidati?\nCPU\n?•L’unico modo è attraverso dei segnali predefiniti (con un ben definito andamento temporale, ciclo di bus)  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#31": "IndirizziBA[K..0]DatiBD[R..0] READControlloWRITEREADYINT\nCPU\nK+1R+1\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#32": "Come avviene la comunicazione?CPU\nK+1MemoriaPeriferica#i\nR+1•Tutto viaggia sui bus di sistema•Tutto è regolato da cicli di bus",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#33": "CKADDRESSMEMRDMEMWRDATADATA_INReady?Esempio di ciclo di lettura",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#34": "CKADDRESSMEMRDMEMWRDATADATA_OUTReady?Esempio di ciclo di scrittura",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#35": "Range di indirizzi•Sesonodisponibili32bit(BA[31..0])èpossibileavereaccessoa2^32elementi(memorie,periferiche)•32segnalidiindirizzo,4GB•Aogniindirizzoèassociatoundispositivo(memorie,periferiche)•E’necessariodecodificarel’indirizzoemessodallaCPUperdeterminareconqualedispositivolaCPUintendecomunicare•Quantibytepossonoesseretrasferitiduranteunciclodibus?Dipendedall’ampiezzadelbusdati",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#36": "Relazione tra hardware e software•ConsideriamounaistruzioneperleggeredallamemoriaunbyteaundeterminatoindirizzoxxxLBdestinazione,indirizzo;letturadiunbytePerprimacosaèeseguitoilfetchdell’istruzioneall’indirizzoxxx.come?Conunciclodibus,naturalmenteComefacciamoaconoscerel’indirizzoxxx?LaCPUhaaccessoalprogramcounterPCUnavoltalettaedecodificata,l’istruzioneèeseguita.Durantel’esecuzioneèeseguitounciclodibusdiletturaDuranteilciclodibusèemessol’indirizzo",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#4": "Materiale didatticoDisponibile in formato PDF sul sito del corso:http://vision.disi.unibo.it/~smatt/Site/Courses.htmlNel sito sono presenti anche numerose prove d’esameI lucidi non sono un libro, seguire con attenzione le lezioni è fondamentale per superare rapidamente e con buoni risultati l’esame...",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#5": "Materiale didattico per approfondimentiSeguendo il corso con attenzione e utilizzando i lucidiforniti NON è necessario utilizzare altro materialeper la preparazione dell’esame.Tuttavia, per chi desiderasse approfondire:–Hennessy & Patterson, \"Computer architecture: a quantitative  approach\u0001, Morgan Kaufmann, Anche in versione italiana. La seconda edizione (inglese, a dx) descrive approfonditamente il processore DLX-G. Bucci, \u0001Architettura e organizzazione dei calcolatori elettronici. Fondamenti\u0002, McGraw-Hill-J. Yiu, \u0001The definitive guide to the ARM Cortex M0\u0002, Newnes-Patterson &  Waterman, \u0001RISC-V\u0002, Strawberry Canyon\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#6": "RequisitiPer superare in modo proficuo il corso di Calcolatori Elettronici T è fondamentale avere compreso bene:1) Fondamenti di Informatica T2) Reti Logiche TPer Reti Logiche T è cruciale la progettazione diretta.Si sconsiglia vivamente di seguire questo corso senza avere solide basi in 1) e soprattutto di Reti Logiche T",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#7": "Avvisi e altre comunicazioni Eventuali comunicazione di carattere generale sarannoinserite nella pagina web del corso, nella sezione “Avvisi” di Calcolatori Elettronici T o chat Teams\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#8": "Modalità di svolgimento dell’esame •L’esame consiste in una prova SCRITTA di 2.5 ore•Nessuna prova orale• Non è possibile portare libri, appunti, computer, telefoni, smartphone, tablet, smartwatch, etc• E’ indispensabile (pena l’esclusione dall’esame) presentarsi con documento di identitàe badge•Esami gravemente insufficienti saranno verbalizzati",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\01_Introduzione.pdf#9": "Prossimi appelli d’esame \n•La date degli esami sono consultabili su Almaesami•L’iscrizioneagli appelli via Almaesamiè obbligatoria e si chiude (circa) una settimana prima• Non è ammessa alcuna deroghe all’iscrizione•Sono previsti 6 appelli all’anno: -3 Dicembre/Febbraio -2 Giugno/Luglio -1 SettembreNessun appello straordinarioSe possibile (aule, etc) il primo appello subito dopo il termine delle lezioni",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#0": "02 Mappinge decodificaCalcolatori Elettronici TIngegneria Informatica\nStefano Mattoccia",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#1": "Spazio di indirizzamento•Una CPU emette un certo numero di indirizzi e altri segnalisui bus di sistema per comunicare con altri moduli  •Il numero di diversi indirizzi emessi dalla CPU costituiscelo spazio di indirizzamento•Una CPU che emette un indirizzo a 20 bit ha uno spazio diindirizzamento di 1 MB (2^20)•Una CPU che emette un indirizzo a 32 bit ha uno spazio diindirizzamento di 4 GB (2^32)•Le prime CPU avevano spazi di indirizzamento molto ridottodi alcuni KB (e.g., 64 KB o meno)•Oggi è consuetudine avere almeno 32 bit di indirizzo",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#10": "11Memorie RAM (SRAM)\n•Memorie volatili, leggibili e scrivibili•Capacità a multipli di 4:8K, 32K, 128K, 512K, etc•DRAM: 1 transistore per  bit, maggiore capacità, più lenteAiCE*OE*I/OiTceTaccToe(Out)ReadCycleAiCE*WE*I/OiTawTwp(In)TdsWriteCycleNCA16A14A12A7A6A5A4A3A2A1A0I/O0I/O1I/O2GNDVCCA15NCWE*A13A8A9A11OE*A10CE*I/O7I/O6I/O5I/O4I/O31234567891011121314151632313029282726252423222120191817128K ´8RAM",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#11": "12\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#12": "13\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#13": "14\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#14": "15\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#15": "16\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#16": "17Integrati Notevoli: \u0001244\u00021A11A21A31A42A12A22A32A41Y11Y21Y31Y42Y12Y22Y32Y4EN1*EN2*74XX244ENx*xAixYiDriver 3-state ad 8-bit(strutturato in 2 gruppi di 4 bit) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#17": "18\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#18": "19\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#19": "20\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#2": "Un indirizzo per distribuire merci\nWR(consegna)RD(preleva)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#20": "21EN*BiDIRAiA1A2A3A4A5A6A7A8B1B2B3B4B5B6B7B8EN*DIR74XX245IntegratiNotevoli: \u0001245\u0002Driver bidirezionale (transceiver) ad 8-bit.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#21": "22\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#22": "23\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#23": "24\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#24": "25Integrati Notevoli: \u0001373\u0002D0D1D2D3D4D5D6D7O0O1O2O3O4O5O6O7COE*74XX373\nCDiQiOE*OiZCQiDCDiOE*OiLatch CD \nLatch a 8-bit con uscite 3-state",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#25": "26Integrati Notevoli: \u0001374\u0002D0D1D2D3D4D5D6D7O0O1O2O3O4O5O6O7CKOE*74XX374\nCKDiQiOE*OiZQiDCKDiOE*OiFlip-Flop D\nRegistro edge-triggeredcon uscite 3-state",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#26": "",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#27": "28\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#28": "29\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#29": "30Registro Edge-Triggered con WE*WE*D0OE*O0Flip-Flop DMUX10CKDQ0O1Flip-Flop DDQ1MUX10ON-1Flip-Flop DDQNMUX10D1\nDN-1D[0..N-1]WE*OE*O[0..N-1]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#3": "CPU\nDecoder(RC)I livelloCS_ACS_BCS_CCS_DCS_ECS_FCS_GCS_HABCD\nEFGHUn indirizzo per distribuire dati (CPU)\nIl decoder di II livello è all’interno di ciascundispositivo (memoria, etc) \nBA[K-1..0]BD[R-1..0]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#30": "Register File (1 read-port, 1 write-port)DEC01M-1EN*m  Read_AddressDEC01M-1EN*m  Write_AddressRD*WR*N  Write_DataRead_DataN CKD[0..N-1]WE*OE*O[0..N-1]R0D[0..N-1]WE*OE*O[0..N-1]R1\nD[0..N-1]WE*OE*O[0..N-1]RM-1N.B. :M=2m",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#31": "Mappingdi dispositivi da 8 bit in sistemicon bus dati da 8 bit•Consideriamodispositiviconportadatia8bit•Imponiamo(temporaneamente)l\u0001ulteriorecondizionecheilparallelismodelbusdatisiaa8bit•Inquesteipotesil\u0001assegnamentoaundispositivodiunafinestradiindirizziinunospaziodiindirizzamentoavverràingeneralenelrispettodelledueseguentiulterioricondizionirestrittive:–ladimensionedellafinestradiindirizziassociataaundispositivoèunapotenzadidue–lafinestraècompostadaindirizzicontigui",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#32": "33Dimensione della finestra occupata da un dispositivo -esempi•Undispositivoaccessibileattraversoilbusoccupaingeneralen=2^Kposizioninellospaziodiindirizzamento•nrappresentailnumerodioggettia8bitindirizzabiliall\u0001internodeldispositivo(es.numerodicelledimemorianelleRAMedEPROM)•K(numerodibitdiindirizzointernialdispositivo)èfortementevariabilealvariaredeldispositivo:–Ingeneraleneidispositividiinput/output(i.e.,leinterfacce)Kèpiccolo(e.g.,2)–ingeneraleneidispositividimemoriaKègrande(e.g.,perunaRAMda128KBsihaK=17)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#33": "Caratteristiche ai morsetti di un dispositivo indirizzabile su una finestra di n = 2^K byteQualunquedispositivoda8bitconall’internon=2^kelementiindirizzabiliseparatamentehaalsuointernoundecoder(IIlivello)diKvariabiliconingressodienablecheselezionaisingolioggettiindirizzabili–Read(RD),dettoancheOutputEnable(OE)èilcomandodilettura.QuandoRDeCSsonoattivi,ildispositivoesponeilsuBD[7..0]ilcontenutodellacellaindirizzata–Write(WR),èilcomandodiscrittura.QuandoCSasseritosulfrontedidiscesediWRècampionatoildaatopresentesuBD[7..0]DISPCSA[K-1..0].RDWRD[R..0]?KBA[K-1..0]CS_DISP8BD[7..0]RDWR",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#34": "350K32K8K8K8K8KSpazio di memoria8KDispositivo di memoria fisico che realizza una zona della memoria logica00001FFF0121314Ind. delBloccoIndirizzo interno al blocco\nCS = A14 AND A13*(Dispositivo da 8K di memoria)Esempio con 15 bit di indirizzo del sistema \nIn questo caso ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#35": "Mapping allineato di dispositivi da 8 bit in sistemi con bus dati da 8 bitSiconsideriundispositivoDdin=2^Kbyteindirizzabili•SidicecheDèmappatoall\u0001indirizzoAsegliindirizzideibytediDsonocompresitraAeA+(n-1),cioèseAèl\u0001indirizzopiùbassotratuttigliindirizziassociatiaD•SidicecheDèallineatoseAèunmultiplodin(numerodibytesinternialdispositivo),cioèse:(indirizzopiùbassodiD)MODn=0(condizionediallineamento)•SeDèallineatoalloraikbitmenosignificatividiAsonougualiazeroEsempi:•Undispositivodaduebyteèallineatoseèmappatoaunindirizzopari•Unadispositivoda8byteèallineatoseèmappatoaunindirizzoilcuivalorecodificatoinbinarioterminacon3zeri•Undispositivoda16byteèallineatoseilsuoindirizzoinizialeincodiceesadecimalehalaciframenosignificativaugualeazero•Undispositivoda64KBèallineatoseilsuoindirizzoincodiceesadecimalehalequattrocifremenosignificativeugualiazero",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#36": "Come individuare univocamente una finestra allineata di 2^K byte in uno spazio di indirizzamento•Supponiamo di mappare un dispositivo D di 2^k bytes(k=4) a un indirizzo A allineato di uno spazio di indirizzamento di 1 MB (bus di indirizzi di 20 bit): •Allora possiamo porre A = α ## (0)k(ex F8570) ove αè una configurazione binaria di 20 -K bit e gli indirizzi associati a D saranno compresi traAmin= A = α ## (0)k e Amax= Amin+ 2k -1 = α## (1)k    (Amin= F8570–Amax=  F857F)•Dunque, possiamo indicare l\u0001indirizzo  Aidell\u0001i-esimo byte di D come l\u0001insieme di due campi concatenati: Ai = α ## i(Ai = F8573)αindividua tra le 2^(20-K) finestre allineate di 2^K byte presenti nello spazio di indirizzamento, quella su cui è mappato (a = F857)iindividua l\u0001offset nel chip del byte indirizzato (i = 3)(NB ## è l’operatore simbolico concatenazione)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#37": "Campi in cui si suddivide l\u0001indirizzo di dispositivi mappati in uno spazio di indirizzamento -esempio n. 1IndirizzamentodiunbytediunaRAMall\u0001indirizzo40010Hinunospaziodiindirizzamentodi1MBnell\u0001ipotesididisporrediunchipda128KBmappatoall\u0001indirizzo40000H:L\u0001indirizzovienesuddivisoinduecampi:ilprimoidentificalafinestradi128KBincuièmappatalaRAM,ilsecondoidentifical\u0001offsetall’internodellaRAM\nA0A19A17A16Identificatore dellafinestra di 128Kin cui si trova la RAMOffset del byte indirizzato all\u0001interno del dispositivo di 128KB 0      1       00  0000  0000  0001  0000iα",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#38": "Campi in cui si suddivide l\u0001indirizzo di dispositivi mappati in uno spazio di indirizzamento -esempio n. 2Indirizzamentodiunbyteall’indirizzo1026HinundispositivodiI/Odi16bytemappatoall’indirizzo1020Hdiunospaziodiindirizzamentodi64KBL\u0001indirizzovienesuddivisoinduecampi:ilprimoidentificalafinestradi16Bincuièmappatoildispositivo,ilsecondoidentifical\u0001offsetneldispositivo\nA0A15A4A3Identificatore dellafinestra di 16Bin cui si trova DOffset del byte indirizzato all\u0001interno del dispositivo Ddi 16 Bindirizzato 0 0 0 1       0 0 0 0         0 0 1 00       1        1        0iα",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#39": "Decodifica degli indirizzi in caso di mappingallineato•Consideriamounospaziodiindirizzamentodi1MBincuisiamappatoundispositivodi2^Kbyte•PerindividuareunacelladiindirizzoAi=α##ipossiamodecodificaretuttii20bitchecompongonoAi•Questadecodificaèeffettuataricorrendoallastrutturadeidecoderadalbero,conalberodiduelivelli:–IlIlivelloèusatoperdecodificareα(cheidentificalaposizioneincuiilchipèmappato);perdecodificareαdobbiamodecodificare20-Kvariabili–ilIIlivellovieneutilizzatoperdecodificarei(cheidentificailbyteall\u0001internodelchip,serveundecoderdikvariabili)•IldecoderdiIIlivellositrovaall\u0001internodelchipmentreladecodificadiαèacaricodelprogettistadelsistemachepuòutilizzareundecoderdi20-kvariabiliconcuisidecodificaα•Ladecodificaècompletasesiutilizzanotuttii20-Kbitperdecodificareα,semplificatasesiutilizzasolounsottoinsieme(minimo)dei20-Kbit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#4": "Condizione di visibilità di un dispositivo da parte del software•Condizionenecessariaaffinchéundispositivofisico(memoria,interfaccia,oaltraentità)siaaccessibilealsoftwareè:–ildispositivodeveesseremappatoinunospaziodiindirizzamento•Mappareinunospaziodiindirizzamentosignifica:–associarealdispositivounafinestradiindirizzidiquellospaziodiindirizzamento•Siaccedeaidispositivimappatiinunospaziodiindirizzamentoconciclidibus",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#40": "0K32K8K8K8K8KSpazio di memoria8KIlmodulodimemoriafisicoèdifattoattivato(mappato)induedifferentizonedellamemorialogica00001FFF0121314Ind. delBloccoIndirizzo interno al bloccoCS = A14 A13*Segliindirizziusatidaunprogrammasonoquellichevannoda16Ka24K(overoda4000Ha5FFFH)equellida24Ka32K(da6000Ha7FFF)nonsonousatialloraèpossibileladecodificaincompletaoparzialeinquantolazona24K-32Knonvienemaiindirizzata.EspressioneCSpiùsemplice",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#41": "0K32K8K8K8K8KSpazio di memoria8KIlmodulodimemoriafisicoèattivato(mappato)induedifferentizonedellamemorialogicanonconsecutive00001FFF0121314Ind. delBloccoIndirizzo interno al bloccoCS = A14 A13Decodifica parziale 2/2",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#42": "EsercizioSiconsideriunsistemaconbusindirizzia16bitebusdatia8bit.Scrivereleespressionididecodificacompletaesemplificata(quelladausareall’esame)neiseguenticasi:1)Dispositivodimemoriada16KBmappatoa8000h2)Dispositivodimemoriada8KBmappatoa0000h3)EntrambiidispositiviprecedentiSec’èunsolodispositivo(casi1e2)ilCSèmoltoparticolare….",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#43": "44",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#44": "Mapping, read, write e set/reset di un FFD\n•Il FFD èun elementaredispositivodi memoria•Con unaCPU, come possiamo:•scriverenelFFD•leggerenelFFD •settareo resettarein modoasincronoilFFD FFDDQA_RESA_SET\nCPUMEMRDMEMWRBD[7..0]BA[19..0]?\nConsideriamo il caso di una CPU con bus dati a 8 bit con 20 bit di indirizzo. 64 K di RAM agli indirizzi alti e 64 K di EPROM agli indirizzi bassi",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#45": "NellepagineseguentiassumiamocheicomandidelFFDsianomappatineiseguentiindirizzi:CS_READ_FFD ->  80003hCS_WRITE_FFD ->  80002hCS_A_RES_FFD ->  80001hCS_A_SET_FFD ->  80000hAssumiamoinoltrediutilizzareilsegnaleBD0delbusdatiperleggereescrivereilsingolobitdidato.Ovviamentesarebbepossibileutilizzarealtriindirizzinonappartenentiallememorieeanchealtrisegnalidelbusdati(anchediversiperlettureescritture).Seiltestodell’esamenonspecificaqualiindirizziusarelasceltaèlasciataallostudente.Spesso,lasceltadegliindirizzisemplifica/complicaisegnalididecodifica.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#46": "CKADDRESSMEMRDMEMWRDATADATA_INReady?Esempio di ciclo di lettura",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#47": "CKADDRESS\nDATADATA_OUTReady?Esempio di ciclo di scrittura\nMEMRDMEMWR",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#48": "FFDDQA_RESA_SETCS_A_RES_FFD\nCS_A_SET_FFDCS_READ_FFDCS_WRITE_FFDBD0BD0MEMWR*CS_RAM_H = BA19·BA18·BA15CS_RAM_H = BA19·BA18·BA15*CS_READ_FFD = BA19·BA18*·BA1·BA0·MEMRD (ist. lettura)CS_WRITE_FFD = BA19·BA18*·BA1·BA0*       (ist. scrittura)CS_A_RES_FFD = BA19·BA18*·BA1*·BA0·MEMWR(ist. scrittura)CS_A_SET_FFD = BA19·BA18*·BA1*·BA0*·MEMWR(ist. scrittura) CS_EPROM = BA19*Vedremo che istruzioni di lettura e scrittura sono (risp.)loadbyte (LB) e storebyte(SB).01",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#49": "FFD(x8)D[7..0]Q[7..0]A_RESA_SETCS_A_RES_FFD\nCS_A_SET_FFDCS_READ_FFDCS_WRITE_FFDBD[7..0]BD[7..0]MEMWR*Estensione a 8 bit\n01\nStessiCSdellapaginaprecedente,cambiasoloilnumerodibitdidatotrasferiti.8888",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#5": "Esempio: una CPU con K=3 bit di indirizzo\nCPU3•Lospaziodiindirizzamentosarebbedisolo8elementi•Supponiamodiavereduedispositividimemoria,da4byte:AeB76543210111110101100011010001000Decoder(RC)I livelloCS_ACS_BBA[2..0]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#50": "Mapping, read, write e set/reset di un latch\n•Anche il latch CD è un elementare dispositivo di memoria•Con una CPU, come possiamo:•scriverenel latch•leggerenel latch •settare o resettare in modo asincrono il latch CDDQA_RESA_SET\nCPUMEMRDMEMWRBD[7..0]BA[19..0]\nConsideriamo il caso di una CPU con bus dati a 8 bit con 20 bit di indirizzo. 64 K di RAM agli indirizzi alti e 64 K di EPROM agli indirizzi bassiC?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#51": "CDA_RESA_SETCS_A_RES_LATCH\nCS_A_SET_LATCHCS_READ_LATCHBD0BD0CS_WRITE_LATCHCS_RAM_H = BA19·BA18·BA15CS_RAM_H = BA19·BA18·BA15*CS_READ_LATCH = BA19·BA18*·BA1·BA0·MEMRD (ist. lettura)CS_WRITE_LATCH = BA19·BA18*·BA1·BA0*·MEMWR(ist. scrittura)CS_A_RES_LATCH = BA19·BA18*·BA1*·BA0·MEMWR(ist. scrittura)CS_A_SET_LATCH = BA19·BA18*·BA1*·BA0*·MEMWR(ist. scrittura) CS_EPROM = BA19*Vedremo che istruzioni di lettura e scrittura sono (risp.)loadbyte (LB) e storebyte(SB).CDQMappiamo i quattro comandi del latch agli stessi indirizzi usati per il FFD.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#52": "Estensione a 8 bit\nStessi CS della pagina precedente, cambia solo il numero di bit di dato trasferiti.CD(x8)A_RESA_SETCS_A_RES_LATCH\nCS_A_SET_LATCHCS_READ_LATCHBD[7..0]CS_WRITE_LATCHCBD[7..0]888D[7..0]Q[7..0]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#53": "Incrementare il parallelismo dei dati\n•Abbiamoconsideratofinoaorasistemiconunparallelismo(busdati)a8bit•Ognitrasferimentorichiedeunciclodibus•Nelementi(byte)->Nciclidibus•Sappiamochelememorie(enonsolo)sonolente(vsCPU)\n1",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#54": "i)\nii)\niii)\n111",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#55": "•Possiamo fare meglio?•Si, aumentando il parallelismo dei dati•Riducendo la dimensione di ciascuna memoria•Trasferendo più dati nello stesso ciclo di bus \n¼¼¼¼i)i)i)i)IS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#56": "•CosaNON fare?•Trasferireglielementisequenzialmentein memoriepiùpiccole•Elementicontiguivannosumemoriediverse \n¼¼¼¼NOilparallelismodi ciascunamemoriaèsempre8 bit!\ni)ii)iii)iv)i)ii)iii)iv)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#57": "Memoria con processori a parallelismo > 8Il caso dei 16 bitIndirizzo fisicomemorie = Indirizzo logico/ 2Sul piedino  A0della memoria -> BA1busA1della memoria ->BA2 bus……………………………..\nMemorialogicaMemoriafisicaBUS ALTOBUS BASSO876543210876543210abcdefghi\nacegibdfhlWord(3) -> Byteh(1) e Byteb(2) -> 2 lettureLogicoFisicoFisicoLe memorie  fisiche vanno sempre in coppiaPer ogni \u0001banco\u0002ci deve essere un ByteEnableBE0 per banco 7-0 e BE1 per banco 15-8078bit 15Indirizzo interno ai chip\nMemorie sempre in coppia Ad esempio 2 x8K = 16 K (Lettura bytes 3 e 4 che però stanno a indirizzi fisici interni delle memorie differenti)(d ,e)(d )(e )",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#58": "Memorie con bus a 16 bitBE1      BE01           1       Word1           0       Byte alto (ind. dispari)0           1       Byte basso (ind. pari)0           0       Non possibile  Lo scambio byte alto esterno, byte basso del registro  e viceversa avviene all\u0001internodel microprocessore\nBA0del processorenon viene generato (di fatto seleziona il banco -al suo posto BE0 e BE1)BA1del processore connesso ai piedini A0delle memorieBA2del processore connesso ai piedini A1delle memorie etc. etc.7      015     8Memorie fisicheMicroprocessoreRiMUX",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#59": "MemorialogicaMemoriafisicaBUS ALTOBUS BASSO128K128K128K128KFFFFFh\n00000h64K64K070bit  740000h5FFFFh0000hFFFFhIndirizzi interni alle EPROM2 x 64K = 128KEPROM1BE1*EPROM0BE0*\nCSEPROM1= BA19*BA18BA17*BE1CSEPROM0= BA19*BA18BA17* BE0Individua la zona di memoria da realizzareLe memorie vanno sempre in coppia (16 bit)La decodifica si fa come se si avesse una memoria a 8 bit. Si usano dispositivi di taglia metà selezionati con BE0 e BE1Memoria con processori a parallelismo > 8Il caso dei 16 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#6": "•Comefacciamoadattivareunadelleduememorieinbaseall’indirizzoBA[2..0]emessodallaCPU?•Ovvero,comeèfattalaretedidecodifica(Ilivello)chegeneraiduesegnaliCS_AeCS_B?•CS_A=BA2CS_B=BA2*•Questisegnalisarannoinviatiaallememorie(decodificadiIlivello)•Poi,saràindividuatol’elementoall’internodellememoriaselezionata(decodificadiIIlivello)76543210111110101100011010001000BA2=1BA2=0II livello",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#60": "MemorialogicaMemoriafisicaBUS ALTOBUS BASSO128K128K128K128KFFFFFh\n00000h64K64K070bit  780000h9FFFFh0000hFFFFhIndirizzi interni alle EPROM2 x 64K = 128KEPROM1BE1*EPROM0BE0*\nCSEPROM1= BA19BA18*BA17*BE1CSEPROM0= BA19BA18*BA17*BE0Individua la zona di memoria da realizzareMemoria con processori a parallelismo > 8Il caso dei 16 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#61": "BUS BASSO4000540004400034000240001400005FFFF5FFFE5FFFD5FFFC5FFFBMemoria Logica\n128K0   Eprom Pin0    Bus Pin77EPROM0BE0 -64KFFFFFFFEFFFDFFFC0003000200010000BUS ALTO07EPROM1BE1 -64KFFFFFFFEFFFDFFFC0003000200010000Eprom PinBus Pin       15              8Indirizzi interni della EPROMIndirizzi interni della EPROM\nIndirizzi della memoria logicaMemoria con processori a parallelismo > 8Il caso dei 16 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#62": "Memoria con processori a parallelismo 32 bitMemorialogicaMemoriafisica\nBUS 3BE3876543210abcdefghi\naeibfl07815cgdh162324bit 3187654210BUS 2BE2BUS 1BE1BUS 0BE0Indirizzo fisico = Indirizzo logico/4",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#63": "64Bus enable con parallelismo 32 bitBE3      BE2     BE1     BE01            1            1           1       Word 32 bit0            0            1           1       Half word bassa 1            1            0           0      Half word alta  0            0            0           1       Byte 0-7  N.B. BA0 e BA1  del processorenon vengono generati (di fatto selezionano uno  dei banchi -al loro posto BE0, BE1, BE2, BE3)BA2del processore connesso ai piedini A0delle memorieBA3del processore connesso ai piedini A1delle memorie etc. etc.etc.0            0            1           0       Byte 15-8  Lo scambio fra i bytes (half word) dei banchi di memoria e i byte (half word)  dei registri  e viceversa avviene all\u0001internodel microprocessore",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#64": "65Memoria logicaMemoria fisica\nBUS 32MB2MB2MBFFFFFFFFh\n00000000h512K243140000000h401FFFFFh00000h7FFFFhIndirizzi interni alle EPROM\n4 Memorie x 512K= 2MBEPROM3BE3*\nCSEPROM3= BA31*BA30BA29*BA28*BA27*BA26*BA25*BA24*BA23*BA22*BA21*BE3CSEPROM2 = BA31*BA30BA29*BA28*BA27*BA26*BA25*BA24*BA23*BA22*BA21*BE2CSEPROM1 = BA31*BA30BA29*BA28*BA27*BA26*BA25*BA24*BA23*BA22*BA21*BE1CSEPROM0= BA31*BA30BA29*BA28*BA27*BA26*BA25*BA24*BA23*BA22*BA21*BE0Individua la zona di  memoria da realizzare512K1623EPROM2BE2*512K815EPROM1BE1*512K07EPROM0BE0*BUS 2BUS 1BUS0Selezionail BUS\nCS espressi in forma veraMemoria allineata11 bitdi indirizzosono fissiMemorie con parallelismo 32 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#65": "Memoria logicaMemoria fisicaBUS 3  -D24-312MB2MB2MBFFFFFFFFh\n00000000h512K40000000h401FFFFFh00000h7FFFFhEPROM3DLX512K512KEPROM2EPROM100000h7FFFFh00000h7FFFFh512KEPROM000000h7FFFFhBUS 2  -D23-16BUS1  -D15-8BUS 0  -D7-0BE0BE1BE2BE3\nEmessi dal processoreal posto di BA1e BA0Memorie con parallelismo 32 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#66": "Memoria logica(come vista dal programmatore)512K\n40000000h401FFFFFh\n00000h7FFFFhEPROM3Memoria fisica(come realizzatafisicamente)\ndh00001h2MB\n512K00000h7FFFFhEPROM2\ncg00001h512K00000h7FFFFhEPROM1\nbf00001h512K00000h7FFFFhEPROM0\nae00001habcde40000001h40000002h40000003h40000004hIndirizzi fisicidei singolidispositivi\n------x\nI dati di indirizzi logiciconsecutivisi trovano su dispositivi diversiLa cella x di indirizzo logico abcdefghsi troverà all\u0001indirizzo fisicoabcdefgh/4 del dispositivo EPROMi ove iè il resto della divisioneabcdefghMemorie con parallelismo 32 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#67": "Esempio:sivuolerealizzarenelDLX(bus32bit)unamemoriaRAMda256Kpostaall\u0001indirizzo84000000(allineata).Campodiindirizzamento84000000-8403FFFF.Dispositivi:8RAMda32K(leRAMda64KstaticheNONesistono!!!!)Difattoquindivisonoduebanchida128Kl\u0001uno:ilprimorealizzalamemoriada84000000a8401FFFFel\u0001altroda84020000a8403FFFF.Ichipdimemoriada32Kutilizzanoallorointerno(fisicamente)comeindirizzidiselezionedellecelleipinA14-A0chesonoperòcollegatirispettivamenteagliindirizziemessidalDLXBA16-BA2(ricordiamoinfatticheBA1eBA0delDLXNONsonoemessiealloropostovengonoemessiBE3,BE2,BE1eBE0).Sinotiilruolodell\u0001indirizzoDLXBA17chedivideiduebanchiPrimobanco(decodificanonsemplificata)CSRAM00=(BA31BA30*BA29*BA28*BA27*BA26.…BA18*BA17*)BE0CSRAM01=(BA31BA30*BA29*BA28*BA27*BA26.…BA18*BA17*)BE1CSRAM02=(BA31BA30*BA29*BA28*BA27*BA26.…BA18*BA17*)BE2CSRAM03=(BA31BA30*BA29*BA28*BA27*BA26.…BA18*BA17*)BE3Secondobanco(decodificanonsemplificata)CSRAM10=(BA31BA30*BA29*BA28*BA27*BA26….BA18*BA17)BE0CSRAM11=(BA31BA30*BA29*BA28*BA27*BA26….BA18*BA17)BE1CSRAM12=(BA31BA30*BA29*BA28*BA27*BA26….BA18*BA17)BE2CSRAM13=(BA31BA30*BA29*BA28*BA27*BA26….BA18*BA17)BE3Ovviamentenelcasodidecodificasemplificata(memorialogicaincompletamenterealizzatafisicamente)lefunzionididecodificavengonoridottedicomplessità.OvequestiduebanchifosserogliunicidarealizzareiCSdipenderebberosolodaBA17edaBEiMemorie con parallelismo 32 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#7": "Come è fatto un generico dispositivo?•Unqualsiasidispositivo(memoria,periferica,etc),comunicaconlaCPUmedianteunainterfacciastandardasxDISPCSA[K-1..0].RDWRD[R-1..0]?KBA[K-1..0]CS_DISPRBD[R-1..0]RDWRCPU•Lacomunicazioneconl’esternoavvienesecondomodalitàchesonospecifichedeldispositivoequindinonstandard•BA[K-1..0]utilizzati(internamente)perdecodificadiIIlivello",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#8": "9Memorie EPROM•Memorie non volatili a sola lettura•Capacità a multipli di 2:32K, 64K, 128K, 256K, etcVPPA16A15A12A7A6A5A4A3A2A1A0D0D1D2GNDVCCPGM*NCA14A13A8A9A11OE*A10CE*D7D6D5D4D3EPROM1234567891011121314151632313029282726252423222120191817128K ´8AiCE*OE*DiTceTaccToeCE*OE*DiCella M/bit i",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\02_Mapping_e_decodifica.pdf#9": "CQiDCella di indirizzo \u0001j\u0002A0  A1    An-1WR RDD0   DiDN-1La cella di una RAMDECODERIIj2n´N",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#0": "03 Linguaggio macchinaCalcolatori Elettronici TIngegneria Informatica\nStefano Mattoccia",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#1": "Instruction Set Architecture•L’insieme delle istruzioni e dei registri di una CPU costituiscono l’InstructionSet Architecture(ISA)•Mediante l’ISA è possibile accedere alle risorseinterne (e.g., registri) ed esterne (e.g., memoria)•Tipicamente le istruzioni in linguaggio macchina sono generate da un compilatore•Più raramente, come in questo corso, scritte daiprogrammatori•Purtroppo, (quasi) ogni CPU possiede un proprio ISA •A proposito di ISA, esistono due linee di pensiero:•RISC: insieme ridotto di istruzioni semplici -> moltiregistri interni (DLX, ARM, RISC-V, etc)•CISC: insieme ampio di istruzioni complesse -> pochiregistri (Intel X86)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#10": "Consentediesprimereefficacementeconfigurazionibinarie:•Traslazionelogicaasinistradinbit:<<n(inserendo“0”adestra)•Traslazionelogicaadestradinbit:>>n(inserendo“0”asinistra)•Concatenazionediduecampi:##•Ripetizionenvoltedix:(x)n•Ennesimobitdiunaconf.binariax:xn(ilpediceselezionaunbit)•Selezionediuncampoinunastringadibitx:xn..m(unrangeinpediceselezionailcampo)•Datalaconfigurazionebinariadi8bitC=011011002:–C<<2:101100002–C3..0##1111:1100|11112–(C3..0)2:110011002–(C6)4##C>>4:1111|000001102Notazioneper la costruzionedi configurazionibinarie",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#11": "•Trasferimentodiundato:←Ilnumerodibittrasferitièdatodalladimensionedelcampodestinazione;lalunghezzadelcampovaspecificatasecondolanotazioneseguentetuttelevoltechenonèaltrimentievidente•Trasferimentodiundatodinbit:←nQuestanotazionesiusapertrasferireuncampodinbit,tuttelevoltecheilnumerodibitdatrasferirenonèevidentesenzalarelativaindicazioneesplicita•Contenutodicelledimemoriaadiacentiapartiredall’indirizzox:M[x]Esempio:R1←32M[x]indicailtrasferimentodallamemoriaversoilregistroR1dei4byte:M[x],M[x+1],M[x+2],M[x+3]Altranotazione",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#12": "READYINTBA[31..2]BE3BE2BE0BE1BD[31..0]MEMRDMEMWRµPDLXCLOCKRESET\nREADYINTRESETBA[31..2]BE3BE2BE1BE0BD[31..0]RDWR\nIlsegnalediRESETèasserito,all’avvio,daunareteesterna.AncheisegnalidiREADY,INTsonogeneratidaretiesternemautilizzatiduranteilnormalefunzionamento.Segnali del processore DLX30\n32",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#13": "•Unicospaziodiindirizzamentodi4G•32registrida32bitGP(R0,…,R31,conR0=0)•Istruzionidilunghezzacostante,32bitallineate•Campidelleistruzionididimensioni/posizionifisse•3formatidiistruzione:I,R,J•Noncisonoistruzionipergestirelostack•Peristruzionicheprevedonounindirizzodiritorno(JAL/JALR),essoèsalvatoinR31•NonesisteunregistrodiFLAGsettatodalleistruzioniALU.Lecondizionisonosettateesplicitamenteneiregistri(istruzioniSET)•E’presenteun’unicamodalitàdiindirizzamentoinmemoria(indiretto,medianteregistro+offset)•Leoperazioniaritmetico/logichesonoeseguitesolotraregistri(nontraregistriememoria)•Esistonoalcuneistruzioni(MOVS2IeMOVI2S)perspostaredatitraregistriGPeregistrispecialieviceversaCaratteristiche dell’ISA DLX (integer)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#14": "Registri del DLX (integer)R0=0R1R2R3R28R29R30R3132PCIARMARMDR32Registri GP*accessibilidirettamentedal codiceRegistri nonaccessibilidirettamentedal codice*\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#15": "DLX (integer): tipi di datoNelDLX(integer)sonodisponibilitretipididato:BYTE(8bit)HALF-WORD(16bit)WORD(32bit)•Idatididimensioneinferiorea32bit(quindia8o16bit)lettidallamemoriadebbonoessereestesia32bitduranteilcaricamentoneiregistri(semprea32bit)•Questaoperazionepuòessereeseguitamantenendoomenoilsegnodeldatolettodallamemoria07015031",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#16": "Estensione del segnoInmolticasiènecessarioèestenderelarappresentazionediundatocodificatoconnbit,inundatoconunarappresentazioneambit(conm>n).Peresempio,volendotrasferireunbyte(n=8)dallamemoriainunregistroa32bit(m=32)ènecessarioconoscerelamodalitàconlaqualeèrappresentatoildatoletto.Esempio:10110101(n=8)Assumendoildatosenzasegno(unsigned),l‘estensionea32bitavvieneaggiungeno24zeri:00000000000000000000000010110101oppure(0)24##10110101Assumendoildatoconsegno(signed),l’estesioneavvienereplicando24volteilbitdisegno11111111111111111111111110110101oppure(1)24##10110101",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#17": "Il set di istruzioni (integer) del DLX•Le principali istruzioni aritmetiche e logiche•Istruzioni logiche anche con op. immediato: AND, ANDI, OR, ORI, XOR, XORI•Istruzioni aritmetiche: ADD, ADDI, SUB, SUBI•Istruzioni di shift(a destra anche aritmetico): SLL1, SRL, SRA2•Istruzioni di SET CONDITION: Sx, con x= {EQ, NE, LT, GT, LE, GE} •Le principali istruzioni di trasferimento dati•Loadbyte signede unsigned(LB, LBU), loadhalfwordsignede unsigned(LH, LHU), loadword (LW)•Storebyte, storehalfword, storeword: SB, SH, SW•Copia un dato da un registro GP a un registro speciale e viceversa MOVS2Ie MOVI2S•Le principali istruzioni di trasferimento del controllo•Istruzioni di salto condizionato (PC+4 relative): BNEZ, BEQZ•Istruzioni di salto incondizionato J: assoluto (con reg.) e PC-relative•Istruzioni di chiamata a procedura Jumpand Link (JAL). L’indirizzo di ritorno viene automaticamente salvato in R31. JAL con registro e immediato  (PC-relative)•Istruzione di ritorno dalla procedura di servizio delle interruzioni: RFE1)Shiftlogicoasinistraeshiftaritmeticoasinistracoincidono(entrano0neibitmenosignificativi).PerquestaragioneNONesisteSLA.Fareattenzioneconshiftasinistra,nonpreservailsegnoepuògenerareoverflow2)Trascinandoadestradiunaposizioneunregistroeinserendoasinistrasempreilbitdelsegnosimantieneilsegnodeldatomentrelosidividesuccessivamenteper2",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#18": "Elencoistruzionidel DLX (integer)Data TransferLWRa,Imm16bit(Rb)LB Ra,Imm16bit(Rb)LBU Ra,Imm16bit(Rb)LH  Ra,Imm16bit(Rb)LHURa,Imm16bit(Rb)SW  Ra,Imm16bit(Rb)SH  Ra,Imm16bit(Rb)SB  Ra,Imm16bit(Rb)MOVS2IRa,Rs*MOVI2SRs*,RaSpecial registerRs* (IAR)Aritmetiche/logicheADD Ra,Rb,RcADDIRa,Rb,Imm16bitADDURa,Rb,RcADDUI Ra,Rb,Imm16bitSUB Ra,Rb,RcSUBIRa,Rb,Imm16bitSUBURa,Rb,RcSUBUIRa,Rb,Imm16bitSLL Ra,Rb,RcSLLI Ra,Rb,Imm16bitSRL Ra,Rb,RcSRLI Ra,Rb,Imm16bitSRA Ra,Rb,RcSRAI Ra,Rb,Imm16bitOR Ra,Rb,RcORIRa,Rb,Imm16bitXORRa,Rb,RcXORIRa,Rb,Imm16bitANDRa,Rb,RcANDIRa,Rb,Imm16bitLHI Ra,Imm16bitControlloSxRa,Rb,RcSxIRa,Rb,Imm16bitBEQZRa,Imm16bitBNEZ Ra,Imm16bitJImm26bitJRRaJALImm26bitJALRRaxpuò essere: LT,GT,LE,GE,EQ,NE\nRa{R0+,R1,..,R30,R31}Rb{R0,R1,..,R30,R31}Rc{R0,R1,..,R30,R31}+RanonpuòessereR0comeregistrodestinazionediistruzioniload,MOV2SI,aritmetico/logiche,LHIeSET∈∈∈\nPer le istruzioni aritmetiche: l’immediato a 16 bit è esteso senza segno se di tipo U (unsigned) altrimenti con segno.  Per istruzioni logiche, sempre estensione senza segno.  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#19": "DLX: formato delle istruzioni\n•Istruzionidiload,store,branch,JeJALconregistro,serconditionSxIeALUconoperandoimmediato.L’immediatoèa16bit•NelleoperazioniloadeALURS2/RdèRd.NellestoreRS2/RdèRS2.InentrambiicasiRS1perindirizzosorgente(loadostore)oregistrosorgente(operazioniALUconconoperandoimmediato)IOpCodeRS2/RdRS1Immediato di 16 bit\nJOpCodeImmediato/offset di 26 bit (PC relative)•Salti incondizionato con e senza ritorno (Je JAL) con immediatoROpCodeRS2RS1RdOpCodeext. (11 bit)•IstruzioniALUdeltipoRd¬Rs1opRs2oppuresetconditionSxtraregistri6 bit5 bit5 bit5 bit11 bit031•In alcuneistruzionidi tipoI (LOAD e ALU), RS2 rappresentailregistrodestinazioneRd•Alcuneistruzioni(e.g., J e JAL con registro) potrebberoesserecodificatecon piùdi un formatotraquellidisponibili",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#2": "Requisitidi un linguaggiomacchina/ISAOltreallapossibilitàdipoterrisolvereunqualsiasiproblema*,unrequisitofondamentalediunlinguaggiomacchina/ISAèquellodiminimizzareiltempodiesecuzionedelcodice*•SeCPImedioèilnumeromediodiclockperl’esecuzionediunaistruzione,l’obiettivoèquellodiminimizzareCPUTime=Nistruzioni*CPImedio*TCK•Lostessoproblema,puòesserequindirisoltoconCPUTimediversiinbasea:•Nistruzioni(RISC,richiedonoingenerepiùistruzioni)•CPImedio(RISC,tipicamenteistruzionipiùveloci)•TCK(Retilogichesemplicipotenzialmentepiùveloci)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#20": "Modalitàdi accessoallamemoria•OgniISAdisponediistruzioniperaccedereallamemoriainletturaescrittura•Normalmenteèpossibilestabilireladimensionedeldatochepuòesseretrasferito(BYTE,HALF-WORD,WORD,etc)•Idueprincipalimetodidiaccessoallamemoria(indirizzamento)sono:–Diretto(indirizzocablatonell’istruzione)–Indiretto(indirizzomodificabilearun-time)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#21": "Indirizzamentodiretto•Conquestamodalitàl’istruzionecontienealsuointernounvalore(cablato)chespecifical’indirizzodiaccessoallamemoriaLBR7,0800h-“LeggiunBYTE(8bit)all’indirizzo0800hememorizzalanelregistroR7”A0870800Ipotetica codifica dell’istruzione con 32 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#22": "Indirizzamentoindiretto•Conquestamodalitàl’indirizzodiaccessoallamemoriaèottenutosommandounvalorecostantepresentenell’istruzioneconilcontenutodiunregistro•Indirizzo=costante+registro•Ilregistroècablatonell’istruzionemailsuocontenutopuòcambiareatempodiesecuzioneLBR7,0800(R3)-“LeggiunBYTE(8bit)all’indirizzoR3+0800hememorizzalanelregistroR7”A3570800Ipotetica codifica dell’istruzione con 32 bit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#23": "Indirizzamentodirettovsindiretto1/2•Ladifferenzatraleduemodalitàdiindirizzamentoènotevole•Perrenderveneconto,poteteconsiderareuncasopiuttostocomune:“sommareglielementidiunarrayAdi8elementimemorizzatoapartiredall’indirizzo0800h”\n00800h10801h20802h30803h40804h50805h60806h70807h\nA[0]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#24": "Indirizzamentodirettovsindiretto2/2Diretto(non usato):XOR  R8,R8,R8; R8=0LBU R7,0800hADD R8,R8,R7 ; R8=R8+R7LBU R7,0801hADD R8,R8,R7 ; R8=R8+R7LBU R7,0802hADD R8,R8,R7 ; R8=R8+R7LBU R7,0803hADD R8,R8,R7 ; R8=R8+R7LBU R7,0804hADD R8,R8,R7 ; R8=R8+R7LBU R7,0805hADD R8,R8,R7 ; R8=R8+R7LBU R7,0806hADD R8,R8,R7 ; R8=R8+R7LBU R7,0807hADD R8,R8,R7 ; R8=R8+R7Indiretto:XOR  R8,R8,R8; R8=0ADDI R9,R8,8; R9=8LOOP: SUBI R9,R9,1; R9=R9-1LBU R7,0800h(R9); legge BYTE ; a 0800+R9ADD R8,R8,R7; R8=R8+R7BNEZ R9,LOOP; se R9!=0; salta a LOOP•Il registro R9 è utilizzato in ogniiterazione per cambiare l’indirizzobase (0800h)•Pensate se l’array fosse di 1000000elementi...",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#25": "DLX: modalitàdi accessoallamemoria•IlDLXprevedeun’unicamodalitàdiindirizzamento:indiretto•L’indirizzo(a32bit)èsempreottenutosommandounregistroa32bitconunvaloreimmediatoa16bitestesoa32bitconsegno.•Esempio:LWR7,Imm16_bit(R8)•CaricainR7,lawordall’indirizzo(a32bit)ottenutosommandoR8ilvaloredell’immediatoestesoa32bitconsegno:R7ç32M[R8+Imm16_bit[15]16##Imm16_bit[15..0]]•Nell’eserciziodellepagineprecedentiabbiamosottointeso,persemplicità,chel’indirizzofossea16bit.Inrealtà,nelDLXl’indirizzoèsemprea32bit(lospaziodiindirizzamentoè4G)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#26": "Come sonomemorizzatiidatiin memoriain un sistemacon parallelismo> 8? •Consideriamounsistemaconbusdatia16bit•Comepossiamomemorizzareilvalorea16bit0468hapartiredall’indirizzo(chesupponiamoa20bit)00010h?•Esistonodueconvenzioni:04680468046800010h00011h00010h00011h880000Fh00012h0000Fh00012h16HL\nLittle Endian(e.g., Intel)Big Endian(e.g., Motorola)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#27": "Istruzioni Aritmetico Logiche (ALU)•Istruzioni a 3 operandi:–2 operandi “sorgente”–1 operando “destinazione”. •“destinazione”: sempre un registro (a 32 bit)•“sorgente”: registro, registro •“sorgente”: operando immediato(16 bit)Esempi: ADD R1,R2,R3; R1 çR2 + R3 formato RADDIR1,R2,3; R1 çR2 + 3 formato I; il valore (3) dell’immediato a 16 bit ; è esteso a 32 bit ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#28": "Istruzioni di Set ConditionQuesteistruzioniconfrontanoidueoperandisorgenteemettonoa“1”oppurea“0”l’operandodestinazioneinfunzionedelrisultatodelconfronto•“SET EQUAL” (SEQ, =) : settase uguale•“SET NOT EQUAL” (SNE, !=): settase diverso•“SET LESSER THAN” (SLT, <) : settase <•. . . Gli operandi possono anche essere unsigned:•“SET LESSER THAN UNSIGNED” (SLTU, <)Esempi SLT R1,R2,R3; R1 ç1 se R2<R3 altrimenti R1 ç0; formato RSLTIR1,R2,3; R1 ç1 se R2<3 altrimenti R1 ç0; formato I",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#29": "Istruzioni per il trasferimento dati •Sonoistruzionicheaccedonoallamemoria(loadestore):LB,LBU,SB,LH,LHU,SH,LW,SW•L’indirizzodell’operandoinmemoriaèlasommadelcontenutodiunregistroa32bitconun“offset”di16bit(Imm16bit)estesoconsegnoa32bit•L’istruzioneècodificatasecondoilformatoIEsempi:LWR1,40(R3);R1←32M[40+R3]LBR1,40(R3);R1←32(M[40+R3]7)24##M[40+R3]LBUR1,40(R3);R1←32(0)24##M[40+R3]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#3": "Istruzionie risorseinterne a unaCPULeistruzionieseguibilidaunaCPU,codificateinbinario,sonoingeneremoltopiùsemplicidelleistruzionicheutilizzateneilinguaggiadaltolivello.Tipicheoperazionisono:-somme,sottrazioni,divisioni,moltiplicazioni,etc-lettureescrittureinmemoriaeperiferiche-confrontotraoperandi(“A>B?”)-salticondizionati(“saltase”)eincondizionati(“salta”)-...E’possibile,medianteleistruzioni,accederearisorseinternedellaCPUcomeregistriarchitetturalietalvoltaaregistridistato(e.g.,AeramaggiorediB?)LerisorsechesonoaccessibilialleistruzioniinlinguaggiomacchinasonodefinitedaiprogettistidallaCPUTuttavia,nontuttiiregistriinternisonoaccessibilialprogrammatore(senzachequestorappresentiunproblema)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#30": "Istruzioni per il trasferimento del controllo:salti incondizionati (con e senza ritorno)•“JUMP”: salto incondizionato•“JUMP AND LINK: salto incondizionato con ritornoEsempiJoffset; PC = PC + 4 + (offset[25])6## offset, tipo JJR R3; PC = R3, tipo RJAL offset; R31 = PC+4; PC = PC + 4 + (offset[25])6## offset, tipo JJALR R5; R31 = PC + 4, PC = R5JR R31; PC = R31; istruzione per tornare da una procedura ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#31": "BCONDRd,Imm16E’ possiibleverificare solo due condizioni (COND):•BEQZ“BEQUAL ZERO”: salta se registro è 0•BNEQZ“BRANCH NOT EQUAL ZERO”: salta se registro è ≠ 0EsempiBEQZR4,Imm16; se R4==0 PC = PC + 4 + Imm16[15]16## Imm16; altrimenti PC = PC + 4BNEZR4,Imm16; se R4!=0 PC = PC + 4 + Imm16[15]16## Imm16; altrimenti PC = PC + 4Conunaistruzioneditiposetseguitadaun’istruzionedibranchsirealizzalafunzionedicompareandbranch(confrontoesaltocondizionatodalrisultatodelconfronto)senzabisognodiflagdedicatiIstruzioni per il trasferimento del controllo: salti condizionati (Branch)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#32": "Come generare valori a 32 bitNelDLXèpresenteunaistruzione,LHI(“LoadHighImmediate”)checonsentedicrearerapidamentevaloria32bit(NONèunaistruzionediaccessoallamemoria!).LHI Rd,Imm16; Rd= Imm16 ## 0000h  InserisceinRdilvaloredell’immediatonei16bitpiùsignificativie0neirimanentibitTipicamente,LHIèutilizzatapergenerareindirizzia32bitpartendodaimmediatia16bit.QualipotrebberoesseredellealternativeallaLHI?Esempio LHI R1,8420; R1 = 8420 ## 0000h = 84200000h",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#33": "Esempio di codice assemblerDLX 1QualevaloreassumonoiregistriR3edR4alterminedell’esecuzionedelcodiceseguente?LHIR1,0xE000ADDUIR2,R0,0x0081SB0x0000(R1),R2LBUR3,0x0000(R1)LBR4,0x0000(R1)R3=?,R4=?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#34": "EsercizioScrivereilcodiceDLXchesommaduewordmemorizzateapartiredallametàdellospaziodiindirizzamento(80000000h).LHIR4,8000h;R4=8000##0000hLWR5,0(R4);R5<-M[R4+0]LWR6,4(R4);R6<-M[R4+4]ADDR7,R5,R6;R7=R5+R6",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#35": "EsercizioProgettareunsistemabasatosulprocessoreDLXcon512MBdiEPROMnellapartebassadellospaziodiindirizzamento,1GBdiRAMapartiredall’indirizzo0x40000000e512MBdiRAMnellapartefinaledellospaziodiindirizzamento.Nelsistema,medianteopportuneistruzionisoftware,ènecessariopoter:-impostareallivellologico0o1unsegnaledenominatoSTARTUP,mappatoa0xC0000000einizialmentealvalore1-invertirelostatodiunLED,inizialmentespentoemappatoall’indirizzo0x90000000,prevedendoanchelapossibilitàdipoterneleggerelostato(ie,determinareseilLEDèaccesoospento)EPROM512 MBRAM 1 GBRAM512 MB\n0xC00000000x90000000\n0x1FFFFFFF\n0x40000000\n0x7FFFFFFFSTARTUPLED\n0xE0000000",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#36": "Struttura di una soluzioneRispostaaeventualidomandespecificheindicateneltestoIndicazionedeidispositiviedellememorieutilizzaticonrelativiindirizzidimappingreali(inizioefineinesadecimale)Scritturadeichip-selectdiciascundispositivocondecodificasemplificataProgettodieventualiretilogichenecessarieperrisolvereilproblema\nScritturadelcodiceinassemblerDLX,necessarioarisolvereilproblemaIndicazionedicomesonoconnessituttiidispostivi(inclusetuttelememorie)presentinellasouzioneaibusdisistemadelDLXCognomeNomeMatricolaDataTipoesame(CalcolatoriToLA)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#37": "Soluzione -chip selectCS_EPROM_512_3=CS_EPROM_512_2=CS_EPROM_512_1=CS_EPROM_512_0=CS_RAM_1_GB_L_3=CS_RAM_1_GB_L_2=CS_RAM_1_GB_L_1=CS_RAM_1_GB_L_0=CS_RAM_1_GB_H_3=CS_RAM_1_GB_H_2=CS_RAM_1_GB_H_1=CS_RAM_1_GB_H_0=CS_READ_LED=(0x90000000)CS_SWITCH_LED=(0x90000004)CS_READ_STARTUP=(0xC0000000)CS_WRITE_STARTUP=(0xC0000004)CS_RAM_512_3=CS_RAM_512_2=CS_RAM_512_1=CS_RAM_512_0=",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#38": "Soluzione –rete segnale STARTUPAll’avviodelsistemaSTARTUP=1",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#39": "Soluzione –rete segnale LEDAll’avviodelsistemaLED=0",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#4": "Registridi unaCPU•OgniCPUpossiedeuncertonumerodiregistriaccessibilialprogrammatore•Ilnumeroeladimensionedeiregistridipendonodall’ISA(equestohaimpattosullaretelogicarisultante)•Ovviamente,averemoltiregistrigeneralpurpose(GP)èvantaggioso(menoaccessiallalentamemoria)•Avereistruzionichepossonousaretutti,oquasi,iregistriGPsenzavincolièvantaggioso•LostessoISApuòessererealizzatoconretilogichecompletamentedifferenti(e.g.,InteleAMD)•Questeretihannoingenereprestazionidiverse(diversoCPUTimesebbeneabbianostessoNistruzioni)•NonsarebbestatomeglioavereunsoloISA?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#40": "Soluzione –connessione memorie\n_3_2_1_0BA[  ..  ]MEMWRMEMRDCS_CS_CS_CS_\nBD[7..0]BD[15..8]BD[23..16]BD[31..24]\nA[  ..  ]RD WR CSRD WR CSRD WR CSRD WR CS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#41": "Soluzione –codice assemblerDLX 1/2 LetturasegnaleSTARTUP\nImpostazionesegnaleSTARTUPalvalorelogico0",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#42": "Soluzione –codice assemblerDLX 2/2 LetturasegnaleLED\nInversionevaloredelsegnaleLED",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#43": "Simulatore DLXNell’ambitodialcunetestidilaureaèstatosviluppatounsimulatorediistruzioniDLXperscopididatticidisponibileaquestoindirizzo:http://dlx-simulator.disi.unibo.it/dlxAncorainfasedisperimentazionemapotrebbeessereunavalidaalternativaalsoftwareindicatonellepagineseguenti.Perchifosseinteressato,sebbenesiaancorainformamoltopreliminare,èpossibilesimulareancheistruzioniRISC-VSonograditesegnalazionidibachiesuggerimentipermigliorareisimulatoriinprossimetesiTesidilaureasvolteinquestocontesto:FedericoPomponii,“SviluppodiunsimulatoreDLXperscopididattici”,AA2019/20FabrizioMaccagnani,“ProgettodiunsimulatorediDLXperscopididattici”,AA2018/19AlessandroFoglia,“ProgettodiunsimulatorediRISC-Vperscopididattici”,AA2018/19",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#44": "AlcunenotesulsimulatoreDLXattuale-Negliimmediatinecessarioilprefisso0X(eg,0x8000)-Nellestoreladestinazioneasinistra(eg,SW0x800(R0),R18)-Altro?Esempio:sommaelementidiunarrayinit:XORR8,R8,R8;R8=0ADDIR9,R8,0x0008;R9=8LOOP:SUBIR9,R9,0x0001;R9=R9-1LBUR7,0x0800(R9);leggeunBYTEa00000800+R9ADDUR8,R8,R7;R8=R8+R7BNEZR9,LOOP;seR9!=0saltaaLOOP\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#45": "",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#46": "Esempio di codice assemblerDLX 2E’correttoilcodiceseguente?LHIR1,0x0000ADDIR2,R0,0x0081SH0x7FF1(R1),R2LHUR3,0x7FF1(R1)LHR4,0x7FF1(R1)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#47": "Esempio di codice assemblerDLX 3ScrivereilcodiceassemblerDLXperinserireinmemoria,apartiredall’indirizzoE0000800hl’arraydiwordindicatoinfigura.\n01234567\n0xE00008000xE00008040xE00008080xE000080C0xE00008100xE00008140xE00008180xE000081C0xE0000820\n32",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#48": "ADDR2,R0,R0;R2usatocomeindicedelcicloLHIR3,0xE000;R3=E0000000Loop:SWR2,0800(R3);scriveindiceinmemoriaADDIR2,R2,1;incrementaindicedelciclodi1ADDIR3,R3,4;incrementaindiceoffestdi4SNEIR4,R2,8;confrontaindiceR2con8BNEZR4,Loop;saltaseR4nonèzeroi)QualevaloreènecessariosostituireaLoopinBNEZ?i)Sipuòfaremeglio(ie,usaremenoistruzioni)?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#49": "76543210\n0x000008000x000008040x000008080x0000080C0x000008100x000008140x000008180x0000081C0x00000820\n32Esempio di codice assemblerDLX 4Scrivereilcodiceassemblerperilcalcolodellasommadeiprimi8elementidiunvettorediWORDmemorizzatoapartiredall’indirizzo00000800h.Ilrisultatodell’elaborazionedeveesserememorizzatoall’indirizzoE0000200h.Σ\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#5": "IA: Intel X86 (CISC)\nIA: ARM (RISC)\nATMEL (RISC 8 bit)ArduinoUnoDesktopSmartphonee Tablet",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#50": "ADDR1,R0,R0;azzeraR1,accumulatoreADDIR2,R0,20h;R2=3210Loop:SUBIR2,R2,4h;R2=R2-410LWR3,0800(R2);leggewordinmemoriaADDR1,R1,R3;aggiornaaccumulatoreR1BNEZR2,Loop;saltaseR2nonèzeroLHIR7,0xE000;R7=E0000000SWR1,0200h(R7);memorizzaaccumulatoreinE0000200h",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#51": "Esercizio",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#6": "RISC-V (1/2)•IlprogettoRISC-Vmiraproprioaquesto:creareunISAunicoeopensource•Ovviamentel’obiettivononèquellodiuniformareleretilogichecheimplementanol’ISA•L’ISAbasedelprogettoRISC-VèmoltosimileaquelladelDLXchestudieremoeprogetteremoinquestocorso•Esistonospecificheperestenderel’ISAbasealfinedicontemplareparticolarifunzionalità(floating-point,SingleInstructionMutipleData(SIMD),32/64/128bit,etc)\nhttps://riscv.org/\nRISC-V: The Free and Open RISC Instruction Set Architecture",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#7": "RISC-V (2/2)\nhttps://www.slideshare.net/KrsteAsanovic/riscv-20160507patterson\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#8": "Codificabinariadelleistruzioni•LeistruzioniperessereeseguitedallaretelogicaCPUdebbonoesserecodificateinbinariosecondounformatonotoedocumentatodalproduttore(datasheet)•Lacodificabinariadeveconteneretutteleinformazioninecessarieall’UnitàdiControlloperpotereseguirel’istruzione•EsistonoCPUconcodificadelleistruzionialunghezza:-costante(e.g.,32bitcasoRISCDLXemoltialtri)-variabiledaistruzioneaistruzione(IntelX86)Esempio:LBR7,0800(R3)-“LeggiunBYTE(8bit)all’indirizzoR3+0800hetrasferiscinelregistroR7”A3570800Ipotetica codifica dell’istruzione con 32 bit. I bit non utilizzati per codificare R3, R7e 0800rappresentano il codice operativo (op code) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\03_Linguaggio_macchina.pdf#9": "Linguaggioassembler•Lacodificadelleistruzioniinlinguaggiomacchinaèpocointuitivapergliesseriumani•Nellinguaggio*assemblersicodificanoleinformazioniinunmodo(unpo’)piùintuitivoMacchina->Assembler014FA27Dh->ADDR1,R2,R3;R1=R2+R3Escludendolacaratteristicaappenaevidenziata,unaltrosignificativovantaggioèquellodipoterdefiniredellelabelutili(spesso)neisaltiLOOP:SUBR1,R1,R3......BNEZ(R1),LOOP;saltaaLOOPseR1!=0",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#0": "04 InterruzioniCalcolatori Elettronici TIngegneria Informatica\nStefano Mattoccia",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#1": "•Inunsistemaamicroprocessoreèdifondamentaleimportanzapotergestireeventichesiverificanoall’esterno(manonsolo)dellaCPU•Peresempio,determinareseèstatopremutountastosullatastiera,seilmouseèstatospostato,etc•Unastrategia,pocoefficiente,perraggiungerequestoscopoconsistenelcontrollareperiodicamentesetalieventisisonoverificati(polling)•Questopuòesserefattointerrogandodicontinuolaperifericachesidesideragestire•Ovviamente,conquestastrategia,laCPUspendemolticiclimacchinaperlaverifica(oleverifiche)•Unastrategiamoltopiùefficiente,basatasuunastrategia“push”,consistenell’usodiinterruptGestioneeventicon unaCPU: polling",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#10": "Interruzionimultiple e priorità\n•Inunsistemanelqualeèpresentepiùdiunasorgentediinterruzioneèfondamentalepoterassociareunlivellodiprioritàaciascunainterruzione•Sarebbeauspicabilepoterinterromperel’interrupthandlerinesecuzionesegiungeunarichiestadiinterruzionepiùprioritaria(annidamento)•Esempio:\nPROBLEMA_SISTEMA_FRENIPROBLEMA_SISTEMA_AUDIOINT?\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#11": "•AssumeremocheilDLXsiasensibileallivellodelsegnalediinterruptINTenonalsuofronte•L’indirizzodiritorno(PC+4)èsalvatoinIAR•Inseguitoall’arrivodiuninterrupt,l’istruzioneincorsoècompletataedèeseguitoilcodiceall’indirizzo00000000h•Ilritornodall’interrupthandler(PCçIAR)avvienemediantel’istruzioneRFE(ReturnFromException)•Ingenere,manonnelDLXbase,gliinterruptpossonoessereabilitatiodisabilitatimedianteistruzioni•Nell’ISADLX,ègestitounsoloindirizzodiritorno.Pertanto,ilDLXdisabilitaleinterruzionimentreeseguel’handlereleriabilitaautomaticamenteritornandodall’handler(RFE).Incasocontrario,nelDLX,servirebbeunostacksoftwareInterrupt nelDLX",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#12": "•Conannidamento(nesting)delleinterruzionisiintendelapossibilitàdipoteravviareuninterrupthandlerdurantel’esecuzionediunaltrohandler•QuestacaratteristicaèstandardnellamaggiorpartedelleCPUincommerciomanonèprevistadalDLXbase•Perpoterannidaregliinterruptsarebbenecessariounostacksoftware(utilizzandol’istruzioneMOVS2I)eaverelapossibilitàdiri-abilitaregliinterruptnell’handlermedianteopportuneistruzioni(ENI)nonprevistadall’ISAbase•Incasomultiplesorgentidiinterruzione,nasceilproblemadicomeassociareunascaladiprioritàalleinterruzioni•Atalfineesistonovariepolitiche:prioritàfissa,variabile,etc.Ovviamentelaprioritàècrucialenonsoloquandoèpossibileannidaregliinterrupt",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#13": "•Lerichiestediinterruptpossonoverificarsiinqualsiasimomento•E’perònecessariomantenerelaconsistenzadeidatiinmodocheilcodiceinesecuzionenonsiamodificatodall’arrivoomenodiinterruptediconseguenzadall’esecuzioneomenodegliinterrupthandler•Perquestaragioneènecessariofareinmodochel’interrupthandler(i.e.,ildriverdeldispositivo)noninterferiscaconilcodicedelprogramma(main)inesecuzione•Comefare?Salvandoeripristinandoiregistrimodificatidall’interrupthandlerall’internodellostessocodice(handler)•Nellapaginaseguenteèmostratol’effettodiunpessimointerrupthandlerchenonpreservairegistriInterrupt handler e consistenzadeidati",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#14": "1\n+\n2\n=\n33\na)b)c)\nd)e)\n1+2=33??Chi ha scrittoildriver/handler del nuovodispositivo, avràsalvatoe ripristinatolo statodeiregistri?Temodi no…",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#15": "Nelcasodiunasingolasorgentediinterruzione,ilcodicediuntipicointerrupthandlerpotrebbeaverelastrutturaseguente:00000000h;Istruzionichesalvanoiregistri;modificatidalleistruzioniseguenti;codicedirispostaallarichiesta;diinterruzione;istruzionidiripristinodeiregistri;modificatiinprecedenzaXXXXXXXXhRFE;ritornodall’interrupt(PCçIAR)Interrupt handlercon singolainterruzione",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#16": "Nelcasodimultiplesorgentidiinterruzione,ilcodicediuntipicointerrupthandlerpotrebbeaverelastrutturaseguente:00000000h;Istruzionichesalvanoiregistri;modificatidalleistruzioniseguenti;Identificazionedell’interruptpiù;prioritariotraquelliasseriti;ripristinaregistriesaltaalcodice;dell’interruptpiùprioritario;salvaregistrimodificatiinseguito;codicehandler_1XXXXXXXXh;ripristinaregistrieritorno(RFE);salvaregistrimodificatiinseguito;codicehandler_2YYYYYYYYh;ripristinaregistrieritorno(RFE)Interrupt handlercon multiple interruzione\nRFERFEPreambolo",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#17": "•Conlastrategiamostratanellapaginaprecedenteèilsoftware,interrogandoognisingolaperiferica,adoverdeterminarequalèl’interruptpiùprioritario•Atalfinesaràanchenecessariaunaopportunainfrastrutturahardware(itri-stateserviranno?)•Tuttavia,èpossibilevelocizzareesemplificareleretilogichedisupportoaquestocompitomediantel’utilizzodiundispositivoadhoc(PIC)•IlPICsioccupadigestiremultiplesorgentidiinterruzioneediforniredirettamenteallaCPU(surichiesta)qualèilcodice/indirizzodell’interruptpiùprioritariotraquelliasseritiinquelmomento•Tipicamente,inunPICèpossibiledisabilitarelesingolesorgentidiinterruzioneestabilireillivellodiprioritàdiciascunainaccordoavariepolitiche(prioritàfissa,variabile,etc)Programmable Interrupt Controller (PIC)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#18": "PICCSA[K-1..0]RDWRD[7..0]KBA[?..?]CS_PIC8BD[7..0]RDWRINT_7INT_6INT_5INT_4INT_3INT_2INT_1INT_0INTINTINT_7INT_6INT_5INT_4INT_3INT_2INT_1INT_0•LastrutturadiunipoteticoPICpotrebbeesserequellamostratainseguito•LevariesorgentidiinterruzioneINT[7..0]sonoinviatealPICchesioccupadiinviarelarichiestasull’unicopinINTdelDLX•Piùavantineprogetteremounomoltosempliceconfunzionalitàdibase(abilita/disabilitaINT_i)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#19": "INTPICCSA[K-1..0]RDWRD[7..0]INT_7INT_6INT_5INT_4INT_3INT_2INT_1INT_0INTCPU•IlPICinviailsegnalediINTefornisceallaCPU,surichiesta,ilcodicedell’interruptconprioritàpiùelevatatraquelliasseritiinquelmomento•PerchénelPICèpresenteancheilsegnaleWR?Perché dei timer?Come può essere realizzato un timer?\nCosa comunicano alla CPU le reti?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#2": "main(){bool tasto_premuto=false;while(1){if (tasto_premuto==true)gestisci_evento();. . .}}void gestisci_evento(){. . .return;}Premuto?Premuto?Premuto?Premuto?\nLaCPUspendemoltotemponelcontrollare(polling)sel’eventosièverificato.Questastrategiarallental’esecuzionedelmainPocoefficiente….",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#20": "(i)(ii)INT\nΔxΔyΔwheelpressed_Lpressed_R•Inrealtàleinformazionisonoconvogliatesuuncanaleseriale(USB,PS/2)perridurreilnumerodiconnessioni/fili•Tuttavia,possiamopensareperlenostrefinalitàchel’interfacciamouse/CPUespongaisegnalidiunaportadiI/Ostandard(CS,RD,WR,D[7..0],indirizzi)\nInrealtàgliinterruptsonoemessiperiodicamente(e.g.,100Hz)esolosenecessario(uneventonelmouse)\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#21": "",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#22": "•InunaCPU(manonnelDLX)puòesserepresenteunulterioresegnale(ininput)denominatoNMI(NotMaskableInterrupt)•Atalesegnalesonocollegateunnumerolimitatodisorgentidiinterruzioniparticolarmentecritiche•Peresempio,l’outputdiunaretecherilevaesegnalaunaimminenteperditadialimentazioneelettrica•UnarichiestadiinterruptinviatasulpinNMInonpuòessereignorata(eventualiistruzionichedisabilitanogliinterruptnonagisconoperquestosegnale)einterrompel’esecuzionedialtrihandler•L’handlerassociatoalpinNMIèaprioritàmassimaedeveessereseguitonelminortempopossibileInterrupt non mascherabili(segnaleNMI)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#23": "•IlsegnaleNMIvausatoconcautelaesolopersegnalazionicriticheallaCPU•NelcasodelDLXutilizzeremosoloINT•Sefossedisponibile,perlagestionedelsegnaleNMIsarebbenecessarioinserireleistruzioninellaprimapartedel“preambolo”all’indirizzo00000000h,primadigestiregliinterruptchesonoinviatiattraversoINT",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#24": "ProgettareunsistemabasatosulprocessoreDLX,conun1GBdiEPROMaindirizzibassie512MBdiRAMaindirizzialti.Intalesistema,utilizzandounpulsante,deveesserepossibileaccendere/spegnereunledmedianteinterrupt.All’avvioilleddeveessereacceso.Sifaccial’ipotesicheR29eR30possanoessereusatisenzalanecessitàdiessereripristinati.Esercizio",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#25": "Alcune considerazioni sul reset asincrono\nFFDDQQ*A_SETA_RESRESETClockRESET_SYNCTuttavia,presentadeiproblemi:•E’semprenecessariounsegnalediclock•QuandoRESETvaa1,RESET_SYNCsiasserisce(ie,diventaattivo)alprimofrontediclockL’applicazionediunsegnaleasincronodireset,puòportareaproblemidimetastabilitànelmomentoincuitalesegnalevienepostoalvalore0(ie,quandosiescedalreset,assumendochetalesegnalesiaattivoalto).Leproblematichesonoanalogheaquelleevidenziateduranteilcampionamentodiunsegnalechenonrispettaitempidisetupehold.Unapossibilesoluzioneèlaseguente:",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#26": "FFDDQQ*A_SETA_RES0ClockRESET_SYNCRESETUnasoluzionecheeliminaidueproblemiprecedenti,echegarantisceun’uscitasincronadalreset,èlaseguente:\nClockRESETRESET_SYNCAttivazionenonsincronadelresetUscitasincronadalreset",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#3": "•UninterruptèuneventocheinterrompelaCPUduranteilregolareflussodiesecuzionedelcodice•L’interruptsegnalachesièverificatouneventochemeritaimmediata*attenzionedapartedellaCPU•SelaCPUèabilitata*ariceveretalesegnalazione,esegueautomaticamenteunaporzionedicodicedenominatainterrupthandleralfinedigestirel’evento•Glieventipossonoessererelativiafattoriesterni(e.g.,premutountasto)ointerni(e.g.,èstataeseguitaunadivisioneperzero,overflow,etc)•Quandodipendonodafattoriinternisiparladieccezioni(exceptions)•Inoltre,èpossibileinvocarel’handlermedianteopportuneistruzioni(e.g.,perinvocaresystemcall)Gestioneeventicon CPU: interrupt",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#4": "READYINTBA[31..2]BE3BE2BE0BE1BD[31..0]MEMRDMEMWRµPDLXCLOCKRESET\nREADYINTRESETBA[31..2]BE3BE2BE1BE0BD[31..0]RDWR\nInogniprocessore,èpresentealmenounsegnaledenominatoINTpergestireleinterruzioni.Inmolticasi,manonnelDLX,èpresenteancheunulterioresegnaledenominatoNMIpergestireinterruzionichenonpossonoessereignorate.Gestione interruzioni nel DLX30\n32NMI(NA nelDLX)NMI",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#5": "•Nelcasodiinterruptgeneratodall’esternolasituazioneèquesta:\n•Lapressionedeltastoinnesca*l’esecuzionedelcodicedell’interrupthandler(2)(1)(2); Interrupt hander. . . . . . . . . .. . . . .RFEINT\nLaCPUnormalmentesvolgeoperazioniutiliedèavvisatasoloquandosiverifical’evento(inquestocaso,lapressionedeltasto)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#6": "•Nelcasodiinterruptgeneratodall’esternolasituazione,dalpuntodivistasoftware,èquesta:main(){Istruzione1;Istruzione2;Istruzione3;Istruzione4;Istruzione5;Istruzione6;Istruzione7;Istruzione8;}\n(i); Interrupt handler ADD R1,R0,R0. . . . . . . . . .RFE(ii)(iii)•L’interruptpuòverificarsiinqualsiasimomento(i.e.,durantel’esecuzionediqualsiasiistruzione)enonèsincronizzatoconilclock•Assumeremosempreche,l’esecuzionedell’istruzionedurantelaqualesiverifical’interruptsiasempreportataatermineprimadieseguirel’handler\nL’is tr uzio n e4èportataatermineprimadieseguirel’interrupthandler",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#7": "•EsistonoCPUsensibiliallivellodelsegnalediinterrupt,altrealfrontedisalitaealtreaentrambelecose•NelcasodelDLXassumeremochelaCPUsiasensibileallivellodelsegnale(1sel’interruptèattivoe0incasocontrario)•Nelcasodeidispositivichegeneranointerrupt,assumeremocheessorimangaa1fintantochélacausachelohageneratononsiastatagestitadallaCPU•Pertanto,seunaperifericahauninterruptalivelloasserito,rimanetalefintantochél’interruptnonègestitodallaCPU(nonnecessariamentesubito*)•Inalcunicasi,nell’handlerpuòesserenecessarioeseguiredelleoperazionisoftwareperpoterportareallivellologico0ilsegnalediinterruptprovenientedall’esternodopoavergestitol’eventoSegnaledi interrupt: fronteo livello",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#8": "FFDDQ1A_RESINT_FRONTEINT_LIVELLOCS_RESET_INT\n•Comefareseildispositivochegeneral’interruptassumechelaCPUsiasensibileaifrontimentrelaCPUèsensibilesoloallivellodelsegnale?•E’necessarioeseguireunatrasformazionedafrontealivellodelsegnaleINT_FRONTE•Inuncasocomequesto,illivellologicodelsegnaleINT_LIVELLOdeveessereportatoazerodaunopportunocomandosoftware(CPU)cheasserisceilsegnaleCS_RESET_INTTrasformazioneda frontea livello",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\04_Interruzioni.pdf#9": "•C’èperòunproblema:escludendoNMI(discussodopo)ilDLXhaunsolosegnalediinterruptdenominatoINT.Comefacciamoagestire,cometipicamenteaccade,multiplesorgentidiinterrupt?•Siconvogliano(e.g.,medianteunORoaltrefunzioniinbaseallespecificheesigenze)tuttigliinterruptversol’unicosegnaleINTpresentenelDLX•Rimaneunaltroproblema:comedeterminarequale/qualiinterruptsonoasseritiinundeterminatoistante?•Atalpropositoè(tipicamente)necessariopoterdeterminarelostatodellerichiestediinterruptmedianteopportuneistruzionisoftware•Vedremocheesistonoanchedellereti,denominatePIC,chepossonoagevolarequestocompitoallaCPUGestionedi interruzionimultiple",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#0": "05 Periferiche di I/O con handshakeCalcolatori Elettronici TIngegneria Informatica\nStefano Mattoccia",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#1": "Porte di Input/Output (I/O)•InprecedenzaabbiamovistocomeprogettaredellesempliciperiferichediI/O,perscambiaredatitraCPUemondoesternomedianteunbuffer•Tuttavia,nonvieranessunagaranziasulcorrettoesitodeitrasferimenti•Infatti,cosaaccadese,mentrelaCPUscrivenellaportainoutput,undispositivoesternoleggedallamedesimaporta?Inoltre,cosaaccadeselaCPUleggeundatocheinrealtànonèmaistatoscrittodaundispositivoesterno?Comepuòsaperlo?Perquesto,itrasferimentisonointrinsecamenteespostiaerrori•Inpiù,lagestionedeltrasferimentieratotalmenteacaricodellaCPU(chepotrebbefarealtro)•LeportediI/Ononeranoingradodigenerareinterruptcontutteleproblematichechenederivano",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#10": "WROBFINT_OACKHandshake (OUTPUT): formed’onda\nNOTA: questo WRdeve essere “diretto”, dal processore, alla porta in OUTPUTOUTPUTINT_OEXTUNITACKOBFD_OUT[7..0]WRBD[7..0]CSWRINTRBD[7..0]CSD[7..0]OBFACK",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#11": "Unesempiodiunitàesternainoutputpotrebbeessererappresentatadaunastampantecheimprimesullacartaunacarattereallavolta.LaCPU,fornisce*idatiallastampanteattraversolaperifericadioutputquandoilsegnaleINT_Oèasserito(questoimplicacheOBFsia0)LastampanteleggeildatosoloquandoilsegnaleOBFèasserito(i.e.,quandolaportainoutputcomunicaallastampantecheunnuovodatoèstatoscrittodallaCPUnelbufferedèquindidisponibile)\nOUTPUTINT_OACKOBFD_OUT[7..0]WRBD[7..0]CSWRINTRBD[7..0]CSD[7..0]OBFACK\nLastampantedevecontenereunasempliceretelogicaingradodigestireilprotocollodihandshake",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#12": "ProgettareunaretelogicabasatasuFFDingradodigestirelecomunicazioniconduedispositiviesterni–unoininputmappatoaCS+0eunoinoutputmappatoaCS+1–utilizzandoilprotocollodihandshakeEsercizio\nParallelI/OBD[7..0]RDINT_ICSA0WRINT_0D_OUT[7..0]OBFACKD_IN[7..0]STBIBFA_RESETRDINT_ICSBA2WRINT_0D_OUT[7..0]OBFACKD_IN[7..0]STBIBFRESET\nBD[7..0]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#13": "Progettodell\u0001interfacciaL’interfacciaparallelaèdotatadidueporte,ciascunaingradoditraferiredatia8bit:•PortainINPUTmappataall’indirizzoCS+0•PortainOUTPUTmappataall’indirizzoCS+1Alfinedirisolvereilproblema,risultautilepensarelaportadiI/Ocomecompostadadueporteindipendenti:unaportaininputeunaportainoutputInoltre,nellasoluzionesidesideraevitareclockgatingIlpuntodipartenzasonoleformed’ondadelprotocollodihandhsake,mostratenellepagineprecedenti",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#14": "STBIBFINT_IRDHandshake (INPUT)\nNOTA: questo RDdeve essere “diretto”, dal processore, alla porta in INPUTINPUTINT_IEXTUNITSTBIBFD_IN[7..0]RDBD[7..0]CSRDINTRBD[7..0]CSD[7..0]IBFSTB",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#15": "STBIBFINT_IRDHandshake (INPUT)\nNOTA: questo RDdeve essere “diretto”, dal processore, alla porta in INPUT01230INPUTINT_IEXTUNITSTBIBFD_IN[7..0]RDBD[7..0]CSRDINTRBD[7..0]CSD[7..0]IBFSTB",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#16": "STBIBFINT_IRDU0U1U2U300 0 010 0 011 0 011 1 011 1 101 1 100 1 100 0 100 0 010 0 011 0 011 1 011 1 1Handshake (INPUT): soluzionesenzaclock gating\n013715141280137151otrasferimento2otrasferimento3otrasferimentoOsservando le forme d\u0001onda, è possibile individuare unasoluzione senza clock gating\nDue trasferimenti, un ciclo completo ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#17": "IlsegnaleDEC(x)identificalaconfigurazionebinariaU3U2U1U0equivalenteaxinbase10.Pertanto,isegnaliIBFeINT_Irisultano:IBF=(DEC(1)+DEC(3)+DEC(7))+(DEC(14)+DEC(12)+DEC(8))INT_I=DEC(3)+DEC(12)Oppure,IBF=U0XORU3INT_I=U1XORU2CSèilchip-selectdellaperifericaininputFFDDQ0A_RESQ0*RESETSTBU0FFDDQ1A_RESQ1*RESETSTB*U1\nFFDDQ3A_RESQ3*RESETRD*U3Q3*Q310CS·A0*FFDDQ2A_RESQ2*RESETRDU2Q2*Q210CS·A0*",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#18": "373D[7..0]Q[7..0]OECCS·RD·A0*D_IN[7..0]BD[7..0]STBHandshake (INPUT): buffer di ingressocon 373 Ipotizzando di mappare la porta in INPUTall’indirizzoCS + 0e di voler utilizzare dei latch 373 come buffer.\nOvviamentesarebbepossibileunasoluzionedeltuttoequivalentecon374comemostratonellapaginasuccessiva",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#19": "374D[7..0]Q[7..0]OECS·RD·A0*D_IN[7..0]BD[7..0]STB*Handshake (INPUT): buffer di ingressocon 374 IpotizzandodimapparelaportainINPUTall’indirizzoCS+0edivolerutilizzaredeiFFD374comebuffer",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#2": "FFD(x8)D[7..0]Q[7..0]CS·RDDI[7..0]BD[7..0]WRITE*Unasempliceperifericaperleggeredatidall’esterno,senzautilizzareinterrupt,èlaseguente:CPU\nEsternoTuttavia,conquestasoluzione,sorgonodeglievidentiproblemi:•ComepuòsaperelaCPUcheèdisponibileunnuovodatoscrittodall’esternonellaporta?•Comesipuòsaperedall’esternochelaCPUhalettoildatoscrittoinprecedenzanellaporta?\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#20": "Esercizio: progettodellaportain outputProgettarelaperifericapergestireitrasferimentiinOutputmediantehandshakeapartiredalleformed\u0001ondamostratenelleslidesuccessive.OUTPUTINT_OEXTUNITACKOBFD_OUT[7..0]WRBD[7..0]CSWRINTRBD[7..0]CSD[7..0]OBFACK",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#21": "WROBFINT_OACKHandshake (OUTPUT)\nNOTA: questo WRdeve essere “diretto”, dal processore, alla porta in OUTPUT01230OUTPUTINT_OEXTUNITACKOBFD_OUT[7..0]WRBD[7..0]CSWRINTRBD[7..0]CSD[7..0]OBFACK",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#22": "Registridi statoe programmazioneSarebbeutileaggiungereallaperifericachegestisceinputeoutputconprotocollodihandshakeiseguentiregistri:•Registrodistato(letturasegnalidistatopergestioneapolling)indirizzoA1A0=10•Registrodiprogrammazione(enable/disablesingolainterfaccia,etc)indirizzoA1A0=11Ovviamente,serveunulteriorebit(A1)perindirizzaregliulterioridueregistriEsercizioComesipotrebbemodificareilprogettodellaportadiI/Oconhandshakeperaggiungerequestenuovefunzionalità?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#23": "ProgettareunsistemabasatosulmicroprocessoreDLX,con1GBdiEPROMagliindirizzibassie2GBdiRAMagliindirizzialti.Nelsistemaèpresenteunaportaininput,giàprogettataedenominataINPUT_PORT,eunpulsantedenominatoP.Ilbyte(unsigned)lettodaINPUT_PORTdeveesserememorizzatoall’indirizzoFFFF0020hmentreilregistroR20deveessereincrementatodiuno,viasoftware,aognipressionediPeinizializzatoa0all’avviodelsistema.Inoltre,siassumache:1)IlpulsantePabbiaprioritàmaggiorediINPUT_PORT2)IlpulsantePnonpossaesserepremutoprimachesiaterminatalagestionediPdapartedell’interrupthandler.Atalfinesegnalare,conunLED,quandoilpulsantenondeveesserepremuto3)IregistridaR25aR30possonoessereutilizzatisenzalanecessitàdiessereripristinati4)IlregistroR20siamodificabilesolodall’handlerchegestisceilpulsanteEsercizio",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#3": "Unesempiocheevidenziaquestiproblemiriguardaloscambiodibeni/datitraunproduttoreeunconsumatore\nSeilproduttoreproduceuncaffècheèprelevatoprimadell’arrivodiunaltrotuttopotrebbeapparentementefunzionarecorrettamente(setupehold?)Tuttavia,comepuòsapereilproduttorecheilcaffèèstatoprelevato?Comepuòsapereilconsumatorecheèdisponibileunnuovocaffèpreparatodalproduttore?Perquesteragioni,sorgonoaltriproblemi...PC\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#4": "Unprimoproblemasiverificaseilconsumatoresmettediprelevarecaffèperchénonèpronto(e.g.,ilconsumatoreèaltelefono).Comepuòsaperloilproduttore?\nPC\nbla,bla,bla",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#5": "Ilproblemadualesiverificaseilproduttoresmettediprepararecaffèperchéimpegnatoafarealtro(e.g.,parlarealtelefono).Comepuòsaperloilconsumatore?\nbla,bla,bla\n•IdueproblemievidenziatipossonoessererisoltiinmodomoltosemplicericorrendoaqualcheformadisincronizzazionetraledueentitàPeC•Perquestoscopol’handshakeèunapprocciosemplice,efficienteeampiamenteutilizzatoPC\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#6": "Segnali del protocollo \u0002handshake\u0003: INPUT\n1.SeIBF=0,quandopossibile*UEpuòscrivereildatonelbufferd\u0001ingressodellaporta2.UE,portandoSTBa1,scriveildatonellaportachecontemporaneamenteasserisceIBF(InputBufferFull)3.QuandoUEportaSTBazero(scritturaterminata),l\u0001interfacciaattivaINT_I(InterruptRequest)4.Quandopossibile*,laCPUandràaleggereildatoscrittonellaportadaUE.Altermine,IBFandràazero(mentreINT_Ivaa0,dall’iniziodellalettura)INPUTUnitàEsterna(UE)InputRDINT_IBD[7..0]D_IN[7..0]STBIBFCSRDINTRBD[7..0]CSD[7..0]STBIBF",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#7": "STBIBFINT_IRDHandshake (INPUT): formed’ondaINPUTINT_IEXTUNITSTBIBFD_IN[7..0]RDBD[7..0]\nNOTA: questo RDdeve essere “diretto”, dal processore, alla porta in INPUTCSRDINTRBD[7..0]CSD[7..0]IBFSTB",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#8": "Unesempiodiunitàesternaininputpotrebbeessererappresentatadaunsensore(e.g.,ditemperatura)IlsensoreinviaidatiallaCPUattraversolaperifericadiinputquandounanuovamisuraèdisponibileeIBF=0.Alterminediogniscritturanellaportadapartedelsensoreditemperatura,ilsegnaleINT_IsiasserisceINPUTINT_ISTBIBFD_IN[7..0]RDBD[7..0]CSRDINTRBD[7..0]CSD[7..0]IBFSTB\nIlsensoredevecontenereunasempliceretelogicaingradodigestireilprotocollodihandshake",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\05_Handshake.pdf#9": "Segnali del protocollo \u0002handshake\u0003: OUTPUTOUTPUTUnità Esterna(UE)Output1.IlsegnaleINT_OasseritocomunicaallaCPUchelaportapuòaccettareunnuovodato2.InrispostoallarichiestadiinterruptlaCPUscrive,quandopossibile*,ildatosulbufferdellaporta1.L\u0001interfacciasegnalaaUEcheèdisponibileunnuovodatoattivandoOBF(OutputBufferFull)2.Quandopossibile*,UEleggeildatoscrittodallaCPUasserendoACK(acknowledge)WRINT_OBD[7..0]D_OUT[7..0]ACKOBFCSWRINTRBD[7..0]CSD[7..0]ACKOBF",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#0": "06 ProgrammableInterrupt Controller (PIC)Calcolatori Elettronici TIngegneria Informatica\nStefano Mattoccia",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#1": "•Abbiamogiàvistocheèpossibile,opzionalmente,utilizzareundispositivoad-hocperlagestionedimultiplesorgentiinterruzionidenominatoPIC•IlPICvelocizzaefacilitalafasedianalisiegestionedegliinterrupt•TipicamenteunPICconsentedi:•Abilitaredisabilitaresingoleinterruzioni•Fornireinformazionisulleinterruzioniasserite•Gestirelaprioritàdelleinterruzioni•Perleragionievidenziate,unPICèprogrammabilemediantel’utilizzodiopportuniregistriinterni•Sebbenesiasemprepossibileunagestioneinteramentesoftwaredelleinterruzioni,l’utilizzodiunPICpuòessereunavalidaalternativa•Perquesteragioni,progettiamounPICmoltosempliceGestionedelleinterruzionicon PIC",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#10": "EN_INT_0INT_0Diconseguenza,inogniistante,gliinterruptabilitatirisultanodalleuscitediquestarete:EN_INT_1INT_1EN_INT_2INT_2EN_INT_3INT_3RAW_ENABLED_INT_0RAW_ENABLED_INT_1RAW_ENABLED_INT_2RAW_ENABLED_INT_3Si ricorda che, come mostrato nello schema ai morsetti del PIC, risulta:INT_0= INT_OUT_PORT_0INT_1= INT_OUT_PORT_1INT_2= INT_IN_PORT_0INT_3= INT_IN_PORT_1Al pin di interrupt del DLX è pertanto inviato il segnale:INT_DLX= RAW_ENABLED_INT_0+RAW_ENABLED_INT_1+RAW_ENABLED_INT_2+RAW_ENABLED_INT_3",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#11": "Perevitareproblemidimetastabiitàèpossibilecampionarelostatodegliinterrupt,primadiprocedereallalorolettura,peresempioconquattroFFDchecampionanogliinterruptsulfrontedisalitadiMEMRD.RESETFFDDQA_RESMEMRDRAW_ENABLED_INT_0SYNC_ENABLED_INT_0RESETFFDDQA_RESMEMRDRAW_ENABLED_INT_1SYNC_ENABLED_INT_1RESETFFDDQA_RESMEMRDRAW_ENABLED_INT_2SYNC_ENABLED_INT_2RESETFFDDQA_RESMEMRDRAW_ENABLED_INT_3SYNC_ENABLED_INT_3",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#12": "Alfinedileggerelostatodegliinterruptasseriti,traquellichesonostatiabilitati,siutilizzanodeibuffertri-statepilotatidalsegnaleCS_PIC_READ_INTscomesegue:CS_PIC_READ_INTsSYNC_ENABLED_INT_0CS_PIC_READ_INTsSYNC_ENABLED_INT_1CS_PIC_READ_INTsSYNC_ENABLED_INT_2CS_PIC_READ_INTsSYNC_ENABLED_INT_3CS_PIC_READ_INTs‘0000’ENABLED_INT[0]ENABLED_INT[1]ENABLED_INT[2]ENABLED_INT[3]ENABLED_INT[7..4]IsegnaliENABLED_INT[7..0]sonoconnessialbusdatiBD[7..0]44",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#13": "Lareteseguente,consentedileggereaCS_PIC_READ_CODEilcodicea16bit(perragionimostratedopo)dell’interruptpiùprioritario(BD[15..0])\nCS_PIC_READ_CODESYNC_ENABLED_INT_0·SYNC_ENABLED_INT_1*· SYNC_ENABLED_INT_2*·SYNC_ENABLED_INT_3* CS_PIC_READ_CODESYNC_ENABLED_INT_1·SYNC_ENABLED_INT_2*·SYNC_ENABLED_INT_3*CS_PIC_READ_CODESYNC_ENABLED_INT_2·SYNC_ENABLED_INT_3*CS_PIC_READ_CODESYNC_ENABLED_INT_3CS_PIC_READ_CODE‘00000000’\nCS_PIC_READ_CODE‘0000’INT_CODE[7..0]INT_CODE[8]INT_CODE[9]INT_CODE[10]INT_CODE[11]INT_CODE[15..12]NOTA: come richiesto dal testo del problema, si assegna il seguente ordine crescente di priorità:0) INT_0 (Minima) 1) INT_12) INT_23) INT_3 (Massima)4488",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#14": "Icodicidipriorità,a16bitpervelocizzarel’handler,associatiallequattrointerruzionielettiall’indirizzoCS_PIC_READ_CODE,risultano:0800hseèasseritoSYNC_ENABLED_INT_3(massimapriorità)0400hseèasseritoSYNC_ENABLED_INT_2enonSYNC_ENABLED_INT_30200h se è asseritoSYNC_ENABLED_INT_1e non SYNC_ENABLED_INT_2oSYNC_ENABLED_INT_30100h se è asserito SYNC_ENABLED_INT_0e nessun altro segnaleIl codice per abilitare le interruzioni dalle 4 porte risulta:LHI R25,8000h; R25 = 80000000hADDI R26,R0,000Fh; R26 = 0 + 0000000FSBR26,(R25)04h; scrive il byte 0Fh contenuto in R26; all’indirizzo CS_PIC_SET_INTs(80000004h)Il codice per leggere quali sono le interruzioni asserite:LHI R25,8000h; R25 = 80000000hLBUR26,(R25)08h; legge in R26 gli interrupt asseriti, tra quelli; abilitati, all’indirizzo CS_PIC_READ_INTs; (80000008h)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#15": "Il codice dell’interrupt handlerè il seguente:00000000: LHI R25,8000h; prepara indirizzo 80000000h00000004: LHU R26,(R25)0Ch; lettura del codice di priorità a 16 bit; all’indirizzo CS_PIC_READ_CODE00000008: LHIR27,FFFF; prepara in R27 l’indirizzo per : operazioni comuni successive al salto 0000000C: JRR26; salta all’indirizzo presente in R26; checorrisponde al codice di interrupt ; più prioritario letto mediante LHU00000100: LBU R28,(R27)10h; legge in memoria un byte a FFFF0010h00000104: SB R28,(R25)2h; scrive quanto letto in OUTPUT_PORT_000000108: RFE; (80000002h) e ritorna dall’interrupt00000200: LBU R28,(R27)20h; legge in memoria un byte a FFFF0020h00000204: SB R28,(R25)3h; scrive quanto letto in OUTPUT_PORT_100000208: RFE; (80000003h) e ritorna dall’interrupt00000400: LBU R28,(R25)0; legge da INPUT_PORT_0(80000000h)00000404: SB R28,(R27)40h; scrive byte in memoria a FFFF0040h  00000408: RFE; ritorna dall’interrupt00000800: LBU R28,(R25)1; legge da INPUT_PORT_1(80000001h)00000804: SB R28,(R27)80h; scrive byte in memoria a FFFF0080h  00000808: RFE; ritorna dall’interrupt",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#16": "RAM_3RAM_2RAM_1RAM_0BA[28..2]Interfacciamento RAMMEMWRMEMRDCS_RAM_0CS_RAM_1CS_RAM_2CS_RAM_3\nBD[7..0]BD[15..8]BD[23..16]BD[31..24]\nA[26..0]RD WR CSRD WR CSRD WR CSRD WR CS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#17": "Interfacciamento EPROM\nBD[7..0]BD[15..8]BD[23..16]BD[31..24]EPROM_3EPROM_2EPROM_1EPROM_0BA[29..2]MEMRDCS_EPROM_0CS_EPROM_1CS_EPROM_2CS_EPROM_3\nA[27..0]RD  CSRD  CSRD  CSRD  CS",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#2": "InunsistemabasatosulDLX,con1GBdiEPROMmappatanegliindirizzibassie512MBdiRAMmappatanegliindirizzialti,sonopresentiegiàprogettate2portea8bitinINPUT(IN_1eIN_0)e2portea8bitinOUTPUT(OUT_1eOUT_0)basatesulprotocollodihandshake.ProgettareunsemplicePICalfinediassegnareleseguentiprioritàstaticheallequattrointerruzioni:IN_1(massimapriorità),IN_0,OUT_1eOUT_0(minimapriorità).IlPICdovràinoltreconsentiredi:a)disabilitare/abilitareselettivamente,medianteparoledicontrollo,ciascunainterruzionegeneratadalle4perifericheb)fornireleinterruzioniasserite(traquelleabilitate)c)fornireuncodicecheindicaqualèl’interruzionepiùprioritaria(traquelleabilitate)inundeterminatoistanteUtilizzandolaretelogicaprogettatagestirelequattrointerruzioniinmodochedurantel’esecuzionedell’interrupthandlersiaeseguito,nelmodopiùrapidopossibile,soloiltrasferimentoattivopiùprioritarioinquelmomento.Eventualialtrerichiesteditrasferimentoattivesarannogestitedurantesuccessiveesecuzionidell’interrupthandler.IdatilettidalleporteinINPUTdovrannoesserescrittiaFFFF0080(IN_1)eFFFF0040(IN_0)mentreidatidascriverenelleporteinOUTPUTdovrannoesserelettidaFFFF0020(OUT_1)eFFFF0010(OUT_0).All’avviodelsistemailPICdovràautomaticamentedisabilitaretuttelerichiestediinterruzioneprovenientidallequattroporte.-ScrivereilcodicecheabilitatutteleinterruzioninelPICeilcodicecheconsentedileggerelostatodegliinterrupt-Scrivereilcodiceottimizzatodell’interrupthandler(iregistridaR25aR29possonoessereutilizzatisenzalanecessitàdidoverliripristinare).Progettodi un semplicePIC",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#3": "PICProgrammableInterruptControllerA_RESETINT_0 (-)INT_1INT_2INT_3 (+)CS_PIC_SET_INTs\nCS_PIC_READ_INTsCS_PIC_READ_INTs_CODED[3..0]INT_TO_DLXENABLED_INT[7..0]INT_CODE[15..0]4816BD[7..0]BD[15..0]BD[3..0]INT(to DLX)CS_PIC_SET_INTs\nCS_PIC_READ_INTsCS_PIC_READ_CODEINT_OUT_PORT_0INT_OUT_PORT_1INT_IN_PORT_0INT_IN_PORT_1RESETIlPIC(ProgrammableInterruptController),ingradodigestire4interruzioni,puòessereschematizzatonelmodoseguente:\nRDWRMEMRDMEMWR",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#4": "NelPIC,lamassimaprioritàèassegnataaINT_3,quellaminimaaINT_0.Leprioritàsonostatiche,comeprevistodaltesto.IsegnalidiingressodelPICINT_3,INT_2,INT_1eINT_0sonoconnessiai4interruptdelleperifericheinmododasoddisfareivincolisullaprioritàprevistidaltestodelproblema.ScrivendoaCS_PIC_SET_INTs,idatipresentisuipinD[3..0]consentonodiabilitare/disabilitareisingoliinterrupt.Gliinterruptasseritidalleperiferiche,traquelliabilitati,possonoessereletti,aCS_PIC_READ_INTs,attraversoisegnaliENABLED_INT[7..0].Essendoprevistisolo4interrupt,4degli8bitsonocablatia0(i4bitpiùsignificativi).Ilcodicechecorrispondeall’interruptpiùprioritario,traquelliabilitati,potràessereletto,aCS_PIC_READ_CODE,attraversoisegnaliINT_CODE[15..0].Diquestiultimi16segnali,12sarannosemprecablatia0perragionichiariteinseguito.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#5": "Dispositivi e segnali presenti nel sistemaMemorie:RAM_512_MBmappata da E0000000h:FFFFFFFFh, 4 banchi da 128 MBEPROM_1_GB mappata da 00000000h:3FFFFFFFh, 4 banchi da 256 MBPorte di input, output e altri chip-selecte/o segnali:CS_INPUT_PORT_0mappato a 80000000hCS_INPUT_PORT_1mappato a 80000001hCS_OUTPUT_PORT_0mappato a 80000002hCS_OUTPUT_PORT_1mappato a 80000003hCS_PIC_SET_INTsmappato a 80000004hCS_PIC_READ_INTsmappato a 80000008hCS_PIC_READ_CODEmappato a 8000000Ch",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#6": "Segnali di decodifica di memorie, periferiche e segnali:CS_RAM_0 = BA31·BA30·BE0CS_RAM_1= BA31·BA30·BE1CS_RAM_2= BA31·BA30·BE2CS_RAM_3= BA31·BA30·BE3CS_INPUT_PORT_0= BA31·BA30*·BA3*·BA2*·BE0·IBF_0mappato a 80000000hCS_INPUT_PORT_1= BA31·BA30*·BA3*·BA2*·BE1·IBF_1mappato a 80000001h CS_OUTPUT_PORT_0= BA31·BA30*·BA3*·BA2*·BE2·OBF_0*mappato a 80000002h CS_OUTPUT_PORT_1= BA31·BA30*·BA3*·BA2*·BE3·OBF_1*mappato a80000003hCS_PIC_SET_INTs= BA31·BA30*·BA3*·BA2mappato a 80000004hCS_PIC_READ_INTs= BA31·BA30*·BA3·BA2*·MEMRDmappato a 80000008hCS_PIC_READ_CODE= BA31·BA30*·BA3·BA2·MEMRDmappato a 8000000ChCS_EPROM_0 = BA31*·BE0 CS_EPROM_1= BA31*·BE1CS_EPROM_2 = BA31*·BE2CS_EPROM_3 = BA31*·BE3",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#7": "INPUTPORT_0D[7..0]RDINT_INPUTCSDATA_IN[7..0]STBIBFCS_INPUT_PORT_0MEMRDUNITA’ESTERNA#0INT_IN_PORT_0BD[7..0]IBF_0Nelsistemasonopresentidueporteininput,collegateaibusdatiBD[7..0](INPUT_PORT_0)eBD[15..8](INPUT_PORT_1)\nINPUTPORT_1D[7..0]RDINT_INPUTCSDATA_IN[7..0]STBIBFCS_INPUT_PORT_1MEMRDUNITA’ESTERNA#1INT_IN_PORT_1BD[15..8]IBF_1STB_1STB_0A_RESETRESET\nA_RESETRESET",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#8": "OUTPUTPORT_0D[7..0]WRINT_OUTPUTCSDATA_OUT[7..0]ACKOBFA_RESETCS_OUTPUT_PORT_0MEMWRRESETUNITA’ESTERNA#2INT_OUT_PORT_0     BD[23..16]NelsistemasonopresentianchedueporteinoutputcollegateaibusdatiBD[23..16](OUTPUT_PORT_0)eBD[31..24](OUTPUT_PORT_1)\nOUTPUTPORT_1D[7..0]WRINT_OUTPUTCSDATA_OUT[7..0]ACKOBFCS_OUTPUT_PORT_1MEMWRUNITA’ESTERNA#3INT_OUT_PORT_1     BD[31..24]ACK_1OBF_1ACK_0OBF_0\nA_RESETRESET",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\06_PIC.pdf#9": "RESETFFDDQA_RES10MEMWR*D0EN_INT_0AlfinediabilitareedisabilitareselettivamentegliinterruptsipossonoutilizzarequattroFFD.IquattrobitD[3..0]sonoconnessiaisegnaliBD[3..0]delbusdatieutilizzatipercondizionareognisingolainterruzionemedianteisegnaliEN_INT_0,EN_INT_1,EN_INT_2eEN_INT_3generatidalleretiseguenti:CS_PIC_SET_INTsEN_INT_0RESETFFDDQA_RES10MEMWR*D1EN_INT_1CS_PIC_SET_INTsEN_INT_1RESETFFDDQA_RES10MEMWR*D2EN_INT_2CS_PIC_SET_INTsEN_INT_2RESETFFDDQA_RES10MEMWR*D3EN_INT_3CS_PIC_SET_INTsEN_INT_3",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#0": "DLX: implementazione sequenziale  Calcolatori Elettronici T Ingegneria Informatica \n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#1": "Datapath e Unità di Controllo  • La struttura di una CPU, come tutte le reti logiche sincrone che  elaborano dati, può essere strutturata in due blocchi: Unità di Controllo e Datapath  • La CPU, per funzionare, ha bisogno della memoria esterna su cui risiedono il programma e i dati \nreset interrupt ready \nCPU istruzioni Dati (in)  indirizzi \nDati (out) U.d.C. Data Path clock memoria Rete logica CPU ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#10": "Estrazione “automatica” dei registri durante la fase di decode di una istruzione (qualsiasi)  I Codice  operativo  RS2/Rd RS1 Operando immediato di 16 bit J Codice  operativo  Offset di 26 bit (PC relative) R Codice  operativo  RS2 RS1 Rd Estensione al Cod. op (11 bit) 0 31 < A B \nQuesti 5 + 5 bit  sono utilizzati per estrarre, preventivamente e ancora prima di conoscere che tipo  di istruzione che è stata letta dalla memoria, dal Register File due registri in A e B. Nel caso di  istruzione J non ci sono registri coinvolti e quindi saranno estratti bit corrispondenti all’offset. Nel  caso di istruzione I, in B potrebbe finire il valore del registro destinazione (e.g. in una LD o  operazione ALU (tipo I)). Infine: i 5 + 5 bit rappresentano gli indici (o presunti tali) ma non il valore dei due registri che è  contenuto nel Register File. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#11": "Gli stati della fase di fetch  • In questa fase si deve verificare se è presente un interrupt (evento esterno asincrono che la CPU deve “servire” con apposito software); • se l’interrupt è presente e può essere servito (IEN = true) si esegue implicitamente l’istruzione di chiamata a procedura all’indirizzo 0, e si salva l’indirizzo di ritorno nell’apposito registro IAR; • se l’interrupt non è presente o le interruzioni non sono abilitate, si va a leggere in memoria la prossima istruzione da eseguire (il cui indirizzo è in PC) MAR ← PC Dall’ultimo stato  dell’istruzione precedente  IAR:  Interrupt  Address  Register  IAR ← PC PC ← 0 IEN ← 0 IEN:  Interrupt  Enable  Flag  (int and IEN) = 1 (int and IEN) = 0 IR ← M(MAR) Alla fase di decodifica Ready = 1 Ready = 0 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#12": "Si modifica il  DATAPATH in  maniera da poter  indirizzare  la memoria dal PC.  Meno stati ma maggiore complessità  Data transfer ALU Set Jump Branch Ready ? INSTRUCTION FETCH INSTRUCTION  DECODE* Tutte le istruzioni impiegano un clock in meno per essere eseguite !  Ma potenzialmente aggiore lentezza  -> minore freq. clock Il diagramma  degli  stati del  controller  PC <- PC +4  A <- RS1 B <- RS2  IR <- M [PC] ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#13": "Ready ? IR <- M [PC] \n MAR <- A + (IR15)16 ## IR15..0 LOAD MDR <- M[MAR]  LB Ready ? C <- (MDR7)24 ## MDR7..0 RD <- C   PC <- PC +4  A <- RS1  B <- RS2  Controllo per  l’istruzione LB  (LOAD BYTE) ALU ALU Parte comune \nRS2 è da intendersi come registro di destinazione (A) = (RS1) Estensione segno ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#14": "Estensione del segno   (IR15)16 ## IR15..0 0    15      31 IR \n31 30…………17  16 BUS S1 o S0 Da UdC \n15-0 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#15": "MDR <- M[MAR]\nMDR <- B.C <- MDRRD <- C M[MAR] <-MDRINIT STORE \nLB LBU LH LHU LW Controllo per  le istruzioni di  DATA  TRANSFER  LB  LBU LH LHU LW STORE  Byte -> SB Half Word –> SH Word -> SW  C <-(MDR7)24 ## MDR7..0C <- (0)24 ## MDR7..0C <- (MDR15)16 ## MDR15..0 C <-(0)16 ## MDR15..0MAR <- A + (IR15)16 ## IR15..0 LOAD \nMancano nell’esempio  SH e SB (sempre unsigned)  che corrispondono a attivazione degli specifici WE delle memorie e “traslatori” dei bytes del registro MDR.  Come si realizzerebbero ?  NB: in lettura la parte meno  significativa del dato viene letta  sempre allineata al registro MDR per permettere il filling SW Il contenuto di A come unsigned Ready ? \nReady ? ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#16": "17  17  Trasferimenti BYTE, HW  •  I trasferimenti di bytes sono SEMPRE considerati allineati •  I trasferimenti di HW debbono avvenire a indirizzi multipli di 2 •  I trasferimenti di Word debbono sempre avvenire a indirizzi multipli di 4 •  In caso di disallineamento: fault •  Nel caso di store di dati di dimensione inferiore alla word NON si ha estensione del segno •  La lettura/scrittura di bytes e HW (a causa del reciproco disallineamento fra i registri e la memoria) implica che fra i registri e la memoria siano interposti dei mux/demux (realizzati con tristate) Registro MDR \nMemoria Come sono attivati i WE delle memorie ? Progettare la rete  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#17": "Trasferimenti BYTE, HW  MDR \nMemoria 31 0 Mux Demux I MUX 23-16 e 31-24 hanno come ingresso anche il bit 7 del byte 7-0 della memoria (LB) e il bit 15 del byte 15-8 della memoria (LH)  Ad esempio in una LB il MUX 7-0 si collega direttamente alla memoria mentre i MUX 15-8, 23-16 e 31-24 si collegano al bit 7 del MUX 7-0 proveniente dalla memoria.  In una SH a indirizzo multiplo di 2 e non di 4  il DEMUX 7-0  dal MDR si collega alla memoria 23-16 e il DEMUX 15-8 alla memoria 31-24. Gli altri due bytes della memoria rimangono invariati Mux Demux “0” Bit più signif. byte precedenti Solo in lettura Trasferimento  “unsigned” 24 23 16 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#18": "C <- A + TempC <- A xor TempC <- A - Temp C <- A and TempC <- A or TempINIT RD <- CRegistro (formato R) Immediato (formato I) \nADD AND SUB XOR OR   Temp <- BTemp <- (IR15)16 ## IR15..0Esempi di istruzioni  ALU  Duplicando i percorsi si potrebbe risparmiare il passaggio in TEMP  Lo stesso schema si può usare per gli shift etc.  Il contenuto dei registri come signed se op aritmetica ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#19": " RD <- C A = Temp\nC <- 1SEQ SLT SGE SNE SGT SLE YES NO  Il risultato del test è un input per il controller ! Registro (formato R) Immediato (formato I) Controllo per  le istruzioni  di SET  (confronto) ex. SLT R1,R2,R3  \nINIT Duplicando i percorsi si potrebbe risparmiare il passaggio in TEMP   Temp<- BTemp <- (IR15)16 ## IR15..0 A < Temp A >= Temp A <= Temp A > Temp A! = TempC <- 0I micropassi sono eseguiti  in ALU ma il risultato  NON è memorizzato in un registro: i flag sono utilizzati dalla ALU per impostare (almeno) il bit 0 del registro C  Il contenuto dei registri come signed ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#2": "•  Datapath: contiene tutte le unità di elaborazione ed  i registri necessari per l’esecuzione delle istruzioni  della CPU. Ogni istruzione appartenente all’ISA è  eseguita mediante una successione di operazioni  elementari, dette micro-operazioni •  Micro-operazione: operazione eseguita all’interno  del DATAPATH in un ciclo di clock ( e s e m p i :  trasferimento di un dato da un registro ad un altro  registro, elaborazione ALU) •  Unità di Controllo: è una RSS che in ogni ciclo di  clock invia un ben preciso insieme di segnali di  controllo al DATAPATH al fine di specificare  l’esecuzione di una determinata micro-operazione  Datapath e Unità di Controllo  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#20": "INIT  C <- PCJALR JAL JMP JR JALR \nJALR JR JAL \nJMP JAL JALR  JAL Controllo per  le istruzioni  di JUMP  (IR15)16 ## IR15..0  C <- PC\nPC <-  PC + (IR25)6 ## IR25..0 PC <- A R31 <- CPer il salvataggio in R31 \nIstruzione  formato I  Istruzione  formato J  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#21": "INIT A = 0 BRANCH \nYES YES NO NO BEQZ BNEZ Controllo per  le istruzioni  di BRANCH  A! = 0 PC <-  PC + (IR15)16 ## IR15..0 Ex. BNEQZ R5, 100 Il controllo se 0 (o !=0) è fatto sull’intero registro A (a 32 bit) e non solo sul bit meno significativo ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#22": "Numero di clock necessari per eseguire le istruzioni  Istruzione Cicli Wait Totale Load 6 2 8 Store 5 2 7 ALU 5 1 6 Set 6 1 7 Jump 3 1 4 Jump and link 5 1 6 Branch (taken) 4 1 5 Branch (not taken) 3 1 4 CPICPIN numero totale di istruzioni iin=i = 1 ∑(*)Esempio su DLX  LOAD: 21%, STORE: 12%, ALU: 37%, SET: 6%, JUMP: 2% BRANCH (taken): 12%, BRANCH (not-taken): 11%    CPI = 6.3 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#23": "Controllo cablato (“hardwired”) Segnali di  controllo \nINSTRUCTION REGISTER (IR) 40 Opcode +  OpCode Extension   6 Datapath Stato presente Rete combinatoria che  genera uscite e stato futuro Int e ready 2 6+11 3 Stato futuro 228 righe Rs1, Rs2, Rd - Indici di Rs1, Rs2 e Rd provengono da IR - IR25..0 sono portati ai bus S1 ed S2 del data path attraverso due buffer tristate IR25..0 U.d.C. \n32 bit dalla  memoria - U.d.C. genera anche i segnali di comando per la memoria (MEMRD e  MEMWR) Flag ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#24": "I passi dell’esecuzione delle istruzioni  Nel DLX l’esecuzione di tutte le istruzioni può essere scomposta in 5 passi, ciascuno eseguito in uno o più cicli di clock.   Tali passi sono detti:  1) FETCH:   l’istruzione viene prelevata dalla memoria e posta in IR.  2) DECODE: l’istruzione in IR viene decodificata e vengono prelevati gli   operandi sorgente dal Register File.   3) EXECUTE: elaborazione aritmetica o logica mediante la ALU.   4) MEMORY: accesso alla memoria e, nel caso di BRANCH aggiornamento   del PC (“branch completion”).   5) WRITE-BACK:   scrittura sul Register File. ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#25": "Le micro-operazioni eseguite  in ciascun passo  1) FETCH MAR   ß PC ;   ß  M[MAR]; 2) DECODE A  ß RS1, B  ß RS2,  PC  ß PC+4 IR   ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#26": "Le micro-operazioni eseguite  in ciascun passo  MEMORIA: MDR   ß   B;        ALU:   BRANCH: 3) EXECUTE MAR      A + (IR15)16 ## IR15..0 ; ß C  <- A op B (oppure A op (IR15)16 ## IR15..0) ; Temp       PC + (IR15)16 ## IR15..0) ; (utilizza ALU, S1, S2, dest: qui non si sa       ancora se si deve saltare) ß (utilizzano ALU, S1, S2, dest)  C <-  sign( A op B (oppure A op (IR15)16 ## IR15..0));  se SCn (NB: serve nelle Store  ove RD=RS2 operazione non significativa nelle LOAD)  J e JAL  Temp       PC + (IR25)6 ## IR25..0) ;  ß JR e JALR Temp       A;  ß ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#27": "Le micro-operazioni eseguite  in ciascun passo  4) MEMORY MDR   ß M[MAR];  (LOAD) ß  MDR;    (STORE)  BRANCH: M[MAR]  If (Cond)      PC       Temp; ß Memoria: \n[A] è il registro che condiziona il salto (Cond) ; JAL e JALR: C       PC; ß ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#28": "5) WRITE-BACK RD  ß C ; C ß MDR; (se è una LOAD – due micropassi)) Le micro-operazioni eseguite  in ciascun passo  \nPC       Temp; ß  istruzioni J, JR, JAL, JALR  istruzioni diverse da J, JR, JAL, JALR RD  ß C ; ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#3": "Register file C A B Struttura del DLX (esecuzione sequenziale)  \nTEMP IAR PC S1 S2 dest alu \nCPU Memoria dati in scrittura dati/istruzioni in lettura Indirizzi Instruction register C O N T R O L U N I T \nfetch MDR MAR execute Parallelismo dell’architettura: 32 bit (bus, alu e registri hanno parallelismo 32) I segnali di controllo non sono riportati !  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#4": "I registri del DLX (tutti a 32 bit)  • Register f i l e: 32 General Purpose Registers R 0 … . R 3 1  con  R0=0 • IAR: Interrupt Address Register –  D e p o s i t o  dell’indirizzo di ritorno in caso di interruzione • PC: Program Counter • MAR: Memory Address Register –  C o n t i e n e  l’indirizzo  del dato da scrivere o leggere in memoria • IR: Instruction Register –  C o n t i e n e  l’istruzione  attualmente in esecuzione • TEMP: Temporary Register –  R e g i s t r o  d i  d e p o s i t o  temporaneo di risultati  • MDR: Memory Data Register –  R e g i s t r o  d i  t r a n s i t o  temporaneo dei dati da e per la memoria • A e B: Registri di uscita dal Register File A parte il Register File questi registri NON sono accessibili  al programmatore. In alcuni casi istruzioni speciali per  accedere ad alcuni (e.g., IAR) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#5": "Funzioni della ALU       Dest (uscite) – 4 bit di comando  S1 + S2 S1 – S2 S1 and S2 S1 or S2 S1 exor S2 Shift S1 a sinistra di S2 posizioni Shift S1 a destra di S2 posizioni Shfit S1 aritmetico a destra di S2 posizioni S1 S2 0 1   Flag di uscita  Zero Segno negativo Carry \n• La ALU è una rete PURAMENTE combinatoria • Non esiste nel DLX un registro di flag ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#6": "Trasferimento dati sul datapath • I bus S1 ed S2 sono multiplexati (tri-state) con parallelismo 32 bit. • I registri campionano sul fronte positivo del clock, hanno due porte di uscita O1 e O2 per i due bus (o i registri A e B) e dispongono di tre ingressi di controllo:  – un ingresso di Write Enable (WE*)  ed  uno di Output Enable per ogni porta di uscita, una per ogni bus S1 e S2 (OE1* e OE2*). • Al fine di valutare la massima frequenza a cui è possibile far funzionare il datapath è importante conoscere le seguenti temporizzazioni: – TC (max) : ritardo max tra il fronte positivo del clock e l’istante in cui i  segnali di controllo generati dall’unità di controllo sono validi; – TOE (max): ritardo max tra l’arrivo del segnale OE e l’istante in cui i dati del registro sono disponibili sul bus; – TALU (max): ritardo massimo introdotto dalla ALU; – TSU (min)  : tempo di set-up minimo dei registri (requisito minimo per il corretto campionamento da parte dei registri).  • La massima frequenza di funzionamento del data path si calcola come segue:      fCK(max) = 1/TCK TCK  > TC (max) + TOE (max) + TALU (max) + TSU (min) ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#7": "Esempio : esecuzione della microistruzione  Rin ← Rout  S2 \nalu WE*  OE1* OE2* WE* OE1* OE2* S1 Rout Rin dest clock O2 O1 O2 O1 I I i1 i2 u = i2 WERin* OE2Rout* I segnali in blu (segnali di controllo) provengono dall’Unità di Controllo \nI segnali di controllo in grassetto sono attivi nel ciclo di clock in cui il micro-step Rin ← Rout viene eseguito  (e.g. TEMP) (e.g. MAR) Clock sempre collegato:  write enable !  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#8": "Il progetto dell’Unità di Controllo  • Una volta definito il Set di Istruzioni e progettato il DATAPATH, il passo successivo del progetto di una CPU è il progetto dell’Unità di Controllo  (CONTROLLER). • Il CONTROLLER è una RSS: il suo funzionamento può essere specificato tramite un diagramma degli stati.  •  Il CONTROLLER (come tutte le RSS) permane in un determinato stato per un ciclo di clock e transita (può transitare) da uno stato all’altro in corrispondenza degli istanti di sincronismo (fronti del clock).  •  Ad ogni stato corrisponde quindi un ciclo di clock.  Le micro-operazioni che devono essere eseguite in quel ciclo di clock sono specificate (in linguaggio RTL) nel diagramma degli stati che descrive il funzionamento del CONTROLLER all’interno degli stati. •  A partire dalla descrizione RTL si sintetizzano poi i segnali di controllo che  devono essere inviati al DATAPATH per eseguire le operazioni elementari  associate ad ogni stato.   ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\07_DLX_sequenziale.pdf#9": "Il diagramma  degli  stati del  controller  Data transfer ALU Set Jump Branch Ready ?  IR <- M [MAR] INSTRUCTION  FETCH INSTRUCTION  DECODE* MAR <- PC  \n PC<- PC+4 A <- RS1 B <- RS2 Oltre a decodificare l’istruzione si prelevano  gli operandi sorgente dal RF (anche se non utilizzati !) e si incrementa il PC.  Qui non si sa ancora quale sia l’istruzione ma il trasferimento ai registri è fatto  comunque !! N.B. I primi tre stadi sono comuni a tutte  le istruzioni  ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#0": "ISA DLX: implementazione pipelinedCalcolatori Elettronici TIngegneria Informatica\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#1": "Principio del PipeliningIl pipeliningè oggi la principale tecnica di base impiegata per rendere \u0002veloce\u0003una CPU . L\u0001idea alla base del pipeliningè generale, e trova applicazione in molteplici settori dell\u0001industria (linee di produzione, oleodotti …)Un sistema, S, deve eseguire Nvolte un\u0001attività A: A1 , A2 , A3…ANSR1 , R2 , R3…RNLatency: tempo che intercorre fra l\u0001inizio ed il completamentodell\u0001attività A(TA).Throughput: frequenza con cui vengono  completate le attività.",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#10": "Requisiti per l’implementazione in pipeline•Ogni stadio deve essere attivo in ogni ciclo di clock. •E\u0001necessario incrementare il PC in IF (invece che in ID).•E\u0001necessario introdurre un ADDER (PC <--PC+4 –PC <-PC+1) nello stadio IF.•Sono necessari due MDR (che chiameremo LMDR e SMDR) per gestire il caso di una LOAD seguita immediatamente da una STORE (WB-MEM sovrapposti –sovrapposizione di due dati in attesa di essere scritti, uno in memoria e l’altro nel RF). •In ogni ciclo di clock devono poter essere eseguiti 2 accessi alla memoria (IF, MEM): InstructionMemory (IM) e Data Memory (DM) ->  Architettura ‘Harvard’•Il clock della CPU è determinato dallo stadio più lento: IM, DM devono essere delle memorie cache(on-chip) •I Pipeline Registerstrasportano sia dati sia informazioni di controllo (l’unità di controllo è ‘distribuita’  fra gli stadi della pipeline)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#11": "GerarchiadellamemoriaL0 (registri CPU)L1L2L3Memoria (DDR)\nDiscoCosto/ByteTempo di accessoH\nLL\nHCPU ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#12": "13MemoriecacheCPUCacheMemoria (DDR)Una(opiùlivelli)memoriavelocemadiridottedimensioni,iecache,ingradodisfruttareilprincipiodilocalitàfannoapparirela(lenta)memoriaDDRmoltopiùveloce",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#13": "14\nArea di silicio occupata da cache L1,L2,L3 in un Intel Core i5Fonte: https://thecodeartist.blogspot.com/2011/12/why-readmostly-does-not-work-as-it.html",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#14": "IFIDEXMEMWBDatapath in Pipeline del DLX\nADD4MUX\nDATAMEMALUMUXMUX=0?INSTRMEMRFSEPCDECMUXIF/IDID/EXEX/MEMMEM/WBEstensionedel segnoNumero reg. dest.nel caso di LOADe ALU instr.JL (il PC in   R31)JLRPer il calcolo del nuovo PC nei salti\nPer le operazioni con immediatiRDDRS1RS2\nNumero del registro di destinazioneDatiPCIn realtà è un contatore programmabile  visto che i due bit meno significativi sono a 0se saltoContiene anche i circuiti di swapPer SCn(anche <0 e >0)[agisce sull\u0001uscita]\n=0?per BranchDurante JMP e BRANCH taken in IF/ID entra PC… Pazienza, sarà eliminata l’istruzione mediante NOP",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#15": "Stadiodi Fetch con contatore1/2\nCounter 30 bitENU[29..0]ClockSTALL*LDJUMPPC[31..2]I[29..0]JUMP_ADDRESS[31..2]\nSempre con riferimento allo stesso schema del DLX. Il segnale JUMPcodifica se il DLX deve saltare alla destinazione specificata daJUMP_ADDRESS[31.2]. Entrambi i segnali sono inviati dallo stadio MEM.Il segnale STALL, è generato dalla Unità di Controllo quando lostadio di IF deve essere bloccato. PC (to memory)ConriferimentoalprimoschemadelDLXpipelinedstudiatoduranteilcorso(maconsiderazionianaloghesiapplicanoallealtreversionidelDLX),lareteseguenteconsentedisostituireloschemabasatosuregistroemultiplexerconuncontatorea30bit(iduebitmenosignificatividell’indirizzosonosuperfluiperchéilDLXesegueilfetchsempreaindirizziallineati).PC +1 (to IF/ID)Come?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#16": "REGISTER30 bitStadio di Fetch con contatore 2/2Un’osservazione: come possiamo generare PC + 1 per lo stadio IF/ID(quando viene eseguito il fetch a PC è necessario fare entrare nellapipeline (stadio IF/ID) PC +1 ?\nCKD[29..0]OUT[29..0]+1PC +1 (to IF/ID)\nPC  (to memory)Stato presenteStato futuro\nPC[31..2]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#17": "Sezione IDIRRFSERDDRS1RS2IF/IDID/EX\nIR25-21IR20-16\nNumero registro destinaz.(dallo stadio WB) Dati (dallo stadio WB)(31-16) Immed./Branch(31-26)  JumpIR15IR25LBSWIR31-26 (Codop)IR15-0    (Offset/Immediato/Branch/Load -Reg. dest.)IR25-16   (J; JL))\nPC31-0    (JAL)PCAB26 (J e JL)\n61632323232\n32Info che viaggiacon l\u0001istruzioneIR10-00 (ExtCO)DEC\nEstensione segno",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#18": "Datapath in Pipeline del DLX ADD4MUX\nDMALUMUXMUXIMRFSEPCDEC\nMUXIF/IDID/EXEX/MEMMEM/WBIR1ABIR2PC2CONDX\nX: ALUOUPUT/DMAR/BTASMDRYLMDR\nY: ALUOUPUT1IFIDEX MEMWBPC1PC3PC4IndirizzoDatiIR3IR4n.  registro di destinazionePer SCn(anche <0 e >0)[agisce sull\u0001uscita]\n=0?=0?per BranchJLJLR(il PC in R31)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#19": "Esecuzione in pipeline di istruzione \u0002ALU\u0003\nX : \u0002ALUOUTPUT\u0003(in EX/MEM),  Y : \u0002ALUOUTPUT1\u0003NB in questa come nelle altre istruzioni RD (RS2) è trasferita fino allo stadio WBIFIDEXMEMY <-X (\u0002parcheggio\u0003in attesa di WB)WBRD <-YIR <-M[PC] ; PC <-PC + 4 ; PC1 <-PC + 4 A <-RS1; B <-RS2; PC2 <-PC1; IR2<-IR1ID/EX <-Decodifica istruzione;X <-A op BoppureX <-A op (IR215)16##IR215..0[PC4 <-PC3][PC3 <-PC2]La decod.ifica  attraversa  tutti gli stadi[IR3  <.-IR2][IR4  <.-IR3]N.B. al passare degli stadi IRperde i bit che non servono più in tutte le istruzioni. Da uno stadio al successivo vengono mantenuti i bit che servono qualunque sia l\u0001istruzione",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#2": "Motore: 2000 ccTipo:BenzinaColore:Rosso\n",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#20": "Esecuzione in pipeline di istruzione \u0001MEM\u0002IFIDEXMEMLMDR <-M[MAR]  (LOAD)oppureM[MAR] <-SMDR  (STORE)WBRD <-LMDR   (LOAD)  [ext. Segno]IR <-M[PC] ; PC <-PC + 4 ; PC1 <-PC + 4 A <-RS1; B <-RS2; PC2 <-PC1; IR2<-IR1ID/EX <-Decodifica istruzione;;MAR <-A op (IR215)16##IR215..0SMDR <-B[PC4 <-PC3][PC3 <-PC2]La decod.ifica  attraversa  tutti gli stadi[IR3  <.-IR2[IR4  <.-IR3]",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#21": "Esecuzione in pipeline di istruzione \u0001BRANCH \u0002\nX : \u0001BTA (BRANCH TARGET ADDRESS)\u0002IFIDEXMEMif (Cond) PC <-XWB(NOP)IR <-M[PC] ; PC <-PC + 4 ; PC1 <-PC + 4A <-RS1; B <-RS2; PC2 <-PC1; IR2<-IR1ID/EX <-Decodifica istruzione;;X <-PC2 op (IR15)16##IR15..0Cond <-A op 0[PC4 <-PC3][PC3 <-PC2]La decod.ifica  attraversa  tutti gli stadi[IR3  <.-IR2][IR4  <.-IR3Il test avviene sul valore del registro",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#22": "Esecuzione in pipeline di un\u0001istruzione \u0002JR\u0003IDMEMWBIFIDEXMEMPC <-XWB(NOP)IR <-M[PC] ; PC <-PC + 4 ; PC1 <-PC + 4A <-RS1; B <-RS2; PC2 <-PC1; IR2<-IR1ID/EX <-Decodifica istruzione;;X <-A [PC4 <-PC3][PC3 <-PC2]La decod.ifica  attraversa  tutti gli stadi[IR3  <.-IR2][IR4  <.-IR3]\nCome sarebbe la sequenza degli stati per una J ?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#23": "Esecuzione in pipeline di istruzione \u0001JL  o JLR\u0002IDIFIDEXMEMPC <-X ; PC4<-PC3WBR31 <-PC4IR <-M[PC] ; PC <-PC + 4 ; PC1 <-PC + 4A <-RS1; B <-RS2; PC2 <-P1; IR2<-IR1ID/EX <-Decodifica  istruzione;PC3 <-PC2X <-A (Se JLR)     X <-PC2 + (IR25)6##IR25..0(Se JL)\nNB: La scrittura in R31 NON può essere anticipata perché potrebbe sovrapporsi ad altra scrittura di registro Decod. in tutti gli stadi[IR4 <-IR3][IR3  <.-IR2]\nEvidenziati perché in questo caso utilizzati",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#24": "25Qual sarebbe la sequenza nel caso di SCN  (ex SLT R1,R2,R3) ?IDIFIDEXMEMWBIR <-M[PC] ; PC <-PC + 4 ; PC1 <-PC + 4A <-RS1; B <-RS2; PC2 <-P1; IR2<-IR1ID/EX <-Decodifica  istruzione;???",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#25": "Alee nelle Pipeline•AleeStrutturali-Unarisorsaècondivisafraduestadidellapipeline:leistruzionichesitrovanocorrentementeintalistadinonpossonoessereeseguitesimultaneamente.•AleediDato–Sonodovuteadipendenzefraleistruzioni.Adesempiounaistruzionecheleggeunregistroscrittodaun\u0001istruzioneprecedente(RAW).•AleediControllo–Leistruzionicheseguonounbranchdipendonodalrisultatodelbranch(taken/nottaken).Siverificaunasituazionedi\u0002Alea\u0003(\u0002Hazard\u0003)quandoinundeterminatociclodiclockun\u0001istruzionepresenteinunostadiodellapipelinenonpuòessereeseguitainquelclock.\nL\u0001istruzionechenonpuòessereeseguitavienebloccata(\u0002stallodellapipeline\u0003),insiemeatuttequellechelaseguono,mentreleistruzionichelaprecedonoavanzanonormalmente(cosìdarimuoverelacausadell\u0001alea).",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#26": "Clk 6Clk 7Clk 8Alee e Stalli\nIDIFIDEXMEMWBIi-3Ii-2Ii-1IiIDEXMEMIDEXIDIFIFIFIFIi+1Clk 1Clk 2Clk 3Clk 4Clk 5WBClk 9Clk 10Clk 11Clk 12WBWBT5=  8 * CLK = (5 + 3) * CLKT5= 5 * (1 + 3/5 ) * CLKCPI  idealeStalli per istruzioneTN= N *  1  * CLKTN= N *  (1 + S) * CLKCPI  effettivoSSSSSIFSMEMWBStallo: blocco del clock dello stadio e di tutti quelli precedentie propagazione progressiva agli stadi successiviEffetto–adesempio-diunaaleadidato:sel\u0001istruzioneIinecessitadiundatoprodottodallaistruzioneIi-1deveaspettarefinoalWBdellaIi-1",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#27": "IFIDEXMEMWBStalli nel salto (1/3)\nADD4MUX\nDATAMEMALUMUXMUX=0?INSTRMEMRFSEPCDECMUXIF/IDID/EXEX/MEMMEM/WBRDDRS1RS2\nDatiPCse salto\n=0?NOPNOPNOPNOP forzate per salto\nAl primo fronte positivo del clock successivoal campionamento della  verifica della condizione di salto sono inserite 3 NOP al posto dei codici operativi provenienti dalla memoria",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#28": "IFIDEXMEMWBStalli nel salto (2/3)\nADD4MUX\nDATAMEMALUMUXMUX=0?INSTRMEMRFSEPCDECMUXIF/IDID/EXEX/MEMMEM/WBRDDRS1RS2\nDatiPCse salto\n=0?NOPNOPNOP forzate per salto\nAl primo fronte positivo del clock successivoalla verifica della condizione di salto sono inserite 2 NOP al posto dei codici operativi provenienti dalla memoriaNB In questo caso la condizione di salto e il nuovo PC sono presentatial MUX nello stesso periodo di produzione  della condizione",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#29": "IFIDEXMEMWBStalli nel salto (3/3)ADD4\nDATAMEMALUMUXMUX=0?INSTRMEMRFSEDECMUXIF/IDID/EXEX/MEMMEM/WBRDDRS1RS2\nDatiPCse salto\n=0?NOPNOP per salto\nAl primo fronte positivo del clock successivoalla produzione della verifica della condizione di salto e inserita una NOP al posto del codice operativo proveniente dalla stadio IF/IDNB In questo caso la condizione di salto e il nuovo PC agisconosul MUX nello stesso periodo di produzione  della condizione\nPCMUX",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#3": "Motore: 2000 ccTipo:BenzinaColore:Rosso\nMotore: 2000 ccTipo:BenzinaColore:Rosso\nMotore: 2000 ccTipo:BenzinaColore:Rosso\nMotore: 2000 ccTipo:BenzinaColore:Rosso\nMotore: 2000 ccTipo:BenzinaColore:Rosso\nt",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#30": "ForwardingADD  R3, R1, R4Clk 6Clk 7Clk 8MEMWBIFIDEXMEMWBIDEXMEMIDEXIDIFIFIFIFClk 1Clk 2Clk 3Clk 4Clk 5WBEXMEMIDEXClk 9MEMWBWBIDIDIDIl forwardingconsente di eliminare quasi tutte le alee di tipo RAW dellapipeline del DLX senza stallarela pipeline. (NB: nel DLX si alteranoi registri  soloin WB)SUB  R7, R3, R5 aleaOR  R1, R3, R5 aleaLW  R6, 100 (R3) aleaAND R9, R5, R3  no aleaAnche qui il dato non è ancora in RF per essere estratto in ID !",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#31": "Implementazione del ForwardingFU\nEX/MEMMUXMEM/WBALUMUXID/EXMUXMUXRS1/RS2CODOPRD2/CodOpRD1 (registro di destinazione/CodOpConfronto fraRS1, RS2 e RD1, RD2 e i cod. Op.RFMUX\nSpesso realizzato all\u0001interno del RFOppure SPLIT-CYCLE(v. dopo)scrittura prima di lettura\nPermette di \u0002anticipare\u0003il registro su ID/EXControllo MUX: codice operativo IF/ID e confronto RD con RS1 e RS2 IF/IDFU –> Forwarding Unit",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#32": "33Split-cycleT\nIn questo semiperiodo si scriveil registroIn questo semiperiodo si leggeil registro",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#33": "34Alea di dato dovuta alle istruzioni di LOAD\nN.B.ildatorichiestodallaADDèpresentesoloallafinediMEM.L\u0001aleanonpuòessereeliminataconilforwarding(amenodinonaprireunaulteriorediingressoaimuxdellaALUdallamemoria–ritardi!)ADD  R4,R1,R7 SUB  R5,R1,R8AND  R6,R1,R7LW    R1,32(R6)MEMWBIFIDEXMEMIFIDEXIFIDIFIDEX\nLW     R1,32(R6)ADD  R4,R1,R7 SUB  R5,R1,R8AND  R6,R1,R7IFIDEXMEMWBIFIDSEXMEMIFSIDEXSIFIDE\u0001necessario stallare lapipelineDi fatto non viene generato il clock. Il blocco di un  clock si propaga lungo  la pipeline uno stadio alla volta. Dalla fine di questo stadio in poi normale forwarding",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#34": "Delayed loadIn diverse CPU RISC l\u0001alea associata alla LOAD non è gestita in HW stallando la pipeline ma è gestita via SW dal compilatore (delayed load): Istruzione LOADdelay slotIstruzione SuccessivaIl compilatore cercadi riempire il delay-slotcon un\u0001istruzione \u0002utile\u0003(caso peggiore: NOP).LW     R1,32(R6)LW     R3,10 (R4)ADD   R5,R1,R3LW     R6, 20 (R7)LW     R8, 40(R9)LW     R1,32(R6)LW     R3,10 (R4)ADD   R5,R1,R3LW     R6, 20 (R7)LW     R8, 40(R9)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#35": "36Alee di Controllo\nBEQZ R4, 200PCBEQZ R4, 200PC+4SUB  R7, R3, R5PC+8OR   R1, R3, R5PC+12LW   R6, 100 (R8)PC+4+200AND R9, R5, R3(BTA)Next InstructionAddressR4 = 0 :    Branch Target Address(taken)R4 ¹0 :   PC+4(not taken)Clk 6Clk 7Clk 8IFIDEXMEMWBIDIDClk 1Clk 2Clk 3Clk 4Clk 5MEMWBEXMEMEXIFIFWBIDIDIDIFEXWBIDMEMFetch connuovo PCNuovo valore PC calcolato (Aluout)SUB R7, R3, R5OR R1, R3, R5LW R6, 100 (R8)Nuovo valore in PC (un clock dopo)  \nIDIFEXWBIDMEM",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#36": "ADD4\nIMRFSEPCDECInstruction FetchInstruction DecodeExecuteMemoryWriteBack\nIF/IDID/EXALUMUXEX/MEMMUXMUXDatapath in Pipeline del DLX  (caso 1/3) -(Branch o JMP)BEQZ R4, 200\nMUXDMMEM/WBNel momento in cui il nuovo PC agisce sulla IM treistruzioni hanno eseguito i primi trestadi (fino a EXincluso)=0?=0?",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#37": "Gestione delle Alee di ControlloBEQZ R4,200Clk 6Clk 7Clk 8IFIDEXMEMWBClk 1Clk 2Clk 3Clk 4Clk 5SSIFSFetch at new PC•Always Stall (blocco di tre clock che si propaga)\nHyp.:  Freq.Branch = 25 %CPI = (1 + S) = ( 1 + 3 * 0.25) = 1.75•Predict Not TakenIFIDEXMEMWBIDIDIDBEQZ R4, 200SUB R7, R3, R5OR R1, R3, R5LW R6, 100 (R8)Clk 6Clk 7Clk 8Clk 1Clk 2Clk 3Clk 4Clk 5MEMWBEXMEMEXIFIFIFWBEXWBIDIDIDMEMBranch CompletionFlush:diventanoNOPNOP           NOP           NOP           IF–maquil\u0001istruzioneprecedentenonancoradecodificataSIFIFIDSSituazione realeIF ripetuto PC <-PC -4Qui il nuovo valoreè campionato dal PC\nNessun danno: nessuna istruzione ha effettuato WB !",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#38": "Delayed branchSimilmentealcasodellaLOAD,indiverseCPUditipoRISCl\u0001aleaassociataalleistruzionidiBRANCHègestitaviaSWdalcompilatore(delayedbranch):Istruzione BRANCHdelay slotIstruzione SuccessivaIl compilatore cercadi riempire i delay-slotcon istruzioni \u0002utili\u0003(caso peggiore: NOP).delay slotdelay slot",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#39": "Delayed branch/jumpAdd  R5, R4, R3Sub   R6, R5, R2Or     R14, R6, R21Sne   R1, R8, R9; condizione di branchBr     R1, +100Sne   R1, R8, R9; condizione di branchBr     R1, +100Add  R5, R4, R3Sub   R6, R5, R2Or     R14, R6, R21CompilatoOriginale\nEseguite inentrambi i casiOvviamente in questo gruppo  non debbono esserci salti !!!!Al posto di una o più istruzioni \u0001posposte\u0002il compilatore mette delle NOP in caso non riesca a trovarne di adatte",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#4": "•Latenza: 5 fasi(clock)•Throughput: a regime, dopo5 fasi(clock), un’automobileper fase(clock)",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#40": "Gestione delle Alee di Controllo con BTBDynamic Prediction: Branch Target Buffer -> nessuno stallo (quasi)T/NTTAGSPredicted PCPC=HIT:  Fetch a PC  predettoMISS: Fetch a  PC + 4Predizione Corretta :    0 stalli Predizione Errata :       da 1 a 3 stalli  (fetch corretto in  ID o EX   v. precedentemente)N.B.  Qui il branch è individuato durante il periodo del clock IF che carica IR1 in IF/ID",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#41": "Buffer di predizione: caso più semplice un bit che  indica cosa è successo l'ultima volta.\nIn presenza di preponderanza di un caso quando si verifica il caso opposto si hannodueerrorisuccessivi.Loop1Loop2Quando esce da loop2 sbaglia (predetto takenma in realtà untaken) ma sbagli ancora quando predice untakenrientrando nuovamente in loop2 a causa di  loop1 ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#42": "Normalmente duebits.TAKENTAKEN\nUNTAKENUNTAKENTAKENUNTAKENTAKENUNTAKENTAKENTAKEN\nUNTAKENUNTAKEN",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#43": "Esempio, molto frequente, di loop annidato:for (i=0; i<5000; i++)for (j=0; j<1000; j++}{x[i,j] = i*j + i + j;...... }",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#5": "Principio del Pipelining1) Sistema Sequenziale A2A3tANA1TALatency(tempo di esecuzione di una istruzione)= TAThroughput=1TA2) Sistema in Pipeline\nSAP1P2P3P4tS1S2S3S4Si: pipeline stage",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#6": "Principio del PipeliningP1TPP2P1A2P2P3P1A3tA1\nSS1S2S3S4P4P3P2P1A4P4P3P4P2P3P4AnLatency(2)= 4 *TP = TAThroughput(2)@1TP4TA==4 * Throughput(1)TP : pipeline cycle ",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#7": "Pipeliningin una CPU (DLX)Attività:    A1 , A2 , A3…ANIstruzioni:    I1 , I2 , I3…INIEXIDtMEMWBIF\nCPI=1 (idealmente !)IF/IDID/EXEX/MEMMEM/WBCPU (datapath)IFIDEXMEMWBPipeline CycleClock CycleRitardo dello stadio piùlentoRegistri(Pipeline)Registers)ReticombinatorieN.B. architettura TOTALMENTEdiversa !!!!!",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#8": "Pipeline del DLXInstr iInstr i+1Instr i+2Instr i+3Instr i+4IFIDEXMEMWB\nTclk=  Td  +  TP+  TsuClock CycleCPI (ideale)  = 1\nOverhead introdotto dai Pipeline Registers:Ritardo registroa monteSet-up registro a valleRitardo stadio combinatorio più lentoIFIDEXMEMWBIFIDEXMEMWBIFIDEXMEMWBIFIDEXMEMWBt",
    "data_test\\rootfolder\\università\\CalcolatoriElettronici\\08_DLX_pipelined.pdf#9": "DDRCTp",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#0": "Strumen￿ matema￿ci per l’analisi deisistemi tempo discreto – LT Cap.￿Controllo DigitaleCorso di Laurea in Ingegneria Informa￿caProf. Federica PascucciMarch ￿, ￿￿￿￿\n￿/￿￿ ",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#1": "Indice￿ Equazioni alle di￿erenzeIEquazioni alle di￿erenzeITrasformata Z\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#10": "Ritardo temporale￿ Trasformata ZSia data la funzionex(t),n u l l ap e rt<0,es i aX(z)laZ-trasformata della sequenzax(kT), che si o￿ene campionandox(t)con periodoT, alloraZ[x(t\u0000nT)] =z\u0000nX(z)Dim.Z[x(kT\u0000nT)],P1k=0x(kT\u0000nT)z\u0000k==z\u0000nP1k=0x(kT\u0000nT)z\u0000(k\u0000n)=[si ponem=k\u0000n]=z\u0000nP1m=\u0000nx(mT)z\u0000m[poich`ex(\u0000kT)=0perk\u00000]=z\u0000n1Xm=0x(mT)z\u0000m=z\u0000nX(z)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#11": "An￿cipo temporale￿ Trasformata ZSia data la funzionex(t),n u l l ap e rt<0,es i aX(z)laZ-trasformata della sequenzax(kT), che si o￿ene campionandox(t)con periodoT, alloraZ[x(t+nT)] =znX(z)\u0000n\u00001Xk=0x(kT)z\u0000k\u0000Dim.Z[x(kT+nT)],P1k=0x(kT+nT)z\u0000k==znP1k=0x(kT+nT)z\u0000(k+n)=[si pone↵=Pn\u00001k=0x(kT)z\u0000k]=zn[P1k=0x(kT+nT)z\u0000(k+n)++↵\u0000↵]==zn[P1m=0x(mT)z\u0000m\u0000↵]==zn[X(z)\u0000↵]￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#12": "Teorema del valor iniziale e ￿nale￿ Trasformata ZSia data la funzionex(t),n u l l ap e rt<0,es i aX(z)laZ-trasformata della sequenzax(kT), che si o￿ene campionandox(t)con periodoT, allora•Teorema del valor iniziale (se esistex(0))x(0)= l i mz!1X(z)•Teorema del valor ￿nale (se esiste il lim)limk!1x(kT)=l i mz!1[(1\u0000z\u00001)X(z)]￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#13": "Teorema del valor iniziale￿ Trasformata ZSia data la funzionex(t),n u l l ap e rt<0,es i aX(z)laZ-trasformata della sequenzax(kT), che si o￿ene campionandox(t)con periodoT, allorax(0)= l i mz!1X(z)Dim.limz!1X(z)= l i mz!11Xk=0x(kT)z\u0000k==l i mz!1[x(0)+x(T)z\u00001+x(2T)z\u00002+...]==x(0)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#14": "Teorema del valor ￿nale￿ Trasformata ZSia data la funzionex(t),n u l l ap e rt<0,es i aX(z)laZ-trasformata della sequenzax(kT), che si o￿ene campionandox(t)con periodoT, alloralimk!1x(kT)=l i mz!1[(1\u0000z\u00001)X(z)]Dim.limz!1[(1\u0000z\u00001)X(z)] = limz!1[X(z)\u0000z\u00001X(z)] ==l i mz!1[P1k=0x(kT)z\u0000k+\u0000P1k=0x((k\u00001)T)z\u0000k]==1Xk=0[x(kT)\u0000x((k\u00001)T)]z\u0000k==l i mk!1x(kT)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#15": "Trasformate notevoli￿ Trasformata Z•Impulso di Kronecker•Gradino unitario•Rampa unitaria•Funzione esponenziale•Funzione sinusoidale￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#16": "Impulso di Kronecker￿ Trasformata Z\u00000(t)=⇢1set=00altrove\u00000(kT)={1,0,0,...}\nZ[\u00000(kT)] =1Xk=0\u00000(kT)z\u0000k==1+0z\u00001+0z\u00002+···==1￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#17": "Gradino unitario￿ Trasformata Z\u0000\u00001(t)=⇢1set\u000000altrove\u0000\u00001(kT)={1,1,...}\nZ[\u0000\u00001(kT)] =1Xk=0\u0000\u00001(kT)z\u0000k==1+z\u00001+z\u00002+···==11\u0000z\u00001=zz\u00001￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#18": "Rampa unitaria￿ Trasformata Z\u0000\u00002(t)=⇢tset\u000000altrove\u0000\u00002(kT)={kT}\nZ[\u0000\u00002(kT)] =T1Xk=0kz\u0000k==T(z\u00001+2z\u00002+...)==Tz\u00001(1+2z\u00001+...)==Tz(z\u00001)2￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#19": "Funzione esponenziale￿ Trasformata Zx(t)=⇢e\u0000atset\u000000altrovex(kT)={e\u0000akT}\nZ[x(kT)] =1Xk=0e\u0000akTz\u0000k==1+e\u0000aTz\u00001+e\u00002aTz\u00002+···==11\u0000e\u0000aTz\u00001=zz\u0000e\u0000aT￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#2": "Equazioni alle di￿erenze (ricorsive)￿ Equazioni alle di￿erenze\nflegame tra le sequenze{ek}ed{uk}uk=f(e0,e1,...,ek;u0,u1,...,uk\u00001)seflineare, tempo invariante, a memoria ￿nitauk=\u0000a1uk\u00001\u0000a2uk\u00002\u0000···\u0000anuk\u0000n++b0ek+b1ek\u00001+···+bmek\u0000msi o￿engonoequazioni alle di￿erenze ricorsive￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#20": "Funzione sinusoidale￿ Trasformata Zx(t)=⇢sin (!t)set\u000000altrovex(kT)={sin (!kT)}\nZ[x(kT)] = [formule di Eulero]=12j✓11\u0000ej!Tz\u00001\u000011\u0000e\u0000j!Tz\u00001◆==12j(ej!T\u0000e\u0000j!T)z\u000011\u0000(ej!T+e\u0000j!T)z\u00001+z\u00002==z\u00001sin(!T)1\u00002z\u00001cos(!T)+z\u00002=zsin(!T)z2\u00002zcos(!T)+1￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#21": "Metodi per an￿trasformare￿ Trasformata Z￿.Lunga divisione(successione)￿.Computazionale(successione)￿.Scomposizione in fra￿ semplici o Heaviside(forma chiusa)￿.Integrale di inversione(forma chiusa)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#22": "Metodo della lunga divisione￿ Trasformata ZConsente di calcolare i valori della sequenza{x(kT)}Ricordando cheX(z)=1Xk=0x(kT)z\u0000k=x(0)+x(1)z+x(2)z2+...ec h eX(z)=N(z)D(z)=c0+c1z+c2z2+...si o￿ene chec0=x(0)c2=x(2)c1=x(1)...￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#23": "Metodo computazionale￿ Trasformata ZConsente di calcolare i valori della sequenza{x(kT)}Ricordando cheX(z)=X(z)U(z)=N(z\u00001)D(z\u00001)doveU(z)=Z[\u00000(kT)]si o￿eneX(z)D(z\u00001)=U(z)N(z\u00001)da cui (trasl. in avan￿)xk+a1xk\u00001+···+anxk\u0000n=b0uk\u0000(n\u0000m)+b1uk\u0000(n\u0000m+1)+···+bmuk\u0000nxk=\u0000a1xk\u00001\u0000···\u0000anxk\u0000nb0uk\u0000(n\u0000m)+b1uk\u0000(n\u0000m+1)+···+bmuk\u0000nMetodo per implementare eq. alle di￿erenze￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#24": "Osservazioni￿ Trasformata Z￿.Il metodo computazionale e quello della lunga divisione possono essere applica￿quando non`e di interesse calcolare una forma chiusa perx(kT),m as iv o g l i o n oconoscere solo alcuni campioni per cara￿erizzare la risposta di un sistema (metodinumerici)￿.Sianom=deg(N(z))en=deg(D(z)), allora si avr`an\u0000m=0)c06=0n\u0000m=k)c0=···=ck\u00001=0￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#25": "Scomposizione in fra￿ semplici o metodo di Heaviside￿ Trasformata ZConsente di calcolare{x(kT)}in forma chiusaSi scomponeX(z)/zin termini di cui l’an￿trasformata`en o t aG(z)=X(z)z=lXi=1riXj=1Ri,j(z\u0000pi)jsi an￿trasformano i singoli termini (prop. linearit`a) dopo aver mol￿plicato perzecalcolatoRi,jcon la formula per i residuiRi,j=1(ri\u0000j)!limz!pi⇢dri\u0000jdzri\u0000j(z\u0000pi)riX(z)z\u0000\u0000NBLa funzione deve essere stre￿amente propria￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#26": "Osservazioni￿ Trasformata Z￿.Si considera la funzionaG(z)=X(z)zperch´e—Le funzioni da an￿trasformare devono essere stre￿amente proprie (m<n)—Si elimina lo zero inz=0(b0=0)—Si possono u￿lizzare con pi`u facilit`a le tabelle￿.In caso di poli a molteplicit`a singola, il residuo pu`o essere calcolato con la formulaRi=l i mz!pi(z\u0000pi)X(z)zche si o￿ene dalla formula generale￿.Ir e s i d u iRiedRi+iassocia￿ ad una coppia (pi,pi+1)d ip o l ic o m p l e s s ic o n i u g a ￿s o n oanch’essi complessi coniuga￿.￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#27": "Metodo dell’integrale di inversione￿ Trasformata ZConsente di calcolare{x(kT)}in forma chiusa ed`ei lm e t o d op i`u generale (valeanche con trasformateZnon razionali fra￿e)Formula matema￿cax(kT)=12⇡jICX(z)zk\u00001dz,k=0,1,2,...\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#28": "Tabella TrasformataZ(￿/￿)￿ Trasformata Z\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#29": "Tabella TrasformataZ(￿/￿)￿ Trasformata Z\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#3": "Indice￿ Trasformata ZIEquazioni alle di￿erenzeITrasformata Z\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#30": "Tabella TrasformataZ(￿/￿)￿ Trasformata Z\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#31": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#32": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#33": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#34": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#35": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#36": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#37": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#38": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#39": "Strumen￿ matema￿ci per l’analisi deisistemi tempo discreto – LT Cap.￿Thanks for sharing your thoughtsTo The TOP￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#4": "TrasformataZ￿ Trasformata Z\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#5": "De￿nizione￿ Trasformata ZSia data una sequenza di valori{xk}2R,d e ￿ n i t ap e rk=0,1,2,...en u l l ap e rk<0.LaZ-trasformata (unilatera) della sequenza{xk}`e la funzione di variabile complessazde￿nita come segueX(z)=Z[xk]==x0+x1z\u00001+x2z\u00002+···+xkz\u0000k+···==1Xk=0xkz\u0000kSe la sequenzaxk`e o￿enuta campionando uniformemente con periodoTil segnalex(t)allora vale la notazioneX(z)=Z[x(t)] =Z[x(kT)]￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#6": "Osservazioni￿ Trasformata Z￿.Dominio di convergenza: zona esterna ad un cerchio di raggioR(raggio diconvergenza) centrato nell’origine￿.Z[x(t)]implica un tempo di campionamentoTX(z)=Z[X(s)]=Z[L\u00001[X(s)]t=kT]￿.Le funzioni considerate qui saranno del ￿po razionale fra￿oX(z)=b0zm+b1zm\u00001+···+bmzn+a1zn\u00001+···+an==b0z\u0000(n\u0000m)+b1z\u0000(n\u0000m+1)+···+bmz\u0000n1+a1z\u00001+...anz\u0000n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#7": "Propriet`ad e l l aZ-trasformata￿ Trasformata Z•Linearit`a•Traslazione nel tempo•Teorema del valor iniziale•Teorema del valor ￿nale￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#8": "Linearit`a￿ Trasformata ZSiano date due sequenzef(kT),g(kT),c o nZ-trasformataF(z),G(z)rispe￿vamente, e due costan￿a,b2C, allora la sequenzax(kT)o￿enuta comex(kT)=af(kT)+bg(kT)haZ-trasformata pari aX(z)=aF(z)+bG(z)Dim.X(z),P1k=0x(kT)z\u0000k==P1k=0[af(kT)z\u0000k+bg(kT)z\u0000k]==aP1k=0f(kT)z\u0000k+bP1k=0g(kT)z\u0000k==aF(z)+bG(z)￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\01StrumentiMatematici.pdf#9": "Traslazione nel tempo￿ Trasformata ZSia data la funzionex(t),n u l l ap e rt<0,es i aX(z)laZ-trasformata della sequenzax(kT), che si o￿ene campionandox(t)con periodoT, allora•Ritardo temporaleZ[x(t\u0000nT)] =z\u0000nX(z)•An￿cipo temporaleZ[x(t+nT)] =znX(z)\u0000n\u00001Xk=0x(kT)z\u0000k\u0000conn=1,2,...￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#0": "Ricostruzione dei segnaliLT Cap.￿Controllo DigitaleCorso di Laurea in Ingegneria Informa￿caProf. Federica PascucciMarch ￿￿, ￿￿￿￿\n￿/￿￿ ",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#1": "Indice￿ Ricostru￿ori realiIRicostru￿ori realiIRicostru￿ore di ordine ￿ (ZOH)IRicostru￿ore di ordine ￿ (FOH)ICon￿nua￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#10": "Ricostru￿ore di ordine uno (FOH)￿ Ricostru￿ore di ordine ￿ (FOH)Segnale ricostruitox1(t)=x(kT)+x(kT)\u0000x((k\u00001)T)T(t\u0000KT)kTt<(k+1)TRisposta impulsivag1(t)=\u0000\u00001(t)+\u0000\u00002(t)T\u00002\u0000\u00001(t\u0000T)+\u00002\u0000\u00002(t\u0000T)T+\u0000\u00001(t\u00002T)+\u0000\u00002(t\u00002T)T\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#11": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#12": "Ricostru￿ore di ordine ￿ (FOH)￿ Ricostru￿ore di ordine ￿ (FOH)L-trasformataH1(s)=1s+1Ts2\u00002e\u0000sTs\u00002e\u0000sTTs2+e\u00002sTs+e\u00002sTTs2==✓1s+1Ts2◆(1\u00002e\u0000sT+e\u00002sT)==1+TsT✓1\u0000e\u0000sTs◆2\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#13": "Analisi frequenziale￿ Ricostru￿ore di ordine ￿ (FOH)Risposta armonicaH1(j!)=1+j!TT✓1\u0000e\u0000j!Tj!◆2==T✓sin(!T/2)!T/2◆2(1+j!T)e\u0000j!TModulo|H1(j!)|=T\u0000\u0000\u0000\u0000sin(!T/2)!T/2\u0000\u0000\u0000\u00002p1+!2T2Fase\\H1(j!)=ArgT✓sin(!T/2)!T/2◆2(1+j!T)e\u0000j!T\u0000== arctan(!T)\u0000!T￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#14": "Analisi frequenziale￿ Ricostru￿ore di ordine ￿ (FOH)\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#15": "Indice￿C o n ￿ n u aIRicostru￿ori realiIRicostru￿ore di ordine ￿ (ZOH)IRicostru￿ore di ordine ￿ (FOH)ICon￿nua￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#16": "Ricostru￿ore a uscita con￿nua￿C o n ￿ n u aSegnale ricostruitox1(t)=x((k\u00001)T)+x(kT)\u0000x((k\u00001)T)T)(t\u0000KT)kTt<(k+1)TRisposta impulsivagc(t)=\u0000\u00002(t)T\u00002\u0000\u00002(t\u0000T)T+\u0000\u00002(t\u00002T)TL-trasformataHc(s)=1\u00002e\u0000sT+e\u00002sTTs2=1T✓1\u0000e\u0000sTs◆2\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#17": "Ricostruzione dei segnaliLT Cap.￿Thanks for sharing your thoughtsTo The TOP￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#2": "Ricostru￿ori reali￿ Ricostru￿ori realiEspansione in serie di Taylorx(t)=x(kT)+dx(t)dt\u0000\u0000\u0000\u0000t=kT(t\u0000kT)+d2x(t)dt2\u0000\u0000\u0000\u0000t=kT(t\u0000kT)22!+...Derivata=rapp. incrementaledx(t)dt\u0000\u0000\u0000\u0000t=kT'x(kT)\u0000x((k\u00001)T)Td2x(t)dt2\u0000\u0000\u0000\u0000t=kT'dx(t)dt\u0000\u0000t=kT\u0000dx(t)dt\u0000\u0000t=(k\u00001)TT''x(kT)\u00002x((k\u00001)T)+x((k\u00002)T)T2￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#3": "Studio dei ricostru￿ori reali￿ Ricostru￿ori reali￿.Segnale ricostruito￿.Risposta impulsiva￿.L-trasformata (Trasformata di Laplace)￿.Analisi in frequenza—Calcolo della funzione in!—Tracciamento della risposta armonica—Considerazioni sul tempo di campionamento￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#4": "Indice￿ Ricostru￿ore di ordine ￿ (ZOH)IRicostru￿ori realiIRicostru￿ore di ordine ￿ (ZOH)IRicostru￿ore di ordine ￿ (FOH)ICon￿nua￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#5": "Ricostru￿ore di ordine zero (ZOH)￿ Ricostru￿ore di ordine ￿ (ZOH)Segnale ricostruitox0(t)=x(kT)kTt<(k+1)TRisposta impulsivag0(t)=\u0000\u00001(t)\u0000\u0000\u00001(t\u0000T)L-trasformataH0(s)=L[g0(t)] =1s\u0000e\u0000sTs=1\u0000e\u0000sTs\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#6": "Analisi in frequenza (￿/￿)￿ Ricostru￿ore di ordine ￿ (ZOH)Risposta armonicaH0(j!)=1\u0000e\u0000j!Tj!=2e\u0000j!T/2!ej!T/2\u0000e\u0000j!T/22j==2e\u0000j!T/2!sin(!T/2)=Tsin(!T/2)!T/2e\u0000j!T/2Modulo|H0(j!)|=T\u0000\u0000\u0000\u0000sin(!T/2)!T/2\u0000\u0000\u0000\u0000Fase\\H0(j!)=ArgTsin(!T/2)!T/2e\u0000j!T/2\u0000=Argsin!T2\u0000\u0000!T2ApprossimazioneH0(j!)'Te\u0000j!T/2￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#7": "Analisi in frequenza (￿/￿)￿ Ricostru￿ore di ordine ￿ (ZOH)\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#8": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\03Ricostruzione.pdf#9": "Indice￿ Ricostru￿ore di ordine ￿ (FOH)IRicostru￿ori realiIRicostru￿ore di ordine ￿ (ZOH)IRicostru￿ore di ordine ￿ (FOH)ICon￿nua￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#0": "Sistemi tempo discreto – LT Cap.￿Controllo DigitaleCorso di Laurea in Ingegneria Informa￿caProf. Federica PascucciMarch ￿￿, ￿￿￿￿\n￿/￿￿ ",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#1": "Indice￿ Funzione di trasferimento tempo discretoIFunzione di trasferimento tempo discretoISchemi a blocchi\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#10": "Blocchi in cascata￿S c h e m i a b l o c c h i\nNel dominio di LaplaceY⇤(s)=G⇤(s)H⇤(s)X⇤(s)Z-trasformataY(z)=G(z)H(z)X(z)FdTY(z)X(z)=G(z)H(z)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#11": "Blocchi in cascata￿S c h e m i a b l o c c h i\nNel dominio di LaplaceY⇤(s)=[G(s)H(s)]⇤X⇤(s)Z-trasformataY(z)=GH(z)X(z)FdTY(z)X(z)=GH(z)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#12": "Esempio: confronto blocchi in cascata￿S c h e m i a b l o c c h i\nY(z)X(z)=H0(z)G(z)=G(z)=Z[G(s)]\nY(z)X(z)=H0G(z)=Z[H0(s)G(s)] = (1\u0000z\u00001)ZG(s)s\u0000￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#13": "Controllo a retroazione (￿/￿)￿S c h e m i a b l o c c h i\nE(s)=R(s)\u0000H(s)Y(s)Y(s)=G(s)E⇤(s)sos￿tuendo si o￿eneE(s)=R(s)\u0000H(s)G(s)E⇤(s)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#14": "Controllo a retroazione (￿/￿)￿S c h e m i a b l o c c h icampionando le relazioni preceden￿ si o￿eneE(s)=R⇤(s)\u0000GH⇤(s)E⇤(s)Y(s)=G⇤(s)E⇤(s)La FdT del sistema campionato nel dominio di Laplace risultaY⇤(s)=G⇤(s)R⇤(s)1+GH⇤(s)nel dominio dellaZ-trasformataY(z)=G(z)R(z)1+GH(z)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#15": "Sistemi tempo discreto – LT Cap.￿Thanks for sharing your thoughtsTo The TOP￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#2": "Integrale vs sommatoria di convoluzione￿ Funzione di trasferimento tempo discreto\nc(t)=Zt0g(⌧)x(t\u0000⌧)d⌧==Zt0x(⌧)g(t\u0000⌧)d⌧C(s)=X(s)G(s)\nmk=kXi=0diek\u0000i=kXi=0eidk\u0000iM(z)=D(z)E(z)￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#3": "Dai sistemi tempo con￿nuo ai sistemi tempo discreto (￿/￿)￿ Funzione di trasferimento tempo discreto\ny(t)=8>>>>><>>>>>:g(t)x(0)0t<Tg(t)x(0)+g(t\u0000T)x(T)Tt<2Tg(t)x(0)+g(t\u0000T)x(T)+g(t\u00002T)x(2T)2Tt<3T......g(t)x(0)+g(t\u0000T)x(T)+···+g(t\u0000kT)x(kT)kTt<(k+1)T￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#4": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#5": "Dai sistemi tempo con￿nuo ai sistemi tempo discreto (￿/￿)￿ Funzione di trasferimento tempo discreto\nIn forma compa￿ay(t)=g(t)x(0)+g(t\u0000T)x(T)+···+g(t\u0000kT)x(kT)=kXh=0g(t\u0000hT)x(hT)0t<(k+1)Tcampionando la sequenza o￿enutay(kT)=kXh=0g(kT\u0000hT)x(hT)=kXh=0x(kT\u0000hT)g(hT)ricordando chex(t)=g(t)=0,p e rt<0y(kT)=1Xh=0g(kT\u0000hT)x(hT)=1Xh=0x(kT\u0000hT)g(hT)￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#6": "Funzione di trasferimento discreta￿ Funzione di trasferimento tempo discretoA par￿re dalla relazione trovatay(kT)=1Xh=0g(kT\u0000hT)x(hT)si arriva alla relazione di funzione di trasferimento discretaY(z)=G(z)X(z)Y(z)=1Xk=0y(kT)z\u0000k=1Xk=01Xh=0g(kT\u0000hT)x(hT)z\u0000k=Y(z)=1Xm=01Xh=0g(mT)x(hT)z\u0000m\u0000h=1Xm=0g(mT)z\u0000m1Xh=0x(hT)z\u0000h==G(z)X(z)￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#7": "Indice￿S c h e m i a b l o c c h iIFunzione di trasferimento tempo discretoISchemi a blocchi\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#8": "Ingresso/uscita campiona￿￿S c h e m i a b l o c c h i\nNel caso di ingresso pari ax⇤(t), nel dominio di Laplace in uscita si avr`aY(s)=G(s)X⇤(s)Campionando l’uscita si o￿eneY⇤(s)=[G(s)X⇤(s)]⇤=G⇤(s)X⇤(s)Passando nel dominio dellaZ-trasformataY(z)=G(z)X(z)￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\04SistemiTempoDiscreto 2.pdf#9": "Ingresso con￿nuo￿S c h e m i a b l o c c h i\nNel caso di ingresso pari ax(t), nel dominio di Laplace in uscita si avr`aY(s)=G(s)X(s)Campionando l’uscita si o￿eneY⇤(s)=[G(s)X(s)]⇤6=G⇤(s)X⇤(s)Passando nel dominio dellaZ-trasformataY(z)=GX(z)6=G(z)X(z)￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#0": "Mappings!zer i s p o s t aa r m o n i c aLT Cap.￿ – Cap.￿Stabilit`ad e is i s t e m iat e m p od i s c r e t oLT Cap.￿Controllo DigitaleCorso di Laurea in Ingegneria Informa￿caProf. Federica PascucciMarch ￿￿, ￿￿￿￿\n￿/￿￿ ",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#1": "Indice￿ Mappings!zIMappings!zIRisposta armonicaIStabilit`a\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#10": "Risposta armonica discreta￿ Risposta armonicaLarisposta armonicadiG(z)`ed e ￿ n i t ac o m eG(ej!T)0!<⇡T￿.La funzione`ed e ￿ n i t as o l oi0!<⇡Tin quanto`ep e r i o d i c ai n!sG(ej(!+!s)T)=G(ej(!T+2⇡TT))=G(ej!T)￿.per!\u00000assume valori complessi coniuga￿ rispe￿o al caso!0G(ej(\u0000!)T)=G⇤(ej!T)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#11": "Diagrammi di Bode￿ Risposta armonica•Ha senso considerare solo il range di frequenze!2⇥0,⇡T⇤•La funzioneG(ej!T)`e trascendente e non valgono le regole per il tracciamento deidiagrammi di Bode asinto￿ci•I diagrammi di Bode vanno traccia￿ per pun￿ (cio`e con l’ausilio del calcolatore) opassando nel dominiow￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#12": "Indice￿ Stabilit`aIMappings!zIRisposta armonicaIStabilit`a\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#13": "De￿nizione di stabilit`a￿ Stabilit`aG(z)=N(z)D(z)•Il sistema`easinto￿camente stabilese e solo se tu￿e le radici del polinomio D(z), cio`ei poli del sistema, si trovano all’interno della circonferenza di raggio unitario centratanell’origine del piano Z•Il sistema`estabile semplicementese i poli del sistema si trovano all’interno dellacirconferenza di raggio unitario centrata nell’origine del piano Z tranne al pi`uu n oc o nmodulo pari ad ￿NBDal momento che si tra￿a di sistemi lineari, tempo invarian￿ stabilit`a asinto￿ca estabilit`a BIBO coincidono.￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#14": "Criteri per determinare la stabilit`a￿ Stabilit`a•Calcolo delle radici diD(z)•Analisi dei coe￿cien￿⇧Passare nel dominiowCriterio di Routh-Hurwitz⇧Analizzare i coe￿cien￿ diD(z)Criterio di Jury￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#15": "Criterio di Routh-Hurwitz￿ Stabilit`a￿.Si trasformaD(z)sos￿tuendoz=1+w1\u0000w￿.Si analizzaD(w)costruendo la tabella di Routh\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#16": "Trasformazione bilineare￿ Stabilit`aPer la stabilit`as ih a|z|<1|z|=\u0000\u0000\u0000\u00001+w1\u0000w\u0000\u0000\u0000\u0000=\u0000\u0000\u0000\u00001+\u0000+j!1\u0000\u0000\u0000j!\u0000\u0000\u0000\u0000=(1+\u0000)2+!2(1\u0000\u0000)2+!2<1￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#17": "Criterio di Jury￿ Stabilit`a•Si consideraD(z)=a0zn+a1zn\u00001+a2zn\u00002+···+an\u00001z+ancona0>0•Si veri￿cano le seguen￿ condizioni:￿.|an|<a0￿.D(z)|z=1>0￿.D(z)|z=\u00001⇢>0npari<0ndispari￿.|bn\u00001|>|b0||cn\u00002|>|c0|...|q2|>|q0|￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#18": "Tabella di Jury￿ Stabilit`a|z0z1z2...zn\u00001zn\u0000+\u0000\u0000 \u0000 \u0000 \u0000 \u0000 \u00001|anan\u00001...a2a1a02|a0a1a2...an\u00001an3|bn\u00001...b2b1b04|b0b1b2...bn\u000015|cn\u00002...c1c06|c0c1...cn\u00002...|2n\u00005|p3p2p1p02n\u00004|p0p1p2p32n\u00003|q2q1q0￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#19": "Coe￿cien￿ di Jury￿ Stabilit`abk=\u0000\u0000\u0000\u0000anan\u0000k\u00001a0ak+1\u0000\u0000\u0000\u0000k=0,1,...,n\u00001ck=\u0000\u0000\u0000\u0000bn\u00001bn\u0000k\u00002b0bk+1\u0000\u0000\u0000\u0000k=0,1,...,n\u00002...qk=\u0000\u0000\u0000\u0000p3p2\u0000kp0pk+1\u0000\u0000\u0000\u0000k=0,1,2￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#2": "Legames!z￿ Mappings!zData una funzionex(t)campionataX⇤(s)=X(z)\u0000\u0000\u0000\u0000z=esTda cui si deduce i legames!zz=esTs`e una variabile complessas=\u0000+j!da cuiz=esT=eT(\u0000+j!)=eT\u0000ejT(!+2k⇡T)8k2Z+￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#20": "Mappings!ze risposta armonicaLT Cap.￿ – Cap.￿Stabilit`ad e is i s t e m iat e m p od i s c r e t oLT Cap.￿Thanks for sharing your thoughtsTo The TOP￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#3": "Suddivisione pianoS￿ Mappings!z\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#4": "Poli stabili￿ Mappings!z\nAsse immaginarioz=e0Tej!TModulo|z|=eT\u0000<1￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#5": "Polo nell’origine (￿-￿)￿ Mappings!z\nModulo|z|=e0T=1Fase\\z=\\ej0\u0000·T=\\ej0+·T=0￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#6": "Poli sull’asse immaginario (￿-￿)￿ Mappings!z\nModulo|z|=e0T=1Fase\\z=\\ej!sT4=\\ej2⇡T4T=⇡2￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#7": "Poli al con￿ne della striscia primaria (￿-￿)￿ Mappings!z\nModulo|z|=e0T=1Fase\\z=\\ej!sT2=\\ej2⇡T2T=⇡￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#8": "Poli al con￿ne della striscia primaria (￿-￿)￿ Mappings!z\nModulo|z|=e\u00001T=0Fase\\z=\\ej!sT2=\\ej2⇡T2T=⇡￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\05MappingSZStabilita 2.pdf#9": "Indice￿ Risposta armonicaIMappings!zIRisposta armonicaIStabilit`a\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#0": "Intro to Digital Control SystemsDigital Control SystemsComputer Science EngineeringProf. Federica PascucciMarch ￿, ￿￿￿￿\n￿/￿￿ ",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#1": "Table of Contents￿C o u r s e i n f oICourse infoITeaching sta￿IResourcesIExamICourse outline￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#10": "Moodle Website￿R e s o u r c e s•Outcomes•Textbooks•Syllabus•Slides and notes•Links•Exams\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#11": "Teams Group￿R e s o u r c e s•Old recordings•Slides and notes•Forms!The course will be in presence only\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#12": "Table of Contents￿ ExamICourse infoITeaching sta￿IResourcesIExamICourse outline￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#13": "Exam￿ Exam•￿ Mid term tests or wri￿en exam (Aula Campus)•Oral exam•Homeworks\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#14": "Table of Contents￿ Course outlineICourse infoITeaching sta￿IResourcesIExamICourse outline￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#15": "Outline￿ Course outline•Digital Control Systems (￿￿%)-Analysis of digital control systems-Design of digital control systems•Microcontroller (￿￿%)-Arduino pla￿orm-Arduino programming•Training: Matlab (￿￿%)￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#16": "Prerequisites￿ Course outline•Con￿nuous ￿me linear systems•Computer architectures•Signal sampling and reconstruc￿on\n￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#17": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#18": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#19": "",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#2": "Digital Control Systems￿C o u r s e i n f oCourse name:Controllo DigitaleSSD:Ing–Inf/￿￿Instructor:Prof. Federica PascucciLectures:Mon-Wed-ThuTimetable:￿￿:￿￿-￿￿:￿￿Room:N￿Textbook:Bonivento, Melchiorri, ZanasiSistemi di controllo digitaleProge￿o LeonardoResources:Moodle, Teams￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#20": "Intro to Digital Control SystemsThanks for sharing your thoughtsTo The TOP￿￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#3": "Table of Contents￿ Teaching sta￿ICourse infoITeaching sta￿IResourcesIExamICourse outline￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#4": "Who am I?￿ Teaching sta￿Federica PascucciAssociate Professor in Automa￿c ControlRobo￿cs and Automa￿on Group (GRA)Chair for Technical Ac￿vi￿es - I-RIM\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#5": "My research interests￿ Teaching sta￿•Cybersecurity for Industry ￿.￿•Wearable Robo￿cs•Autonomous Naviga￿on•Localiza￿on\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#6": "How to contact me￿ Teaching sta￿•Online mee￿ng (Teams)•In presence mee￿ng (DIA ￿.￿￿)•Email: federica.pascucci@uniroma￿.it•Phone: ￿￿ ￿￿￿￿ ￿￿￿￿\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#7": "Tutor￿ Teaching sta￿•Valeria BonaguraEmail: valeria.bonagura@uniroma￿.it•Laura FilardoEmail: laura.￿lardo@uniroma￿.it•Jacopo PisaniEmail: jacopo.pisani@uniroma￿.it￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#8": "Email rules￿ Teaching sta￿•Subject: [CD]•Insert yournameesurnamein the email text!Please no￿ce that email without [CD] in the subject will not beread\n￿/￿￿",
    "data_test\\rootfolder\\università\\ControlloDigitale\\IntroCorsoCD-4.pdf#9": "Table of Contents￿R e s o u r c e sICourse infoITeaching sta￿IResourcesIExamICourse outline￿￿/￿￿",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#0": "Machine Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nDeep Learning: Introduzione\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#1": "Sommario\nInformazioni sul corso  \nProgramma, testi consigliati, precondizioni  \nSoftware tools  \nCos'è il deep learning  \nRepresentation learning  \nAutoencoders  \nFactors of variation  \nApprocci di IA ed evoluzione delle architetture  \nCurse dimensionality  \nLocal constancy & smoothness regularization  \nLibreria D2L",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#10": "Google Colaboratory (o Colab)  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#11": "Google Colaboratory (o Colab)  \nGPUs includes Tesla \n P100\n  (used in Colab), Tesla \n V100\n  (equipped in \nAmazon EC2 P3 instance), and Tesla \n T4\n (equipped in Amazon EC2 \nG4 instance). \n TPUs\n  are tensor processing units developed by Google \nto accelerate operations on a Tensor\n ﬂ\now Graph. Each TPU packs up \nto 180 tera\n ﬂ\nops of \n ﬂ\noating-point performance and 64 GB of high-\nbandwidth memory onto a single board.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#12": "Cos'è il Deep Learning\nIl \nmachine learning\n  riguarda tecnologie capaci di acquisire conoscenza \nrelativamente all'ambiente di interesse allo scopo di risolvere problemi in \nmodo automatizzato, cioè senza l'intervento dell'utente.  \nPer problemi complessi occorre rappresentare la conoscenza come \nconcetti su vari livelli di astrazione, creando dipendenze tra gli stessi, in \nmodo simile a come avviene nella mente umana.  \nDa queste strutture deriva il termine \n deep learning\n . \nStoricamente i computer sono stati impiegati per rappresentare conoscenza \nformale (es. regole per giocare a scacchi) su cui implementare meccanismi \ndi \nreasoning\n  (es. regole logiche) mentre è stato più dif\n ﬁ\ncile rappresentare la \nconoscenza informale.  \nEsempio: Cyc inference engine e \"Fred shaving in the morning\"\n13",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#13": "Representation learning (1)\nSuccessivamente sono state introdotte tecniche di machine learning per \nacquisire la conoscenza (\n know-how\n ) estraendo \n patterns\n  dai dati in modo \nautomatico, es. logistic regression e naive Bayes.  \nLe prestazioni di tali approcci dipendono dalla scelta con cui i dati sono \nrappresentati. È importanti scegliere le informazioni più rilevanti (\n features\n ) \nper il task che si intende risolvere (approccio \n hand-designed\n ). \nLXI + XCIX = ?  \nPer alcuni task de\n ﬁ\nnire una rappresentazione dei task è arduo.  \nEs. identi\n ﬁ\ncare un auto potrebbe ridursi al task di riconoscere le ruote; \ncome puoi farlo a partire da una rappresentazione a pixel?  \nA differenza del hand-designed, il \n representation learning\n  introduce un \nsotto-task nel processo di ML che mira a riconoscere le feature più rilevanti \nin modo automatico. \n14",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#14": "Representation learning (2)\nIdenti\n ﬁ\ncare le features in modo automatico garantisce vantaggi:  \nL'approccio hand-designed è lungo e richiede risorse  \nSi può facilmente adattare l'addestramento a nuovi tasks\n15",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#15": "Autoencoders\nGli \nautoencoders\n  sono composti da un \n encoder\n  e un \n decoder\n . Il primo \nconverte l'input in una rappresentazione compatta, cioè con \ndimensionalità ridotta,, il decoder mira a ricostruire l'input originale da tale \nrappresentazione.  \nL'addestramento degli autoencoders crea uno \n spazio \n che mira a \nrappresentare solo le features salienti necessarie per identi\n ﬁ\ncare una certa \nistanza, tralasciando informazioni non utili.  \nNel corso vedremo diversi tipi di autoencoders. Le \n Generative Adversarial \nNetwork (GAN)\n  impiegano tali tecnologie.\n16",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#16": "Factors of variation\nLe features identi\n ﬁ\ncate con un approccio hand-designed, oppure \nriconosciute in modo automatico durante il learning, devono saper \ndistinguere i \n fattori di variazione\n . \nSono spesso considerati degli elementi astratti (non misurabili) che \nin\nﬂ\nuenzano il modo in cui le istanze vengono viste dagli approcci di ML. \nSe identi\n ﬁ\ncati ci permettono di capire meglio la grande variabilità di \nistanze in certi domini.  \nAd esempio, età, sesso, un certo accento possono in\n ﬂ\nuenzare le parole \npronunciate da una certa persona in un task di speech-recognition. \nOsservando un automobile, la posizione, il colore, l'angolo di incidenza \ndei raggi solari sono altri tipici fattori per l'analisi di una immagine.  \nSe riusciamo a riconoscerli e ignorarli durante il processamento saremmo \nin grado di sempli\n ﬁ\ncare molti task di ML.\n17",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#17": "Esercizio\nProva a identi\n ﬁ\ncare un ulteriore task adatto ad un approccio di ML, ed \nelenca qualche fattore di variazione. \n18",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#18": "Deep Learning\nIl Deep learning segue un approccio di \n representation learning\n , dove certe \nrappresentazioni sono espresse mediante altre più semplici, es., l'immagine \ndi un uomo viene composta da angoli e contorni, che a sua volta sono \nrappresentati con piccoli segmenti.  \nUn esempio di una architettura Deep, il \n multilayer perception:\n19\nImmagine tratta da Zeiler and Fergus (2014).",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#19": "Schema sempli\n ﬁ\ncato approcci di IA\nI box scuri includono fasi esplicite di apprendimento.\n20\n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#2": "Il corso\nL'obiettivo del \n corso di Deep Learning (DL)\n  di 6 CFU è fornire competenze \navanzate e speci\n ﬁ\nche nell'ambito delle architetture di reti neurali Deep.  \nIl corso è strutturato in una parte teorica e \n metodologica\n  sui concetti \nfondamentali, e da una \n attività di programmazione \n in cui tali concetti sono \napplicati nella risoluzione di problemi mediante recenti framework di \nsviluppo (Keras & PyTorch).  \nAl termine del corso lo studente sarà in grado di:  \naddestrare e ottimizzare in maniera adeguata reti neurali Deep;  \nsaper distinguere tra diverse soluzioni, e saper selezionare e \npersonalizzare le architetture di reti più ef\n ﬁ\ncaci da utilizzare in ambiti \napplicativi reali, supervised, unsupervised o seguendo un approccio \nbasato su un apprendimento per rinforzo.  \nIl corso prevede lo svolgimento di progetti.\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#20": "Evoluzione delle architetture\nOgni 2,4 anni il numero di neuroni nascosti è all'incirca raddoppiato.\n21\n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#21": "I.A. & Big Data: il contesto attuale\n2222\n\u0000\n\u0000\u0000Large amount of \nHuman -generated content\nPosizione GPS, “mi piace”, precedenti \nacquisiti, immagini sui social.\nInfrastructures\nCloud computing, GPU -enabled \ninfrastructure\nEU GDPR \npersonal information as \neconomic assetAI-enabled frameworks\nRecommender systems, Speech and Text \nprocessing, Video and Image analysis\nOligopoly on data\nPoche imprese possiedono \ngrandi quantità di dati sull’utente.\nAI-based interpretation of content\nby natural language processing, personality \ntraits, object recognition, etc.AI & Big Data\nLa distribuzione traintelligenza artificiale , e-commerce e abitudini di consumo -12 aprile 2018 Fabio Gasparetti",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#22": "Curse of dimensionality\nNei corsi di I.A. e M.L. sono stati descritti numerosi algoritmi che si \nadattano facilmente i vari task. Ma solo pochi riescono ad affrontare \nproblemi centrali come riconoscere il parlato od oggetti arbitrari.  \nTali task causano il cosiddetto \n curse of dimensionality\n : il numero di \npotenziali con\n ﬁ\ngurazioni di variabili di ingresso cresce in modo \nesponenziali col numero di variabili considerate.  \nNe segue: istanze di training << # potenziali con\n ﬁ\ngurazioni \n23\nIncrementando il numero di dimensioni (da 1d a 3d) il \nnumero di regioni di interesse (box colorati) incrementa, \ne abbiamo necessità di un numero elevato di istanze per \ncaratterizzarne ognuna.",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#23": "Local constancy & Smoothness regularization\nImponiamo che la distribuzione di probabilità a priori, che in\n ﬂ\nuenza i \nparametri ma anche la funzione che stiamo cercando di stimare, sia \nsmoothness prior\n  o \nlocal constancy prior\n . \nSotto questa assunzione, se l'output di una funzione è OK per una certa \nistanza \n x\n, allora l'output è buono anche per istanze vicine ad \n x\n: \nEsempio\n : algoritmo di \n k\n-nearest neighbors.  \nPer dimensionalità elevate, una funzione smooth potrebbe cambiare (in \nmodo smooth) in modo diverso a seconda della dimensione. Occorrono \nmolte istanze di training per caratterizzarle.  \nIl deep learning introduce dipendenze tra le regioni di interesse (cioè nelle \ndistribuzioni dei dati) per ridurre il numero di istanze necessarie. \n24\n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#24": "La libreria D2L\nDurante il corso faremo uso di una libreria Python di supporto chiamata \nD2L.ai  \nLa libreria d2l mette a disposizione alcune funzionalità per rendere il \ncodice più interpretabile e compatto.  \nVediamone alcuni esempi di impiego:  \n01-d2l_3.2.ipynb  \n02-d2l_regressione_3.3.ipynb\n25",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#3": "Il programma\n4\n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#4": "Testi consigliati e altri riferimenti\n• I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning\", MIT Press, \n2016.  \n• A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, \"Dive into Deep Learning\", \n2020 (free online).  \n• A. Geron, “Hands-on Machine Learning with Scikit-Learn, Keras, and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems”, \nO'Reilly Media, Inc, USA, 2019.  \n• M. Nielsen, \"Neural Networks and Deep Learning\", 2019 (free online).  \nAltri riferimenti a codice, tutorial, e altre fonti saranno dati durante il corso.\n5",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#5": "Precondizioni\nLe seguenti lezioni del corso di Machine Learning sono requisiti per il \ncorso di DL:  \nIntroduzione alla Regressione  \nLa Valutazione nella Regressione  \nOver\n ﬁ\ntting, Cross Validation  \nIntroduzione alle Reti Neurali Arti\n ﬁ\nciali (es. algoritmo di \nbackpropagation)  \nSebbene alcuni dei concetti saranno ripresi per introdurre i formalismi \nnecessari al resto del corso.\n6",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#6": "Software Tools\nDocker  \nhttps://www.docker.com/community-edition#/download  \nJupyter Notebook Scienti\n ﬁ\nc Python Stack + Tensor\n ﬂ\now + Tensorboard  \nhttps://github.com/lspvic/jupyter_tensorboard  \ndocker pull lspvic/tensorboard-noteboo\n k\ndocker run -it --rm -p 8888:8888 lspvic/tensorboard-noteboo\n k\nDocker Engine Utility for NVIDIA GPUs  \nhttps://github.com/NVIDIA/nvidia-docker   \nAnaconda + Tensor\n ﬂ\now \nhttps://docs.anaconda.com/anaconda/user-guide/tasks/tensor\n ﬂ\now/ \n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#7": "Jupiter\nhttps://jupyter.readthedocs.io/en/latest/  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#8": "Google Colaboratory (o Colab)  \nServizio di calcolo online basato su Nvidia Tesla T4 GPUs  \n12 GB of RAM  \nﬁ\nno a 12 ore di seguito  \nSupporto multi-ambiente: TensorFlow, Keras, PyTorch, e OpenCV.  \nMolti dataset disponibili nell'ambiente  \nhttps://www.tensor\n ﬂ\now.org/datasets/catalog/overview   \nInterfaccia Jupyter ben nota.  \nDefault: Runtime Python 3 e nessun acceleratore hardware.  \nMenu Runtime -> Change runtime type  \nPossibilità di trasferire l’esecuzione in locale (per elaborazioni molto \nlunghe)  \nhttps://research.google.com/colaboratory/local-runtimes.html  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\01-Introduzione-sbloccato.pdf#9": "Google Colaboratory (o Colab)  \nAcceleratore: GPU\n",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nMultilayer Perceptrons, One-hot encoding e Softmax\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#1": "Sommario\nMonotonicità  \nMLP e Hidden layers  \nNon linearità  \nFunzioni di attivazione  \nDatasets  \nMLP e Tensor\n ﬂ\now \nDa regressione lineare a classi\n ﬁ\ncazione  \nFunziona softmax  \nOne-hot encoding e misure di distanza",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#10": "Alcuni toy datasets \nElenchiamo alcuni dataset che vengono spesso impiegati negli approcci di \nML e DL:  \nMNIST  \nnotMNIST  \nfashion-MNIST  \nDataset più complessi saranno introdotti più avanti. \n11",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#11": "Dataset MNIST\nComposto da cifre numeriche, usato per addestrare sistemi OCR.  \n\"If it doesn't work on MNIST, it won't work at all”; \"Well, if it does work on \nMNIST, it may still fail on others.\"  \nContiene 60K immagini di addestramento e 10K di training.  \n1998: un linear classi\n ﬁ\ner ha ottenuto 7.6% di errore rate.  \n2012: per mezzo di una architettura DL (convolutional neural networks) si è \narrivati al 0.23%.  \nOgni immagine è rappresentata in scala di grigi (256 livelli). Le cifre sono \ncentrate in un box 28x28 pixel: abbiamo 784 valori in [0-255] per rappresentare \nuna cifra.  \nhttp://yann.lecun.com/exdb/mnist/  \nhttps://www.kaggle.com/c/digit-recognizer/data   \nImplementazione online JS (ott’17) \n http://myselph.de/neuralNet.html\n12",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#12": "Dataset MNIST: train.csv e test.csv\nIl \nﬁ\nle train.csv contiene una matrice con 785 colonne. La prima colonna è il \nlabel\n della cifra (es. 3) e le restanti colonne sono la rappresentazione \nsequenziale dell’immagine:  \nIl \nﬁ\nle test.csv ha la stessa rappresentazione senza la prima colonna.  \nEsempio di immagini:\n13\n",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#13": "Dataset MNIST - svantaggi\nTroppo semplice: algoritmi classici di ML raggiungono i 97% di precisione, \narchitetture DL il 99.7%  \nSi rischia di ideare nuove architetture adatte solo per questo dataset e \ndif\nﬁ\ncilmente adattabili in altri contesti.  \nMolto diverso dai task studiati attualmente nell’ambito del DL.\n14",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#14": "Dataset notMNIST\nSimile a MNIST: contiene 10 labels (lettere da A a J), ma ogni lettera nel \ndataset ha un font molto diverso dalle altre, es.:  \nhttp://yaroslavvb.blogspot.\n ﬁ\n/2011/09/notmnist-dataset.html   \nDownload \n http://yaroslavvb.com/upload/notMNIST/  \nnotMNIST_large.tar.gz -> training e validazione  \nnotMNIST_small.tar.gz -> test \n15\n",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#15": "Dataset fashion-MNIST\nFornito da Zalando: 10 classi che fanno riferimento a generi di vestiario (es. \nsandali, t-shirt, borse, etc).  \nContiene 60K immagini di addestramento e 10K di training.  \nOgni immagine è rappresentata in scala di grigi di 28x28 pixel  \nhttps://github.com/zalandoresearch/fashion-mnist   \nSide-by-side accuracy MNIST vs fashion MNIST:  \nhttp://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#\n16\n",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#16": "Altri dataset popolari sulle immagini\nCIFAR-10 (e 100)\n : 60K 32x32 colour images in 10 classes.  \nImageNet\n : 1,5 milioni di immagini organizzate etichettate su WordNet. In \nmedia 1K immagini per concetto.  \nILSVRC2012 task 1\n : 10 milioni di immagini e +1K classi.  \nOpen Image\n : 9 milioni di URLs di immagini annotate con bounding boxes e \nmigliaia di classi.  \nVisualQA\n : open-ended questions su 265K immagini. In media 5.4 questions \nper immagini con 10 ground truth answers per question.  \nThe Street View House Numbers\n : 600K immagini di numeri civici.  \nRisultati sperimentali ottenuti per varie architetture  \nhttp://rodrigob.github.io/are_we_there_yet/build/#datasets  \n17",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#17": "MLP e Tensor\n ﬂ\now\nProviamo a costruire una MLP con Tensor\n ﬂ\now (Keras).  \nCoalb 04-mlp_5.2.1.ipynb\n18",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#18": "Da regressione lineare a classi\n ﬁ\ncazione\nNei problemi di regressione rispondiamo a domande del tipo \"\n Quale \nquantità o valore?\n \". Ma molti problemi mirano a trovare una classe di \nappartenenza,  \nes. è una email di spam? è più probabile che un utente si iscriva ad un \nabbonamento oppure no?  \nCi può interessare la classe più verosimile (\n hard assignements\n ), oppure la \ndistribuzione di probabilità sulle classi possibili (\n soft assignements\n ), o siamo \nin presenza di più classi di appartenenza (\n multi-label classi\n ﬁ\ncation\n ). \nIn caso di più valori in output (es. un layer di output con più nodi), ogni \nvalore può essere interpretato come \n il grado di appartenenza dell'istanza in \ningresso ad una certa classe\n . La loss misura il discostamento tra classe attesa \ne valori prodotti dal modello. \n19",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#19": "Classi\n ﬁ\ncazione binaria\nSi ha interesse ad associare una istanza in input ad un valore in y\n ∈\n{0,1} \nSe usiamo un modello di regressione, estraiamo dall'istanza x features \nnumeriche e le combinano linearmente. Il risultato dipende dalle somme \ndei valori di input e dei parametri del modello.  \nAl risultato del modello applichiamo la funzione \n logistic\n , che restituisce un \nvalore in [0,1]. La funzione è facilmente differenziabile.  \nInterpretiamo tale valore come la probabilità di appartenenza ad una delle \ndue classi.  \nSi ottiene una \n logistic regression\n . \nVogliamo generallizzare la logistic regression al caso K classi, con K>2\n20!\"=\targmax!*(!|-)",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#2": "Motivazioni\nTale dispensa richiama in modo sommario molti concetti trattati nel corso di \nML con particolare attenzione ai concetti che interessano maggiormente lo \nsviluppo di architetture DL (architetture MLP).  \nSi rimanda al materiale del corso di ML per i dettagli\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#20": "Esempio  \nSupponiamo di avere 3 classi e l’output della combinazione lineare sia:  \nSebbene la classe più probabile sia associata all’indice 1, i valori non sono \ndirettamente interpretabili come distribuzioni di probabilità, infatti:  \nI valori non sono in in [0,1]  \nLa somma non è pari 1\n21y=2.0\n1.0\n0.1⎡\n⎣⎢\n⎢\n⎢⎤\n⎦⎥\n⎥\n⎥",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#21": "La funzione Softmax\nLa funzione \n softmax\n  prende in input un vettore in \n ℝ\nT\n e dà in output un \nvettore \n ℝ\nT  \nnell'intervallo (0,1] la cui somma è pari a 1. È de\n ﬁ\nnita: \nL'output può essere interpretato come distribuzione di probabilità su K \nclassi, a differenza di altri modelli (es. classi\n ﬁ\ncatore SVM).\n22S(yi)=eyi\neyj\njK\n∑",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#22": "Layer softmax nelle reti neurali\nLa funzione softmax è tipicamente applicata all’output di un layer fully-\nconnected, creando un nuovo layer chiamato \n softmax\n . \nIl seguente esempio rappresenta un singolo layer, con funzione di attivazione \nsoftmax su T classi.  \nNota\n : la funzione softmax introduce non linearità.\n23\ny s(y)",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#23": "Softmax in Keras\nIn Keras è semplice implementare il modello precedente con il parametro \nactivation\n  di layer Dense:  \n24\n",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#24": "One-hot encoding\nNel ML le rappresentazioni dell’input e output sono sottoinsiemi dei \ndomini \n ℕ\n e \nℝ\n. Tali insieme introducono implicitamente ordinamenti.  \nEs. se abbiamo 3 categorie (es. rosso=1, bianco=2 e nero=3) e gli \nassegniamo 3 numeri, introduciamo una relazione di ordinamento che \nnon esiste nei dati.  \nDurante l’addestramento tali relazioni possono essere considerate \npotenziali features, e di conseguenza apprese dall'algoritmo  \nEs. Le due istanze Rosso-Nero possono considerarsi più distanti rispetto \na Rosso-Bianco  \nLa rappresentazione \n one-hot\n  caratterizza ogni istanza con una \ncon\nﬁ\ngurazione univoca, costituita da una sequenza binaria di zero, tranne \nun solo elemento pari a 1.\n25",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#25": "One-hot encoding in Python\nColab 05_onehot.ipynb\n26",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#26": "Loss e one-hot encoding\nSe la \n softmax\n  genera una distribuzione di probabilità su K possibili, la \ncodi\nﬁ\nca one-hot genera una distribuzione che \"concentra\" tutta la densità di \nprobabilità sulle classi corrette, es.:  \n[0, …, 0, 1, 0 …, 0].  \nPer addestrare il modello occorre de\n ﬁ\nnire una misura di loss che tenga conto \ndella distanza tra le due distribuzioni.\n27",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#27": "Misura di Distanza: cross entropy\nPer confrontare due generici vettori \n p\n e \nq\n che rappresentano distribuzioni di \nprobabilità si impiega la misura \n cross entropy\n : \nDove \n x\n si estende su tutte i valori potenziali della variabile causale su cui \nsono de\n ﬁ\nnite le probabilità, cioè le classi in output.  \nAttenzione: la funzione H non è simmetrica:  \nSe uno dei parametri (\n p\n o \nq\n) è codi\n ﬁ\ncato one-hot, in che posizione conviene \naverlo?\n28H(p,q)≠H(q,p)H(p,q)=−p\nx∑ (x)⋅log\tq(x)",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#28": "Misura di Distanza: cross entropy\nNella fase di addestramento un parametro della cross entropy è l’output \ndella funzione softmax s(y), mentre il secondo è la codi\n ﬁ\nca one-hot che \nindica una o più classi di appartenenza.  \nSupponiamo di usare la codi\n ﬁ\nca one-hot per il calcolo dei logaritmi:  \nAnche il layer softmax può generare valori 0, ma è un problema raro e \nfacilmente risolvibile (es. aggiungendo un \n ε\n).\n29D(s(y),ˆy)=−s(y1)⋅log\t1.0+s(y2)⋅log\t0+s(y3)⋅log\t0 ( ) ˆy=1.0\n0.0\n0.0⎡\n⎣⎢\n⎢\n⎢⎤\n⎦⎥\n⎥\n⎥",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#29": "Multinomial logistic classi\n ﬁ\ncation\n30x=2.0\n0.7\n1.5\n...\n8.0⎡\n⎣⎢\n⎢\n⎢\n⎢\n⎢\n⎢⎤\n⎦⎥\n⎥\n⎥\n⎥\n⎥\n⎥S(y)=0.659\n0.242\n0.099⎡\n⎣⎢\n⎢\n⎢⎤\n⎦⎥\n⎥\n⎥y=2.0\n1.0\n0.1⎡\n⎣⎢\n⎢\n⎢⎤\n⎦⎥\n⎥\n⎥ˆy=1.0\n0.0\n0.0⎡\n⎣⎢\n⎢\n⎢⎤\n⎦⎥\n⎥\n⎥D(ˆy,S(y))Input\nLinear model Softmax Onehot rep.\nCross entropy distanceLabels",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#3": "Monotonicità\nLe architetture lineare impongono l'assunzione di \n monotonicità\n : \nl'incremento di una feature generare un incremento/decremento nel valore \nin output del modello, a seconda del valore dei pesi (o parametri).  \nPer certi task è verosimile, sebbene non sempre vero, ad esempio:  \nTask: \"\n un individuo sarà regolare con le rate del mutuo?\n \". Se il salario \npassa da 0K a 50K la probabilità che ripaghi il mutuo sarà molto diversa; \nmentre se il salario passa da 1M a 1,05M la probabilità non cambierà \nmolto.  \nTask: \"\n predire se un individuo è malato in base alla temperatura\n \".  \nT << 37 o T >> 37 indica una possibile patologia.  \nCome pensi si può risolvere il problema impiegando un algoritmo di \nregressione lineare?\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#30": "Esercizio\nSupponiamo di avere 3 istanze di addestramento che consistono in varie \nfeatures (es. sex, age, etc) e vogliamo predire se un elettore voterà \ndemocratico o repubblicano con una rete neurale.  \nAvendo due reti che producono in output i seguenti valori:  \nCalcola l’errore impiegando: (1) cross entropy, (2) mean squared error, (3) \naccuratezza (binaria).\n31\n#1\n#2",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#31": "Confronto tra misure di loss\nCross entropy  \n#1: -(ln(0.4) + ln(0.4) + ln(0.1)) / 3 = 1.38  \n#2: -(ln(0.7) + ln(0.7) + ln(0.3)) / 3 = 0.64 (smaller)  \nMean squared error  \n#1: [(0.3 - 0)^2 + (0.3 - 0)^2 + (0.4 - 1)^2 + …] / 3  \n(0.54 + 0.54 + 1.34) / 3 = 0.81  \n#2: (0.14 + 0.14 + 0.74) / 3 = 0.34 (smaller)  \nAccuratezza (binaria)  \nEntrambi: classi\n ﬁ\ncation error 1/3 = 0.33, accuracy 2/3 = 0.67  \nNota\n : le implementazione delle misure discusse sono in \n sklearn.metrics  \n32",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#32": "Confronto tra misure di loss\nCross entropy  \n#1: 1.38  \n#2: 0.64 (migliore)  \nMean squared error  \n#1: 0.81  \n#2: 0.34 (migliore)  \nAccuratezza (binaria)  \nEntrambi: 0.67  \nRispetto alla cross entropy, MSE da molta importanza agli output sbagliati, \nma allo stesso tempo, se la rete si avvicina ai risultati corretti, i gradienti \ndiventano assai bassi, rallentando notevolmente la convergenza.\n33\n#1\n#2",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#4": "Monotonicità\nPer un task \"\n l'immagine contiene un cane\n ?\", come possiamo creare una \nrelazione tra un certo pixel è una classe in output?  \nL'assunzione di linearità ci impone un vincolo tra:  \nluminosità del pixel <-> classe di appartenenza;  \nignorando però il contesto (altri pixel) e la complesse relazioni tra essi che \nportano a rappresentare visivamente un oggetto.  \nInvece di de\n ﬁ\nnire una rappresentazione adeguata, impieghiamo reti neurali \nmultistrato\n , dove gli \n hidden layer\n  si occupano di riconoscere una \nrappresentazione adeguata dei dati in input, che viene impiegata da un \npredittore lineare per generare l'output. \n5",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#5": "Hidden layers e MLP\nL'approccio più semplice per aggiungere strati nascosti è \n impilarli\n  (stack) uno \ndopo l'altro, ottenendo L layers.  \nInterpretiamo gli L layer, tranne l'ultimo, come l'insieme di nodi impiegati \nper la rappresentazione, e l'ultimo come predittore lineare.  \nOtteniamo una architettura \n Multilayer perceptron (MLP)\n  fully connected.\n6\n",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#6": "Da lineare a non lineare\nIndichiamo con \n  un \nminibatch\n  (una sottoinsieme del dataset di \ntraining) di \n n\n istanze dove ogni istanza ha \n d\n features.  \nPer un hidden layer con \n h\n unità, indichiamo con \n  il relativo \noutput. Avendo layer fully connected, abbiamo come parametri:  \ni pesi \n   e i  bias \n  . \nIl layer di output avrà parametri:   \n    e    \nL'output è ricavato nel seguente modo:  \n \n \nSecondo te, combinando più funzioni af\n ﬁ\nni, siamo riusciti a introdurre non \nlinearità nel modello?\nX\n∈\nℝ\nn\n×\nd\nH\n∈\nℝ\nn\n×\nh\nW\n(\n1\n)\n∈\nℝ\nd\n×\nh\nb\n(\n1\n)\n∈\nℝ\n1\n×\nh\nW\n(\n2\n)\n∈\nℝ\nh\n×\nq\nb\n(\n2\n)\n∈\nℝ\n1\n×\nq\nH\n=\nX\nW\n(\n1\n)\n+\nb\n(\n1\n)\nO\n=\nH\nW\n(\n2\n)\n+\nb\n(\n2\n)\n7",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#7": "Da lineare a non lineare\nCombinando le equazioni viste in precedenza otteniamo un modello \nequivalente ad un singolo layer:  \n \nLa non linearità viene espressa mediante funzioni di attivazione \n  non \nlineari (es. ReLU) impiegate all'interno delle unità nascoste, a valle della \ntrasformazione af\n ﬁ\nne. \nFacendo \n stacking\n  di più hidden layer con funzioni non lineari, es:  \n \n \nsi ottengono architetture \n deep\n , che approssimano funzioni più complesse.\nO\n=\n(\nX\nW\n(\n1\n)\n+\nb\n(\n1\n)\n)\nW\n(\n2\n)\n+\nb\n(\n2\n)\n=\nX\nW\n(\n1\n)\nW\n(\n2\n)\n+\nb\n(\n1\n)\nW\n(\n2\n)\n+\nb\n(\n2\n)\n=\nX\nW\n+\nb\nσ\nH\n(\n1\n)\n=\nσ\n1\n(\nX\nW\n(\n1\n)\n+\nb\n(\n1\n)\n)\nH\n(\n2\n)\n=\nσ\n2\n(\nH\n(\n1\n)\nW\n(\n2\n)\n+\nb\n(\n2\n)\n)\n8",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#8": "Universal approximators\nCi si può chiedere quanta capacità rappresentativa (i.e. quanto è \n potente\n ) è \nespressa da una rete neurale.  \nAlcuni risultati suggeriscono che \n per\nﬁ\nno con un solo hidden layer\n  è possibile \napprossimare qualsiasi funzione con un numero adeguato di unità.  \nUna rete neurale deep può essere pensata come un programma in C,  \ncioè puoi risolvere qualsiasi problema software, ma i programmi possono \nraggiungere complessità molto elevate.  \nVedremo architetture di reti deep possono risolvere gli stessi task in modo \nmolto più ef\n ﬁ\nciente. \n9",
    "data_test\\rootfolder\\università\\DeepLearning\\02-MLP_onehot_softmax-sbloccato.pdf#9": "Funzioni di attivazione\nNe esistono molte, ad esempio:  \nFunzione Sigmoide  \nTangente iperbolica (tanh)  \nRelu \nLeaky Relu  \nSwish  \nRelu parametrizzato  \nELU \nSoftplus e Softsign  \nSelu \nGelu  \nDurante il corso discuteremo pro e contro delle principali.  \nColab 03-funzioni_di_attivazione_5.1.2\n10",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nKayers e moduli in Keras\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#1": "Sommario\nModuli e Keras  \nSequential  \nModuli custom  \nGestione dei parametri: lettura, condivisione, inizializzazione  \nInizializzazione lazy  \nLayer custom  \nI/O \nGPU e Keras",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#10": "Gestione dei parametri: accesso\nIl loop di addestramento mira a trovare i parametri che minimizzano la funzione di \nloss. Le architetture più classiche hanno implementazioni che si occupano \ninteramente della gestione dei parametri. In altri casi è necessario accedervi \ndurante l'esecuzione (es. debugging, riuso dei parametri in parti diverse del \nmodello).  \nVediamo come accedere ai parametri. Costruiamo un semplice modello:  \nimport \ntensorflow  \nas \ntf \nnet = tf.keras.models.Sequential([  \n    tf.keras.layers.Flatten(),  \n    tf.keras.layers.Dense(\n 4\n, activation=tf.nn.relu),  \n    tf.keras.layers.Dense(\n 1\n), \n]) \nX = tf.random.uniform((\n 2\n, \n4\n)) \nnet(X).shape  \nI layer nel modello sono memorizzati mediante liste. I parametri sono facilmente \naccedibili:  \nnet.layers[\n 2\n].weights       # secondo layer: 4 pesi e 1 bias  \n[<tf.Variable \n 'dense_1/kernel:0'\n  shape=(\n 4\n, \n1\n) dtype=float32, numpy=  \n array([[-\n 0.6941955\n  ], \n        [-\n 0.9906301\n  ], \n        [-\n 0.13128954\n ], \n        [ \n 0.22367525\n ]], dtype=float32)>,  \n <tf.Variable \n 'dense_1/bias:0'\n  shape=(\n 1\n,) dtype=float32, numpy=array([\n 0.\n], dtype=float32)>]  \n11",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#11": "Gestione dei parametri: lettura\nIn Keras i parametri sono salvati in particolari classi. Per ottenere il valore \nbisogna convertire le classi in tensori, ad esempio, per ottenere il bias dal \nsecondo layer della rete:  \ntype\n(net.layers[\n 2\n].weights[\n 1\n]), tf.convert_to_tensor(net.layers[\n 2\n].weights[\n 1\n]) \n(tensorflow.python.ops.resource_variable_ops.ResourceVariable,  \n <tf.Tensor: shape=(\n 1\n,), dtype=float32, numpy=array([\n 0.\n], dtype=float32)>)  \nMentre per ottenere tutti i parametri:  \nnet.get_weights()  \n[array([[-\n 0.20149094\n ,  \n0.69364685\n , -\n0.12403131\n ,  \n0.81778544\n ], \n        [ \n 0.3347332\n  ,  \n0.43645364\n ,  \n0.18376476\n , -\n0.5020199\n  ], \n        [-\n 0.7681664\n  , -\n0.14477473\n , -\n0.6313741\n  ,  \n0.8246415\n  ], \n        [-\n 0.8074637\n  , -\n0.20050609\n ,  \n0.4308104\n  ,  \n0.69257575\n ]], \n       dtype=float32),  \n array([\n 0.\n, \n0.\n, \n0.\n, \n0.\n], dtype=float32),  \n array([[-\n 0.6941955\n  ], \n        [-\n 0.9906301\n  ], \n        [-\n 0.13128954\n ], \n        [ \n 0.22367525\n ]], dtype=float32),  \n array([\n 0.\n], dtype=float32)]  \n12",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#12": "Condivisione dei parametri\nIn alcune architetture è conveniente condividere i parametri in layer distinti, \nin modo che la modi\n ﬁ\nca dei parametri di un layer si ri\n ﬂ\netta sull'altro.  \nshared = tf.keras.layers.Dense(\n 4\n, activation=tf.nn.relu)  \nnet = tf.keras.models.Sequential([  \n    tf.keras.layers.Flatten(),  \n    shared,  \n    shared,  \n    tf.keras.layers.Dense(\n 1\n), \n]) \nnet(X) \nIn questo caso, i gradienti del secondo e terzo layer sono sommati.\n13",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#13": "Inizializzazione dei parametri\nAll'interno del modulo Python \n keras.initializers\n  sono contenute le \nimplementazioni di vari tipi di inizializzazione dei parametri. Tali approcci \ndipendono solitamente dall'input e dell'output e i valori dei bias sono \nimpostati a zero.  \nPer default, l'inizializziazione dei parametri è basata su una distribuzione \nuniforme (\n glorot initializer\n ) nell'intervallo [-k,k], dove k = sqrt(6/(\n ﬁ\nn_in + \nfan_out)).  \nimport \ntensorflow  \nas \ntf \nnet = tf.keras.models.Sequential([  \n    tf.keras.layers.Flatten(),  \n    tf.keras.layers.Dense(\n 4\n, activation=tf.nn.relu),  \n    tf.keras.layers.Dense(\n 1\n), \n]) \nX = tf.random.uniform((\n 2\n, \n4\n)) \nnet(X).shape  \nVedi: \n https://keras.io/api/layers/initializers/  \n14",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#14": "Inizializzazione dei parametri\nNell'esempio si impiega una inizializzazione basata su una distribuzione \ngaussiana con deviazione standard 0.01.  \nnet = tf.keras.models.Sequential([  \n    tf.keras.layers.Flatten(),  \n    tf.keras.layers.Dense(  \n        \n 4\n, activation=tf.nn.relu,  \n        kernel_initializer=tf.random_normal_initializer(mean=\n 0\n, stddev=\n 0.01\n), \n        bias_initializer=tf.zeros_initializer()),  \n    tf.keras.layers.Dense(\n 1\n)]) \nnet(X) \nnet.weights[\n 0\n], net.weights[\n 1\n] \n(<tf.Variable \n 'dense_2/kernel:0'\n  shape=(\n 4\n, \n4\n) dtype=float32, numpy=  \n array([[-\n 0.00021173\n ,  \n0.00316905\n , -\n0.00598176\n ,  \n0.00144992\n ], \n        [-\n 0.00882782\n ,  \n0.01484077\n , -\n0.00652608\n , -\n0.00581241\n ], \n        [ \n 0.00398763\n , -\n0.01069997\n , -\n0.01145216\n , -\n0.00430671\n ], \n        [ \n 0.00342147\n , -\n0.01215916\n ,  \n0.01345742\n ,  \n0.01632656\n ]], \n       dtype=float32)>,  \n <tf.Variable \n 'dense_2/bias:0'\n  shape=(\n 4\n,) dtype=float32, numpy=array([\n 0.\n, \n0.\n, \n0.\n, \n0.\n], dtype=float32)>)  \nInvece per una inizializzazione con valori costanti:  \n    tf.keras.layers.Dense(  \n        \n 4\n, activation=tf.nn.relu,  \n        kernel_initializer=tf.keras.initializers.Constant(\n 1\n), \n        bias_initializer=tf.zeros_initializer()),  \nNota\n : è possibile impiegare inizializzazioni distinte per ogni layer.\n15",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#15": "Inizializzazione dei parametri - custom\nPer una inizializzazione custom dei parametri bisogna creare una classe a \npartire dalla classe Initializer, e de\n ﬁ\nnire la funzione __call__() che restituisce \nil tensore in base alle dimensioni passate come parametro, es:  \nclass \nMyInit\n(tf.keras.initializers.Initializer):  \n    \n def \n__call__\n (\nself\n, shape, dtype=\n None\n): \n        data=tf.random.uniform(shape, -\n 10\n, \n10\n, dtype=dtype)  \n        factor=(tf.abs(data) >= \n 5\n) \n        factor=tf.cast(factor, tf.float32)  \n        \n return\n data * factor  \nnet = tf.keras.models.Sequential([  \n    tf.keras.layers.Flatten(),  \n    tf.keras.layers.Dense(  \n        \n 4\n, \n        activation=tf.nn.relu,  \n        kernel_initializer=MyInit()),  \n    tf.keras.layers.Dense(\n 1\n), \n]) \nnet(X) \nprint\n(net.layers[\n 1\n].weights[\n 0\n]) \n<tf.Variable \n 'dense_8/kernel:0'\n  shape=(\n 4\n, \n4\n) dtype=float32, numpy=  \narray([[-\n 0.\n       , -\n 6.526873\n  ,  \n8.615063\n  ,  \n5.7617836\n ], \n       [ \n 0.\n       ,  \n 0.\n       ,  \n 6.0559807\n , -\n0.\n       ],  \n       [-\n 6.7486644\n ,  \n8.665197\n  ,  \n0.\n       , -\n 7.035637\n  ], \n       [-\n 0.\n       , -\n 0.\n       , -\n 7.608464\n  ,  \n0.\n       ]], dtype=float32)>  \n16",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#16": "Inizializzazione dei parametri - custom (2)\nIn alternativa si possono impostare i parametri direttamente:  \nnet.layers[\n 1\n].weights[\n 0\n][:].assign(net.layers[\n 1\n].weights[\n 0\n] + \n1\n) \nnet.layers[\n 1\n].weights[\n 0\n][\n0\n, \n0\n].assign(\n 42\n) \nnet.layers[\n 1\n].weights[\n 0\n] # stampa  \n<tf.Variable \n 'dense_8/kernel:0'\n  shape=(\n 4\n, \n4\n) dtype=float32, numpy=  \narray([[\n 42.\n       , -\n 5.526873\n  ,  \n9.615063\n  ,  \n6.7617836\n ], \n       [ \n 1.\n       ,  \n 1.\n       ,  \n 7.0559807\n ,  \n1.\n       ],  \n       [-\n 5.7486644\n ,  \n9.665197\n  ,  \n1.\n       , -\n 6.035637\n  ], \n       [ \n 1.\n       ,  \n 1.\n       , -\n 6.608464\n  ,  \n1.\n       ]], dtype=float32)>  \nNell'esempio aggiorno i pesi del primo layer (+1) e imposto uno speci\n ﬁ\nco \npeso al valore 42.\n17",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#17": "Inizializzazione lazy\nNel codice visto, il risultato di alcune istruzioni dipende da iperparametri \nquali la dimensione dei layer (es. inizializzazione dei parametri, inserire un \nlayer senza indicarne il numero di nodi), sebbene tali iperparametri non \nsono esplicitamente indicati.  \nCon la inizializzazione differita (o lazy) è possibile de\n ﬁ\nnire una architettura \nin modo più possibile parametrico, in modo da speci\n ﬁ\ncare solo gli \niperparametri essenziali e derivare gli altri in modo automatico.  \nIn questo esempio manca la dimensione del layer di input, perciò Keras non \npuò de\n ﬁ\nnire completamente gli iperparametri:  \nimport \ntensorflow  \nas \ntf \nnet = tf.keras.models.Sequential([  \n    tf.keras.layers.Dense(\n 256\n, activation=tf.nn.relu),  \n    tf.keras.layers.Dense(\n 10\n), \n]) \n[net.layers[i].get_weights() \n for\n i \nin \nrange\n(\nlen\n(net.layers))]  \n[[], []]  \n...\n18",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#18": "Inizializzazione lazy (2)\nSe proviamo a de\n ﬁ\nnire le dimensioni di un certo input, Keras può \ncompletare l'inizializzazione, ad esempio:  \nX = tf.random.uniform((\n 2\n, \n20\n)) \nnet(X) \n[w.shape \n for\n w \nin\n net.get_weights()]  \n[(\n20\n, \n256\n), (\n256\n,), (\n256\n, \n10\n), (\n10\n,)]\n19",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#19": "Layer custom\nPossiamo de\n ﬁ\nnire layer anche senza parametri da sottoporre ad \naddestramento. Nell'esempio implementiamo una sorta di normalizzazione \nsottraendo la media dai valori in input. Tale operazioni vanno inserite nella \nfunzione call().  \nimport \ntensorflow  \nas \ntf \nfrom \nd2l \nimport\n tensorflow \n as\n d2l \nclass \nCenteredLayer\n (tf.keras.Model):  \n    \n def \n__init__\n (\nself\n): \n        \n super\n().\n__init__\n () \n    \n def \ncall\n(\nself\n, inputs):  \n        \n return\n inputs - tf.reduce_mean(inputs)  \nlayer = CenteredLayer()  \nlayer(tf.constant([\n 1.0\n, \n2\n, \n3\n, \n4\n, \n5\n])) \n<tf.Tensor: shape=(\n 5\n,), dtype=float32, numpy=array([-\n 2.\n, -\n1.\n,  \n0.\n,  \n1.\n,  \n2.\n], dtype=float32)>  \nImpieghiamo il layer custom nel nostro modello, e veri\n ﬁ\nchiamo che con dait \nrandom otteniamo un valore medio in output quasi 0:  \nnet = tf.keras.Sequential([tf.keras.layers.Dense(\n 128\n), CenteredLayer()])  \nY = net(tf.random.uniform((\n 4\n, \n8\n))) \ntf.reduce_mean(Y)  \n<tf.Tensor: shape=(), dtype=float32, numpy=\n 9.313226e-10\n >\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#2": "Deep networks e moduli\nAbbiamo visto come una MLP sia composta da uno o più layer, ognuno \ncomposto da uno o più nodi che costituiscono l'unità elementare di \nelaborazione.  \nAlcuni risultati ci suggeriscono che questo sia un modello suf\n ﬁ\ncientemente \ngenerale per simulare un dominio molto vasto funzioni. Ma risultati \nsperimentali hanno dimostrato che modelli intermedi, più grandi del singolo \nneurone, ma più piccoli dell'intero modello computazionale siano più adatti \nper costruire architetture deep.  \nEsempio\n : l'architettura \n ResNet-152\n  (Residual NN) sviluppata nell'ambito \ndella computer vision è una delle prime architetture con 100ia di layers. \nLa rete è costituita da schemi di nodi e connessioni (\n moduli\n ) che si \nripetono. Particolari tecniche (\n skip connections\n ) sono impiegate per \nrisolvere il vanishing problem.  \nUn modulo può essere un layer, più layers, o l'intero modello; e generalizza \nun elemento computazionale che può essere ripetuto, o riutilizzato in diverse \narchitetture.\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#20": "Layer custom con parametri\nNell'esempio ricreiamo un layer fully connected con una classe custom. \nNella funzione \n __init__\n () creiamo i parametri che non dipendono dalla \ndimensione dell'input, mentre in \n build\n () de\nﬁ\nniamo quelli che dipendono, \ncon eventuale inizializzazione. La funzione \n build\n () viene invocata \nautomaticamente prima di call().  \nLa funzione \n add_weight\n () automatizza la creazione dei parametri da \nsottoporre ad addestramento.  \nclass \nMyDense\n(tf.keras.Model):  \n    \n def \n__init__\n (\nself\n, units):  \n        \n super\n().\n__init__\n () \n        # il secondo parametro indica la dimensione dell'input  \n        \n self\n.units = units  \n    # il secondo parametro indica la dimensione dell'input  \n    \n def \nbuild\n(\nself\n, X_shape):  \n        \n self\n.weight = \n self\n.add_weight(name=\n 'weight'\n , \n            shape=[X_shape[-\n 1\n], \nself\n.units],  \n            initializer=tf.random_normal_initializer())  \n        \n self\n.bias = \n self\n.add_weight(  \n            name=\n 'bias'\n, shape=[\n self\n.units],  \n            initializer=tf.zeros_initializer())  \n    \n def \ncall\n(\nself\n, X): \n        linear = tf.matmul(X, \n self\n.weight) + \n self\n.bias \n        \n return\n tf.nn.relu(linear)  \n# vedi anche https://www.tensorflow.org/guide/keras/custom_layers_and_models\n21",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#21": "Layer con parametri\nNell'esempio ricreiamo un layer fully connected con una classe custom. \nNella funzione \n __init__\n () creiamo i parametri che non dipendono dalla \ndimensione dell'input, mentre in \n build\n () de\nﬁ\nniamo quelli che dipendono, \ncon eventuale inizializzazione. La funzione \n build\n () viene invocata \nautomaticamente prima di call().  \nLa funzione \n add_weight\n () automatizza la creazione dei parametri da \nsottoporre ad addestramento.  \nclass \nMyDense\n(tf.keras.Model):  \n    \n def \n__init__\n (\nself\n, units):  \n        \n super\n().\n__init__\n () \n        # il secondo parametro indica la dimensione dell'input  \n        \n self\n.units = units  \n    # il secondo parametro indica la dimensione dell'input  \n    \n def \nbuild\n(\nself\n, X_shape):  \n        \n self\n.weight = \n self\n.add_weight(name=\n 'weight'\n , \n            shape=[X_shape[-\n 1\n], \nself\n.units],  \n            initializer=tf.random_normal_initializer())  \n        \n self\n.bias = \n self\n.add_weight(  \n            name=\n 'bias'\n, shape=[\n self\n.units],  \n            initializer=tf.zeros_initializer())  \n    \n def \ncall\n(\nself\n, X): \n        linear = tf.matmul(X, \n self\n.weight) + \n self\n.bias \n        \n return\n tf.nn.relu(linear)  \n# vedi anche https://www.tensorflow.org/guide/keras/custom_layers_and_models\n22",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#22": "I/O su \n ﬁ\nle - tensori\nGli addestramenti di reti deep possono essere molto lunghe. È necessario \nsalvare i risultati parziale e \n ﬁ\nnali su \n ﬁ\nle in modo da poterli recuperare \nfacilmente.  \nAd esempio, per salvare e recuperare i tensori:  \nimport \nnumpy \nas \nnp \nimport \ntensorflow  \nas \ntf \n# salvataggio  \nx = tf.range(\n 4\n) \nnp.save(\n 'x-file.npy'\n , x) \n# recupero  \nx2 = np.load(\n 'x-file.npy'\n , allow_pickle=\n True\n) \n# salvataggio di più sensori  \ny = tf.zeros(\n 4\n) \nnp.save(\n 'xy-files.npy'\n , [x, y])  \nx2, y2 = np.load(\n 'xy-files.npy'\n , allow_pickle=\n True\n) \n# o salvare dizionari stringa-tensore  \nmydict = {\n 'x'\n: x, \n'y'\n: y} \nnp.save(\n 'mydict.npy'\n , mydict)  \nmydict2 = np.load(\n 'mydict.npy'\n , allow_pickle=\n True\n) \n23",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#23": "I/O su \n ﬁ\nle - modelli\nPer i modelli, occorre distinguere architettura e parametri. Per la prima, ci si \nbasa sul codice che si usa per crearla, perciò senza salvataggio su \n ﬁ\nle. \nMentre per i parametri si sfruttano le funzionalità di Keras.  \nAd esempio, de\n ﬁ\nniamo una architettura, salviamo i parametri e ricostruiamo \nla rete con il recupero dei parametri:  \nclass \nMLP\n(tf.keras.Model):  \n    \n def \n__init__\n (\nself\n): \n        \n super\n().\n__init__\n () \n        \n self\n.flatten = tf.keras.layers.Flatten()  \n        \n self\n.hidden = tf.keras.layers.Dense(units=\n 256\n, activation=tf.nn.relu)  \n        \n self\n.out = tf.keras.layers.Dense(units=\n 10\n) \n    \n def \ncall\n(\nself\n, inputs):  \n        x = \n self\n.flatten(inputs)  \n        x = \n self\n.hidden(x)  \n        \n return \nself\n.out(x) \nnet = MLP()  \nX = tf.random.uniform((\n 2\n, \n20\n)) \nY = net(X)  \nnet.save_weights(\n 'mlp.params'\n ) \n...  \nclone = MLP()  \nclone.load_weights(\n 'mlp.params'\n )\n24",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#24": "GPU e Keras\nPer default i tensori sono creati in memoria e le computazioni sono sulla \nCPU. Ma possiamo comunque controllare l'elaborazione:  \nimport \ntensorflow  \nas \ntf \nfrom \nd2l \nimport\n tensorflow \n as\n d2l \ndef \ncpu\n():   \n    \n return\n tf.device(\n '/CPU:0'\n ) \ndef \ngpu\n(i=\n0\n):   \n    \n return\n tf.device(\n f'/GPU:\n{\ni\n}\n'\n) \ncpu(), gpu(), gpu(\n 1\n) \n(<tensorflow.python.eager.context._EagerDeviceContext at \n 0x7fafa2b271c0\n >, \n <tensorflow.python.eager.context._EagerDeviceContext at \n 0x7fafa257a100\n >, \n [<tensorflow.python.eager.context._EagerDeviceContext at \n 0x7fafa253ad00\n >, \n  <tensorflow.python.eager.context._EagerDeviceContext at \n 0x7fafa253ab40\n >]) \ndef \nnum_gpus\n ():   \n    \n return \nlen\n(tf.config.experimental.list_physical_devices(\n 'GPU'\n)) \ndef \ntry_gpu\n(i=\n0\n):   \n    \n# restituisce gpu(i) se esiste, altrimenti cpu()  \n    \n if\n num_gpus() >= i + \n 1\n: \n        \n return\n gpu(i) \n    \n return\n cpu() \ndef \ntry_all_gpus\n ():   \n    # \nNumero di GPU disponibili, o CPU se le GPU non ci sono  \n    \n return\n [gpu(i) \n for\n i \nin \nrange\n(num_gpus())]  \ntry_gpu(), try_gpu(\n 10\n), try_all_gpus()\n25",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#25": "GPU e Keras (2)\n# su quale device è allocato il tensore  \n# Nota: è fondamentale avere tutti i parametri di una operazione sullo stesso device  \nx = tf.constant([\n 1\n, \n2\n, \n3\n]) \nx.device  \n'/job:localhost/replica:0/task:0/device:GPU:0'  \n# alloca un tensore su una GPU  \nwith\n try_gpu():  \n    X = tf.ones((\n 2\n, \n3\n)) \n<tf.Tensor: shape=(\n 2\n, \n3\n), dtype=float32, numpy=  \narray([[\n 1.\n, \n1.\n, \n1.\n], \n       [\n 1.\n, \n1.\n, \n1.\n]], dtype=float32)>  \n# alloca un tensore sulla seconda GPU  \nwith\n try_gpu(\n 1\n): \n    Y = tf.random.uniform((\n 2\n, \n3\n)) \n<tf.Tensor: shape=(\n 2\n, \n3\n), dtype=float32, numpy=  \narray([[\n 0.44844735\n , \n0.7493162\n  , \n0.5692874\n  ], \n       [\n 0.10097635\n , \n0.81023645\n , \n0.5274769\n  ]], dtype=float32)>  \n# per calcolare X + Y, dobbiamo averli sullo stesso device  \n# spostiamo X sulla stessa GPU di Y  \nwith\n try_gpu(\n 1\n): \n    Z = X  \nprint\n(X) \nprint\n(Z) \ntf.Tensor(  \n[[\n1. \n1. \n1.\n] \n [\n1. \n1. \n1.\n]], shape=(\n 2\n, \n3\n), dtype=float32)  \ntf.Tensor(  \n[[\n1. \n1. \n1.\n] \n [\n1. \n1. \n1.\n]], shape=(\n 2\n, \n3\n), dtype=float32)  \n# ora possiamo calcolarlo  \nY + Z\n26",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#26": "GPU e Keras (3)\nÈ possibile indicare a Keras di impiegare le GPU disponibili per \nl'elaborazione di un certo modello:  \nstrategy = tf.distribute.MirroredStrategy()  \nwith\n strategy.scope():  \n    net = tf.keras.models.Sequential([  \n        tf.keras.layers.Dense(\n 1\n)]) \nINFO:tensorflow:Using MirroredStrategy \n with\n devices (\n '/job:localhost/replica:0/task:0/device:GPU:0'\n , \n'/\njob:localhost/replica:0/task:0/device:GPU:1'\n ) \nnet(X) \n<tf.Tensor: shape=(\n 2\n, \n1\n), dtype=float32, numpy=  \narray([[-\n 1.1522729\n ], \n       [-\n 1.1522729\n ]], dtype=float32)>  \n# vediamo la conferma cheanche i parametri sono memorizzati nello stesso device  \nnet.layers[\n 0\n].weights[\n 0\n].device, net.layers[\n 0\n].weights[\n 1\n].device  \n(\n'/job:localhost/replica:0/task:0/device:GPU:0'\n , \n \n'/job:localhost/replica:0/task:0/device:GPU:0')\n27",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#3": "Moduli in Keras\nUn modulo è rappresentato da una classe Python che implementa la forward \npropagation, memorizza i parametri, e fornisca la backpropagation. \nQuest'ultimo aspetto può essere delegato alla tecnica autodiff, senza perciò \nde\nﬁ\nnire manualmente i singoli gradienti.  \nAd esempio il seguente codice genera due layer: il primo con 256 nodi \n fully \nconnected\n  (o \ndenso\n ) ed uno di output con 10 nodi.  \nimport \ntensorflow  \nas \ntf \nnet = tf.keras.models.Sequential([  \n    tf.keras.layers.Dense(\n 256\n, activation=tf.nn.relu),  \n    tf.keras.layers.Dense(\n 10\n), \n]) \nX = tf.random.uniform((\n 2\n, \n20\n)) \nnet(X).shape  \nIl modello è costruito istanziando la classe \n Sequential\n  e passandogli i singoli \nlayer come parametri. Sia \n Sequential\n  che \n Dense\n  sono istanze di \n keras.Model\n .  \n4",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#4": "Sequential in Keras\nSequential crea una lista ordinata di layer.  \nLa procedura di forward propagation è de\n ﬁ\nnita implicitamente: l'output di \nun layer corrisponde all'input del secondo.  \nNell'esempio si invoca net(X), che corrisponde alla funzione net.call(X), per \nottenere l'output dal modello appena creato.\n5",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#5": "Moduli custom\nPer creare nuovi moduli occorre tener presente come vengono impiegati \ndurante l'esecuzione:  \n1.\nI dati di input vengono mandati in input alla forward propagation  \n2.\nLa funzione di propagazione restituisce i valori in output  \n3.\nSi calcolano i gradienti dell'output rispetto agli input per mezzo del \nmetodo di backpropagation. Uno step solitamente gestito in automatico  \n4.\nMemorizzare i parametri ottenuti necessari per la successiva forward \npropagation\n6",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#6": "Moduli custom\nAd esempio, la rete precedente (un layer da 256 nodi seguito da un layer di \n10 nodi) si codi\n ﬁ\nca nel seguente modo:  \nclass \nMLP\n(tf.keras.Model):  \n    \n def \n__init__\n (\nself\n): \n        \n # Sempre necessario richiamare il costruttore della superclass  \n        \n super\n().\n__init__\n () \n        \n self\n.hidden = tf.keras.layers.Dense(units=\n 256\n, activation=tf.nn.relu)  \n        \n self\n.out = tf.keras.layers.Dense(units=\n 10\n) \n    \n# forward propagation  \n    \n def \ncall\n(\nself\n, X): \n        \n return \nself\n.out(\nself\n.hidden((X)))  \nde\nﬁ\nnendo il costruttore e la funzione che si occupa della forward \npropagation.  \nIl metodo call permette di creare layer che richiedono particolari \nelaborazioni (es. controllare il \n ﬂ\nusso di esecuzione durante la forward \npropagation) che non corrispondono a quelle prede\n ﬁ\nnite in Keras.\n7",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#7": "Moduli custom - esempio\nDurante l'elaborazione possiamo aver bisogno di accedere a costanti, cioè valori \nche non sono associati a parametri da stimare durante l'apprendimento, perciò \nnon soggetti a back propagation.  \nNell'esempio, istanziamo i parametri in modo casuale, e rimarranno costanti \ndurante il training. Restituiamo la somma dei valori in output.  \nL'esempio è di scarsa utilità ma dimostra le potenzialità dei moduli custom.  \nclass \nFixedHiddenMLP\n (tf.keras.Model):  \n    \n def \n__init__\n (\nself\n): \n        \n super\n().\n__init__\n () \n        \n self\n.flatten = tf.keras.layers.Flatten()  \n       \nself\n.rand_weight = tf.constant(tf.random.uniform((\n 20\n, \n20\n))) \n        \n self\n.dense = tf.keras.layers.Dense(\n 20\n, activation=tf.nn.relu)  \n    \n def \ncall\n(\nself\n, inputs):  \n        X = \n self\n.flatten(inputs)  \n        \n # Usiamo i parametri costanti per generare l'output  \n        X = tf.nn.relu(tf.matmul(X, \n self\n.rand_weight) + \n 1\n)        \n        X = \n self\n.dense(X)  \n        \n # Control flow: simil l1 regularization  \n        \n while\n tf.reduce_sum(tf.math.abs(X)) > \n 1\n: \n            X /= \n 2 \n        \n # reduce_sum() calcola la somma dei valori per una certa dimensione del tensore  \n        \n # senza secondo parametro la somma è operata su tutte le dimensioni del tensore  \n        \n return\n tf.reduce_sum(X)  \nnet = FixedHiddenMLP()  \nnet(X) \n<tf.Tensor: shape=(), dtype=float32, numpy=\n 0.88945085\n >\n8",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#8": "Moduli custom - esempio\nNell'esempio de\n ﬁ\nnisco un altro modello (NestMLP) e successivamente un \nnuovo modello che include il primo come layer:  \nclass \nNestMLP\n(tf.keras.Model):  \n    \n def \n__init__\n (\nself\n): \n        \n super\n().\n__init__\n () \n        \n self\n.net = tf.keras.Sequential()  \n        \n self\n.net.add(tf.keras.layers.Dense(\n 64\n, activation=tf.nn.relu))  \n        \n self\n.net.add(tf.keras.layers.Dense(\n 32\n, activation=tf.nn.relu))  \n        \n self\n.dense = tf.keras.layers.Dense(\n 16\n, activation=tf.nn.relu)  \n    \n def \ncall\n(\nself\n, inputs):  \n        \n return \nself\n.dense(\nself\n.net(inputs))  \nchimera = tf.keras.Sequential()  \nchimera.add(NestMLP())  \nchimera.add(tf.keras.layers.Dense(\n 20\n)) \nchimera.add(FixedHiddenMLP())  \nchimera(X)\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\03-Layers_moduli_keras-sbloccato.pdf#9": "Esercizio\nImplementare un modulo che prende l'output di due moduli (es. \n net1\n e \nnet2\n) e restituisce un output concatenato durante la forward propagation.\n10",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nConvolutional Neural Networks (CNN) - parte 1\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#1": "Sommario\nIntroduzione  \nArchitettura Visual cortex  \nMLP fully connected e limiti  \nInvarianza (spaziale) e principio di località  \nConvolutional layer e canali",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#10": "MLP - fully connected\nLe reti MLP sono comunque ef\n ﬁ\ncaci in molti contesti, ad esempio:  \nIn presenza di dati in formato \n tebellare\n , dove non assumiamo a priori \nuna struttura che mette in correlazione le features per ogni istanza, \nsebbene possano esserci potenziali correlazioni e dipendenze.  \nDati da cui si possono estrarre un numero di features non elevatissimo \n(<<1000), che perciò necessitano di un numero di parametri da stimare \nlimitato.\n11",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#11": "Invarianza (spaziale) #1\n12\nNella identi\n ﬁ\ncazione delle targhe, per addestrare una MLP dobbiamo \ncostruire un training set con molte istanze, in modo da :  \navere lo stesso oggetto che compare in varie posizioni, angolazioni e \ndimensioni.  \noggetto visualizzato parzialmente (es. sul bordo).  \ncasi di overlap tra oggetti etc.",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#12": "Invarianza (spaziale) #2\nNel task \"Where's Waldo?\" non siamo interessati alla posizione, ma solo \nalla presenza o meno di una certa istanza.  \nIl modello dovrebbe tentare di analizzare piccole zone dell'immagine e \nconfrontarle con il pattern \"Waldo\".\n13\n",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#13": "Proprietà locali\n14Per riconoscere certe caratteristiche speci ﬁche analizziamo informazioni \"locali\" o ravvicinate, cioè con una \ndistanza relativa limitata . Non c'è bisogno di considerare l'intera immagine iniziale.  \nUn output associato ad una certa feature (es. occhio o naso) è associato solo un certo numero di pixel in input.  ",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#14": "MLP: Proprietà desiderate\n15\nNei primi layer la rete dovrebbe comportarsi in modo simile \nindipendentemente dalla posizione di una certa regione di interesse \n(\ntranslation invariance\n  o \ntranslation equivariance\n ). \nNei primi layer l'analisi deve essere limitata a piccole regioni \ndell'immagine in input, e non sull'intera immagine (\n principio di località\n ).  \nNei successivi layer, tali analisi considerano regioni più vaste, combinando \nl'output delle analisi precedenti, \n ﬁ\nno ad arrivare all'intera immagine.",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#15": "Esempio MLP e immagini\n16\nSupponiamo di avere una MLP con uno strato nascosto \n H\n. L'input \n X\n è 2d, ed è \nrappresentato da un tensore, anch'esso 2d. Supponiamo per ora che \n H\n abbia la \nstessa struttura di \n X\n. \nIndichiamo con [\n X\n]\ni,j\n e [\nH\n]\ni,j\n il pixel nella posizione <i,j> e il corrispettivo nodo nel \nlayer nascosto.  \nIndichiamo con \n W\n e \nU\n pesi e bias della rete. Poiché ogni nodo di \n H\n riceve input \nda tutti i pixel in input, usiamo matrici-tensori di ordine 4.  \nDove [\n V\n]\ni,j,a,b\n := [\nH\n]\ni,j,i+a,j+b \n , \nperciò introduciamo un semplice cambio notazione. \nGli indici \n a\n e \nb\n sono offset rispetto a <i,j> e scorrono l'intera immagine in input, \nperciò possono assumere valori negativi.  \nNumero di parametri: per immagini 1000x1000 abbiamo 10\n12\n parametri, infatti \nogni nodo in \n H\n deve essere connesso con tutti i nodi del layer precedente.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#16": "Invarianza spaziale nella pratica\n17\nTale proprietà impone che, se abbiamo uno \n shift\n nell'input \n X\n, anche la \nrappresentazione \n H\n deve subire lo stesso \n shift\n, in modo da mantenere lo \nstesso output. Ma questo è possibile solo se \n U \ne \nV\n non dipendono da \n <i,j>\n, \ncioè [\n V\n]\ni,j,a,b\n := [\nV\n]\na,b  \ne \nU\n è una costante.  \nRappresenta l'operatore di \n convoluzione\n . Il pixel <i+a,j+b>, vicino alla \nlocation <i,j>, è pesato con il coef\n ﬁ\nciente [\n V\n]\na,b\n per ottenere l'output [\n H\n]\ni,j\n. \n[\nV\n]\na,b  \nrichiede meno coef\n ﬁ\ncienti poiché è indipendente dalla location. I \nparametri passano da 10\n12\n a 4·10\n6\n, con \n a\n e \nb\n in (-1000,1000).\n",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#17": "Principio di località nella pratica\n18\nLimitiamo l'analisi per determinare [\n H\n]\ni,j\n a una zona \n Δ\n×\nΔ\n, con \nΔ\n<<1000 \n(es. \nΔ\n=10), perciò evitando di considerare l'intera immagine:  \nI parametri si riducono ulteriormente da 4·10\n6\n a 4·\nΔ\n2\n, sebbene il layer \nnascosto mantenga la dimensione iniziale, e perciò la quantità di \ninformazione originale.  \nLa regione \n Δ\n×\nΔ\n che genera le attivazioni nel successivo strato è chiamata \nLocal receptive \n ﬁ\neld (LRF)\n .\n",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#18": "CNN - Convolutional layer e LRF\n19\nnodo\ninput per il successivo hidden layer\n25x2521x21\nEsempio di input:  \nimmagine 25x25 pixe l\nin bianco e neroOutput dopo il primo  \nlayer convolutivo .local receptive ﬁeldOgni nodo è attivato in base \nall'input determinato  \nda una certa posizione del \nLRF  che scorre lungo l'input.input\nConvolutional layer\nnotiamo la riduzione della  \ndimensione rispetto all'inputmatrice delle attivazioni\nelaborazione",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#19": "Convolutional neural networks\n20\nIl layer \n H\n che abbiamo introdotto prende il nome di \n convolutional layer,\n  e \nle rete basate su tale layer \n Convolutional neural networks\n  (CNNs).  \nV\n è comunemente chiamato \n convolution kernel\n  o \nﬁ\nltro\n. \nPer rappresentare features più complesse e ad alto livello, si impiegano più \nlayer convolutivi alternati a non linearità.",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#2": "Introduzione\nAlcune so\n ﬁ\nsticate \n architetture ML\n  sono riuscite a ottenere \n performance superiori a \nquelle umane\n  (es. il gioco degli scacchi con IBM Deep Blue). Ma solo intorno al \n2000\n  si sono ottenute\n  buone performance per  \ntask apparentemente più semplici\n , \ncome:  \n•\nRiconoscere un giocattolo in una immagine  \n•\nSpeech recognition - riconoscimento vocale  \nPer noi sono task semplici perché l'evoluzione ha portato il cervello a costruire \nstrutture con funzioni speci\n ﬁ\nche.  \nQuando le informazioni arrivano alle parti deputate al ragionamento ad alto \nlivello, sono già arricchite di features ad alto livello elaborate da queste strutture.  \n•\nSebbene siamo coscienti che esiste un giocattolo, non sappiamo spiegare quale \nprocesso abbiamo seguito per identi\n ﬁ\ncarlo.  \n•\nLe architetture \n Convolutional Neural Networks (CNN)\n  sono state sviluppate negli \nanni '80 in base agli studi della zona della corteccia deputata al riconoscimento \nvisivo. Nelle ultime 2 decadi si sono diffuse grazie alla presenza di \n GPU\n .\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#20": "Channels (canali)\n21\nLe immagini a colori hanno 3 canali RGB, perciò, ai due assi principali che \nidenti\n ﬁ\ncano le relazioni spaziali, ne aggiungiamo un terzo ottenendo \ntensori 3d [\n X\n]\ni,j,k\n con \n ﬁ\nltri del tipo [\n V\n]\na,b,c\n . \nMuovendoci in profondità, possiamo creare una terza dimensione per ogni \nstrato hidden. In pratica si ha uno \n stack\n  di griglie, chiamato \n feature maps\n , \ndove ogni griglia è creata con un \n ﬁ\nltro distinto. Il numero di griglie \ncorrisponde ai canali per quello strato.  \nGeneralizzando, supponendo di avere più canali in input (\n c\n) e più canali \nnell'hidden layer (\n d\n), si ha:  \nIl successivo layer userà i \n d\n canali dell'hidden layer che diverranno i \n c \ncanali di input.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#21": "Esercizio\n22\nImpiega il dataset di cifre MNIST e crea una rete convolutiva per la \nclassi\n ﬁ\ncazione.  \nColab \n 07-lenet.ipynb \n",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#3": "L'architettura della Visual cortex\nNegli anni '60 \n Hubel e Wiesel\n  hanno dimostrato che  \n•\nmolti neuroni nella parte di corteccia deputata al riconoscimento di \nimmagini possiedono un piccolo \n Local receptive \n ﬁ\neld (LRF)\n , cioè possono \nreagire agli stimoli situati in regioni limitate del campo visuale.  \n•\nsebbene condividano il LRF, \n alcuni neuroni si attivano \n solo\n in presenza di \nlinee orizzontali\n , \naltri \nsolo \ncon quelle \n verticali\n . \n•\nalcuni neuroni hanno LRF più estesi\n  e \nsi attivano in presenza di certe \ncon\nﬁ\ngurazioni di più caratteristiche a basso livello\n .  \n•\nsi può desumere che l'attivazione di neuroni ad alto livello é basata \nsull'output di neuroni a basso-livello che sono ritenuti \"vicini\".  \nAumentando la complessità, ripetendo più volte in cascata i passi riportati, \npossiamo riconoscere \n patterns visuali \n anche molto \n complessi.  \nNota\n : il resto della lezione suppone di considerare \n immagini\n  come istanze di \ninput, ma le tecnologie introdotte possono essere usate anche per altri input.\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#4": "L'architettura della Visual cortex\n5\nSecondo te è una MLP?Ad ogni livello saliamo di astrazione  \nnei pattern individuati\nLocal receptive ﬁelds",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#5": "L'architettura della Visual cortex\n6\nÈ simile a una MLP ,  \nma ogni nodo e connesso solo  \na un piccolo insieme di neuroni viciniAd ogni livello saliamo di astrazione  \nnei pattern individuati\nLocal receptive ﬁelds",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#6": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti?\n7",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#7": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti?  \n1.\nA causa dell'\n elevato numero di parametri da stimare  \n•\nSupponiamo di avere in input una piccola immagine di 100x100 pixel  \n•\nCreiamo un primo layer di appena 1000 nodi, che perciò \n ﬁ\nltra \nnotevolmente le informazioni passata ai successivi layer.  \n•\nPer questo primo strato abbiamo già \n 10 milioni di parametri da stimare\n .\n8",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#8": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti?  \n1.\nA causa dell'\n elevato numero di parametri da stimare  \n•\nSupponiamo di avere in input una piccola immagine di 100x100 pixel  \n•\nCreiamo un primo layer di appena 1000 nodi, che perciò \n ﬁ\nltra \nnotevolmente le informazioni passata ai successivi layer.  \n•\nPer questo primo strato abbiamo già \n 10 milioni di parametri \n da stimare.  \n2.\nSupponiamo che \n certi nodi \n del primo strato \n si specializzino su un certo \ntask\n, es. riconoscere linee orizzontali.  \n•\nI neuroni specializzati sono attivati se il pattern da identi\n ﬁ\ncare è \nlocalizzato in una certa zona.  \n•\nMa vorremmo poter identi\n ﬁ\ncare lo stesso pattern indipendentemente da \ndove compare.\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\04-CNN parte 1-sbloccato.pdf#9": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti?  \n3. Le reti \n MLP \nnon riescono a codi\n ﬁ\ncare esplicitamente l'organizzazione \nspaziale delle features\n . \n•\nNel Visual cortex i neuroni degli strati più vicini all'input identi\n ﬁ\ncano \nfeatures analizzando piccole aree dell'immagine.  \n•\nI neuroni \"ad alto livello\" combinano tali features per identi\n ﬁ\ncare features \nspazialmente più estese.\n10",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nConvolutional Neural Networks (CNN)  \n2a parte\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#1": "Sommario\nConvolutional Neural network  \n•\nConvolutional layer  \n•\nLocal receptive \n ﬁ\neld \n•\nStride e Padding  \n•\nFilters e Feature Maps  \n•\nPooling Layer  \nArchitettura LeNet-5",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#10": "CNN - Stride\n11•La distanza s tra due LRF  adiacenti è chiamata stride . \n•Finora abbiamo visto stride di 1 pixel, ma la LRF  può scorrere di più pixel . \n•Le CNN spesso impiegano kernels di dimensione 1,3,5 o 7. Questo rende più facile mantenere \nla dimensionalità con padding (vedi di seguito) che consistono nello stesso numero di righe in \ncima e in fondo, e colonne a sinistra e a destra dell'immagine. \nOutput layer precedenteLayer convoluzionale\n<------ padding ------>\n<------ padding ------>\nLRF di 3x3  \nStride = 2",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#11": "CNN: Padding\n12•Supponendo stride > 1 , può accadere che il convolutional layer (comunque ridotto di fw-1 e \nfh-1 a causa del LRF ) non abbia le stesse dimensioni del layer precedente poiché la LRF non \npuò scorrere l'intera instanza in input.  \n•Il padding  aggiunge dimensioni  ai dati in input. Normalmente i dati inseriti sono valori nulli \n(0-padding ). Si hanno i seguenti vantaggi :\n•Permettere alla LRF  di scorrere per intero l'immagine in input senza ignorarne delle parti .\n•Un LRF potrebbe \"imparare\" a riconosce una certa feature  quando è centrata \nnell'immagine. Se la feature è posizionata molto vicino al bordo , senza padding potrebbe \nessere ignorata.\n0-padding\n✓LRF\nOutput layer precedente senza padding Output layer precedente con padding",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#12": "CNN: Riduzione dimensionalità e Stride\n13Output layer precedente•La presenza di stride > 1  altera gli indici iniziali e ﬁnali che identi ﬁcato il LRF associato ad \nun certo nodo.  \n•L'attivazione di un nodo nella posizione (i, j) di un certo layer è determinato dagli output dei \nnodi nel layer precedente posizionati nella righe da i × s h  a  i × s h + f h - 1, e nelle colonne da  \nj × s w  a  j × s w + f w - 1.\n•Per s pari a 1, si torna alla formulazione già vista .\n•Stride > 1 riducono la dimensione  del layer convoluzionale a scapito della precisione .\nLayer convoluzionale\n<------ padding ------>\n<------ padding ------>stride verticale\nstride orizzontaleLRF di 3x3  \nStride = 2",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#13": "CNN: Filters\n14Filters•Supponiamo di poter rappresentare gra ﬁcamente i pesi associati a un certo nodo , usati per \nil calcolo della sua attivazione. Tali pesi prendono il nome di ﬁlters  o convolution kernels  \n(o kernels )\n•Ad esempio, una LRF  77 corrisponderà ad un ﬁltro con medesime dimensioni. ×\nNell'esempio ci sono due ﬁltri Vertical ﬁlter e Horizontal ﬁlter entrambi con matrice tutta di 0, \ntranne una colonna di 1 e una riga di 1, rispettivamente.\nInput",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#14": "Esempi di \n ﬁ\nltri e attivazioni (1)\n15Esempio di input  \nimmagine 25x25 pixel\nOutput dopo il primo  \nlayer convolutivo .\nImmagine in input\nImmagine in inputOutput\nOutputFiltro\nFiltroAttivazioni\nAttivazioni",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#15": "Esempi di \n ﬁ\nltri e attivazioni (2)\nhttp://brohrer.github.io/how_convolutional_neural_networks_work.html\n1-1-1\n-11-1\n-1-11\n0.33 -0.11 0.55 0.33 0.11 -0.11 0.77\n-0.11 0.11 -0.11 0.33 -0.11 1.00 -0.11\n0.55 -0.11 0.11 -0.33 1.00 -0.11 0.11\n0.33 0.33 -0.33 0.55 -0.33 0.33 0.33\n0.11 -0.11 1.00 -0.33 0.11 -0.11 0.55\n-0.11 1.00 -0.11 0.33 -0.11 0.11 -0.11\n0.77 -0.11 0.11 0.33 0.55 -0.11 0.33-1-1-1-1-1-1-1-1-1\n-11-1-1-1-1-11-1\n-1-11-1-1-11-1-1\n-1-1-11-11-1-1-1\n-1-1-1-11-1-1-1-1\n-1-1-11-11-1-1-1\n-1-11-1-1-11-1-1\n-11-1-1-1-1-11-1\n-1-1-1-1-1-1-1-1-1=0.77 -0.11 0.11 0.33 0.55 -0.11 0.33\n-0.11 1.00 -0.11 0.33 -0.11 0.11 -0.11\n0.11 -0.11 1.00 -0.33 0.11 -0.11 0.55\n0.33 0.33 -0.33 0.55 -0.33 0.33 0.33\n0.55 -0.11 0.11 -0.33 1.00 -0.11 0.11\n-0.11 0.11 -0.11 0.33 -0.11 1.00 -0.11\n0.33 -0.11 0.55 0.33 0.11 -0.11 0.77\n-1-11\n-11-1\n1-1-11-11\n-11-1\n1-110.33 -0.55 0.11 -0.11 0.11 -0.55 0.33\n-0.55 0.55 -0.55 0.33 -0.55 0.55 -0.55\n0.11 -0.55 0.55 -0.77 0.55 -0.55 0.11\n-0.11 0.33 -0.77 1.00 -0.77 0.33 -0.11\n0.11 -0.55 0.55 -0.77 0.55 -0.55 0.11\n-0.55 0.55 -0.55 0.33 -0.55 0.55 -0.55\n0.33 -0.55 0.11 -0.11 0.11 -0.55 0.33=\n=-1-1-1-1-1-1-1-1-1\n-11-1-1-1-1-11-1\n-1-11-1-1-11-1-1\n-1-1-11-11-1-1-1\n-1-1-1-11-1-1-1-1\n-1-1-11-11-1-1-1\n-1-11-1-1-11-1-1\n-11-1-1-1-1-11-1\n-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1\n-11-1-1-1-1-11-1\n-1-11-1-1-11-1-1\n-1-1-11-11-1-1-1\n-1-1-1-11-1-1-1-1\n-1-1-11-11-1-1-1\n-1-11-1-1-11-1-1\n-11-1-1-1-1-11-1\n-1-1-1-1-1-1-1-1-1ﬁltroattivazioni\nﬁltro\nﬁltro",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#16": "CNN: Feature Maps\n17Filters•Le LRF  scorrono sulla immagine in input. Supponiamo di mantenere costante i valori del \nﬁltro usato per il calcolo dell'attivazione . Tale approccio prende il nome di shared weights . \n•L'insieme delle attivazioni ottenute (output) con lo stesso ﬁltro viene chiamato feature map \npoiché rappresenta le features apprese nella dimensione spaziale. Esse possono essere \nvisualizzate come una immagine.\nNell'esempio si nota che il Vertical ﬁlter crea una feature map  dove le zone dell'input simili a una \nlinea verticale  sono più evidenziate  (cioè più attivazione), mentre le zone  meno simili saranno \npiù scure e sfocate . Discorso duale per il ﬁltro Horizontal ﬁlter.Feature maps\nInput",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#17": "CNN: Stacking feature maps\nIn \nogni layer\n  possiamo avere \n più \nﬁ\nltri con le medesime dimensioni\n . Ogni \nﬁ\nltro produrrà una diversa feature map. Ogni layer sarà così costituito da \nuna sequenza di matrici di attivazioni, perciò una \n struttura 3d\n . \nDurante il \n forward propagation\n  è fondamentale che i \n ﬁ\nltri, cioè i parametri \npesi\n e \nbias\n che costituiscono il layer convoluzionale, rimangano costanti, \nsebbene il valore delle attivazioni, ovvero la \n feature map\n , cambiano in base \nalla posizione del \n LRF\n. Questo permette di:  \n•\nAvere un numero molto minore di parametri da stimare rispetto a un layer \nMLP.  \n•\nDurante la backpropagation, adattare ogni \n ﬁ\nltro ad una particolare \ncaratteristica saliente.  \n•\nLa possibilità di usare lo stesso \n ﬁ\nltro in diverse zone dell'immagine garantisce la \ntranslational simmetry\n , cioè possiamo riconoscere la caratteristica in diverse \nposizioni. Una rete Fully connected (\n FC\n) potrebbe riconoscere una caratteristica \nin una posizione stimando certi parametri, ma non sarebbe in grado di \nriutilizzarli in altre posizioni.\n18",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#18": "Canali multipli in input\nAbbiamo già visto l'operatore di convoluzione in presenza di più canali.  \nSe in ingresso abbiamo più canali, es. RGB, \n c\ni\n > 1, allora il \n ﬁ\nltro \nrappresentato dal tensore \n k\nh\n × \nk\nw\n dovrà essere ripetuto per ogni canale. Se \nconcateniamo i tensori abbiamo un tensore \n c\ni \n× k\nh\n × \nk\nw\n. \nIl risultato sarà un tensore 2d poiché il risultato delle singole convoluzioni \nsarà sommato nella dimensione dei canali.  \nAd esempio, considerando 2 canali in input:\n19\n",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#19": "Canali multipli in output\nNelle CNN tradizionalmente il numero di canali aumentano con il numero \ndi layer processati, generalmente riducendo allo stesso tempo la risoluzione \nspaziale degli input.  \nIdealmente ogni canale rappresenterà un different set di features, ma in \nrealtà le features possono essere \n sparse\n  su più canali.  \nPer avere un output multicanale, creiamo più tensori \n c\ni \n× k\nh\n × \nk\nw \n, ognuno \nper singolo canale in output. Se li concateniamo otteniamo un kernel  \nc\no \n× c\ni \n× k\nh\n × \nk\nw\n .\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#2": "CNN - Struttura gerarchica\n3Input layer:  \nÈ un layer costituito da unità  \na cui viene associato il valore  \ndei singoli pixel dell'immagine.  \nNon c'è reale elaborazione.Primo convolutional layer\nSecondo convolutional layerData una instanza in input, nodi vicini nel convolutional layer layer saranno attivati in base \nalle features estratte da una certa zona dell'input. Astrazione delle features\nNota : Nelle tradizionali MLP , input bidimensionali [N, M] (es. immagini in bianco e nero) sono \ncomunemente ridimensionati a vettori , ovvero matrici di dimensioni [NxM, 1].  \nNelle CNN  tale ridimensionamento è controproducente  poiché si perderebbe l'informazione relativa alla \nvicinanza delle features in input. Struttura gerarchica\n Nell' input layer  le features  \ncorrispondono ai singoli pixel",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#20": "CNN: Feature Maps e Canali\n21...Input\nConvolutional layer 2\nConvolutional layer 1\nUna immagine a colori con 3 matrici \nassociate ai canali RGB, cioè 3 \ncanali.Possiamo de ﬁnire un certo numero di \nﬁltri (es. 12) per riconoscere diverse \ncaratteristiche salienti dell'immagine \niniziale. I ﬁltri analizzano \ncontemporaneamente 3 canali RGB, \nperciò i ﬁltri saranno de ﬁniti con \nmatrici a 3 dimensioni. Un ﬁltro \napplicato all'immagine in input \nproduce un singolo convolutional layer.I successivi layer convoluzionali \nanalizzato le attivazioni di più ﬁltri \ncontemporaneamente. I ﬁltri di questo \nlayer riconosceranno caratteristiche \npiù astratte.depth = 3 depth = 12 depth = 7",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#21": "TensorFlow: Padding\nTensorFlow fornisce il parametro \n padding\n  che può assumere due valori:  \n•\n\"\nVALID\n \" nel caso in cui si voglia ignorare il padding  \n•\n\"\nSAME\n \" per aggiungere automaticamente righe e colonne composte da \nvalori 0 in modo bilanciato per garantire che il LRF scorra l'intera matrice \nin input.\n2201234567891011121300 12345678910111213\nsenza padding ('VALID' ) con padding ('SAME' )ignorati\nstride=5padding P=+3",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#22": "Stride, padding e Keras\nLe dimensioni del kernel e i restanti iperparametri sono de\n ﬁ\nniti via \ncostruttore del modello Conv2D:  \n# numero di kernels pari a 1  \nconv2d = tf.keras.layers.Conv2D(\n 1\n, kernel_size=\n 3\n, padding=\n 'same'\n, strides=\n 2\n) \nconv2d = tf.keras.layers.Conv2D(\n 1\n, kernel_size=(\n 3\n,\n5\n), padding=\n 'valid'\n, strides=(\n 3\n, \n4\n)) ",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#23": "Tuning delle CNN\nRispetto a una MLP abbiamo \n molti più iperparametri da stimare\n : \nNumero di \n ﬁ\nltri per layer (o \n depth\n ) \nDimensione del LRF  \nStride e padding  \nInvece di usare tecniche automatiche per il tuning,\n  ci si ispira ad \narchitetture già studiate \n in letteratura per avere una con\n ﬁ\ngurazione \nverosimilmente già ottimizzata.\n24",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#24": "Risorse di memoria: considerazioni\nLa backpropagation richiede di memorizzare tutti i valor intermedi calcolati \ndurante la forward propagation\n . \n•\nAd esempio, \n convolutional layer \n con \nﬁ\nltri 5\n 5 e con 200 feature maps di \ndimensione 150\n 100 con stride 1 e padding SAME: se in input abbiamo \nimmagini RGB 150\n 100, il numero di \n parametri\n  è (5\n 5\n3+1)\n 200 = \n 15.200  \n•\nNella \n MLP\n, un layer 150\n 100 completamente connesso col layer in input \nrichiederebbe 150\n 100\n 150\n 100\n 3 = \n67.5M di parametri\n . \n•\nOgnuna delle 200 mappe contiene 150\n 100 nodi, ed ogni nodo ricava \nl'attivazione valutando 5\n 5\n3 input, che corrispondono a \n 225 milioni di \nmoltiplicazioni\n  in virgola mobile.  \n•\nCon \nﬂ\noat di \n 32bit\n  il layer di output impiega 200\n 150\n 100\n 32 = \n 11.5Mb \ncirca\n  per ogni istanza. Per 100 istanze il layer occuperebbe più di un \n 1Gb\n. \nIn produzione, le attivazioni di un layer possono essere dimenticate appena i \ncalcoli sul layer successivo sono terminati, richiedendo molta meno memoria \n(cioè al massimo quella di 2 layer contemporaneamente). \n×\n×\n×\n ×\n×\n ×\n×\n×\n ×\n ×\n ×\n×\n×\n×\n×\n ×\n ×\n25",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#25": "Pooling layer\nRipetendo i layer convolutivi, ad ogni layer il \n receptive \n ﬁ\neld\n sarà \n sensibile  \nad una parte sempre maggiore in riferimento all'immagine iniziale. Perciò \ngli ultimi nodi della rete saranno attivati in base all'intera informazione \npresente nell'immagine iniziale.  \nSpesso l'informazione spaziale esatta delle features riconosciute non è \nimportante, soprattutto se ci interessa l'invarianza ad eventuali translazione \ndell'input.  \nI pooling layer sono utili per:  \nmitigare la sensitività\n  dei layer convolutivi rispetto alle posizioni delle \nfeatures  \nridurre la dimensionalità dell'input da elaborare\n .\n26",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#26": "Pooling layer (1)\nI \nlayer di pooling \n ha lo scopo di \n ridurre il numero di parametri \n operando un \ncampionamento\n  (o \ndown-sampling\n ) dei dati. I vantaggi sono i seguenti:  \n•\nMeno complessità computazione  \n•\nMeno risorse di memoria  \n•\nMeno parametri (e ridurre l'over\n ﬁ\ntting come effetto collaterale)  \nCome nel convolutional layer, \n ogni nodo è connesso con un numero limitato di \nnodi del layer precedente \n posizionati in un certo LRF.  \n•\nOccorre de\n ﬁ\nnire dimensione, stride e padding  \nIl \npooling layer non ha parametri.\n  Opera semplicemente una \"\n aggregazione\n \" dei \nvalori associati ai nodi, ad esempio calcolando \n media\n  o \nvalore massimo\n . \nSpesso il calcolo è fatto per ogni canale in input, cioè su un singolo strato alla volta \nrispetto all'intera profondità del layer precedente (es. sul canale R, G e B \nseparatamente).  \n•\nLa profondità (numero di layer) in uscita corrisponderà a quella che si ha in \ningresso. \n27",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#27": "Pooling layer (2)\nNon ha parametri da inferire\n , ma solo iperparametri, cioè dimensione del \nﬁ\neld (\npooling size\n ), il \npooling stride\n , e tipo di aggregazione.  \n•\nSpesso pooling size e stride corrispondono.  \nIn molti scenari \n non è fondamentale la posizione esatta di una certa \ncaratteristica\n , ma il fatto che esista in una certa zona, o che sia identi\n ﬁ\ncata \nuna certa sequenza (o pattern) di features senza considerare esattamente le \nrispettive distanze reciproche.  \n•\nAd esempio, nella face detection ho interesse a riconoscere due occhi \nvicini, ma non mi interessa la distanza esatta.  \nEsistono \n due tipi principali di aggregazione\n : \n•\nmax-pooling:  \nun nodo assume l’attivazione massima tra i valori presenti \nnel \nﬁ\neld considerato.  \n•\naverage pooling:\n  considero il valor medio nel \n ﬁ\neld.\n28",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#28": "Esempio: Pooling layer\nNell'esempio il pooling kernel è di 2\n 2, lo stride pari a 2, padding VALID e \naggregazione max.  \n•\nIl layer di output contiene il 75% in meno dei valori del layer precedente.\n×\n29\nA causa del padding VALID  \nil valore di alcuni nodi sarà ignorato.\nSe in input abbiamo un canale con un layer NN,  fpo è il pooling size , spo il pooling stride ,  \n \nuna dimensione del layer di output è:  ×\nN−fpo\nspo+1",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#29": "CNN: Convolutional layer e dimensione output\nLa dimensione dell'output di un \n convolutional layer\n  si ricava a \npartire dalla dimensione dell'input e dal valore degli iperparametri.  \nSe per semplicità assumiamo input \n N\nN\n, e la dimensione del \n LRF \n \nf\nh\n = f\n w\n = \nf\n, lo stride \n s,\n e le righe (o colonne) \n p\n aggiunte come \npadding, allora una delle due dimensione del layer di output è la \nseguente:  \n \nLa dimensione in output perciò corrisponde a \n O\nO.\n×\nO\n=\nN\n−\nf\n+\np\ns\n+\n1\n×",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#3": "CNN - Esempio di attivazione di un nodo \nL'attivazione di un nodo in un layer convoluzionale si ottiene \nanalizzando l’output dal layer precedente per mezzo del \n LRF\n. \nEsempio: la funzione d’attivazione (\n σ\n) per il nodo <\n l\n,\nk\n> si valuta \nconsiderando il bias \n b\n e la matrice \n W\n di dimensione \n f\nh  \nf\nw \nassociati al \nLRF, in questo caso pari a 3\n 3. \n \nW\n e \nb\n sono i parametri da determinare.  \ni\n e \nj\n sono gli offset riferiti al \n LRF\n. \nSe la \n ﬁ\nnestra scorre un passo alla volta allora \n l\n e \nk\n fanno riferimento \nall’origine della \n ﬁ\nnestra del \n LRF\n.\n×\n×\nσ\n(\nb\n+\n2\n∑\ni\n=\n0\n2\n∑\nj\n=\n0\nw\ni\n,\nj\n⋅\nx\ni\n+\nl\n,\nj\n+\nk\n)\nijlk\nLRF",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#30": "AlexNet\n  (2012) è una delle prime architetture di reti neurali che combina CNN e \nGPU nell'ambito della classi\n ﬁ\ncazione degli oggetti.\nEsempio: calcolo parametri AlexNetoutput depth = 96input depth = 3\nRicordiamoci  che il local receptive ﬁeld  \nha una profondità pari a quella dell’inputEsempi di calcolo dei parametri nel primo layer \nhidden:  \n•Dim. immagine in Input = 227 227 3 \n•Dim. LRF = 11 11 \n•Stride = 4; padding VALID  \n•Numero ﬁltri (o depth) = 96  \n•L’output per ogni ﬁltro avrà dimensione di lato \n(227-11)/4 + 1 = 55. Cioè 55 55 per ﬁltro. \n•Considerando la profondità si ha: 55x55x96 \n=290.400 nodi.  \n•L'attivazione di un nodo si ricava considerando \n11x11x3 nodi del layer precedente.  \n•In una MLP si avrebbero 105.415.200 parametri.  \n•Per la proprietà degli shared weights, nella CNN il \nnumero di parametri sarà 11x11x3x96 + 96 = \n34.944.  × ×\n×\n×\nfeature mapscomputazione",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#31": "Architettura LeNet-5 per OCR\nLeNet-5\n  (1989) è una delle prime architetture CNN.  \n•\nE' stata ideata per fare OCR garantendo un errore <1% su MNIST.  \nCombina layers \n CNN\n  con una rete tradizionale \n MLP\n a valle.  \n•\nLo scopo è di impiegare le caratteristiche salienti identi\n ﬁ\ncate dalle CNN \nper fare classi\n ﬁ\ncazione per mezzo della MLP.  \n•\nUna rete interamente \n MLP fully connected avrebbe richiesto molti più \nparametri\n  per ottenere le stesse prestazioni.",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#32": "Demo LeNet-5\nda http://yann.lecun.com/exdb/lenet/  ",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#33": "Architettura LeNet-5\nconvolutional layer#1 conv. laye r\nfeature maps:  \n28x28, depth 6#3 conv. layer  \nfeature maps:  \n10x10, depth 16\navg.  \npoolingconv. layeravg.  \npooling#2 pooling laye r\nfeature maps:  \n14x14, depth 6#4 pooling laye r\nfeature maps:  \n5x5, depth 16\nconv. layer#6 fully connected layer  \nnodi 84#5 conv. layer  \nfeature maps:  \n1x1, depth 120\nImmagini  \n32x32x1 (gray scale)LRF\nL'output dell'ultimo \nconvolution layer è \nconvertito in un vettore \n120x1, adatto come input di \nun fully connected layer.\nLa ReLU non era ancora \nstata approfondita ai tempi di \nLeNet-5. Si è impiegata la \npiù tradizionale tanh.#7 fully connected layer  \nnodi 10\nLa con ﬁgurazione degli \niperparametri e la dimensione \ndell'input non necessita di \nimpiegare il padding.",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#34": "LeNet-5: esempio di \n ﬁ\nltri\nNel caso del dataset MNIST di caratteri numerici (immagini 28x28), \notteniamo \n ﬁ\nltri di questo tipo:  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#4": "CNN - LRF\n5Output layer precedente•In un certo layer convoluzionale , un nodo con indice (i, j) prende in input  gli output dei nodi \ndel layer precedente posizionati all'interno del LRF .\n•la regione LRF  va dalla riga  i alla riga i+f h-1, e dalla colonna j alla colonna j+f w-1\n•fh e fw corrispondono all'altezza e larghezza del LRF . \n•i e j sono indici che scorrono da 1 a dim x-fh-1 e dim y-fh-1\nIl convolutional layer è \nrappresentato da una \ngriglia bidimensionale \nche contiene il risultato \ndelle attivazioni .forward propagation\nEsempio con LRF 3x3  \ncon stride pari a 1.<------ padding ------>\n<------ padding ------><------ dim x ------>\n<--- dimy -->\nPadding  \n(discusso più avanti)\n•Notazioni: rispetto alle slide precedenti 2Δ=fh=fw",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#5": "Cross-correlazione\nSupponendo \n K\n il kernel e \n X\n l'input 2d, possiamo de\n ﬁ\nnire la funzione \n corr2d\n () che \nrestituisce un output di dimensioni pari all'input, meno la dimensione del kernel \n + \n1\n: \nimport \ntensorflow  \nas \ntf \nfrom \nd2l \nimport\n tensorflow \n as\n d2l \ndef \ncorr2d\n(X, K):  \n    h, w = K.shape  \n    Y = tf.Variable(tf.zeros((X.shape[\n 0\n] - h + \n 1\n, X.shape[\n 1\n] - w + \n 1\n))) \n    \n for\n i \nin \nrange\n(Y.shape[\n 0\n]): \n        \n for\n j \nin \nrange\n(Y.shape[\n 1\n]):         \n            # estraggo la parte di X che mi interessa  \n            # calcolo una moltiplicazione element-wise tra le matrici  \n            # ricavo infine la somma  \n            Y[i, j].assign(tf.reduce_sum(  \n                X[i: i + h, j: j + w] * K))  \n    \n return\n Y \nX = tf.constant([[\n 0.0\n, \n1.0\n, \n2.0\n], [\n3.0\n, \n4.0\n, \n5.0\n], [\n6.0\n, \n7.0\n, \n8.0\n]]) \nK = tf.constant([[\n 0.0\n, \n1.0\n], [\n2.0\n, \n3.0\n]]) \ncorr2d(X, K)  \n<tf.Variable \n 'Variable:0'\n  shape=(\n 2\n, \n2\n) dtype=float32, numpy=  \narray([[\n 19.\n, \n25.\n], \n       [\n 37.\n, \n43.\n]], dtype=float32)>  \nNota\n : l'operatore di \n convoluzione\n  è simile all'operatore \n cross-correlazione\n , ma nel \nprimo il kernel è \"\n capovolto\" \n durante il calcolo. Nelle CNN si impiega usualmente \nla cross-correlazione. Non c'è differenza poiché i pesi ricavati durante \nl'addestramento sono i medesimi, ma con ordine invertito. Spesso nei testi e nel \ncodice i due termini assumono lo stesso signi\n ﬁ\ncato.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#6": "Esempio di modello Conv2D\nclass \nConv2D\n(tf.keras.layers.Layer):  \n    \n def \n__init__\n (\nself\n): \n        \n super\n().\n__init__\n () \n    \n def \nbuild\n(\nself\n, kernel_size):  \n        initializer = tf.random_normal_initializer()  \n        \n self\n.weight = \n self\n.add_weight(name=\n 'w'\n, shape=kernel_size,  \n                                      initializer=initializer)  \n        \n self\n.bias = \n self\n.add_weight(name=\n 'b'\n, shape=(\n 1\n, ), \n                                    initializer=initializer)  \n    \n def \ncall\n(\nself\n, inputs):  \n        \n return\n corr2d(inputs, \n self\n.weight) + \n self\n.bias",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#7": "Esempio: riconoscimento bordi\nSupponiamo che vogliamo riconoscere il bordoe in una immagine \nmonitorando il cambio del valore dei pixel.  \nCostruiamo una immagine 6x8 nel seguente modo:  \nX = tf.Variable(tf.ones((\n 6\n, \n8\n))) \nX[:, \n2\n:\n6\n].assign(tf.zeros(X[:, \n 2\n:\n6\n].shape))  \n<tf.Variable \n 'Variable:0'\n  shape=(\n 6\n, \n8\n) dtype=float32, numpy=  \narray([[\n 1.\n, \n1.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n, \n1.\n, \n1.\n], \n       [\n 1.\n, \n1.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n, \n1.\n, \n1.\n], \n       [\n 1.\n, \n1.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n, \n1.\n, \n1.\n], \n       [\n 1.\n, \n1.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n, \n1.\n, \n1.\n], \n       [\n 1.\n, \n1.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n, \n1.\n, \n1.\n], \n       [\n 1.\n, \n1.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n, \n1.\n, \n1.\n]], dtype=float32)>  \nCostruiamo un kernel 1x2  \nK = tf.constant([[\n 1.0\n, -\n1.0\n]]) \nCon la crosscorrelazione, l'output è 0 quando due elementi adiacenti \ndell'input sono uguali, altrimenti un valore diverso da 0.  \nNota\n : la crosscorrelazione corrisponde ad una approssimazione \ndiscreta della derivata del primo ordine.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#8": "Esempio: riconoscimento bordi\nSi nota il risultato +1 nei bordi da bianco a nero, -1 da nero a bianco:  \nY = corr2d(X, K)  \nY \n<tf.Variable \n 'Variable:0'\n  shape=(\n 6\n, \n7\n) dtype=float32, numpy=  \narray([[ \n 0.\n,  \n1.\n,  \n0.\n,  \n0.\n,  \n0.\n, -\n1.\n,  \n0.\n], \n       [ \n 0.\n,  \n1.\n,  \n0.\n,  \n0.\n,  \n0.\n, -\n1.\n,  \n0.\n], \n       [ \n 0.\n,  \n1.\n,  \n0.\n,  \n0.\n,  \n0.\n, -\n1.\n,  \n0.\n], \n       [ \n 0.\n,  \n1.\n,  \n0.\n,  \n0.\n,  \n0.\n, -\n1.\n,  \n0.\n], \n       [ \n 0.\n,  \n1.\n,  \n0.\n,  \n0.\n,  \n0.\n, -\n1.\n,  \n0.\n], \n       [ \n 0.\n,  \n1.\n,  \n0.\n,  \n0.\n,  \n0.\n, -\n1.\n,  \n0.\n]], dtype=float32)>  \nTrasponendo l'immagine, il kernel non individua più i bordi:  \ncorr2d(tf.transpose(X), K)  \n<tf.Variable \n 'Variable:0'\n  shape=(\n 8\n, \n5\n) dtype=float32, numpy=  \narray([[\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n], \n       [\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n], \n       [\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n], \n       [\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n], \n       [\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n], \n       [\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n], \n       [\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n], \n       [\n 0.\n, \n0.\n, \n0.\n, \n0.\n, \n0.\n]], dtype=float32)>",
    "data_test\\rootfolder\\università\\DeepLearning\\05-CNN parte 2-sbloccato.pdf#9": "Kernel: training\nAl principio non abbiamo kernel precostituiti, dobbiamo ottenerli \ndurante l'addestramento, soprattutto se abbiamo molti layer convolutivi \nin cascata. Il procedimento è simile al caso MLP, es:  \n# Un layer convolutivo, 2d con 1 canale in output, un kernel 1x2  \n# Per semplicità ignoriamo i bias ora  \nconv2d = tf.keras.layers.Conv2D(\n 1\n, (\n1\n, \n2\n), use_bias=\n False\n) \n# L'input è nella forma (batch_size, height, width, channel),  \n# dove batch size e canali sono entrambi 1  \nX = tf.reshape(X, (\n 1\n, \n6\n, \n8\n, \n1\n)) \nY = tf.reshape(Y, (\n 1\n, \n6\n, \n7\n, \n1\n)) \nlr = \n3e-2  \n# Learning rate  \nY_hat = conv2d(X)  \nfor\n i \nin \nrange\n(\n10\n): \n    \n with\n tf.GradientTape(watch_accessed_variables=\n False\n) \nas\n g: \n        \n # indichiamo noi le variabili su cui operare il gradiente  \n        g.watch(conv2d.weights[\n 0\n]) \n        Y_hat = conv2d(X)  \n        l = (\n abs\n(Y_hat - Y)) ** \n 2 \n        \n # aggiornamento kernel  \n        update = tf.multiply(lr, g.gradient(l, conv2d.weights[\n 0\n])) \n        weights = conv2d.get_weights()  \n        weights[\n 0\n] = conv2d.weights[\n 0\n] - update  \n        conv2d.set_weights(weights)  \n        \n if\n (i + \n1\n) % \n2\n == \n0\n: \n            \n print\n(\nf'epoch \n {\ni + \n1\n}\n, loss \n{\ntf.reduce_sum(l)\n :\n.3f\n}\n'\n) \nepoch \n2\n, loss \n17.533 \nepoch \n4\n, loss \n3.607 \nepoch \n6\n, loss \n0.878 \nepoch \n8\n, loss \n0.259 \nepoch \n10\n, loss \n0.089 \n# monitoriamo i tensori ottenuti  \ntf.reshape(conv2d.get_weights()[\n 0\n], (\n1\n, \n2\n))",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nConvolutional Neural Networks (CNN)  \n3a parte\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#1": "Sommario\nCalcolo del numero dei parametri  \nLeNet-5 e calcolo dei parametri  \nArchitettura AlexNet  \n1x1 convolution",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#10": "CNN - Esercizio\nSe le tue GPU non hanno memoria suf\n ﬁ\nciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema?\n11",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#11": "CNN - Esercizio\nSe le tue GPU non hanno memoria suf\n ﬁ\nciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema?  \n1.\nRidurre la \n dimensione del mini-batch\n .\n12",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#12": "CNN - Esercizio\nSe le tue GPU non hanno memoria suf\n ﬁ\nciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema?  \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers.  \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere.\n13",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#13": "CNN - Esercizio\nSe le tue GPU non hanno memoria suf\n ﬁ\nciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema?  \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers.  \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere.  \n3.\nCambiare l'architettura \n rimuovendo un layer\n .\n14",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#14": "CNN - Esercizio\nSe le tue GPU non hanno memoria suf\n ﬁ\nciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema?  \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers.  \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere.  \n3.\nCambiare l'architettura \n rimuovendo un layer\n . \n4.\nUsare rappresentazioni  \nﬂ\noat a 16\n  bit invece che 32.\n15",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#15": "CNN - Esercizio\nSe le tue GPU non hanno memoria suf\n ﬁ\nciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema?  \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers.  \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere.  \n3.\nCambiare l'architettura \n rimuovendo un layer\n . \n4.\nUsare rappresentazioni  \nﬂ\noat a 16\n  bit invece che 32.  \n5.\nDistribuire la computazione\n  su più elaboratori.\n16",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#16": "Architettura CNN più recenti\nNe sono state proposte molte. Anche se sviluppate in un particolare task di \ncomputer vision, esse sono state impiegate in modo pro\n ﬁ\ncuo in altri domini, es. \ntracking, segmentation, object detection, style transformation.  \nLa challenge ImageNet (dal 2010) è favorito lo sviluppo di molte architetture.  \nLe CNN sono relativamente semplici, ma creare una architettura ef\n ﬁ\nciente \nrichiede intuizione, una base algebrica, e molti tentativi.  \nSpeso nuove architetture sfruttano elementi di architetture precedenti.",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#17": "Architettura CNN più recenti #1\nSebbene LeNet sia ef\n ﬁ\nciente per il problema OCR, non si adatta facilmente a \ndataset più grandi ed eterogenei. Effettivamente dal 1995 (LeNet) al 2012 (AlexNet) \nsono state proposte tecniche ML alternative (es. kernels, ensemble, structured \nestimation) ef\n ﬁ\ncienti in molti tasks.  \nPerché abbiamo atteso così a lungo per avere una rete più versatile e capace di \ncompetere con le altre architetture ML?  \nNel anni '90 una scheda GPU come la NVIDIA GeForce 256 era capace di 480 \nMFLOP, senza la disponibilità di framework software per sempli\n ﬁ\ncare la \nprogrammazione. Oggi la NVIDIA Ampere A100 raggiunge i 300 TFLOPS . Un \ndataset di cifre a bassa risoluzione (28x28) era considerato molto arduo da trattare.  \nIn pratica, era molto complesso testare architetture GPU-based anche su \ndataset semplici.  \nI \ndati disponibili\n  adatti all'addestramento sono aumentati sensibilmente, e questo \nha garantito la sperimentazione di un numero maggiore di architetture.  \nImageNet è stato costruito mediante Google Image e per mezzo di Amazon \nMechanical Turk per la classi\n ﬁ\ncazione manuale.",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#18": "LeNet vs AlexNet\nAlexNet ha 8 layers: 5 convolutivi, 2 FC nascosti, 1 FC output.  \nUsa la ReLU invece delle sigmoid o tanh.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#19": "Architettura AlexNet\nArchitettura CNN vincitrice della challenge object detection ILSVRC 2012 con un \ntop-5 error del 17% (il secondo ha ottenuto 26%) sviluppata da Alex Krizhevsky e \nIlya Sutskever.  \nPrimo tentativo di sfruttare piattaforme hardware GPU-enabled per addestrare reti \ncomplesse.  \nE' molto simile a \n LeNet-5\n  ma con più profondità.\nDopo i 5 convolutional \nlayers (11x11, 5x5 e 3x3) \nc'è il max pooling, e una \nrete FC da 3 layer.  \nImpiega ReLI, SGD e \nmomentum.  \n \nLa doppia pipeline è \ndovuta all’hardware \nimpiegato per \nl’addestramento (2 NVIDIA \nGTX 580s con 3Gb).",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#2": "Calcolo del numero di parametri di una rete neurale\nIl calcolo del numero di parametri è\n  fondamentale per \ncomprendere la complessità \n della rete e apportare miglioramenti \nall'architettura (es. introducendo pooling layer per ridurre i \nparametri).  \nIl calcolo dipende dal tipo di layer che stiamo considerando e dai \nvalori ricevuti dal layer precedente.  \nConsideriamo il calcolo del numero di parametri per le seguenti \ncon\nﬁ\ngurazioni:  \nUn \nConvolutional layer \n seguito da un \n FC layer\n   (\nCONV\n FC\n) \nUn\n Input layer \n seguito da un \n Convolutional layer\n  (\nI\nFC\n) \nUn \nFC layer \n seguito da un \n FC layer \n (\nFC\n FC\n)\n→\n→\n→",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#20": "Architettura AlexNet (2)\nLe immagini di ImageNet sono 8x più grandi rispetto a MNIST.  \nI LRF del primo strato sono 11x11, 5x5 nel secondo e 3x3 nel terzo.  \nDopo il primo, il secondo e 5o strato convolutivo, c'è un \n max-pooling layers\n  con \nﬁ\nnestra 3x3 e uno stride pari a 2.  \nAlexNet ha 10 volte i canali di LeNet.  \nLa rete FC multi-layer ha 1Gb di parametri. La doppia pipeline di elaborazione \npermetteva di suddividere l'occupazione.  \nIl numero elevato di parametri rende AlexNet poco ef\n ﬁ\nciente rispetto ad \narchitetture più recenti.  \nLa ReLU rende la computazione dei gradienti più rapida. Inoltre se \nl'inizializzazione dei parametri porta a valori di attivazione vicini ad 1 o 0 \n(estremi dell'intervallo) la derivata è vicina allo 0, e questo rallenta \nl'aggiornamento dei pesi. Il gradiente della ReLU è sempre 1 per valori positivi.  ",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#21": "Architettura AlexNet (3)\nImpiega \n dropout\n  sugli strati FC, e \n data augumentation\n . Nei layer C1 e C3 impiega \nla \nLocal response normalization:\n  se un nodo riceve una attivazione signi\n ﬁ\ncativa, \nsi inibiscono i nodi nella stessa posizione ma in altre feature maps.  \nIl dropout nei layer FC prende il posto del weight decay della LeNet. Questo \ngarantisce una sorta di regolarizzazione dei parametri  \n•\nL'ipotesi è quella di favorire la competitività, specializzando ogni feature map su \ncaratteristiche distinte.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#22": "Esempio: Filtri di AlexNet\nEsempi di \n ﬁ\nltri dei primi layer di Alex Net dopo l'addestramento:\n",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#23": "AlexNet e Keras\n08-AlexNet.ipynb",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#24": "La \n1\n1 convolution\n  è un \n ﬁ\nltro di dimensione \n 1\n1\nC\n e (ovviamente) si \napplica a input con profondità \n C\n. \n•\nPuò essere vista come una \n rete neurale con un layer,\n  che prende in input \nun vettore di \n C\n elementi.  \n•\nPer \nC\n pari a \n 1 \nnon viene impiegato  \n•\nUn \nﬁ\nltro 1\n 1\n1 corrisponde ad una moltiplicazione per uno scalare, operazione \ninutile in una rete neurale.  \nA cosa può servire?\n×\n ×\n×\n×\n×\nCNN: 1\n 1 convolution\n×\noutput layer precedente :\nsupponi una profondità C > 1feature map  \navrà la stessa \ndimensione dell'input \nma profondità pari a 1\ndimensione LRF : \n11C××",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#25": "CNN: 1x1 convolution (2)\nEffettua un \n feature pooling\n  cioè combina linearmente più features legate tra \nloro da un certa legame spaziale (es. i 3 valori dei canali RGB di un pixel).  \n•\nUtile quando si hanno feature maps con grande profondità e si vuole ridurre \nil numero di paremetri nei layer successivi.  \n•\nMentre il \n pooling\n  tradizionale aggrega più feature vicine all'interno della \nstessa feature map.  \nSe in input abbiamo una feature maps con profondità \n C\n, ogni mappa \nrappresenterà l'importanza di una diversa feature in una certa posizione. La  \n1\n1 convolution\n  raccoglie le informazioni di \n C\n features diverse valutate nella \nstessa posizione per determinare un singolo output.\n×\n1x1 conv\nLa profondità è passata da 32 a 1.  \nImpiegano n ﬁltri 1x1 conv, \notteniamo una profondità n della \nfeature maps in output.",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#26": "CNN: 1x1 convolution (3)\nOltre a ridurre il numero di parametri nei layer successivi in presenza di \nfeature maps con grande profondità, la \n 1\n1 convolution\n  viene usata \nanche per creare nuove \n proiezioni  \nlineari\n  a partire dalle feature map \ncorrenti.  \n•\nLe proiezioni creazioni \n nuove features\n  determinate dalla combinazioni \ndi più feature maps nei layer precedenti. \n×\n+verso i layer successivi...",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#3": "I \nparametri\n  consistono nell’insieme dei \n pesi e bias\n  nel layer convoluzionale, che \nprodurranno i valori delle attivazioni nelle feature maps.  \n•\nIl layer di input non ha pesi associati.  \nSe indichiamo con:  \n•\n#\nW\nc\n e #\nB\nc \nil numero di pesi e bias del layer convoluzionale  \n•\nf\n la dimensione del LRF  \n•\nN\nc\n numero dei \n ﬁ\nltri nel convolutional layer  \n•\nC\n profondità delle istanze in input (es. 3 per immagine a colori RGB)  \nallora si ha:  \n#\nW\nc\n = f\n2 \n C \n N\nc \n    e    #\n B\nc\n = N\n c \nLo stesso risultato si ottiene per con\n ﬁ\ngurazioni \n CONV\n CONV\n , considerando come \nprofondità \n C\n la profondità delle feature maps nel layer precedente.  \nSi nota come il numero di parametri è indipendente dalla dimensione X,Y dell'input.\n×\n×\n→\nCalcolo del numero dei parametri: \n I\nFC\n→",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#4": "I \nparametri\n  consistono nell’insieme dei \n pesi e bias\n  del layer FC connesso alle \nfeature maps prodotte dal convolutional layer precedente.  \nSe indichiamo con:  \n•\n#\nW\ncf\n e #\nB\ncf  \nil numero di pesi e bias del layer FC  \n•\nO\n dimensione delle feature maps nel convolutional layer, supponendo larghezza e \naltezza coincidenti.  \n•\nN\nc\n numero dei \n ﬁ\nltri nel convolutional layer  \n•\nF\n numero dei nodi nel layer FC  \nallora si ha:  \n#\nW\ncf\n = O\n2 \n N\nc \n F \n   e   \n #B\ncf\n = F  \nSpesso si opera una \"\n linearizzazione\n \" dell'output del convolutional layer. Se \nabbiamo \n N\nc \nﬁ\nltri e una dimensione delle feature maps pari a OxO, introduciamo \nuna rappresentazione 1-dimensionale con un vettore di \n O\n2\n• N\n c\n elementi, passato in \ninput al layer fully-connected.\n×\n×\nCalcolo del numero dei parametri: \n CONV\n FC\n→",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#5": "I \nparametri\n  consistono nell’insieme dei \n pesi e bias\n  del layer FC connesso al FC \nprecedente.  \nSe indichiamo con:  \n•\n#\nW\nff\n e #\nB\nff \n il numero pesi e bias del layer FC  \n•\nF\n il numero di nodi nel layer FC  \n•\nF\n-1\n il numero di nodi nel layer FC precedente  \nallora si ha:  \n#W\n ff\n = F\n -1 \n F\n    e   \n #B\nff\n = F\n ×\nCalcolo del numero dei parametri: \n FC\n FC\n→",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#6": "LeNet-5: numero di parametri\nIndichiamo con \n f\n, \ns\n e \np\n rispettivamente la dimensione del \n ﬁ\nltro, stride e \npooling (dove 0 corrisponde al pooling VALID).\nNon è un vero layer ,\nma una linearizzazione dei \ndati: rendiamo ﬂat la \nrappresentazion e\n5x5x16 -> 40028x28x6  \nfeature maps di 6 ﬁltri \ndi dimensione 28x28 l'uno\n14x14x6  \nun pooling layer con f e s pari \na 2 dimezza le dimensioni ,\nma mantiene uguale la dept h\ndella feature maps.",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#7": "LeNet-5: numero di parametri (2)\nDopo la convoluzione \nabbiamo 6 ﬁltri 28x28. Dopo il pooling abbiamo  \n6 ﬁltri 14x14\nInvece di avere 240000 \nparametri ne abbiamo \n151600 (vedi commento \ndopo).Stesso procedimento di S2 \nma ora abbiamo 16 ﬁltriPooling\nPoolingncl-1 è la profondità  \ndel  layer precedente. \nsupponiamo input depth = 1  \ncioè scala di grigiil pooling layer non  \naltera la profondità\nognuno dei 28x28 in output \nha 5x5x6 connessioni col \nlayer precedente, cioè \nl’immagine in input.Connections =  \n28x28 x 5x5x1x6 = 117600\nConnections =  \n10x10x5x5x6x10 = 150000",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#8": "LeNet-5: peculiarità\nNel #3 hidden layer, con lo scopo di ridurre potenziali simmetrie e il numero di \nconnessioni, gli autori hanno deciso che\n  solo 10 delle 16 features maps sono connesse \ncon le 6 features maps del layer precedente\n .  \nLa tecnica \n dropout\n  introdotta solo successivamente ha automatizzato questo step, \nperciò non si riscontra in architetture più recenti.\nSchema di interconnessione tra feature maps impiegato.\nConnections =  \n10x10x5x5x6x10 = 150000",
    "data_test\\rootfolder\\università\\DeepLearning\\06-CNN parte 3-sbloccato.pdf#9": "LeNet-5: numero di parametri (3)\nRendiamo “ ﬂat” l’output \nprecedente.  Abbiamo 400 \n(5x5x16) nodi dal layer S4. \nIl primo strato fully \nconnected layer ha 120 nodi. \nOgni nodo del layer è \nconnesso con i 400 nodi \ndello strato precedente.Fully connected layer con \n84 neuroni.softmax",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nConvolutional Neural Networks (CNN)  \n4a parte - Architetture\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#1": "Sommario\nArchitetture avanzate CNN  \nVGG  \nNiN \nGoogleNet",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#10": "Poiché le risorse di calcolo necessarie alla VGG sono molto \nmaggiori di AlexNet, costruiamo una rete con un numero minore di \ncanali, suf\n ﬁ\ncienti per il dataset Fashion-MNIST.  \ntrainer = d2l.Trainer(max_epochs=\n 10\n) \ndata = d2l.FashionMNIST(batch_size=\n 128\n, resize=(\n 224\n, \n224\n)) \nwith\n d2l.try_gpu():  \n    model = VGG(arch=((\n 1\n, \n16\n), (\n1\n, \n32\n), (\n2\n, \n64\n), (\n2\n, \n128\n), (\n2\n, \n128\n)), lr=\n0.01\n) \n    trainer.fit(model, data)  \nC'è una similarità tra val_loss e train_loss, con un discostamento \nminimale che può rappresentare un piccolo over\n ﬁ\ntting.\nTraining VGG Network e Keras\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#11": "L'upsampling di Fashion-MNIST di un fattore 8 (da 28x28 a \n224x224) è molto inef\n ﬁ\nciente. Prova a modi\n ﬁ\ncare l'architettura per \ntrattare immagini 28x28.\nVGG - Esercizio",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#12": "Rispetto a LeNet, AlexNet e VGG intervengono principalmente \ncreando strutture (conv_layer + pooling) più \"profonde\" e più \n\"ampie\".  \nMa i layer FC \n ﬁ\nnali richiedono ancora molti parametri.  \nUna semplice VGG-11 richiede matrici 25088x4096, con una \noccupazione di 400Mb di RAM (FP32). Non adatti a sistemi \nembedded e mobile.  \nL'architettura Network in network (NiN) blocks consiste in un 1x1 \nconv layer che aggiunge non-linearità tra le attivazioni dei canali, e \nun \nglobal average pooling\n  nell'ultimo layer.\nNetwork in Network (NiN)",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#13": "Rimpiazza i layer FC di una rete CNN tradizionale con un pooling.  \nPrende in input un tensore 3d (height,width,channels) e ricava \nl'avg rispetto al dimensione channels.  \nL'idea è generare una feature map per ogni categoria di interesse nel \ntask nei layer \n ﬂ\nattening, inviando l'output direttamente alla softmax.  \nIntroduce una sorta di codi\n ﬁ\nca più diretta tra feature maps e \ncategorie di interesse. Le feature maps possono essere interpretate \ncome \n mappe di con\n ﬁ\ndenze con le categorie\n . \nNon ci sono i parametri tradizionali, e si evitano fenomeni di \nover\nﬁ\ntting.  \nÈ più robusto a traslazioni spaziali poiché l'operazione considera \ntutte le informazioni spaziali disponibili.\nGlobal average pooling\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#14": "Ricordiamo che l'input e output dei conv layers sono tensori 4d: \nistanze, channel, height e width.  \nL'input e output di un layer FC sono tensori 2d (istanze, features).  \nL'idea è applicare un FC layer a ogni posizione di pixel (per ogni \nheight e width). La rete risultante 1x1 conv può essere interpretata \ncome un layer FC indipendente per ogni pixel.  \nIl blocco NiN è costituito da un conv layer seguito da convoluzioni \n1x1. \nIn questo modo non c'è necessità di una grossa rete FC al termine \ndell'architettura.\nBlocchi NiN",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#15": "La rete NiN usa le stesse dimensioni dei \n ﬁ\nltri di AlexNet: 11x11, 5x5 \ne 3x3; e le stesse dimensioni dei canali di output.  \nLe conv net sono seguite da pooling layer 3x3 con stride 2.  \nNiN non include FC layer. Il numero dei canali di output dei blocchi \nNiN corrispondono al numero di classi del task, seguite da un \nglobal average pooling\n , ottenendo un vettore di logits.  \nL'architettura riduce il numero di parametri a scapito del tempo di \ntraining, più lungo.\nArchitettura NiN",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#16": "Architettura NiN\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#17": "NiN in codice Keras:  \nimport\n tensorflow \n as\n tf\n!\npip install d2l==\n 1.0.0\na1.post0\nfrom\n d2l \nimport\n tensorflow \n as\n d2\nl\ndef \nnin_block\n (out_channels, kernel_size, strides, padding):  \n    \n return\n tf.keras.models.Sequential([  \n    tf.keras.layers.Conv2D(out_channels, kernel_size, strides=strides,  \n                           padding=padding),  \n    tf.keras.layers.Activation(\n 'relu'\n), \n    tf.keras.layers.Conv2D(out_channels, \n 1\n), \n    tf.keras.layers.Activation(\n 'relu'\n), \n    tf.keras.layers.Conv2D(out_channels, \n 1\n), \n    tf.keras.layers.Activation(\n 'relu'\n)])\nBlocco NiN in Keras",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#18": "class \nNiN\n(d2l.Classifier):  \n    \n def \n__init__\n (\nself\n, lr=\n0.1\n, num_classes=\n 10\n): \n        \n super\n().\n__init__\n () \n        \n self\n.save_hyperparameters()  \n        \n self\n.net = tf.keras.models.Sequential([  \n            nin_block(\n 96\n, kernel_size=\n 11\n, strides=\n 4\n, padding=\n 'valid'\n), \n            tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n), \n            nin_block(\n 256\n, kernel_size=\n 5\n, strides=\n 1\n, padding=\n 'same'\n), \n            tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n), \n            nin_block(\n 384\n, kernel_size=\n 3\n, strides=\n 1\n, padding=\n 'same'\n), \n            tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n), \n            tf.keras.layers.Dropout(\n 0.5\n), \n            nin_block(num_classes, kernel_size=\n 3\n, strides=\n 1\n, padding=\n 'same'\n), \n            tf.keras.layers.GlobalAvgPool2D(),  \n            tf.keras.layers.Flatten()])  \nmodel = NiN()  \nX = tf.random.normal((\n 1\n, \n224\n, \n224\n, \n1\n)) \nfor\n layer \n in\n model.net.layers:  \n    X = layer(X)  \n    \nprint\n(layer.\n__class__\n .\n__name__\n ,\n'output shape:\n \\t\n'\n, X.shape)  \nSequential output shape:     (\n 1\n, \n54\n, \n54\n, \n96\n) \nMaxPooling2D output shape:   (\n 1\n, \n26\n, \n26\n, \n96\n) \nSequential output shape:     (\n 1\n, \n26\n, \n26\n, \n256\n) \nMaxPooling2D output shape:   (\n 1\n, \n12\n, \n12\n, \n256\n) \nSequential output shape:     (\n 1\n, \n12\n, \n12\n, \n384\n) \nMaxPooling2D output shape:   (\n 1\n, \n5\n, \n5\n, \n384\n) \nDropout output shape:        (\n 1\n, \n5\n, \n5\n, \n384\n) \nSequential output shape:     (\n 1\n, \n5\n, \n5\n, \n10\n) \nGlobalAveragePooling2D output shape:         (\n 1\n, \n10\n) \nFlatten output shape:        (\n 1\n, \n10\n)\nArchitettura NiN in Keras",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#19": "model = NiN(lr=\n 0.05\n) \ntrainer = d2l.Trainer(max_epochs=\n 10\n, num_gpus=\n 1\n) \ndata = d2l.FashionMNIST(batch_size=\n 128\n, resize=(\n 224\n, \n224\n)) \nmodel.apply_init([\n next\n(\niter\n(data.get_dataloader(\n True\n)))[\n0\n]], d2l.init_cnn)  \ntrainer.fit(model, data)  \nTraining NiN in Keras\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#2": "Networks Using Blocks (VGG)\nSebbene \n AlexNet\n  abbia permesso di ottenere buone performance in \ndiversi task, non fornisce dei \n template\n  per la realizzazione di nuove \narchitetture.  \nIl Visual Geometry Group Oxford University ha de\n ﬁ\nnito \nl'architettura VGG che consiste in strutture ripetute de\n ﬁ\nnite per \nmezzo di istruzioni di loop e subroutines.  \nIl \nblocco\n  fondamentale della CNN è una sequenza di (i) \nconvolutional layer con padding (ii) nonlinearità come la ReLU, (iii) \npooling layer per ridurre la risoluzione.  \nIl problema di questo approccio è che la risoluzione spaziale si \nriduce abbastanza rapidamente. Introduce il limite rigido di \n log\n2\nd \nlayer convolutivi prima che tutte le dimensioni (\n d\n) si esauriscano.  \nPer esempio per \n ImageNet\n  non si possono avere più di 8 layer.",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#20": "CNN: Alcune problematiche \nNei seguenti esempi riconosciamo un cane, ma la posizione e \ndimensione dell’animale sono molto diverse tra loro.  \n•\nNon è facile determinare la giusta dimensione (e il numero) dei \nﬁ\nltri negli strati iniziali.  \nE nonostante le tecnologie di apprendimento introdotte, in \narchitetture molto deep (con molti strati) può sempre riproporsi il \nvanishing gradient problem\n . \n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#21": "CNN: Inception module (GoogleNet)\nL'\ninception module\n  si basa sulla ipotesi che \n combinare\n  le \ninformazioni provenienti da diverse pipeline di processamento basate \nconvolutional layer permetta di estendere le caratteristiche salienti \nidenti\n ﬁ\ncate. \n•\nPiù convolution layer in parallelo\n , ognuno con una \n diversa \ndimensione dei \n ﬁ\nltri\n. Gli output dei convolution layers sono \n\"combinati\" in una singola struttura che consisterà nell'input per il \nlayer successivo.  \n•\nSi impiegano \n ﬁ\nltri con dimensioni pari a \n 1x1\n, \n3x3\n e \n5x5\n, tutti con \nstride 1\n , \nSAME\n  padding e \n ReLU\n  activation function.  \nIn pratica si processa lo stesso input contemporaneamente \nconsiderando più dimensioni di LRF.  \nL'inception module è stato impiegato per la prima volta \nnell'architettura \n GoogleLeNet\n .",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#22": "CNN: Inception module (GoogleNet)\nGoogleNet ha vinto la challenge ImageNet 2015 con una struttura \nche combina le caratteristiche di NiN, blocchi ripetuti, e un mix di \nkernel convolutivi.  \nCrea una distinzione tra:  \nstem\n (data ingest), primi 2-3 conv layers per estrarre feature a \nbasso livello  \nbody\n  (data processing), serie di blocchi convolutivi  \nhead\n  (prediction), per problemi di classi\n ﬁ\ncation, segmentation, \ndetection, o tracking.  \nL'idea è combinare l'output di più conv layer con diverse \ndimensioni in un unico output ",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#23": "Inception module\nL'input è dato contemporaneamente a \n 3 convolution layers\n  e un \n 3x3 \nmax pooling\n .  \n•\nLe \n1x1 convolution \n \"\ncomprimono\n \" la profondità dell'input, utili \nsoprattutto per \n sempli\n ﬁ\ncare i dati in input \n alle convoluzioni 3x3 e \n5x5 che richiedono risorse computazionali.  \n•\nLa combinazione \n 1x1+3x3\n  e \n1x1+5x5\n  hanno più possibilità di \nrappresentare \n feature più complesse \n rispetto ai singoli 3x3 e 5x5.  \n•\nSperimentalmente si nota come gli inception module sono più \nef\nﬁ\ncienti se usati negli layer più a valle.\nInception module\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#24": "Architettura GoogLeNet v1\nL’architettura vincitrice della object detection challenge ILSRC 2014 \nraggiungendo un top-5 error < 7%.  \nLa principale caratteristica è la profondità: \n 22 layer\n  (27 considerando anche i \npooling layers) con 9 \n inception module\n  in cascata.  \n•\nDopo ogni \n inception module\n  si opera una average pooling per ridurre il \nnumero di parametri.  \n•\nSebbene più profonda, possiede 1/10 dei parametri di AlexNet (6 milioni \ncirca)\nAltre tecniche impiegate: batch \nnormalization, image distortions e RMSprop?? inception module",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#25": "Architettura GoogLeNet v1 (2)\nL’\noutput di due inception module intermedi (3º e 6º inception module) è valutato \npreliminarmente nel task della classi\n ﬁ\ncazione \n per mezzo di una softmax.  \nSi affrontare il problema del \n vanishing gradient problem\n , dato che si generano \ngradienti addizionali negli hidden layer lontani dall'ultimo layer.  \nIl valore della loss intermedia è chiamato \n auxiliary loss\n . Durante il training \nviene combinato linearmente (scalato del 70%) con la loss dell'intera rete.  \nIn produzione e nel test set non vengono impiegati.  \nNota\n : le versioni v2, v3 e v4 di GoogleNet introducono molti espedienti per \nrendere più ef\n ﬁ\nciente il training e migliorare l’accuracy.\nauxiliary classi ﬁerauxiliary classi ﬁer",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#26": "GoogLeNet: esempio di \n ﬁ\nltri\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#27": "Inception e Keras\nimport\n tensorflow \n as\n tf\n!\npip install d2l==\n 1.0.0\na1.post0\nfrom\n d2l \nimport\n tensorflow \n as\n d2\nl\nclass \nInception\n (tf.keras.Model):  \n    \n# `c1`--`c4` sono il numero di canali in output per ogni ramo  \n    \n def \n__init__\n (\nself\n, c1, c2, c3, c4):  \n        \n super\n().\n__init__\n () \n        \n self\n.b1_1 = tf.keras.layers.Conv2D(c1, \n 1\n, activation=\n 'relu'\n) \n        \n self\n.b2_1 = tf.keras.layers.Conv2D(c2[\n 0\n], \n1\n, activation=\n 'relu'\n) \n        \n self\n.b2_2 = tf.keras.layers.Conv2D(c2[\n 1\n], \n3\n, padding=\n 'same'\n, \n                                           activation=\n 'relu'\n) \n        \n self\n.b3_1 = tf.keras.layers.Conv2D(c3[\n 0\n], \n1\n, activation=\n 'relu'\n) \n        \n self\n.b3_2 = tf.keras.layers.Conv2D(c3[\n 1\n], \n5\n, padding=\n 'same'\n, \n                                           activation=\n 'relu'\n) \n        \n self\n.b4_1 = tf.keras.layers.MaxPool2D(\n 3\n, \n1\n, padding=\n 'same'\n) \n        \n self\n.b4_2 = tf.keras.layers.Conv2D(c4, \n 1\n, activation=\n 'relu'\n) \n    \n def \ncall\n(\nself\n, x): \n        b1 = \n self\n.b1_1(x)  \n        b2 = \n self\n.b2_2(\nself\n.b2_1(x))  \n        b3 = \n self\n.b3_2(\nself\n.b3_1(x))  \n        b4 = \n self\n.b4_2(\nself\n.b4_1(x))  \n        \n return\n tf.keras.layers.Concatenate()([b1, b2, b3, b4])",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#28": "GoogleNet e Keras\nUno stack di 9 blocchi \n inception, \n organizzati in 3 gruppi \nintramezzati da max-pooling per ridurre le dimensioni, e un global \naverage pooling per generare l'ultimo output prima del FC layer.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#29": "GoogleNet e Keras\nclass \nGoogleNet\n (d2l.Classifier):  \n    \n def \nb1\n(\nself\n): \n        \n return\n tf.keras.models.Sequential([  \n            tf.keras.layers.Conv2D(\n 64\n, \n7\n, strides=\n 2\n, padding=\n 'same'\n, \n                                   activation=\n 'relu'\n), \n            tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n, \n                                      padding=\n 'same'\n)]) \n@d2l\n.add_to_class(GoogleNet)  \ndef \nb2\n(\nself\n): \n    \n return\n tf.keras.Sequential([  \n        tf.keras.layers.Conv2D(\n 64\n, \n1\n, activation=\n 'relu'\n), \n        tf.keras.layers.Conv2D(\n 192\n, \n3\n, padding=\n 'same'\n, activation=\n 'relu'\n), \n        tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n, padding=\n 'same'\n)]) \n@d2l\n.add_to_class(GoogleNet)  \ndef \nb3\n(\nself\n): \n    \n return\n tf.keras.models.Sequential([  \n        Inception(\n 64\n, (\n96\n, \n128\n), (\n16\n, \n32\n), \n32\n), \n        Inception(\n 128\n, (\n128\n, \n192\n), (\n32\n, \n96\n), \n64\n), \n        tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n, padding=\n 'same'\n)]) ",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#3": "VGG Blocks\nL'idea è di impiegare convoluzioni multiple e distinte tra periodici \ndownsampling (eg. max-pooling) sotto forma di unico blocco \nfunzionale.  \nL'ipotesi che \n diverse dimensioni di convoluzioni (deep e wide) \npossono meglio rappresentare le features signi\n ﬁ\ncative\n .  \nPer esempio 3x3 convolutions interessa gli stessi pixel della 5x5 \nconvolutions. Ma l'ultima usa un numero di parametri (\n 25•c\n2\n) come \ntre 3x3 convolutions (\n 3•9•c\n2\n), cioè uno \n stacking\n  di convoluzioni \n3x3. Dimostrano che tali con\n ﬁ\ngurazioni (deep & narrow) ottengono \nprestazioni migliori.  \nLa dimensione della rete con stacking 3x3 può oltrepassare i 100 \nlayers, un approccio molto comune nelle moderne architetture.",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#30": "GoogleNet e Keras\n@d2l\n.add_to_class(GoogleNet)  \ndef \nb4\n(\nself\n): \n    \n return\n tf.keras.Sequential([  \n        Inception(\n 192\n, (\n96\n, \n208\n), (\n16\n, \n48\n), \n64\n), \n        Inception(\n 160\n, (\n112\n, \n224\n), (\n24\n, \n64\n), \n64\n), \n        Inception(\n 128\n, (\n128\n, \n256\n), (\n24\n, \n64\n), \n64\n), \n        Inception(\n 112\n, (\n144\n, \n288\n), (\n32\n, \n64\n), \n64\n), \n        Inception(\n 256\n, (\n160\n, \n320\n), (\n32\n, \n128\n), \n128\n), \n        tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n, padding=\n 'same'\n)]) \n@d2l\n.add_to_class(GoogleNet)  \ndef \nb5\n(\nself\n): \n    \n return\n tf.keras.Sequential([  \n        Inception(\n 256\n, (\n160\n, \n320\n), (\n32\n, \n128\n), \n128\n), \n        Inception(\n 384\n, (\n192\n, \n384\n), (\n48\n, \n128\n), \n128\n), \n        tf.keras.layers.GlobalAvgPool2D(),  \n        tf.keras.layers.Flatten()])  \n@d2l\n.add_to_class(GoogleNet)  \ndef \n__init__\n (\nself\n, lr=\n0.1\n, num_classes=\n 10\n): \n    \nsuper\n(GoogleNet, \n self\n).\n__init__\n () \n    \nself\n.save_hyperparameters()  \n    \nself\n.net = tf.keras.Sequential([  \n        \n self\n.b1(), \nself\n.b2(), \nself\n.b3(), \nself\n.b4(), \nself\n.b5(), \n        tf.keras.layers.Dense(num_classes)])  ",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#31": "GoogleNet e Keras\nmodel = GoogleNet().layer_summary((\n 1\n, \n96\n, \n96\n, \n1\n)) \nSequential output shape:     (\n 1\n, \n24\n, \n24\n, \n64\n) \nSequential output shape:     (\n 1\n, \n12\n, \n12\n, \n192\n) \nSequential output shape:     (\n 1\n, \n6\n, \n6\n, \n480\n) \nSequential output shape:     (\n 1\n, \n3\n, \n3\n, \n832\n) \nSequential output shape:     (\n 1\n, \n1024\n) \nDense output shape:  (\n 1\n, \n10\n) \nmodel = GoogleNet().layer_summary((\n 1\n, \n96\n, \n96\n, \n1\n)) \nSequential output shape:     (\n 1\n, \n24\n, \n24\n, \n64\n) \nSequential output shape:     (\n 1\n, \n12\n, \n12\n, \n192\n) \nSequential output shape:     (\n 1\n, \n6\n, \n6\n, \n480\n) \nSequential output shape:     (\n 1\n, \n3\n, \n3\n, \n832\n) \nSequential output shape:     (\n 1\n, \n1024\n) \nDense output shape:  (\n 1\n, \n10\n) \ntrainer = d2l.Trainer(max_epochs=\n 10\n) \ndata = d2l.FashionMNIST(batch_size=\n 128\n, resize=(\n 96\n, \n96\n)) \nwith\n d2l.try_gpu():  \n    model = GoogleNet(lr=\n 0.01\n) \n    trainer.fit(model, data)  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#32": "GoogleNet - vantaggi\nGoogleNet richiede meno potenza di calcolo rispetto alle \narchitetture precedenti mantenendo una precisione più elevata.  \nL'approccio è basato sulla approssimazione dell'architettura senza \nandare a scapito delle prestazioni.  \nIntroduce un \n design by block\n , con iperparametri più \"ad alto livello\".",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#33": "Architetture AlexNet, VGG, NiN, GoogleNet\nEsercizio\n : valuta la differenza di prestazioni e i tempi di \naddestramento su medesimi dataset (FashionMNIST) o subset di \ndataset più complessi (ImageNet).",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#34": "Architetture CNN\nPrincipali architetture CNN per le immagini, complessità, numero di operazioni \nrichieste per l'addestramento e accuratezza.  \n35\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#35": "Esercizio su Inception v3 e classi\n ﬁ\ncazione di immagini\nProblema di classi\n ﬁ\ncazione di immagini usando Inception v3  \n•\nScarica alcune immagini di animali, ad esempio usando la funzion \nmatplotlib.image.mpimg.imread(). Ridimensionali e fai crop 299x299 pixel, \ncon 3 canali RGB.  \n•\nScarica i modelli pre-addestrati di Inception v3  \n•\nhttps://github.com/tensor\n ﬂ\now/models/tree/master/research/slim  \n•\nCrea il modello Inception v3 usando la funzione inception_v3() con \nis_training=False, num_classes=1001 nel seguente modo:  \nfrom \n tensor\n ﬂ\now.contrib.slim.nets \n import \n inceptio\n n\nimport \n tensor\n ﬂ\now.contrib.slim \n as \nsli\nm\nX \n= \ntf\n.\nplaceholder\n (\ntf\n.\nﬂ\noat32\n , \nshape\n =[\nNone\n , \n299\n, \n299\n, \n3\n]\n)\nwith \nslim\n.\narg_scope\n (\ninception\n .\ninception_v3_arg_scope\n ())\n:\nlogits\n , \nend_points \n = \ninception\n .\ninception_v3\n (\nX\n, \nnum_classes\n =\n1001\n , \nis_training\n =\nFalse\n )\npredictions \n = \nend_points\n [\n\"Predictions\"\n ]\nsaver \n = \ntf\n.\ntrain\n.\nSaver\n (\n)\n•\n...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#36": "Esercizio su Inception v3 e classi\n ﬁ\ncazione di immagini\n... Problema di classi\n ﬁ\ncazione di immagini usando Inception v3  \n•\nCrea una sessione e usa Saver per recuperare il modello pre-addestrato.  \n•\nLancia il modello per addestrare le immagini che hai scaricato visualizzando \nle top-5 predictions e la relativa probabilità.  \n•\nI nomi delle categorie le trovi qui: \n https://github.com/ageron/handson-ml/\nblob/master/datasets/inception/imagenet_class_names.txt  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#4": "VGG Blocks in Keras\nUn funzione che prende come parametri il numero di layer \nconvolutivi e il numero di channel di output  \nimport\n tensorflow \n as\n tf\n!\npip install d2l==\n 1.0.0\na1.post0\nfrom\n d2l \nimport\n tensorflow \n as\n d2l\ndef \nvgg_block\n (num_convs, num_channels):  \n    blk = tf.keras.models.Sequential()  \n    \n for\n _ \nin \nrange\n(num_convs):  \n        blk.add(  \n            tf.keras.layers.Conv2D(num_channels, kernel_size=\n 3\n, \n                                   padding=\n 'same'\n, activation=\n 'relu'\n)) \n    blk.add(tf.keras.layers.MaxPool2D(pool_size=\n 2\n, strides=\n 2\n)) \n    \n return\n blk",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#5": "L'architettura VGG e AlexNet a confronto, con i blocchi funzionali \nche si ripetono:\nVGG Network e AlexNet\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#6": "L'aspetto distintivo sono i layer convolutivi,raggruppati in \ntrasformazioni non lineari che medesima dimensione per gruppo.  \nSi impiegano \n ﬁ\nltri \n3x3\n con zero padding in modo da scorrere \nl'intera l'immagine.  \nSuccessivamente c'è lo step di riduzione della risoluzione (2x2 \npooling)  \nAl termine ci sono layer FC  \nNota: 100M di parametri nei FC in confronto dei 40M degli strati \nconvolutivi\nVGG Network",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#7": "VGG Network - Dettaglio parametri\n",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#8": "La prima parte della rete VGG è una successione di blocchi VGG.  \nLa conv_arch consiste in una lista di tuple (una per blocco), ognuna \nche contiene 2 valori: il numero di conv layers e il numero di \ncanali.  \nclass \nVGG\n(d2l.Classifier):  \n    \n def \n__init__\n (\nself\n, arch, lr=\n 0.1\n, num_classes=\n 10\n): \n        \n super\n().\n__init__\n () \n        \n self\n.save_hyperparameters()  \n        \n self\n.net = tf.keras.models.Sequential()  \n        \n for\n (num_convs, num_channels) \n in\n arch: \n            \n self\n.net.add(vgg_block(num_convs, num_channels))  \n        \n self\n.net.add(  \n            tf.keras.models.Sequential([  \n            tf.keras.layers.Flatten(),  \n            tf.keras.layers.Dense(\n 4096\n, activation=\n 'relu'\n), \n            tf.keras.layers.Dropout(\n 0.5\n), \n            tf.keras.layers.Dense(\n 4096\n, activation=\n 'relu'\n), \n            tf.keras.layers.Dropout(\n 0.5\n), \n            tf.keras.layers.Dense(num_classes)]))  \nVGG Network e Keras",
    "data_test\\rootfolder\\università\\DeepLearning\\07-CNN parte 4-sbloccato.pdf#9": "La VGG originale chiamata \n VGG-11\n  ha 5 blocchi: i primi 2 con un \nconv layer ognuno, e gli 3 con 2 conv layer ognuno. Il 1o blocco ha \n64 canali, e i successivi raddoppiato i canali, \n ﬁ\nno a 512.  \nVGG(arch=((\n 1\n, \n64\n), (\n1\n, \n128\n), (\n2\n, \n256\n), (\n2\n, \n512\n), (\n2\n, \n512\n))).layer_summary(  \n    (\n1\n, \n224\n, \n224\n, \n1\n)) \nSequential output shape:     (\n 1\n, \n112\n, \n112\n, \n64\n) \nSequential output shape:     (\n 1\n, \n56\n, \n56\n, \n128\n) \nSequential output shape:     (\n 1\n, \n28\n, \n28\n, \n256\n) \nSequential output shape:     (\n 1\n, \n14\n, \n14\n, \n512\n) \nSequential output shape:     (\n 1\n, \n7\n, \n7\n, \n512\n) \nSequential output shape:     (\n 1\n, \n10\n) \nLa dimensione \n ﬁ\nnale dopo la sequenza dei blocchi è 7x7, seguita \ndal \nﬂ\nattening e il successivo processamento FC.\nVGG Network e Keras",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nConvolutional Neural Networks (CNN)  \n5a parte - Batch Normalization e ResNet\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#1": "Sommario\nInternal covariate shift  \nBatch normalization  \nArchitettura Residual Network (ResNet)",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#10": "Batch Normalization layer\nPoiché la BN dipende dalla dimensione del mini batch, e perciò dai dati di \ntraining, non possiamo ignorarla quando de\n ﬁ\nniamo la nostra architettura.  \nPer reti FC si può applicare la BN tra la trasformazione lineare e il calcolo \ndella funzione di attivazione.  \nPer conv layers l'approccio è simile, ma consideriamo la BN per ogni \nsingolo canale, valutandola sui i dati sparsi spazialmente. Perciò ogni canale \navrà una stima diversa di media e deviazione standard.  \nQuesto è in linea col principio di \n invarianza spaziale\n , cioè nel calcolo \npossiamo ignorare l'informazione relativa alla posizione.\n11",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#11": "Batch Normalization - Altri vantaggi\nSi dimostra empiricamente che la BN, oltre a ridurre il vanishing gradients, \ngarantisce ulteriori bene\n ﬁ\nci:\n12",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#12": "Batch Normalization - Altri vantaggi\nSi dimostra empiricamente che la BN, oltre a ridurre il vanishing gradients, \ngarantisce ulteriori bene\n ﬁ\nci: \n•\nPermette di impiegare\n  funzioni di attivazione che saturano\n  per input molto \ngrandi o piccoli (es. logistic e tanh).\n13",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#13": "Batch Normalization - Altri vantaggi\nSi dimostra empiricamente che la BN, oltre a ridurre il vanishing gradients, \ngarantisce ulteriori bene\n ﬁ\nci: \n•\nPermette di impiegare\n  funzioni di attivazione che saturano\n  per input molto \ngrandi o piccoli (es. logistic e tanh).  \n•\nRiduce la dipendenza \n sugli effetti di una certa \n scelta dei parametri iniziali\n .\n14",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#14": "Batch Normalization - Altri vantaggi\nSi dimostra empiricamente che la BN, oltre a ridurre il vanishing gradients, \ngarantisce ulteriori bene\n ﬁ\nci: \n•\nPermette di impiegare\n  funzioni di attivazione che saturano\n  per input molto \ngrandi o piccoli (es. logistic e tanh).  \n•\nRiduce la dipendenza \n sugli effetti di una certa \n scelta dei parametri iniziali\n . \n•\nRichiamo: Introduce una certa \n regolarizzazione \n dei parametri, sebbene non \nsostituisce le tecniche più ef\n ﬁ\ncaci (es. dropout)  \n•\nRichiamo: Permette l'uso di \n learning rate più elevati\n , riducendo i tempi di \napprendimento.  \n•\nEs. Per un tipico task di image classi\n ﬁ\ncation, si ottengono incrementi x14.\n15",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#15": "Batch Normalization\nPerché non ci limitiamo a normalizzare i dati in input ad ogni layer  \ne lasciare alla rete determinare i parametri W per l'ottimalità input-output?\n16",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#16": "Batch Normalization\nPerché non ci limitiamo a normalizzare i dati in input ad ogni layer  \ne lasciare alla rete determinare i parametri W per l'ottimalità input-output?  \nSe impieghiamo funzioni di attivazioni logistiche, \n forziamo al rete a \nlavorare in regime di quasi-linearità\n , riducendo la capacità di costruire \nrelazioni input-output non lineari.\n17",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#17": "Batch Normalization e LeNet - Keras\nimport\n tensorflow \n as\n tf\n!\npip install d2l==\n 1.0.0\na1.post0\nfrom\n d2l \nimport\n tensorflow \n as\n d2\nl\nclass \nBNLeNet\n(d2l.Classifier):  \n    \ndef \n__init__\n (\nself\n, lr=\n0.1\n, num_classes=\n 10\n): \n        \n super\n().\n__init__\n () \n        \n self\n.save_hyperparameters()  \n        \n self\n.net = tf.keras.models.Sequential([  \n            tf.keras.layers.Conv2D(filters=\n 6\n, kernel_size=\n 5\n, \n                                   input_shape=(\n 28\n, \n28\n, \n1\n)), \n            tf.keras.layers.BatchNormalization(),  \n            tf.keras.layers.Activation(\n 'sigmoid'\n ), \n            tf.keras.layers.AvgPool2D(pool_size=\n 2\n, strides=\n 2\n), \n            tf.keras.layers.Conv2D(filters=\n 16\n, kernel_size=\n 5\n), \n            tf.keras.layers.BatchNormalization(),  \n            tf.keras.layers.Activation(\n 'sigmoid'\n ), \n            tf.keras.layers.AvgPool2D(pool_size=\n 2\n, strides=\n 2\n), \n            tf.keras.layers.Flatten(), tf.keras.layers.Dense(\n 120\n), \n            tf.keras.layers.BatchNormalization(),  \n            tf.keras.layers.Activation(\n 'sigmoid'\n ), \n            tf.keras.layers.Dense(\n 84\n), \n            tf.keras.layers.BatchNormalization(),  \n            tf.keras.layers.Activation(\n 'sigmoid'\n ), \n            tf.keras.layers.Dense(num_classes)])  \ntrainer = d2l.Trainer(max_epochs=\n 10\n) \ndata = d2l.FashionMNIST(batch_size=\n 128\n) \nwith\n d2l.try_gpu():  \n    model = BNLeNet(lr=\n 0.5\n) \n    trainer.fit(model, data)\n18\n",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#18": "Architettura Residual Network - Motivazioni\nIn generale, architetture di reti complesse (es. più profonde) possono stimare \nuna classe più ampia di funzioni. Ma se aggiungiamo layer, nessuno ci \ngarantisce che l'apprendimento ci permette di trovarle, anzi in taluni casi \npossiamo allontanarci dall'ottimo.  \nInoltre reti profonde potrebbero soffrire del vanishing gradient problem.  \nMa se aggiungiamo layer che mirano a stimare una funzione identità, i.e., \nf(x)=x, sicuramente manteniamo la stessa ef\n ﬁ\ncacia della rete iniziale.  \nL'ipotesi è che, i layer che aggiungiamo alla rete dovrebbero avere più \nprobabilità nel rappresentare funzioni identità per garantire prestazioni ottimali.",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#19": "Architettura Residual Network (ResNet)\nResNet\n  è stata presentata a ILSVRC 2015 (top-5: 3.6%) e consiste in 152 layers.  \nSi introducono le \n skip connections\n , che propagano l'output di un certo layer \nnell'input di un layer che è posizionato più a valle.   \n•\nL'ipotesi è di rendere \n più semplice e veloce propagare segnali \n su varie parti \ndella rete.  \n•\nNelle fasi iniziali (comportamento random) si obbliga parti della rete ad \ncomportarsi in modo da riproporre i valori in input, rendendo \n più veloce \nl'apprendimento\n .\n",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#2": "Motivazioni\nL'addestramento delle architetture Deep mostra alcune problematiche \naggiuntive oltre quelle già discusse per le MLP. Una signi\n ﬁ\ncativa è il tempo \nnecessario per addestrarle.  \nSpesso si operano \n standardizzazioni\n  nei valori delle features in ingresso con \nforme di pre-processamento, es:  \nimporre µ=0 (zero mean) o la unit-variance (cioè dividere per la stddev)  \nzero mean\n  sul valore delle features, considerando la singola istanza; spesso \nutile per dati con informazioni spaziali.  \nCi garantisce che durante l'addestramento i valori dei parametri rimangano in \nintervalli ottimali, sia considerando i layer per l'intera profondità della rete, sia \ntra i nodi di un singolo layer, sia tra i valori di ogni parametro per la durata \ndell'addestramento.",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#20": "ResNet: Residual learning e residual block\nAddestrare una rete neurale può essere interpretato come approssimare una \nfunzione h(\n x\n). Se aggiungi un valore x all'output della rete, allora la rete è \nobbligata a modellare la funzione f(\n x\n) = h(\n x\n) - \nx\n. Tale approccio è chiamato \nresidual learning o mapping\n . \nDal punto di vista operativo, è suf\n ﬁ\nciente combinare l'output di un layer con \nl'output di un layer posizionato più a monte prima di valutare la funzione di \nattivazione (ReLU).\n",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#21": "Architettura ResNet\nL'architettura ResNet impiega conv layer 3x3 (simili a VGG).  \nOgni blocco ResNet ha due 3x3 conv layer seguite dalla batch normalization e \nattivazione ReLU. Prima dell'ultima ReLU sommiamo l'input dalla skip \nconnection.  \nLa 1x1 conv layer è necessaria per adattare i canali dell'input con quelli \nottenuti a valle del blocco.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#22": "Blocco ResNet e Keras\nclass \nResidual\n (tf.keras.Model):  \n    \ndef \n__init__\n (\nself\n, num_channels, use_1x1conv=\n False\n, strides=\n 1\n): \n        \n super\n().\n__init__\n () \n        \n self\n.conv1 = tf.keras.layers.Conv2D(num_channels, padding=\n 'same'\n, \n                                            kernel_size=\n 3\n, strides=strides)  \n        \n self\n.conv2 = tf.keras.layers.Conv2D(num_channels, kernel_size=\n 3\n, \n                                            padding=\n 'same'\n) \n        \n self\n.conv3 = \n None \n# dipende se vogliamo usare o meno il 1x1 conv layer  \n        \n if\n use_1x1conv:  \n            \n self\n.conv3 = tf.keras.layers.Conv2D(num_channels, kernel_size=\n 1\n, \n                                                strides=strides)  \n        \n self\n.bn1 = tf.keras.layers.BatchNormalization()  \n        \n self\n.bn2 = tf.keras.layers.BatchNormalization()  \n    \ndef \ncall\n(\nself\n, X): \n        Y = tf.keras.activations.relu(\n self\n.bn1(\nself\n.conv1(X)))  \n        Y = \n self\n.bn2(\nself\n.conv2(Y))  \n        \n if \nself\n.conv3 \nis \nnot \nNone\n: \n            X = \n self\n.conv3(X)  \n        Y += X  \n        \n return\n tf.keras.activations.relu(Y)  \nblk = Residual(\n 3\n) \nX = tf.random.normal((\n 4\n, \n6\n, \n6\n, \n3\n)) \nY = blk(X)  \nY.shape \nTensorShape([\n 4\n, \n6\n, \n6\n, \n3\n]) ",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#23": "Architettura ResNet-18\nI primi layer di ResNet sono simili a GoogleNet, ma in ResNet si usa la Batch \nnormalization.  \nSeguono vari moduli ripetuti ResNet. La ResNet-18 include 18 layer totali, ma \nsi hanno modelli addestrati con molti più layer, es. ResNet-152.\n",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#24": "Architettura ResNet e Keras\nclass \nResNet\n(d2l.Classifier):  \n    \ndef \nb1\n(\nself\n): \n        \n return\n tf.keras.models.Sequential([  \n            tf.keras.layers.Conv2D(\n 64\n, kernel_size=\n 7\n, strides=\n 2\n, \n                                   padding=\n 'same'\n), \n            tf.keras.layers.BatchNormalization(),  \n            tf.keras.layers.Activation(\n 'relu'\n), \n            tf.keras.layers.MaxPool2D(pool_size=\n 3\n, strides=\n 2\n, \n                                      padding=\n 'same'\n)]) \n@d2l\n.add_to_class(ResNet)  \ndef \nblock\n(\nself\n, num_residuals, num_channels, first_block=\n False\n): \n    blk = tf.keras.models.Sequential()  \n    \nfor\n i \nin \nrange\n(num_residuals):  \n        \n if\n i == \n0 \nand \nnot\n first_block:  \n            blk.add(Residual(num_channels, use_1x1conv=\n True\n, strides=\n 2\n)) \n        \n else\n: \n            blk.add(Residual(num_channels))  \n    \nreturn\n blk \n@d2l\n.add_to_class(ResNet)  \ndef \n__init__\n (\nself\n, arch, lr=\n 0.1\n, num_classes=\n 10\n): \n    \nsuper\n(ResNet, \n self\n).\n__init__\n () \n    \nself\n.save_hyperparameters()  \n    \nself\n.net = tf.keras.models.Sequential(\n self\n.b1()) \n    \nfor\n i, b \nin \nenumerate\n (arch): \n        \n self\n.net.add(\n self\n.block(*b, first_block=(i==\n 0\n))) \n    \nself\n.net.add(tf.keras.models.Sequential([  \n        tf.keras.layers.GlobalAvgPool2D(),  \n        tf.keras.layers.Dense(units=num_classes)]))  ",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#25": "Architettura ResNet e Keras\nclass \nResNet18\n (ResNet):  \n    \ndef \n__init__\n (\nself\n, lr=\n0.1\n, num_classes=\n 10\n): \n        \n super\n().\n__init__\n (((\n2\n, \n64\n), (\n2\n, \n128\n), (\n2\n, \n256\n), (\n2\n, \n512\n)), \n                       lr, num_classes)  \nResNet18().layer_summary((\n 1\n, \n96\n, \n96\n, \n1\n)) \nSequential output shape:     (\n 1\n, \n24\n, \n24\n, \n64\n) \nSequential output shape:     (\n 1\n, \n24\n, \n24\n, \n64\n) \nSequential output shape:     (\n 1\n, \n12\n, \n12\n, \n128\n) \nSequential output shape:     (\n 1\n, \n6\n, \n6\n, \n256\n) \nSequential output shape:     (\n 1\n, \n3\n, \n3\n, \n512\n) \nSequential output shape:     (\n 1\n, \n10\n) \ntrainer = d2l.Trainer(max_epochs=\n 10\n) \ndata = d2l.FashionMNIST(batch_size=\n 128\n, resize=(\n 96\n, \n96\n)) \nwith\n d2l.try_gpu():  \n    model = ResNet18(lr=\n 0.01\n) \n    trainer.fit(model, data)  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#26": "CNN - Esercizio\nQuali sono le innovazioni di AlexNet rispetto a LeNet-5? E per quanto riguarda \nGoogleLeNet e ResNet?\n27",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#27": "CNN - Esercizio\nQuali sono le innovazioni di AlexNet rispetto a LeNet-5? E per quanto riguarda \nGoogleLeNet e ResNet?  \n•\nAlexNet\n  è più profonda e ampia rispetto a LeNet-5, e crea stack di \nconvolutional layer uno sull'altro, invece di alternarli con pooling layer.\n28",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#28": "CNN - Esercizio\nQuali sono le innovazioni di AlexNet rispetto a LeNet-5? E per quanto riguarda \nGoogleLeNet e ResNet?  \n•\nAlexNet\n  è più profonda e ampia rispetto a LeNet-5, e crea stack di \nconvolutional layer uno sull'altro, invece di alternarli con pooling layer.  \n•\nGoogleNet\n  impiega inception modules, che permettono di avere reti ancora \npiù profonde ma con meno parametri rispetto alle precedenti.\n29",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#29": "CNN - Esercizio\nQuali sono le innovazioni di AlexNet rispetto a LeNet-5? E per quanto riguarda \nGoogleLeNet e ResNet?  \n•\nAlexNet\n  è più profonda e ampia rispetto a LeNet-5, e crea stack di \nconvolutional layer uno sull'altro, invece di alternarli con pooling layer.  \n•\nGoogleNet\n  impiega inception modules, che permettono di avere reti ancora \npiù profonde ma con meno parametri rispetto alle precedenti.  \n•\nResNet\n  introduce le skip connections, che permettono un numero di layer \noltre i 100. Anche la relativa semplicità la contraddistingue. \n30",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#3": "Motivazioni\nInoltre un layer che produce valori di attivazione molto elevati rispetto agli altri \n(es. x100) richiede aggiustamenti (es. modi\n ﬁ\ncando il learning rate in modo \nadattivo per produrre variazioni più ef\n ﬁ\ncaci durante il training).  \nIn\nﬁ\nne, per affrontare l'over\n ﬁ\ntting è spesso utile introdurre \n regolarizzazioni\n  sul \nvalore dei parametri (es. aggiungendo del rumore).",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#30": "Esercizio su CNN e MNIST\nProva costruire una tua architettura CNN (cioè con uno o più convolution \nlayers, pooling layers, etc) per raggiungere la migliore accuratezza per i \ndataset MNIST.  \n•\nMNIST dataset: \n http://yann.lecun.com/exdb/mnist/   \n•\nMNIST e Tensor\n ﬂ\now: \nhttps://www.tensor\n ﬂ\now.org/quantum/tutorials/mnist  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#4": "Internal covariate shift\nCon \nInternal covariate shift\n  si indica la circostanza in cui \n la distribuzione \ndei valori di attivazione nella rete cambia a causa della variazione dei \nparametri durante il training\n .  \n•\nFenomeno fondamentale nelle architetture deep (con molti layers).  \n•\nE' chiaro che i parametri in\n ﬂ\nuenzano le attivazioni, ma \n la distribuzione \ndei valori \n non dovrebbe alterarsi a causa dei parametri.  \n•\nIl vanishing/exploding gradient ricade in questa circostanza.  \n•\nReLU, e le sue varianti, riducono il fenomeno ma non lo escludono.  \nL'obiettivo è ridurre il \n covariance shift\n  all'interno della reti.\n5",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#5": "Batch Normalization\nLa \nbatch normalization\n  (\nBN\n) è una tecnica per affrontare tale problema. \nPrima della funzione di attivazione di ogni layer:  \n•\nNormalizza gli input\n , centrandoli in 0 e dividendoli per la deviazione \nstandard \n σ\n. \n•\nIntroduce \n 2 parametri\n , uno per determinare la \n scalatura\n  e uno per lo \n shift\n. \nTali parametri saranno soggetti ad addestramento.  \nDopo la normalizzazione \n la rete apprende il valore medio e la scala più \ngiusta degli input per ogni layer\n . \n•\nLa normalizzazione è frequente nei dati in ingresso degli approcci basati su \nML. La tecnica proposta estende tale tecnica ad ogni layer della rete.  \nPer normalizzare bisogna prima conoscere valor medio e varianza dei dati. \nSi stimano entrambi \n impiegando \n mini-batch,  \ncioè un piccolo sottoinsieme \ndel training set. Da questo il termine \n batch normalization\n .\n6",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#6": "Batch Normalization\nConsiste in un \n algoritmo\n  applicato ad ogni singola istanza in input \n x\ni\n, \nconsiderando un mini-batch \n B\n di \nm\n istanze con \n media  \n e \nvarianza   \nI parametri da apprendere durante il training sono \n γ\n (\nscale\n ) e \nβ\n (\noffset\n ). \nε\n è una costante aggiunta alla varianza per evitare divisione per 0 (es. 10\n-3\n)\nμ\nB\n σ\n2\nB\n7\nda Ioffe e Szegedy \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" 2015Trasformazione lineare",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#7": "Batch Normalization - Considerazioni\nLa tecnica BN può essere impiegata sui singoli layer, soprattutto sugli \nhidden, oppure all'intera rete.  \nLa stima di media e deviazione standard sono ricavate sul mini batch \ncorrente.  \nPossiamo interpretare i parametri \n scale\n  e \noffset\n  stimati durante \nl'apprendimento come un mezzo per \"recuperare\" i gradi di libertà persi a \ncausa della normalizzazione e limitarsi a considerare mini-batch invece \ndell'intero dataset.  \nCon mini-batch di dimensione adeguata (un iperparametro da de\n ﬁ\nnire) si \nraggiungono buoni incrementi di prestazioni e una \n stabilità\n  nell'andamento. \nMa richiede un tuning che dipende dai dati impiegati.  \nLa tecnica non permette al valore dei parametri di divergere. Inoltre permette \ndi incrementare il \n learning rate\n . \n8",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#8": "Batch Normalization - Considerazioni (2)\nPuò sembrare illogico introdurre approssimazioni nei valori di media e \ndeviazione standard, ma nella pratica rappresentano una sorta di rumore \nintrodotto arti\n ﬁ\ncialmente che garantisce tempi più rapidi e minor effetto \nover\nﬁ\ntting.  \nValori spesso ottimali della dimensione del mini batch sono 50-100 istanze, \nche garantiscono la giusta \n quantità di rumore\n  introdotto durante \nl'apprendimento.\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\08-CNN parte 5-sbloccato.pdf#9": "Batch Normalization - Considerazioni (3)\nIn casi particolari \n γ\n e \nβ\n possono assumere valori tali da \"invertire\" il processo \ndi normalizzazione degli input del processo di \n batch normalization\n , se \nquesto garantisce l'ottimalità durante il training.  \nLa BN può essere vista come una \n trasformazione lineare\n , perciò facilmente \ndifferenziabile\n  durante il calcolo dei gradienti.  \nLa normalizzazione basata su mini-batch è essenziale per garantire \nl'ef\nﬁ\ncienza di tale tecnica, ma è inutile nel test e in produzione.  \nIn \nproduzione\n  vogliamo una rete che renda l'output dipendente \nunicamente e deterministicamente dall'input\n , perciò non in\n ﬂ\nuenzata \ndallo speci\n ﬁ\nco mini-batch.  \nPer tale motivo la normalizzazione sarà calcolata sull'intera popolazione \n {x} \ncon valori di media e varianza costanti durante l'elaborazione:\n10\n",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nL'addestramento delle reti neurali Deep\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#1": "Sommario\nMotivazioni dell'apprendimento speci\n ﬁ\nco per reti Deep  \nVanishing/Exploding gradients  \nLa funzione di attivazione softmax  \nInizializzazione dei parametri  \nFunzione di attivazione ReLU e variazioni  \nBatch normalization  \nGradient clipping  \nReusing Pretrained layers - Transfer learning  \nUnsupervised Pretraining  \nPretraining su Auxiliary tasks",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#10": "Inizializzazione di Xavier e He  \nSchema di dimostrazione\nImpiegare una distribuzione casuale gaussiana con media 0 sembra ragionevole, ma perché \nporre un vincolo sulla varianza?  \nConsideriamo un singolo neurone. Per un certo layer, se in input un vettore abbiamo un vettore \nX\n di \nn\n elementi, con una matrice di pesi \n W \nsi ha:  \n                             \nVale anche:  \n                      \nSupponendo \n X\n e \ny\n indipendenti, vale la seguente uguaglianza:  \n             \nAvendo media pari a 0, si riduce a:   \nSupponendo \n w\ni\n e \nx\ni  \nindipendenti, e ogni \n w\ni\n (e \nx\ni\n) generato con stessa distribuzione, si ha:  \n  \nSe imponiamo che le due varianza \n  e \n  siano identiche, allora otteniamo:  \ny\n=\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\nσ\n2\n(\ny\n)\n=\nσ\n2\n(\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nμ\n(\nx\ni\n)\n2\nσ\n2\n(\nw\ni\n)\n+\nμ\n(\nw\ni\n)\n2\nσ\n2\n(\nx\ni\n)\n+\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\n=\nn\n⋅\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\n)\n=\n1\nn\n11",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#11": "Inizializzazione di Xavier e He  \nSchema di dimostrazione\nImpiegare una distribuzione casuale gaussiana con media 0 sembra ragionevole, ma perché \nporre un vincolo sulla varianza?  \nConsideriamo un singolo neurone. Per un certo layer, se in input un vettore abbiamo un vettore \nX\n di \nn\n elementi, con una matrice di pesi \n W \nsi ha:  \n                             \nVale anche:  \n                      \nSupponendo \n X\n e \ny\n indipendenti, vale la seguente uguaglianza:  \n             \nAvendo media pari a 0, si riduce a:   \nSupponendo \n w\ni\n e \nx\ni  \nindipendenti, e ogni \n w\ni\n (e \nx\ni\n) generato con stessa distribuzione, si ha:  \n  \nSe imponiamo che le due varianza \n  e \n  siano identiche, allora otteniamo:  \ny\n=\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\nσ\n2\n(\ny\n)\n=\nσ\n2\n(\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nμ\n(\nx\ni\n)\n2\nσ\n2\n(\nw\ni\n)\n+\nμ\n(\nw\ni\n)\n2\nσ\n2\n(\nx\ni\n)\n+\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\n=\nn\n⋅\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\n)\n=\n1\nn\n12",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#12": "Inizializzazione di Xavier e He  \nSchema di dimostrazione\nImpiegare una distribuzione casuale gaussiana con media 0 sembra ragionevole, ma perché \nporre un vincolo sulla varianza?  \nConsideriamo un singolo neurone. Per un certo layer, se in input un vettore abbiamo un vettore \nX\n di \nn\n elementi, con una matrice di pesi \n W \nsi ha:  \n                             \nVale anche:  \n                      \nSupponendo \n X\n e \ny\n indipendenti, vale la seguente uguaglianza:  \n             \nAvendo media pari a 0, si riduce a:   \nSupponendo \n w\ni\n e \nx\ni  \nindipendenti, e ogni \n w\ni\n (e \nx\ni\n) generato con stessa distribuzione, si ha:  \n  \nSe imponiamo che le due varianza \n  e \n  siano identiche, allora otteniamo:  \ny\n=\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\nσ\n2\n(\ny\n)\n=\nσ\n2\n(\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nμ\n(\nx\ni\n)\n2\nσ\n2\n(\nw\ni\n)\n+\nμ\n(\nw\ni\n)\n2\nσ\n2\n(\nx\ni\n)\n+\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\n=\nn\n⋅\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\n)\n=\n1\nn\n13",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#13": "Inizializzazione di Xavier e He  \nSchema di dimostrazione\nImpiegare una distribuzione casuale gaussiana con media 0 sembra ragionevole, ma perché \nporre un vincolo sulla varianza?  \nConsideriamo un singolo neurone. Per un certo layer, se in input un vettore abbiamo un vettore \nX\n di \nn\n elementi, con una matrice di pesi \n W \nsi ha:  \n                             \nVale anche:  \n                      \nSupponendo \n X\n e \ny\n indipendenti, vale la seguente uguaglianza:  \n             \nAvendo media pari a 0, si riduce a:   \nSupponendo \n w\ni\n e \nx\ni  \nindipendenti, e ogni \n w\ni\n (e \nx\ni\n) generato con stessa distribuzione, si ha:  \n  \nSe imponiamo che le due varianza \n  e \n  siano identiche, allora otteniamo:  \ny\n=\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\nσ\n2\n(\ny\n)\n=\nσ\n2\n(\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nμ\n(\nx\ni\n)\n2\nσ\n2\n(\nw\ni\n)\n+\nμ\n(\nw\ni\n)\n2\nσ\n2\n(\nx\ni\n)\n+\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\n=\nn\n⋅\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\n)\n=\n1\nn\n14",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#14": "Inizializzazione di Xavier e He  \nSchema di dimostrazione\nImpiegare una distribuzione casuale gaussiana con media 0 sembra ragionevole, ma perché \nporre un vincolo sulla varianza?  \nConsideriamo un singolo neurone. Per un certo layer, se in input un vettore abbiamo un vettore \nX\n di \nn\n elementi, con una matrice di pesi \n W \nsi ha:  \n                             \nVale anche:  \n                      \nSupponendo \n X\n e \ny\n indipendenti, vale la seguente uguaglianza:  \n             \nAvendo media pari a 0, si riduce a:   \nSupponendo \n w\ni\n e \nx\ni  \nindipendenti, e ogni \n w\ni\n (e \nx\ni\n) generato con stessa distribuzione, si ha:  \n  \nSe imponiamo che le due varianza \n  e \n  siano identiche, allora otteniamo:  \ny\n=\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\nσ\n2\n(\ny\n)\n=\nσ\n2\n(\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nμ\n(\nx\ni\n)\n2\nσ\n2\n(\nw\ni\n)\n+\nμ\n(\nw\ni\n)\n2\nσ\n2\n(\nx\ni\n)\n+\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\n=\nn\n⋅\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\n)\n=\n1\nn\n15",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#15": "Inizializzazione di Xavier e He  \nSchema di dimostrazione\nImpiegare una distribuzione casuale gaussiana con media 0 sembra ragionevole, ma perché \nporre un vincolo sulla varianza?  \nConsideriamo un singolo neurone. Per un certo layer, se in input un vettore abbiamo un vettore \nX\n di \nn\n elementi, con una matrice di pesi \n W \nsi ha:  \n                             \nVale anche:  \n                      \nSupponendo \n X\n e \ny\n indipendenti, vale la seguente uguaglianza:  \n             \nAvendo media pari a 0, si riduce a:   \nSupponendo \n w\ni\n e \nx\ni  \nindipendenti, e ogni \n w\ni\n (e \nx\ni\n) generato con stessa distribuzione, si ha:  \n  \nSe imponiamo che le due varianza \n  e \n  siano identiche, allora otteniamo:  \ny\n=\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\nσ\n2\n(\ny\n)\n=\nσ\n2\n(\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nμ\n(\nx\ni\n)\n2\nσ\n2\n(\nw\ni\n)\n+\nμ\n(\nw\ni\n)\n2\nσ\n2\n(\nx\ni\n)\n+\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\n=\nn\n⋅\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\n)\n=\n1\nn\n16",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#16": "Inizializzazione di Xavier e He  \nSchema di dimostrazione\nImpiegare una distribuzione casuale gaussiana con media 0 sembra ragionevole, ma perché \nporre un vincolo sulla varianza?  \nConsideriamo un singolo neurone. Per un certo layer, se in input un vettore abbiamo un vettore \nX\n di \nn\n elementi, con una matrice di pesi \n W \nsi ha:  \n                             \nVale anche:  \n                      \nSupponendo \n X\n e \ny\n indipendenti, vale la seguente uguaglianza:  \n             \nAvendo media pari a 0, si riduce a:   \nSupponendo \n w\ni\n e \nx\ni  \nindipendenti, e ogni \n w\ni\n (e \nx\ni\n) generato con stessa distribuzione, si ha:  \n  \nSe imponiamo che le due varianza \n  e \n  siano identiche, allora otteniamo:  \ny\n=\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\nσ\n2\n(\ny\n)\n=\nσ\n2\n(\nw\n1\nx\n1\n+\nw\n2\nx\n2\n+\n.\n.\n.\n+\nw\nn\nx\nn\n+\nb\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nμ\n(\nx\ni\n)\n2\nσ\n2\n(\nw\ni\n)\n+\nμ\n(\nw\ni\n)\n2\nσ\n2\n(\nx\ni\n)\n+\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\nx\ni\n)\n=\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\n=\nn\n⋅\nσ\n2\n(\nw\ni\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\ny\n)\nσ\n2\n(\nx\ni\n)\nσ\n2\n(\nw\ni\n)\n=\n1\nn\n17",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#17": "Altre inizializzazioni\nRicerche più recenti adattano la suddetta inizializzazione considerando \ndiversi scenari di funzione attivazione\n , dove si distinguono per ogni layer il \nnumero di connessioni in input (\n n\ninputs\n) e in output (\n n\noutputs\n ). \nRiferimenti:  \n•\nHe et al. \n Delving Deep into Recti\n ﬁ\ners: Surpassing Human-Level Performance on ImageNet Classi\n ﬁ\ncation\n  2015\n18\nHu initialization →",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#18": "Inizializzazione dei pesi\nPerché non usiamo la \n zero initialization\n  (valori iniziali tutti uguali a 0)?\n19",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#19": "Inizializzazione dei pesi\nPerché non usiamo la \n zero initialization\n  (valori iniziali tutti uguali a 0)?  \n1.\nValori dei pesi prossimi allo 0 favoriscono i vanishing gradients \nproblem\n . \n•\nAllo stesso modo, valori troppo grandi \"saturano\" la logistic function, \ngenerando gradienti vicini allo 0.\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#2": "Motivazioni\nAbbiamo visto architetture di Reti neurali (\n NN\n) con più strati (\n layer\n ), ognuno \ncomposto da molti nodi completamente connessi con i layer precedenti e \nsuccessivi (\n fully connected\n ). \nL'addestramento (training) di tali architetture mostra le seguenti \nproblematiche:  \n•\nVanishing gradients\n  o \nExploding gradients\n : che rendono la ricerca dei \nparametri molto dif\n ﬁ\ncile \n•\nLentezza\n : la stima di molti parametri richiede molto tempo  \n•\nOver\n ﬁ\ntting\n: la presenza di molti parametri aumenta la possibilità di \nover\nﬁ\ntting (cioè mancanza di generalizzazione).  \nPer tale motivo introduciamo\n  tecniche di addestramento speci\n ﬁ\nche\n per \naffrontarle, che permettono di de\n ﬁ\nnire architetture NN più complesse e \ndeep\n .\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#20": "Inizializzazione dei pesi\nPerché non usiamo la \n zero initialization\n  (valori iniziali tutti uguali a 0)?  \n1.\nValori dei pesi prossimi allo 0 favoriscono i vanishing gradients \nproblem\n . \n•\nAllo stesso modo, valori troppo grandi \"saturano\" la logistic function, \ngenerando gradienti vicini allo 0.  \n2.\nPer \nvalori prossimi allo 0 la logistic function si comporta in modo \nlineare\n .  \n•\nTale comportamento ci preclude l'addestramento di funzioni complesse e non \nlineari, anche in presenza di più layer.\n21",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#21": "Inizializzazione dei pesi\nPerché non scegliere un singolo valore random diverso da 0 per tutti i \npesi?\n22",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#22": "Inizializzazione dei pesi\nPerché non scegliere un singolo valore random diverso da 0 per tutti i \npesi?  \n1.\nAvere \n una rete inizializzata con gli stessi valori implica avere stessi \ngradienti e stessi aggiornamenti per ogni nodo di un layer\n .  \n•\nUno degli obiettivi della inizializzazione dei parametri è \n rompere eventuali \nsimmetrie\n  nel comportamento della rete.  \n•\nLa simmetria \n non permette di specializzare diversi neuroni su diversi scopi\n .  \n•\nUn layer con tutti nodi con lo stesso peso è equivalente ad un layer con un \nsingolo nodo.  \n•\nLa \nbackpropagation  \nnon è in grado di risolvere in modo adeguato questo tipo \ndi simmetrie\n .\n23",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#23": "Inizializzazione dei pesi\nPossiamo inizializzare il valore dei bias a 0?\n24",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#24": "Inizializzazione dei pesi\nPossiamo inizializzare il valore dei bias a 0?  \n•\nÈ possibile inizializzare i \n bias\n a 0, oppure seguire il procedimento di \ninizializzazione usato per i pesi \n w\n.\n25",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#25": "Inizializzazione dei pesi e softmax\nLa \nsoftmax\n  è spesso usata nella classi\n ﬁ\ncazione multi label.  \nTende a dare \n molta più probabilità\n  alle classi associate ai nodi che hanno in \noutput \n attivazione superiori agli altri,\n  soprattutto se l'intervallo dei valori \ndelle attivazioni è esteso.  \nSi cerca di ridurre questo intervallo (ad esempio con vincoli sulla varianza) \nper alleviare questo comportamento \"\n opinionated\n \", soprattutto nelle prime \nfasi di training.  \n•\nL'inizializzazione dei pesi è fondamentale.  \n•\nSi garantisce una esplorazione più ampia dello spazio di ricerca.\n26\n",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#26": "Nonsaturing activation functions\nLa \nlogistic function\n  è molto popolare, ed è in parte ispirata al \ncomportamento di un neurone \n ﬁ\nsico.  \nMa nelle architetture \n deep\n  è più conosciuta la:  \nRecti\n ﬁ\ned Linear Function  \nReLU :  \nf(x)=max(0,x)  \nFino a pochi anni fa la più popolare nelle architetture deep.\n27\n",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#27": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:\n28",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#28": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:  \n•\nGarantisce la \n non linearità\n .\n29",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#29": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:  \n•\nGarantisce la \n non linearità\n  anche per valori prossimi allo 0.  \n•\nNon satura per valori positivi elevati\n . I gradienti sono sempre signi\n ﬁ\ncativi. \nRispetto alla logistic (o tanh) \n riduce il vanishing problem\n .\n30",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#3": "Vanishing/Exploding gradients\nL'\nalgoritmo di backpropagation\n  usato per addestrare una NN segue questi passi:  \n•\nPer ogni coppia input-output si valuta l'errore tra output ottenuto dalla NN e \noutput atteso mediante la \n loss function \n (o \ncost function\n ). \n•\nSi calcola il \n gradiente\n , cioè l'insieme delle derivate parziali dell'errore rispetto \nai parametri (pesi), mediante la \n chain rule\n . \n•\nIn base a tali valori si aggiornano i pesi in modo da ridurre l'errore, ad esempio \nmediante il \n gradient descent.  \nImpiegando i gradienti per addestrare la rete può capitare di ottenere valori \nmolto piccoli (\n vanishing gradients\n ), soprattutto per i layer vicini all'input.  \n•\nIl gradiente nei primi strati si ottiene come \n prodotto\n  dei gradienti degli strati più \nlontani.  \n•\nQuesto implica che nei primi layer i pesi non vengono pressoché alterati \ndurante il training \n e dif\nﬁ\ncilmente si converge ad una soluzione\n .\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#30": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:  \n•\nGarantisce la \n non linearità\n  anche per valori prossimi allo 0.  \n•\nNon satura per valori positivi elevati\n . I gradienti sono sempre signi\n ﬁ\ncativi. \nRispetto alla logistic (o tanh) \n riduce il vanishing problem\n . \n•\nRiduce la complessità computazione\n  non essendoci la componente \nesponenziale da differenziare.\n31",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#31": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:  \n•\nGarantisce la \n non linearità\n  anche per valori prossimi allo 0.  \n•\nNon satura per valori positivi elevati\n . I gradienti sono sempre signi\n ﬁ\ncativi. \nRispetto alla logistic (o tanh) \n riduce il vanishing problem\n . \n•\nRiduce la complessità computazione\n  non essendoci la componente \nesponenziale da differenziare.  \nMa ha dei \n svantaggi\n :\n32",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#32": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:  \n•\nGarantisce la \n non linearità\n  anche per valori prossimi allo 0.  \n•\nNon satura per valori positivi elevati\n . I gradienti sono sempre signi\n ﬁ\ncativi. \nRispetto alla logistic (o tanh) \n riduce il vanishing problem\n . \n•\nRiduce la complessità computazione\n  non essendoci la componente \nesponenziale da differenziare.  \nMa ha dei \n svantaggi\n : \n•\nSperimentalmente \n è più prona all'over\n ﬁ\ntting\n, perciò si usa insieme a tecniche \nper ridurre questo problema (es. dropout).  \n33",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#33": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:  \n•\nGarantisce la \n non linearità\n  anche per valori prossimi allo 0.  \n•\nNon satura per valori positivi elevati\n . I gradienti sono sempre signi\n ﬁ\ncativi. \nRispetto alla logistic (o tanh) \n riduce il vanishing problem\n . \n•\nRiduce la complessità computazione\n  non essendoci la componente \nesponenziale da differenziare.  \nMa ha dei \n svantaggi\n : \n•\nSperimentalmente \n è più prona all'over\n ﬁ\ntting\n, perciò si usa insieme a tecniche \nper ridurre questo problema (es. dropout).  \n•\nSe durante l'apprendimento l'input \n x\n combinato coi pesi \n w\n genera un valore \nnegativo, e l'output è pari a \n 0\n. Può capitare che \n i neuroni smettano di generare \nvalori diversi da 0\n  (\ndying ReLUs\n ). \n34",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#34": "ReLU activation function\nI principali \n vantaggi\n  della ReLU sono:  \n•\nGarantisce la \n non linearità\n  anche per valori prossimi allo 0.  \n•\nNon satura per valori positivi elevati\n . I gradienti sono sempre signi\n ﬁ\ncativi. \nRispetto alla logistic (o tanh) \n riduce il vanishing problem\n . \n•\nRiduce la complessità computazione\n  non essendoci la componente \nesponenziale da differenziare.  \nMa ha dei \n svantaggi\n : \n•\nSperimentalmente \n è più prona all'over\n ﬁ\ntting\n, perciò si usa insieme a tecniche \nper ridurre questo problema (es. dropout).  \n•\nSe durante l'apprendimento l'input \n x\n combinato coi pesi \n w\n genera un valore \nnegativo, e l'output è pari a \n 0\n. Può capitare che \n i neuroni smettano di generare \nvalori diversi da 0\n  (\ndying ReLUs\n ). \n•\nL'intervallo dell'output è \n [\n0,\n∞\n]\n35",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#35": "Leaky ReLU\nPer risolvere il problema\n  dying ReLUs\n  si introduce la \n Leaky ReLU \n o \nLReLU:  \n                               \ndove \n α\n è un \n iperparametro\n  (es. \n0.01\n) che garantisce un valore diverso da \n 0 \nper \nx < 0\n . \nf\n(\nx\n)\n=\nm\na\nx\n(\nα\n⋅\nx\n,\nx\n)\n36\n",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#36": "Altre ReLU\nParametric ReLU (PReLU)\n :  \n•\nα\n diviene un parametro che viene stimato durante il training\n . \n•\nRispetto alla ReLU incrementa le performance in dataset di immagini molto grandi.  \nRandomized Leaky (RReLU):  \nα\n viene impostato in modo casuale durante il training,  \ne tenuto \n ﬁ\nsso durante il test.  \nPuò ridurre fenomeni di over\n ﬁ\ntting.  \nExponential Linear Unit (ELU):     \nHa un gradiente <> 0 per x < 0\n  (contro il dying ReLU)\n .\n La media degli output di un layer \nè più vicina allo 0 rispetto a quella della ReLU, e si riduce il vanishing problem.  \nNon ha singolarità nello 0\n , cioè ha sempre derivate <> 0.  \nSebbene richieda più tempo per il calcolo del gradiente, compensa con un tasso di \nconvergenza più veloce.\nE\nL\nU\nα\n(\nz\n)\n=\n{\nα\n(\ne\nx\n−\n1\n)\n,\nx\n<\n0\nx\n,\nx\n≥\n0\n37",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#37": "Altre ReLU\nParametric ReLU (PReLU)\n :  \n•\nα\n diviene un parametro che viene stimato durante il training\n . \n•\nRispetto alla ReLU incrementa le performance in dataset di immagini molto grandi.  \nRandomized Leaky (RReLU)\n : \n•\nα\n viene alterato in modo casuale durante il training,  \ne tenuto \n ﬁ\nsso durante il test.  \n•\nPuò ridurre fenomeni di over\n ﬁ\ntting.  \nExponential Linear Unit (ELU):     \nHa un gradiente <> 0 per x < 0\n  (contro il dying ReLU)\n .\n La media degli output di un layer \nè più vicina allo 0 rispetto a quella della ReLU, e si riduce il vanishing problem.  \nNon ha singolarità nello 0\n , cioè ha sempre derivate <> 0.  \nSebbene richieda più tempo per il calcolo del gradiente, compensa con un tasso di \nconvergenza più veloce.\nE\nL\nU\nα\n(\nz\n)\n=\n{\nα\n(\ne\nx\n−\n1\n)\n,\nx\n<\n0\nx\n,\nx\n≥\n0\n38",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#38": "Altre ReLU\nParametric ReLU (PReLU)\n :  \n•\nα\n diviene un parametro che viene stimato durante il training\n . \n•\nRispetto alla ReLU incrementa le performance in dataset di immagini molto grandi.  \nRandomized Leaky (RReLU)\n : \n•\nα\n viene alterato in modo casuale durante il training,  \ne tenuto \n ﬁ\nsso durante il test.  \n•\nPuò ridurre fenomeni di over\n ﬁ\ntting.  \nExponential Linear Unit (ELU)\n :     \n•\nHa un gradiente <> 0 per x < 0\n  (contro il dying ReLU)\n .\n Producendo valori <0, la media \ndegli output di un layer è più vicina allo 0 rispetto a quella della ReLU, e si riduce il \nvanishing problem.  \n•\nNon ha singolarità nello 0\n , cioè è sempre derivabile.  \n•\nSebbene richieda più tempo per il calcolo del gradiente, compensa con un \n tasso di \nconvergenza più veloce \n della ReLU.\nE\nL\nU\nα\n(\nz\n)\n=\n{\nα\n(\ne\nx\n−\n1\n)\n,\nx\n<\n0\nx\n,\nx\n≥\n0\n39\n",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#39": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?\n40",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#4": "Vanishing/Exploding gradients\nIn modo simile i \n gradienti possono aumentare \n e in alcuni layer il valore può \neccedere gli intervalli rappresentabili nei framework di NN (\n exploding \ngradients\n ). \n•\nFenomeno che capita spesso nelle \n Recurrent NN\n  che studieremo più avanti  \nPiù in generale, \n strati diversi della rete possono aggiornarsi con \n\"velocità\" \n (cioè valori di gradienti)\n  molto diverse\n . \nTali problemi sono ancora più evidenti \n con funzioni di attivazione con \nvalore medio <> 0\n , e \ninizializzazione dei pesi casuale \n con distribuzione \ngaussiana\n . \n•\nAd esempio, nel caso della \n logistic \n (o\n sigmoid\n )\n function\n , per valori in input \ngrandi in modulo, la funzioni \n satura a 0 o 1\n , con \n derivate\n  tendenti allo \n 0\n. \nSi ha perciò \n vanishing gradients\n . \n5",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#40": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?  \n1.\nLa \nELU\n è una buona scelta iniziale.  \n41",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#41": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?  \n1.\nLa \nELU\n è una buona scelta iniziale.  \n2.\nPer reti veloci, meglio una \n leaky ReLU\n , o varianti.  \n42",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#42": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?  \n1.\nLa \nELU\n è una buona scelta iniziale.  \n2.\nPer reti veloci, meglio una \n leaky ReLU\n , o varianti.  \n3.\nLa semplicità della \n ReLU\n  motiva la sua diffusione, sebbene \n ELU\n e \nleaky  \nReLU\n  si comportino generalmente meglio.  \n43",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#43": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?  \n1.\nLa \nELU\n è una buona scelta iniziale.  \n2.\nPer reti veloci, meglio una \n leaky ReLU\n , o varianti.  \n3.\nLa semplicità della \n ReLU\n  motiva la sua diffusione, sebbene \n ELU\n e \nleaky  \nReLU\n  si comportino generalmente meglio.  \n4.\nLa possibilità della \n ReLU\n  di produrre esattamente 0 torna utile in certi task.  \n44",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#44": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?  \n1.\nLa \nELU\n è una buona scelta iniziale.  \n2.\nPer reti veloci, meglio una \n leaky ReLU\n , o varianti.  \n3.\nLa semplicità della \n ReLU\n  motiva la sua diffusione, sebbene \n ELU\n e \nleaky  \nReLU\n  si comportino generalmente meglio.  \n4.\nLa possibilità della \n ReLU\n  di produrre esattamente 0 torna utile in certi task.  \n5.\nLa tangente iperbolica \n tanh\n è usata nell'output layer per produrre valori tra \n-1\n e \n1\n, ma non viene più usata negli hidden layers.  \n45",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#45": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?  \n1.\nLa \nELU\n è una buona scelta iniziale.  \n2.\nPer reti veloci, meglio una \n leaky ReLU\n , o varianti.  \n3.\nLa semplicità della \n ReLU\n  motiva la sua diffusione, sebbene \n ELU\n e \nleaky  \nReLU\n  si comportino generalmente meglio.  \n4.\nLa possibilità della \n ReLU\n  di produrre esattamente 0 torna utile in certi task.  \n5.\nLa tangente iperbolica \n tanh\n è usata nell'output layer per produrre valori tra \n-1\n e \n1\n, ma non viene più usata negli hidden layers.  \n6.\nLa \nlogistic\n  è usata nell'output layer per stimare probabilità (es. \nclassi\n ﬁ\ncazione binaria), ma è raramente usata per gli hidden layers.  \n46",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#46": "Funzioni di Attivazione\nIn quali casi impiegheresti ELU, leaky ReLU (e sue varianti), ReLU, tanh, \nlogistic, softmax?  \n1.\nLa \nELU\n è una buona scelta iniziale.  \n2.\nPer reti veloci, meglio una \n leaky ReLU\n , o varianti.  \n3.\nLa semplicità della \n ReLU\n  motiva la sua diffusione, sebbene \n ELU\n e \nleaky  \nReLU\n  si comportino generalmente meglio.  \n4.\nLa possibilità della \n ReLU\n  di produrre esattamente 0 torna utile in certi task.  \n5.\nLa tangente iperbolica \n tanh\n è usata nell'output layer per produrre valori tra \n-1\n e \n1\n, ma non viene più usata negli hidden layers.  \n6.\nLa \nlogistic\n  è usata nell'output layer per stimare probabilità (es. \nclassi\n ﬁ\ncazione binaria), ma è raramente usata per gli hidden layers.  \n7.\nLa \nsoftmax\n  è adatta per ottenere distribuzioni di probabilità su classi \nmutuamente esclusive.\n47",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#47": "Gradient clipping\nUna tecnica molto facile per ridurre il fenomeno del \n exploding gradients\n  è \nintrodurre \n una soglia per limitare il valore dei gradienti\n  durante il \nbackpropagation, chiamata \n gradient clipping\n . \nChiaramente ponendo valori soglia rischiamo di ridurre l'informazione che \ntali parametri possono propagare.  \nLa rivedremo tra poco ma nel dominio della regolarizzazione dei parametri \nW\n.\n48",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#48": "Reusing pretrained layers\nAddestrare un rete deep complessa \n richiede molte risorse\n , a volte \nimpossibili da avere, ad eccezione di pochi laboratori al mondo.  \nIl \ntransfer learning\n  è l'approccio che ha l'obiettivo di\n  ri-utilizzare parametri \nottenuti da architetture già addestrate \n su obiettivi simili.  \nHa il duplice vantaggio di ridurre:  \n•\nil \ntempo di addestramento,  \n•\nla \ndimensione del training set\n  relativo all'obiettivo di interesse.  \nAd esempio, una rete è addestrata a riconoscere animali, piante, automobili, \netc., mentre siamo interessati a distinguere il modello di certe auto.  \n•\nConviene riutilizzare i parametri che permettono di rappresentare speci\n ﬁ\nche \nfeatures della classe auto (es. forme di fanali e paraurti) e sfruttarli per \nadattare la rete alle nuove classi.\n49",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#49": "Esempio di complessità della architettura GPT-3\nGenerative Pre-trained Transformer 3\n  (\nGPT-3\n ) è un architettura deep creata \nda \nOpenAI\n  per ottenere modelli (transformer) di linguaggio naturale.  \nApparsa nel 2020 come evoluzione delle versioni v2 e v1, è popolare \nper la capacità di redigere testo (es. news) simili a quelle scritte da \npersone umane. E' suf\n ﬁ\nciente dare poche parole per farla partire.  \nContiene 175 miliardi di parametri.  \nAddestrata su quasi 500 miliardi di parole estratte da varie fonti \n(CommonCrawl, WebText2, Books, Wikipedia)  \nCon una GPU cloud Tesla V100 richiederebbe $4.6M e 355 anni per \nl'addestramento.  \nEsempi di applicazione: \n https://beta.openai.com/examples/  \n50",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#5": "Logistic (sigmoid) function\n6\nSaturazione  \n(risposta max)  \ngradiente basso\nSaturazione  \n(risposta max )\ngradiente basso Quasi-lineare \nderivata costante \ncioè indipendente dagli input\n",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#50": "Reusing pretrained layers\nNelle architetture deep i \n layer rappresentano features \n rilevanti per la \nclassi\n ﬁ\ncazione \n con diversi livelli di \n astrazione\n . Ad esempio nel task della \nclassi\n ﬁ\ncazione delle immagini:  \n•\nI \nprimi layer\n  (vicini all'input) si specializzano su  \nfeatures di base\n , come \nline, angoli, variazioni cromatiche  \n•\nI \nlayer più vicini all'output\n  legano le feature precedenti per rappresentare \noggetti complessi e relative \n caratteristiche salienti\n  (es. il muso e le orecchie \ndi un animale).  \nIl transfer learning \n mira a riutilizzare i parametri (e il tipo di feature) più \nimportanti.  \n•\nI pesi che si riusano si possono \n congelare, \n e focalizzare l'addestramento \nsolo sui nuovi parametri che dipendono dal nuovo task.  \n•\nSi sempli\n ﬁ\nca l'addestramento poiché alcuni parametri non si alterano. \n51",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#51": "Transfer learning - procedimento\nGli output layers della rete iniziale spesso si scartano perché tipicamente \nnon riadattabili al nuovo task, cioè non rappresentano le feature signi\n ﬁ\ncative \nper il nuovo task e ai nuovi output di interesse.  \nIl \nprocedimento\n  generale del \n transfer learning\n  è il seguente:  \n1.\nSi tenta di \n riutilizzare tutti i parametri della vecchia rete\n  e si valutano le \nperformance.  \n2.\nSi \n\"scongelano\" gli ultimi 1 o 2 layer\n  e si permette il loro addestramento, \nvalutando miglioramenti.  \n3.\nIn caso il training set sia limitato, \n si scartano gli ultimi layer,\n  e si \ncongelano i restanti. Si valutano le performance.  \n4.\nSe si hanno suf\n ﬁ\ncienti dati, i\n  layer scartati si rimpiazzano con nuovi \nlayer\n , eventualmente aumentano la profondità della rete rispetto a quella \ndi partenza.\n52",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#52": "Reusing pretrained layers\nNota: \n Il transfer learning è adatto quando le istanze in input hanno feature a basso livello  \n(es. numero di pixel di una immagine) simili a quelle delle istanze in input alla nuova rete.\n53\nParametri da addestrare,  \nparzialmente riutilizzati\nParametri ﬁssi ottenuti  \ndalla rete già addestrata.\nRete già addestrata  \nper un certo task.Rete da addestrare  \nper un task simile.CongelatiNon congelati\n}}",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#53": "Transfer learning\nDove posso trovare parametri già addestrati?\n54",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#54": "Transfer learning\nDove posso trovare parametri già addestrati?  \nSui repository dei framework di DL, o su blog specializzati, si trovano \nelenchi aggiornati di modelli per diversi task, es:  \n•\nhttps://github.com/tensor\n ﬂ\now/models  \n•\nhttps://pytorch.org/docs/stable/torchvision/models.html   \nI modelli fanno riferimento ad architetture DL conosciute in letteratura  \n•\nEs. AlexNet, VGG, ResNet, SqueezeNet, DenseNet, Inception v3, GoogLeNet, \nShuf\nﬂ\neNet v2, MobileNet v2, ResNeXt, Wide ResNet, MNASNet\n55",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#55": "Unsupervised Pretraining\nPuò capitare di lavorare su un task dove abbiamo \n un training set di istanze \nlabelled\n  ridotto\n , e non esistono modelli pre-addestrati da impiegare.  \nNel \nunsupervised pretraining\n  la rete viene addestrata \n uno strato alla volta\n , \npartendo da quello più vicino agli input.  \n•\nOgni layer è addestrato impiegando l'output del layer precedente, quindi in \nmodo \n unsupervised\n . \n•\nTutti i layer sono congelati, tranne quello sotto addestramento.  \n•\nQuando tutti i layer sono stati addestrati, la rete può essere addestrata con \nun approccio \n ﬁ\nne-tuned \n supervised\n .  \n•\nTipicamente (1) si aggiunge un ultimo layer, (2) si congelano i pesi dei \nlayer precedenti, e (3) si considera il training set disponibile.\n56",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#56": "Unsupervised Pretraining\nLa fase di unsupervised permette di creare \n una approssimazione dei \nparametri (o inizializzazione) \n utile per la fase di addestramento reale.  \nIpotesi sostengono che tale fase individua il sottoinsieme di \n minimi\n  più \nprobabili nello spazio di ricerca.  \nÈ un approccio piuttosto lungo da completare, ma è stato impiegato di \nfrequente \n ﬁ\nno alla comparsa delle prime tecniche che hanno affrontato il \nvanishing gradients\n  problem. \n57",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#57": "Unsupervised Pretraining - considerazioni\nRicordiamoci che in una architettura deep è facile ricavare valori di errore \nsugli strati \n ﬁ\nnali, vicini all'output atteso. Ma a causa del vanishing problem, \n i \nprimi strati potrebbero non avere suf\n ﬁ\nciente informazione per il relativo \naddestramento\n . \nL'approccio iterativo del \n unsupervised pretraining  \naddestra \n uno strato alla \nvolta\n , mantenendo costanti gli altri parametri, perciò senza la possibilità che \npossano in\n ﬂ\nuenzare il training in modo sub-ottimale.  \n•\nL'addestramento è suddiviso in più fasi, e in ogni fase abbiamo pochi \nparametri da determinare.  \n•\nL'\noutput atteso \n di un layer sotto addestramento \n corrisponde all'output dello \nstrato precedente\n  (approccio unsupervised)  \n•\nIn ogni fase si tenta di identi\n ﬁ\ncare \n minimi locali \n utili per la fase \n ﬁ\nnale.  \nCon \npretraining\n  si indica in generale \n il procedimento di addestrare modelli \nsempli\n ﬁ\ncati su dati sempli\n ﬁ\ncati \nprima di arrivare al modello \n ﬁ\nnale su dati \ncomplessi.\n58",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#58": "Unsupervised Pretraining\n59\nLayer addestratoLayer addestratoLayer addestratoRete addestrata",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#59": "Pretraining su auxiliary task\nUn modo alternativo per addestrare una rete con scarsi dati di training e \ntrovare un \n auxiliary task\n , cioè un task simile che condivide un insieme di \nfeature detectors\n  salienti con il task di nostro interesse.  \nIpotesi\n : se riusciamo ad addestrare la rete sul task alternativo, potremmo \nriutilizzare i primi layer dato che si sono specializzati sulle features di \ncomune interesse.  \nEsempio: \n face detection  \nTipicamente ci sono\n  poche istanze \n per ogni viso da riconoscere.  \nPossiamo andare su Google e collezionare facilmente molti visi di \ncelebrity. La rete imparerà a riconoscere le \n feature salienti\n  che potranno \nessere impiegate sul nostro training set.  \nUn approccio alternativo è \"corrompere\" un sottoinsieme di istanze \ndisponibili di una certa classe per associarle ad una classe negativa.\n60",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#6": "Vanishing/Exploding gradients\nUna possibilità sarebbe \n comprimere\n  i valori delle attivazioni in un intervallo \nristretto, ma intorno allo 0 la logistic mostra comportamenti prettamente \nlineari, che non permettono di rappresentare funzioni complesse.\n7",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#7": "Vanishing/Exploding gradients\n8\n",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#8": "Vanishing/Exploding gradients problem:  \nCosa succede durante l'addestramento?\nComportamenti tipici in presenza di vanishing gradients problem sono:  \n•\nLe performance migliorano \n troppo lentamente\n , o \nnon migliorano  \n•\nPrematura convergenza\n  (ma non a valori ottimi)  \n•\nAnalizzando i parametri appresi si notano \n variazioni più signi\n ﬁ\ncative negli \nultimi strati\n , vicini all'output, \n rispetto ai primi strati\n .\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\09-Training Deep 1-sbloccato.pdf#9": "Inizializzazione di Xavier e He \nL'obiettivo è garantire la propagazione delle attivazioni (forward) e dei \ngradienti (backward) in modo corretto, cioè senza \n exploding\n  e \nvanishing\n . \nXavier e He\n  propongono di mantenere uguali:  \n•\ni valori della varianza degli output di ogni layer con la varianza degli input \ndel layer successivo\n  (forward propagation).  \n•\nle varianze dei gradienti ottenuti prima e dopo un certo layer\n  (backward \npropagation)  \nTali condizioni possono essere veri\n ﬁ\ncate solo se ogni layer ha lo \n stesso \nnumero di connessioni in entrata e in uscita\n .  \nIntroducendo una approssimazione de\n ﬁ\nniamo la \n Xavier initialization\n  così:\n10I pesi della rete sono inizializzati per ogni layer in modo casuale  \ncon distribuzione gaussiana  con media 0  e varianza pari a n-1, \ndove n il numero di nodi del layer precedente.",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nL'addestramento delle reti neurali Deep  \nParte 2: Optimizers, learning rates adattivi\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#1": "Sommario\nFaster Optimizer  \nMomentum optimization  \nNesterov Accelerated Gradient  \nAdaGrad Algorithm  \nRMSProp algorithm  \nAdam Optimization  \nLearning rate schedule   \nAdaptive learning rate algorithms",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#10": "Richiami: Gradient descent\n•\nAlterazioni troppo piccole allungano i tempi di esplorazioni.  \n•\nAlterazioni troppo grandi (es. learning rate elevati) generare comportamenti \nche possono allontanarci dall'ottimo.\n11\nRicerca lenta Ricerca imprecisa",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#11": "Gradient descent vs Stochastic vs Minibatch\n•\nGradient descent\n : iteriamo sull'intero training set, cioè su tutte le istanze, \nprima di aggiornare i pesi.  \n•\nSebbene la stima dell'errore sia molto precisa, per training set grandi \ndobbiamo attendere molto prima di aggiornare i parametri. Gli output \nsono ricavati con i parametri ricavati nel ciclo precedente, senza poterli \naggiornare durante l'epoca.  \n•\nStochastic Gradient descent (SGD)\n : ad ogni istanza estratta dal training set \n(in modo random) aggiorniamo i parametri.  \n•\nLa stima dei gradienti è approssimata su una singola istanza, perciò poco \nprecisa. Ma aggiorniamo i parametri istantaneamente. Convergenza più \nrapida, ma meno probabilità di raggiungere l'ottimo.  \n•\nMinibatch SGD\n : dopo un minibatch di istanze aggiorniamo i pesi.  \n•\nSi suppone che il minibatch stimi meglio le variazioni dei parametri \nsimulando la stima sull'training set. In altre parole, riduce la varianza sui \nvalori stimati. Combina i vantaggi di entrambi.\n12",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#12": "Richiami: (Mini)Batch Normalization\nConsiste in un \n algoritmo\n  applicato ad ogni singola istanza in input \n x\ni\n, \nconsiderando un mini-batch \n B\n di \nm\n istanze con \n media  \n e \nvarianza   \nI parametri da apprendere durante il training sono \n γ\n (\nscale\n ) e \nβ\n (\noffset\n ). \nε\n è una costante aggiunta alla varianza per evitare divisione per 0 (es. 10\n-3\n)\nμ\nB\n σ\n2\nB\n13\nda Ioffe e Szegedy \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" 2015Trasformazione lineare",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#13": "Gradient vs Stochastic Gradient descent\nIn condizioni ideali (es. convex function) un gradient descent tradizionale è \nl'approccio ottimale per raggiungere il minimo in poche iterazioni.  \nL'approccio stocastico introduce rumore che può rallentare il \nraggiungimento del minimo (es. a dx dopo 50 iterazioni non si hanno ancora \nvalori ottimali, a sx dopo 20 iterazioni possiamo fermarci).  \nMa generalmente nel DL non abbiamo \n convex functions\n .\n14\n",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#14": "Gradient descent e learning rate\n•\nPer rendere la ricerca \n più ef\n ﬁ\nciente \n potremmo pensare di \n variare il learning \nrate \n  durante l'esplorazione:  \n•\nAumentandolo ulteriormente al principio\n , per rendere la ricerca più rapida,  \ne riducendolo alla \n ﬁ\nne\n, soprattutto nel caso del SGD e minibatch SGD, per \nridurre gli effetti che il rumore possa avere sulla ricerca dell'ottimo.  \n•\nNell'esempio, aumentiamo la velocità durante la discesa della curva, e la \nriduciamo quando la curva riduce la pendenza.\nη\n15\nCosto\nΘ",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#15": "Momentum optimization\nLa \nmomentum optimization \n introduce il concetto di \n accelerazione\n  e \nvelocità  \ndurante l'esplorazione.  \nIntroduco il \n vettore momentum  \nm\n, usato per aggiornare i pesi \n . Il suo compito è \ninterpretare il gradiente \n  come una \n accelerazione,\n  che altera la \n velocità \ncorrente\n  rappresentata da \n . \n \n \n è chiamato \n parametro momentum\n , o semplicemente \n momentum\n , e ha lo scopo \ndi evitare che la velocità cresca eccessivamente.  \n•\n=0 resistenza massima (corrisponde al gradient descent), \n =1 nessuna resistenza.  \nSi veri\n ﬁ\nca facilmente che, se il il valore del gradiente rimane costante, la variazione \nmassima dei pesi è \n . \n•\nPer \n =0.9 e \n =1\n, si ottiene 10 volte il valore del gradiente.\nΘ\nη\n∇\nΘ\nJ\n(\nΘ\n)\nβ\n⋅\nm\nm\n←\nβ\n⋅\nm\n+\nη\n∇\nΘ\nJ\n(\nΘ\n)\nΘ\n←\nΘ\n−\nm\nβ\nβ\n β\nη\n1\n1\n−\nβ\nβ\n η\n16",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#16": "Momentum optimization (2)\nAnalogia con una biglia su una super\n ﬁ\ncie. La direzione non corrisponde più \na quella determinata dal gradiente calcolato attualmente, ma in base alle \nmedia pesata di tutti i gradienti precedenti.  \nL'approccio rientra nella classi dei \n accelerated gradient methods\n . \nHa molteplici \n vantaggi\n : \n•\nRende più stabile la ricerca quando qualche gradiente risulta scarsamente \naccurato (es. scelta sbagliata della istanza/minibatch), mediando su una serie \ndi valori.  \n•\nSi \naccelera l'esplorazione\n  quando stiamo esplorando spazi dei parametri \nampi, e dove le super\n ﬁ\nci de\n ﬁ\nnite dalla funzione di costo variano lentamente.  \n•\nL'accelerazione permette più facilmente di \n evitare (o uscire) minimi locali \nrispetto al gradient descent tradizionale\n . \nIl momentum \n  è un \n iperparametro da stimare\n  caso per caso, ma valori \nintorno allo \n 0.9\n sono molto comuni.\nβ\n17",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#17": "Momentum optimization\n18apprendimento lento\napprendimento veloce\nDiscesa del gradiente  \ncon Momentum optimizationDiscesa del gradiente  \ntradizionale",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#18": "Momentum optimization: esempio\n10-momentum.ipynb\n19",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#19": "Nesterov Accelerated Gradient\nIl \nNesterov Accelerated Gradient\n  (\nNAG\n ) è una variazione del momentum \noptimization dove il \n gradiente\n  viene valutato non nella posizione corrente \nma nella direzione del momentum  \n : \n \n \nIn genere il vettore momentum indica la direzione verso l'ottimo, perciò \nsembra più logico misurare il gradiente verso quella direzione.  \nQueste piccole variazioni si sommano e il NAG si dimostra essere \n più \nrapido rispetto al momentum optimization\n .\nΘ\n+\nβ\n⋅\nm\nm\n←\nβ\n⋅\nm\n+\nη\n∇\nΘ\nJ\n(\nΘ\n+\nβ\n⋅\nm\n)\nΘ\n←\nΘ\n−\nm\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#2": "Gli optimizer\nUn \noptimizer\n  è un algoritmo usato per alterare i parametri della rete, ed \neventualmente alcuni iperparametri quali il learning rate, in modo da \nminimizzare la misura di loss.  \nPer tale motivo la funzione di loss è anche chiamata \n objective function\n . \nNelle architetture deep si impiegano spesso \n optimizer\n  alternativi alla discesa \ndel gradiente.  \nConcettualmente, l'apprendimento delle architetture DL è ricavare un \nmodello adatto al task in base ai dati disponibili, mentre l'optimizer si \nfocalizza sulla objective function.  \nL'ottimizzazione si focalizza sulla loss è sui dati disponibili, perciò ul \ntraining error\n . \nAltrettanto fondamentale nel DL è minimizzare l'\n over\nﬁ\ntting\n, cioè \nmassimizzare le capacità di \n generalizzazione\n . Per questo durante \nl'ottimizzazione dobbiamo includere ulteriori analisi. \n3",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#20": "Nesterov Accelerated Gradient\nNello scenario classico di una discesa verso l'ottimo il NAG \n riduce eventuali \noscillazioni \n causate dall'accelerazione puntuale del momentum \noptimization.\n21\nβm",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#21": "AdaGrad Algorithm: Motivazioni\nIn presenza di \n dati sparsi\n , i parametri \n  associati alle\n  features poco \nfrequenti riceveranno update signi\n ﬁ\ncativi molto raramente\n .  \n•\nSe decrementiamo il learning rate durante l'esplorazione, nel caso tali \nfeatures non compaiono al principio del training, \n è probabile che i relativi \npesi non verranno aggiornati adeguatamente prima di raggiungere la \ncondizione di ottimo.  \nFacciamo l'ipotesi  \nche il \n learning rate  \n è legato \n al numero di volte  \ns(i,t)\n che \nabbiamo \n \"notato\" una certa features \n i\n durante il training \n ﬁ\nno al tempo \n t\n. \n•\nFeatures frequenti vedranno il learning rate associato ai relativi parametri \ndiminuire più velocemente:  \n \nMa se contiamo solo le occorrenze \n non teniamo in considerazione il valore \ndel gradiente\n , a volte molto grande, a volte irrilevante.\nΘ\nη\nη\n=\nη\n0\ns\n(\ni\n,\nt\n)\n+\nϵ\n22",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#22": "AdaGrad Algorithm\nIl \nAdaGrad algorithm\n  rende il \n learning rate adattivo\n , dove \n ogni parametro \nha un proprio rate\n . \nPer tale motivo si considera un \n vettore  \ns\n che \n memorizza gli update per ogni \nparametro\n  calcolato nel seguente modo:  \n \n \nsi assume \n s\n0\n = 0. \nLa prima espressione accumula nel vettore \n s\n i quadrati dei gradienti rispetto \nai parametri  \nﬁ\nno all'istante attuale.  \n•\nSe \nla funzione di costo è ripida\n  rispetto ad una direzione \n i\n, la sequenza dei \ngradienti assumeranno un valore elevato\n  in modulo, e la componente  \naumenterà ad ogni iterazione.\ns\n←\ns\n+\n∇\nΘ\nJ\n(\nΘ\n)\n⊗\n∇\nΘ\nJ\n(\nΘ\n)\nΘ\n←\nΘ\n−\nη\n∇\nΘ\nJ\n(\nΘ\n)\n⊘\n s\n+\nϵ\nΘ\ns\ni\n23...cont",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#23": "AdaGrad Algorithm (\n cont.\n )\nLa seconda è simile alla discesa del gradiente, ma \n il vettore dei gradienti è \nscalato del fattore  \n , dove \n  è il solito parametro di smoothing per \nevitare divisioni per 0.  \n•\nLe coordinate che mostreranno spesso gradienti elevati saranno \nmaggiormente \n ridimensionate\n , al contrario, gradienti signi\n ﬁ\ncativi sporadici \no in valore ridotto corrisponderanno a learning rate più importanti.  \nI principali vantaggi di \n AdaGrad\n  sono i seguenti:  \n•\nAdatto a training data sparsi e addestramenti molto lunghi.  \n•\nIl\n tuning del learning rate super\n ﬂ\nuo\n. Si imposta a un valore comune,  \nes. \n=0.01, evitandolo di considerare come iperparametro da ottimizzare.  \n•\nLa \ncomplessità computazione è paragonabile alla discesa del gradiente \ntradizionale.\ns\n+\nϵ\n ϵ\nη\n24",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#24": "AdaGrad Algorithm\n25\nparametro soggetto a gradienti \nelevati e legati a features frequenti\nparametro soggetto a gradienti  \nridotti e legati a features sparse",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#25": "RMSProp algorithm\nAdaGrad può rallentare la discesa del gradiente interrompendo \nanticipatamente il training.  \nL'\nalgoritmo RMSProp\n  è una variazione di AdaGrad dove \n il vettore \naccumulatore \n s\n considera maggiormente gli ultimi gradienti calcolati\n .  \n•\nSi introduce un \n fattore di decay  \n. \n \n \n•\nSebbene \n  sia un iperparametro, valori intorno allo 0.9 mostrano un buon \ncomportamento.  \nRMSProp\n  si dimostra\n  spesso migliore di AdaGrad\n  e di altre ottimizzazioni \n(es. Momentum optimization e NAG).\nβ\ns\n←\nβ\ns\n+\n(\n1\n−\nβ\n)\n∇\nΘ\nJ\n(\nΘ\n)\n⊗\n∇\nΘ\nJ\n(\nΘ\n)\nΘ\n←\nΘ\n−\nη\n∇\nΘ\nJ\n(\nΘ\n)\n⊘\n s\n+\nϵ\nβ\n26",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#26": "Adam Optimization\nAdam (Adaptive Moment Estimation)  \nè una combinazione di Momentum \noptimization e RMSProp\n . \n \n \n \n \n \ndove \n T\n indica l'iterazione corrente  \nRispetto al Momentum, nella prima espressione si introduce il decay dei gradienti \ncon  \nLa 3\na\n e 4\na\n espressione sono utili per incrementare il valore di \n m\n ed \ns\n all'inizio del \ntraining, essendo i valori iniziali pari a 0.\nm\n←\nβ\n1\nm\n+\n(\n1\n−\nβ\n1\n)\n∇\nΘ\nJ\n(\nΘ\n)\ns\n←\nβ\n2\ns\n+\n(\n1\n−\nβ\n2\n)\n∇\nΘ\nJ\n(\nΘ\n)\n⊗\n∇\nΘ\nJ\n(\nΘ\n)\nm\n←\nm\n1\n−\nβ\nT\n1\ns\n←\ns\n1\n−\nβ\nT\n2\nΘ\n←\nΘ\n−\nη\n⋅\nm\n⊘\n s\n+\nϵ\nβ\n1\n27Momentum :  \n \nRMSProp :  \n \n \n \n \n \nRMSProp : m←β⋅m+η∇ΘJ(Θ)\ns←βs+(1−β)∇ΘJ(Θ)⊗∇ΘJ(Θ)\nΘ←Θ−η∇ΘJ(Θ)⊘ s+ϵ",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#27": "Adam Optimization\nRiguardo il \n tuning\n  dell'Adam Optimization:  \n•\nValori tipici per \n  e \n sono 0.9 e 0.999, rispettivamente.  \n•\nCome per gli altri \n algoritmi di adaptive learning rate\n , il valore iniziale di  \npuò essere impostato ad un valore tipico di 0.001 senza ulteriore tuning.\nβ\n1\nβ\n2\nη\n28",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#28": "Tecniche di ottimizzazione e complessità\nGli approcci \n ﬁ\nnora trattati si basano su \n derivate parziali del primo ordine  \n(Jacobians). Il numero di output per ogni dimensione in input è \n n\n,\n con \n n \nnumero di parametri.  \nEsistono molti approcci del \n secondo ordine (\n Hessians), ma richiedono \n n\n2 \nHessians per output.  \n•\nRecenti architetture Deep contengono oltre 10\n8\n parametri.  \n•\nLimiti sulla memoria di calcolo non permettono di usare tali approcci.\n29",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#29": "Momentum\nCosa succede se usiamo il momentum optimizer con iperparametro \nquasi 1 (es. 0.99999)?\n30",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#3": "Gli optimizer (2)\nNel DL le funzioni di \n loss\n sono complesse e non hanno una \n soluzione \nanalitica\n , cioè non possono essere formalizzati in modo da poter ricavare la \nsoluzione ottima con le risorse (tempo e hardware) disponibili in una serie di \nstep. Per questo si impiegano \n soluzioni numeriche\n  che seguono un \nprocedimento di trail & error su un insieme di soluzioni candidate.  \nEs.\n i coef\n ﬁ\ncienti di una regressione lineare possono essere ricavati \nanaliticamente via algebra lineare (es. \n matrix factorization\n ), oppure \nnumericamente (es. \n gradient descent\n ) quando i dati non sono \ninteramente memorizzabili.\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#30": "Momentum\nCosa succede se usiamo il momentum optimizer con iperparametro \nquasi 1 (es. 0.99999)?  \n1.\nLa ricerca sarà molto veloce verso l'ottimo, ma a causa del \nmomentum, la ricerca oltrepasserà il momentum. \n31",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#31": "Momentum\nCosa succede se usiamo il momentum optimizer con iperparametro \nquasi 1 (es. 0.99999)?  \n1.\nLa ricerca sarà molto veloce verso l'ottimo, ma a causa del \nmomentum, la ricerca oltrepassera il momentum.  \n2.\nRallenterà, tornerà indietro, accelererà e lo oltrepasserà di nuovo.\n32",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#32": "Momentum\nCosa succede se usiamo il momentum optimizer con iperparametro \nquasi 1 (es. 0.99999)?  \n1.\nLa ricerca sarà molto veloce verso l'ottimo, ma a causa del \nmomentum, la ricerca oltrepassera il momentum.  \n2.\nRallenterà, tornerà indietro, accelererà e lo oltrepasserà di nuovo.  \n3.\nPotrà oscillare molte volte prima di arrivare al minimo.\n33",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#33": "Keras e optimizer\nSi può de\n ﬁ\nnire facilmente via compile()  \nfrom\n tensorflow \n import\n keras\nfrom\n tensorflow.keras \n import\n layers\nmodel = keras.Sequential()\nmodel.add(layers.Dense(\n 64\n, kernel_initializer=\n 'uniform'\n , input_shape=(\n 10\n,)))\nmodel.add(layers.Activation(\n 'softmax'\n ))\nopt = keras.optimizers.Adam(learning_rate=\n 0.01\n)\nmodel.\ncompile\n(loss=\n'categorical_crossentropy'\n , optimizer=opt\n )\n# oppur\ne\nmodel.compile(loss='categorical_crossentropy', \n optimizer='adam'\n )\nOppure all'interno del training loop:  \noptimizer = tf.keras.optimizers.Adam()\n# Iterate over the batches of a dataset.\nfor\n x, y \nin\n dataset:\n   \nwith\n tf.GradientTape() \n as\n tape:\n        \n # Forward pass.\n        logits = model(x)\n        \n # Loss value for this batch.\n        loss_value = loss_fn(y, logits)\n    \n# Get gradients of loss wrt the weights.\n    gradients = tape.gradient(loss_value, model.trainable_weights)\n    \n# Update the weights of the model.\n    \noptimizer.apply_gradients\n (\nzip\n(gradients, model.trainable_weights))\n34\nOptimizer disponibili:  \n•\nSGD,  \n•\nRMSprop,  \n•\nAdam,  \n•\nAdadelta,  \n•\nAdagrad,  \n•\nAdamax,  \n•\nNadam,  \n•\nFtrl",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#34": "Richiami: Learning rate\nImpostare un \n learning rate troppo alto\n  può far \n divergere\n  l'apprendimento \ndall'ottimo.  \nValori \n troppo bassi \n provocano \n tempi di addestramento lunghi\n .  \nValori \n elevati\n  rendono il processo più rapido avvicinandosi all'ottimo ma \nsenza convergere realmente\n .  \n•\nTecniche quali\n  AdaGrad, RMSProp\n  e \nAdam\n  affrontano questo problema ma \nrichiedono comunque tempo, perciò \n risorse di calcolo\n . \nAvviando il training più volte \n su un training set ridotto e con diversi learning \nrate ci permette di \n stimare\n  quello più adatto.\n35\nEpocheLoss",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#35": "Richiami: Learning rate\n36\nMinimo cost function\nlearning rate elevato\nlearning rate basso\nlearning rate ideale",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#36": "Learning rate schedule\nLe strategie di \n learning schedules\n  mirano ad \n adattare il valore del learning rate  \ndurane il training.  \nI più popolari algoritmi di \n adaptive learning rate\n  sono:  \n•\nPredetermined piecewise constant learning rate\n : \n•\nOgni \n n\n epoche decrementa il rate di un valore predeterminato.  \n•\nPerformance scheduling\n : \n•\nMisura le perfomance (es. validation error) ogni \n n\n steps e riduce il rate di un \nfattore prede\n ﬁ\nnito quando le performance non migliorano.  \n•\nExponential scheduling\n : \n•\nIl rate si riduce di \n  dopo \n r\n steps:  \n•\nPower scheduling\n : \n•\nSimile al precedente ma il rate decresce più lentamente: \n1\n10\nη\n(\nt\n)\n=\nη\n0\n⋅\n10\n−\nt\nr\nη\n(\nt\n)\n=\nη\n0\n(\n1\n+\nt\nr\n)\n−\nc\n37",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#37": "Learning rate schedule: considerazioni\nNel dominio dello \n speech recognition\n  e impiegando il Momentum \noptimization, il \n performance scheduling\n  e \nexponential scheduling  \ndimostrano \n migliori performance\n . \n•\nL'\nexponential scheduling \n è da preferire perché \n più facile nel tuning\n . \nSe si impiegano i seguenti optimizer \n AdaGrad, RMSProp\n  e \nAdam \n non è \nnecessario implementare il learning rate schedule.\n38",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#38": "Keras e learning rate\nNell'esempio si de\n ﬁ\nnisce un learning rate adattivo basato su exponential \ndecay:  \nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=\n 1e-2\n,\n    decay_steps=\n 10000\n,\n    decay_rate=\n 0.9\n)\noptimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n39",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#4": "Local vs Global minimum\nUn approccio numerico tradizionale che porta lo stato vicino ad un minimo \nlocale restituirà una soluzione sub-ottima. Introducendo un certo grado di \nrumore abbiamo possibilità di continuare la ricerca altrove  \nNel \nminibatch stoachastic gradient descent\n , si introducono variazioni \ndovute ai gradienti calcolati sui minibatch e non sull'intero batch. \n5\n",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#5": "Saddle points\nI punti di sella generano vanishing gradients, e creano problemi se non \nsiamo in minimi locali o globali.  \nNell'esempio \n f(x)=x\n3\n6\n",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#6": "Saddle points (2)\nNell'esempio sotto abbiamo \n f(x,y)=x\n2\n-y\n2 \ncon punto di sella in (0,0), massimo \nper y e minimo per x. Se assumiamo l'input di f un vettore k-dimensionale e \nl'output scalare, abbiamo  \nDerivate parziali  \nLa \nmatrice di Hessian\n  (H) consiste nelle derivate parziali del secondo \nordine. Essa rappresenta proprietà geometriche della super\n ﬁ\ncie, \nsoprattutto quando i gradienti sono pari a 0.\n7\n∇f(x)=[∂f(x)\n∂x1,∂f(x)\n∂x2,⋯,∂f(x)\n∂xk]\nHf=∂2f\n∂x1∂x1∂2f\n∂x1∂x2⋯∂2f\n∂x1∂xk\n∂2f\n∂x2∂x1∂2f\n∂2x2⋯∂2f\n∂x2∂xk\n⋯\n∂2f\n∂x1∂xk∂2f\n∂x2∂xk⋯∂2f\n∂2xk",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#7": "Punti critici\nIn particolare gli autovalori e il determinante di H ci danno indicazioni sui punti \ncritici e sulla funzione di costo.  \nUna \n convex function\n  (cioè con un unico minimo) ha sempre autovalori non negativi.  \nMa il calcolo di H è oneroso di risorse (spazio e calcolo). Inoltre i task su cui si \napplicano architetture di DL non sono tipicamente associati a convex functions.\n8\n",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#8": "Formalismo\nNei lucidi seguenti useremo il formalismo:  \n•\n:   i pesi attuali \n w\n e \nb\n della RN (in passato \n W\n) \n•\n :  \nfunzione di costo\n  (in passato \n E \no \nf\n) \n•\n : \ngradiente\n  della funzione di costo  \n•\n:   \n learning rate \n o step size (in passato \n ) \n•\n, \n:  \nmoltiplicazione\n  e \ndivisione element-wise\n , \n   cioè posizione \n  posizione  \nΘ\nJ\n(\nΘ\n)\n∇\nΘ\nJ\n(\nΘ\n)\nη\n α\n⊗\n⊘\n×\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\10-Training Deep 2-sbloccato.pdf#9": "Richiami: Gradient descent\nIl processo di ottimizzazione \n Gradient descent\n  opera una \n sequenza di step \nregolari\n  per ogni punto verso la direzione di massima discesa che \ncorrisponde a quella determinata dall'opposto del suo gradiente in quel \npunto.  \n \nIn questo modo si ha che:  \n•\nL'\naggiornamento dipende solo dal gradiente calcolato localmente\n , e non \nda quelli precedenti.  \n•\nSe il \n gradiente locale è piccolo\n , l'aggiornamento sarà \n poco signi\n ﬁ\ncativo\n .\nΘ\n←\nΘ\n−\nη\n∇\nΘ\nJ\n(\nΘ\n)\n10",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nL'addestramento delle reti neurali Deep  \nParte 3: Over\n ﬁ\ntting \n1",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#1": "Sommario\nAffrontare l'Over\n ﬁ\ntting \nRichiami  \nEarly stopping  \n1 e \n 2 regularization  \nDropout  \nMax-Norm regularization  \nData Augumentation  \nEsercizi\nℓ\n ℓ",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#10": "Early stopping - ipotesi\nAlcuni studi mostrano che le reti DL hanno la capacità di fare \n ﬁ\ntting di label \narbitrarie, per\n ﬁ\nno generate casualmente, ma solo dopo un numero elevato \ndi iterazioni. In presenza di label ben de\n ﬁ\nnite nel training set, la rete tende a \nrappresentarle per prime, e poi interpolare i dati associati a label \"rumore\".  \nCi garantisce la capacità di generalizzazione: è suf\n ﬁ\nciente riuscire ad \naddestrare il modello sui dati con label ben de\n ﬁ\nnite, ed evitare di \ncontinuare su dati mal addestrati.\n11",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#11": "Early stopping\nInvece di introdurre vincoli sui parametri (l1 e l2 regularization), il vincolo \npuò essere sul numero di epoche del training.   \nNella tecnica di regolarizzazione \n early stopping\n  interrompiamo il training \nquando le prestazioni della rete non migliorano, ad esempio:  \n•\nAlla \nﬁ\nne di ogni epoca possiamo valutare le prestazioni sul \n validation set\n .  \n•\nTeniamo traccia dell'ultima volta in cui il modello ha migliorato le \nprestazioni.  \n•\nSe dopo un certo numero di epoche non ci sono stati miglioramenti \nsigni\nﬁ\ncativi (> \n ε\n), spesso chiamata \n patience criteria\n , interrompiamo e \nscegliamo lo snapshot del modello che in passato si è dimostrato migliore.  \nIl vantaggio è aumentare il potere di generalizzazione evitando di \nconsiderare label noisy. Inoltre riduce il tempo di training.  \nÈ spesso utile combinarlo ad altre tecniche di regolarizzazione.\n12",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#12": "1\n e \n 2\n regularization\n ℓ\nℓ\n1\n e \n 2 \nregularization\n  introducono limiti sul valore dei parametri e possono \nprevenire l'over\n ﬁ\ntting.  \n•\nCorrispondono rispettivamente alle tecniche di \n Lasso\n  e \nRidge \n nella \nregressione e \n 1\n e \n 2\n-penalty nella classi\n ﬁ\ncazione.  \n•\nIn pratica, oltre alla corrispondenza tra output attesi e output prodotti dalla \nrete, aggiungiamo un ulteriore vincolo da soddisfare durante il training.  \nModelli complessi tendono a rappresentare anche \n ﬂ\nuttuazioni causate dal \nrumore.  \nLe due regolarizzazioni spingono i parametri del modello ad assumere valori \nvicini allo 0 e, come effetto collaterale, a ridurre gli effetti dei layer nascosti \ndella rete, perciò rendendo il modello meno complesso.\nℓ\nℓ\nℓ\nℓ\n13",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#13": "1\n regularization\nℓ\nNella \n 1\n  si \naggiunge la magnitudo sui pesi \n (valore assoluto) come \ncoef\nﬁ\nciente di penalità\n  (o \ntermine di regolarizzazione\n ) alla funzione di loss.  \n1\n riduce signi\n ﬁ\ncativamente il valore dei pesi associati alle feature meno \nimportanti, operando una sorta di \n feature selection\n , che\n  riduce complessità \ne signi\n ﬁ\ncatività di alcune feature\n .  \nAlfa è l'iperparametro \n regularization rate\n . Valori \n troppo elevati \n comportano \nmodelli semplici e potenziali \n under\n ﬁ\ntting, valori molto bassi \n annullano la \nregolarizzazione.\nℓ\nℓ\n14\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#14": "2\n regularization\nℓ\nNella \n 2\n  si \naggiunge la magnitudo al quadrato sui pesi \n (o norma Ecluidea)  \ncome coef\n ﬁ\nciente di penalità.\nℓ\n15\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#15": "Dropout - motivazioni\nAbbiamo visto come modelli semplici possono garantire la generalizzazione. \nPossiamo intervenire (1) riducendo il numero delle dimensioni, o (2) \nriducendone l'importanza \n (\n2\n regularization), oppure (3) imponendo che la \nfunzione stimata sia \n smooth\n  cioè poco sensibile a piccoli cambiamenti \ndell'input.  \nAlcune teorie mettono in correlazione la smoothness con la capacità di \nessere resilienti alle perturbazioni nell'input. In base ad esse è stata proposta \nla tecnica di \n iniettare\n  rumore durante la forward propagation negli strati \nintermedi. L'obiettivo è minimizzare la situazione in cui un layer si \nspecializza solo su un sottoinsieme di pattern di attivazione del layer \nprecedente (\n co-adaptation\n ). \nNella pratica, si disabilitano una frazione di nodi del layer precedente così \nda contribuire con un valore pari a 0 nell'input del layer attuale\nℓ\n16",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#16": "Dropout (1)\nNel \ndropout\n  si assegna ad ogni nodo una probabilità \n p\n di essere disattivato \n(ignorato) in un certo step durante la fase di forward e backward propagation, \nad eccezione dell'output layer.  \n•\np\n è un iperparametro chiamato \n dropout rate\n  (es. p=0.5).  \n•\nOgni attivazione di un layer intermedia è sostituita con:  \n•\nNel caso fosse 0 i gradienti svaniscono durante il backpropagation.  \nDopo la fase di training (es. in produzione) tutti i nodi saranno attivati.  \nAd ogni step abbiamo una diversa con\n ﬁ\ngurazione di rete. \n Con N nodi \npossiamo avere 2\nN\n con\nﬁ\ngurazioni diverse, tutte addestrate per lo stesso scopo.\n17\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#17": "Dropout (2)\nUna impresa funziona meglio senza un dipendente?  \n•\nSì, se i lavoratori sanno adattarsi, cioè: ognuno si occupa di più cose, \nmaggiore cooperazione, e non contare solo sui vicini.  \nGarantisce reti più \n robuste \n e con capacità di \n generalizzazione\n . \nSi ottiene un incremento delle prestazioni del \n 1-2% \n per\nﬁ\nno nelle architetture \npiù ottimizzate.\n18",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#18": "Dropout - esempio\n19\nsenza Dropout con Dropout",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#19": "Dropout: altre considerazioni\nUna \n rete complessa\n  con molti parametri facilmente incorpora \ndipendenze che rappresentano feature dei dati di ingresso di scarso \ninteresse (\n over\nﬁ\ntting\n).  \n•\nSe ad ogni step proponiamo dati a diverse con\n ﬁ\ngurazioni di layer è \nmeno probabile che un certo peso si focalizzi su una feature poco \nsigni\nﬁ\ncativa.  \nLa tecnica dropout \n raddoppia circa il numero di iterazioni per \nraggiungere la convergenza\n , ma il \n tempo di addestramento per una \nepoca è più breve \n dato che ho meno nodi funzionanti.  \nPer avere aggiornamenti più lenti si può considerare un singolo mini-\nbatch per ogni con\n ﬁ\ngurazione considerata.\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#2": "Richiami: Over\n ﬁ\ntting\nLe reti deep contengono molti parametri che mirano a modellare un insieme \nvasto di funzioni, anche complesse.  \nL'obiettivo dell'\n addestramento\n  è ottenere una rete che mostra \n buone \nprestazioni sia sul training set, sia in produzione \n (cioè su dati mai visti).  \n•\nIn queste condizioni si ha \n generalizzazione\n . \nIl \ntest set  \npermette di \n valutare l'over\n ﬁ\ntting\n del modello \n ﬁ\nnale testandolo su \ndati mai visti in precedenza durante il training, ma con distribuzione di \nprobabilità simile.  \n•\nSe un modello \n ﬁ\ntta\n i dati di training e di test contemporaneamente, si ha \nminimo over\n ﬁ\ntting.  \nIl \nvalidation set\n  è usato più raramente per \n valutare la combinazione migliore \ndegli iperparametri\n  durante lo sviluppo della rete. Non è impiegato durante il \ntraining né Nonostante ciò, le sue caratteristiche possono essere parzialmente \nrappresentate all'interno della rete, \n rendendo la valutazione meno oggettiva\n .\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#20": "Dropout e analogia col boosting\nSupponiamo di avere un problema di \n classi\n ﬁ\ncazione\n . Il \ndropout  \ninterpreta la rete come un insieme (molto grande) di classi\n ﬁ\ncatori \n“\nweak\n ”.  \n•\nDurante l’addestramento \n disattivo una parte della rete per \nsfruttare solo un sotto-modello alla volta\n . \n•\nL'\naccuratezza dei singoli sotto-modelli è minore di quella che \npotrei ottenere addestrando l'intera rete \n su tutto il training set.  \n•\nMa alla \n ﬁ\nne considero la \n rete nella sua interezza\n , cioè con tutti i \nsotto-modelli attivati, \n ottenendo un aumento delle prestazioni\n . \n•\nNel \nboosting si suppongono modelli indipendenti\n  mentre nel \ndropout c’è inevitabilmente dipendenza\n  dovuta alla condivisione \ndei parametri tra sotto-modelli.\n21",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#21": "Dropout nella pratica\nDal punto di vista operativo, con \n p=0.5,\n  durante il test ogni nodo di \nun qualsiasi hidden layer riceve il doppio degli input rispetto alla \nfase di training.  \n•\nDopo il training è importante moltiplicare il valore degli input per  \n1-p\n o avremmo dei segnali di ingresso con magnitudine troppo \nelevata. In alternativa, si può scalare l'output di ogni neurone.  \nDurante lo sviluppo della rete, se notiamo che il modello mostra \nover\nﬁ\ntting possiamo introdurre il dropout, ovvero incrementare \n p\n. \nSe mostra unde\n ﬁ\ntting lo decrementiamo.  \nDropconnect\n  è una variazione del dropout, dove sono gli archi ad \nessere disattivati.\n22",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#22": "Dropout\nIn una rete multi-layer, consideriamo il dropout su ogni layer?",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#23": "Dropout\nIn una rete multi-layer, consideriamo il dropout su ogni layer?  \nNegli \n hidden layers\n , per creare diverse con\n ﬁ\ngurazioni di rete da \naddestrare singolarmente.",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#24": "Dropout\nIn una rete multi-layer, consideriamo il dropout su ogni layer?  \nNegli \n hidden layers\n , per creare diverse con\n ﬁ\ngurazioni di rete da \naddestrare singolarmente.  \nNon lo consideriamo nel \n output layer \n essendo quello che genera \nil feedback necessario per addestrare la con\n ﬁ\ngurazione corrente.  \nEs. nel caso della classi\n ﬁ\ncazione, se omettiamo un nodo nel \nlayer di output, non otteniamo il comportamento della rete \nper quella classe. ",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#25": "Dropout\nIn una rete multi-layer, consideriamo il dropout su ogni layer?  \nNegli \n hidden layers\n , per creare diverse con\n ﬁ\ngurazioni di rete da \naddestrare singolarmente.  \nNon lo consideriamo nel \n output layer \n essendo quello che genera \nil feedback necessario per addestrare la con\n ﬁ\ngurazione corrente.  \nEs. nel caso della classi\n ﬁ\ncazione, se omettiamo un nodo nel \nlayer di output, non otteniamo il comportamento della rete \nper quella classe.  \nLo possiamo usare nel \n input layer \n perché permette di addestrare \nil modello ignorando alcune feature in ingresso che possono \nin\nﬂ\nuenzare negativamente l'addestramento (es. p=0.8)  \nE simile ad una feature selection.",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#26": "Dropout\n10-dropout\n  (python)\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#27": "Max-Norm regularization\nLa \nMax-norm regularization\n  introduce il \n vincolo sul modulo dei \npesi\n con un iperparametro \n r\n: \n ,            dove\n  indica la \n 2\n-norm  \nAd ogni training step normalizziamo i pesi:  \n \nRiducendo \n r\n, oltre a regolarizzare i pesi, si affronta anche il \nvanishing/exploding problem.  \nw\n2\n≤\nr\n ⋅\n2\nℓ\nw\n←\nw\nr\nw\n2",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#28": "Data Augumentation\nLa \ndata augumentation\n  genera nuove istanze da dare in input\n , \naumentando la dimensione del training set.  \nNel caso delle immagini (\n image augumentation\n ), si automatizza \nil processo con tecniche tradizionali quali:  \n•\nRuotare, spostare, ridimensionare, aggiungere un rumore, copie \nspeculari, variazioni di luce e contrasto, etc.  \n•\nEs. fare crop dell'immagine in modo che il soggetto compaia in \ndiverse posizioni, modi\n ﬁ\ncare l'intensità dei colori per ridurre la \nrelativa sensitività del modello.  \nLo scopo e (1) rendere la rete meno dipendente da queste \nvariazioni e (2) incrementare il set di training nel caso ci fossero \nun numero insuf\n ﬁ\nciente di istanze.",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#29": "Data Augumentation in Keras\n11-data_augmentation.ipynb\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#3": "Richiami: Over\n ﬁ\ntting\nPer ridurre l'over\n ﬁ\ntting si può intervenire:  \n•\nCambiando la struttura della rete\n  (es. riducendo il numero di nodi/pesi/\nlayer).  \n•\nAlterando i valori del parametri\n  durante l'addestramento.\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#30": "Sparse Model\nElenca 2 modi per creare modelli sparsi  \nCon \nmodello sparso\n  indichiamo una versione \"sempli\n ﬁ\ncata\" di \nmodello tipicamente più complesso, utile per elaboratori con \nmeno risorse computazionali.  \nAd esempio: mini computers e smartphones.",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#31": "Sparse Model\nElenca 2 modi per creare modelli sparsi  \nUna volta addestrato il modello si possono azzerare i parametri \nvicini allo 0",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#32": "Sparse Model\nElenca 2 modi per creare modelli sparsi  \nUna volta addestrato il modello si possono azzerare i parametri \nvicini allo 0  \nLa \n 1\n regularization incrementa il numero di parametri vicino \nallo 0 che possono essere azzerati  \nIl \nFTRLOptimizer\n  è una altro algoritmo adatto per questo scopo.\nℓ",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#33": "Tool: Gradient Descent Visualization\nθ1\nθ2loss",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#34": "Richiami \n \n \n \n \nm\n←\nβ\n1\nm\n+\n(\n1\n−\nβ\n1\n)\n∇\nΘ\nJ\n(\nΘ\n)\ns\n←\nβ\n2\ns\n+\n(\n1\n−\nβ\n2\n)\n∇\nΘ\nJ\n(\nΘ\n)\n⊗\n∇\nΘ\nJ\n(\nΘ\n)\nm\n←\nm\n1\n−\nβ\nT\n1\ns\n←\ns\n1\n−\nβ\nT\n2\nΘ\n←\nΘ\n−\nη\n⋅\nm\n⊘\n s\n+\nϵ\n35Momentum :  \n \nRMSProp :  \n \n \n \n \n \nRMSProp : m←β⋅m+η∇ΘJ(Θ)\ns←βs+(1−β)∇ΘJ(Θ)⊗∇ΘJ(Θ)\nΘ←Θ−η∇ΘJ(Θ)⊘ s+ϵ\n s←s+∇ΘJ(Θ)⊗∇ΘJ(Θ)\nΘ←Θ−η∇ΘJ(Θ)⊘ s+ϵAdaGradAdam",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#35": "Qualche indicazione pratica\nPer problemi di classi\n ﬁ\ncazione, inizia da una con\n ﬁ\ngurazione di default,  \ncome la seguente:  \nOppure cerca modelli pre-addestrati per compiti uguali o simili.  \nModi\n ﬁ\nca la con\n ﬁ\ngurazione intervenendo:  \n•\nSe \nconverge troppo lentamente incrementa il learning rate  \n•\nSe converge ma \n con performance non adeguate\n , prova un \n learning schedule\n , es. \nexponential decay.  \n•\nSe \nil training set è troppo piccolo, fai data augumentation\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#36": "Kaggle: piattaforma per competizioni ML-based\nOgni competizione consiste in un dataset di training e uno di test.  \nIl partecipante suddivide il training set in due, una parte per la \nvalidazione, oppure opera una cross-fold validation.  \nIl test set completo rimane privato \n ﬁ\nno alla \n ﬁ\nne della competizione.\n37\nhttps://www.kaggle.com/c/digit-recognizer/data",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#37": "Over\n ﬁ\ntting: un caso reale\nIl ranking \n ﬁ\nnale viene calcolato sul \n test set\n .\n38\nDifferenza rispetto al training (e validation) set pubblico\nScendendo si hanno di solito valori negativi: approcci che \nsi comportano molto bene nel training ma non nel test set.",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#38": "La regola del 30\nLa \nregola del 30  \nè un semplice procedimento empirico adatto per \nclassi bilanciate\n  (cioè con lo stesso numero di istanze per ogni label).  \nFornisce una idea \n se un incremento di prestazioni è signi\n ﬁ\ncativo o \nmeno\n  (es. dovuto solo al caso).  \nSe dopo aver aggiornato il classi\n ﬁ\ncatore ottengo un incremento \n(o decremento) di accuratezza che riguarda almeno 30 istanze, \nallora il miglioramento (peggioramento) è signi\n ﬁ\ncativo.  \n39",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#39": "La regola del 30\nSe il set di validazione ha \n N\n istanze, qual è la differenza minima \npercentuale per dire che l’incremento di accuratezza è signi\n ﬁ\ncativo? \n40",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#4": "Richiami: Over\n ﬁ\ntting\n5Training/Validation\n Produzione/\nTest\nlabel:A\nlabel:J\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#40": "La regola del 30: soluzione\nSe il set di validazione ha \n N\n istanze, qual è la differenza minima percentuale per \ndire che l’incremento di accuratezza è signi\n ﬁ\ncativo?  \nL’incremento percentuale si calcola:  \nEsempio:  \nSe N = 1000 -> 3%  \nSe N = 3000 -> 1%  \nSe N = 30.000 -> 0.1%  \nSeguendo la regola, è meglio usare un validation set ampio, così anche \nincrementi (es. 0.1%) possono essere tenuti in considerazione.  \nIndicazioni più accurate sono ottenute con \n test di signi\n ﬁ\ncatività\n .\n41(N+30)−N\nN⋅100",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#41": "Esercizio su Deep NN\nCostruisci una rete Deep, con 5 layer hidden da 100 nodi l'uno, \ninizializzazione Xavier e He, e funzione di attivazione ELU  \nUsa la Adam optimization con early stopping.  \nImpiega il dataset di cifre MNIST, ma solo da 0 a 4. Usa come output \nun layer softmax da 5 neuroni.  \nRicordati di salvare periodicamente i checkpoints, e il modello \n ﬁ\nnale.  \nFai tuning sugli iperparametri usando la cross-validation, e vedi se \npuoi incrementare la precisione.  \nProva ad aggiungere la Batch normalization e confronta le curve di \nlearning. Converge prima? Produce un modello migliore?  \nSecondo te c'è over\n ﬁ\ntting sul training set? Prova ad aggiungere il \ndropout ad ogni layer e valuta miglioramenti.\n42",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#42": "Esercizio sul Transfer Learning\nCrea una nuova Deep NN e riusa i parametri degli hidden layer della \nrete precedente. Congelali e rimpiazza il layer softmax con uno \nnuovo.  \nAddestra la rete sulle cifre 5-9, usando solo 100 immagini per cifra, e \nvedi quanto impiega. Nonostante le poche immagini riesci ad avere \nuna buona precisione?  \nProva a fare caching dei layer congelati, e addestra di nuovo il \nmodello. Quanto è veloce ora?  \nProva di nuovo usando solo 4 hidden layer. La precisione aumenta?  \nOra scongela i primi 2 layer e continua il training. Ottieni maggiori \nperformance?\n43",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#43": "Esercizio sul Pretraining su auxiliary task\nCostruiamo una Deep NN che confronta due cifre MNIST per veri\n ﬁ\ncare se sono le \nstesse. Dopodiché impieghiamo gli stessi layer \n ﬁ\nnali della rete per addestrare un \nclassi\n ﬁ\ncatore MNIST su pochissimi dati di training.  \nInizia da 2 Deep NN (A e B), simili a quanto costruito nel esercizio su Deep NN. \nAggiungi un singolo layer di output che connette l'output di ambedue le reti. Usa la \nfunzione concat() di Tensor\n ﬂ\now con axis=1. Usa quanto ottieni come input al nuovo \noutput layer. L'output layer contiene un singolo nodo e usa la logistic come funzione \ndi attivazione.  \nSuddividi MNIST in 2 sets: primo split da 55,000 immagini, secondo split da 5,000. \nCrea una funzione che genera un batch dove ogni istanza è una coppia di immagini \nprese dallo split #1. Metà devono appartenere alla classe \"stessa classe\" (label 0), \nl'altra metà \"classi diverse\" (label 1).  \nAddestra la rete sul training set. Per ogni coppia manda in input in simultanea \nl'immagina da A e l'immagine da B.  \nOra crea una nuova rete riutilizzando e congelando i pesi degli hidden layers di A e \naggiungendo una softmax layer di 10 nodi. Addestra la rete sullo split #2 e vedi se \nottieni prestazioni elevate avendo solo 500 immagini per classe.\n44",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#5": "Affrontare l'Over\n ﬁ\ntting\nValori dei pesi limitati in modulo signi\n ﬁ\ncano spesso modelli meno complessi, e meno \nin\nﬂ\nuenzati da \n ﬂ\nuttuazioni statistiche dei dati in input.  \n•\nValori elevati nei pesi implicano attivazioni molto diverse per leggere variazioni in input.  \nTranne nei casi di training set molto grandi, si impiegano sempre \n tecniche di \nregolarizzazione\n , tra le quali:  \n•\nEarly stopping\n : \n•\nterminare l'addestramento quando le performance degradano  \n•\n 1\n e \n 2\n regularization (o weight regularization)\n :  \n•\npenalizzare il modello in base alla magnitudo dei pesi  \n•\nDropout\n : \n•\nper ogni layer ignorare alcuni input durante l'addestramento  \n•\nMax-Norm regularization (o weight constraint)\n :  \n•\nintrodurre un range di ammissibilità per il valore dei pesi  \n•\nData Augumentation (non regolarizza i pesi, ma aumenta la dimensione del training set)\n : \n•\nmodi\n ﬁ\ncare il training set, es. aggiungendo del rumore\nℓ\nℓ\n6",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#6": "Richiami: Over\n ﬁ\ntting\n7\nComplessità del modello",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#7": "Richiami: bias e varianza \nHigh Validation error\nHigh Test errorSi\nSiNo\nNo\nDone! •Bigger mode l\n•Train longer  \n•New model architecture \n•More data  \n•Regularization  \n•New model architecture Bias \n(unde ﬁt)\nVarianc e\n(over ﬁt)\n",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#8": "Over\n ﬁ\ntting e regolarizzazione\nIl procedimento di training \n ﬁ\nnora seguito è:  \n•\naddestrare\n  il modello su un training set, e valutare l'\n errore di \ngeneralizzazione\n  su holdout data (test set).  \nLa differenza di performance tra i due set si chiama \n generalization gap\n . Se \nla differenza è elevata si ha \n over\nﬁ\ntting\n sul training data.  \nNello scenario del DL, prendendo l'esempio del task della classi\n ﬁ\ncazione, \nsi hanno tipicamente modelli complessi a suf\n ﬁ\ncienza per \n ﬁ\nttare\n ogni istanza \ndi training, anche per training set molto grandi. Farebbe pensare che per \nridurre il generalization error siamo costretti a introdurre regolarizzazioni \n(es. riducendo la complessità, vincoli sul valore dei parametri).  \nIn realtà si nota come nel DL si raggiungano spesso 0 training error, perciò \nl'unico aspetto da ottimizzare è il generalization error. Inoltre, contrario alla \nlogica, l'errore si può ridurre anche rendendo l'architettura più complessa \n(es. più layer e nodi). I progettisti hanno più possibilità di affrontare \nl'over\n ﬁ\ntting rispetto alle architetture NN tradizionali.\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\11-Training Deep 3-sbloccato.pdf#9": "Ispirazione ai modelli non parametrici\nGli approcci parametrici possono essere de\n ﬁ\nniti in vari modi, es. sono \nbasati su modelli statistici che rappresentano i parametri con distribuzioni \nstandard. Nei modelli nonparametrici la variabilità dei parametri è più \nampia e ci sono meno vincoli da rispettare (es. su media, varianza).  \nSpesso i modelli nonparametrici confrontano le istanze e si basano \nsull'ipotesi che istanze simili in input producono output simili (es. k-NN).  \nUn altro modo per caratterizzare i modelli nonparametrici è sulla \ncomplessità che tende a crescere al crescere del numero di dati disponibili \ndi training.  \nLe reti NN sono spesso viste come modelli non parametrici\n . Si hanno un \nnumero molto abbondate di parametri che tendono a interpolare i training \ndata.\n10",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nRecurrent Neural Networks (RNN) - parte #1\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#1": "Sommario\nNodi e layers ricorrenti  \n•\nCalcolo degli output  \nPredizione  \nArchitetture RNN  \n•\nSequence-to-sequence  \n•\nSequence-to-vector  \n•\nVector-to-sequence  \n•\nEncoder-decoder  \nMemory cells  \n•\nLSTM  \n•\nGRU",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#10": "Architetture RNN (1) \nCi sono vari tipi di architetture RNN, che dipendono dal task che si vuole \naffrontare.  \nLa rete \n sequence-to-sequence\n  è utile per task di \n predizione\n .  \n•\nSupponiamo di avere la quotazione di chiusura di un titolo in borsa, misurata \nnegli ultimi N giorni. La rete deve produrre in output le stesse quotazioni \ntraslate di un giorno nel futuro.  \nLa rete \n sequence-to-vector\n  è simile alla precedente ma \n scarta tutti i valori in \noutput tranne l'ultimo\n . \n•\nSe in input abbiamo una sequenza di \n id\n di parole, l'ultimo output può \ncorrispondere al \n sentiment\n  (es. -1 [hate], +1 [love).\n11\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#11": "Architetture RNN (2) \nLa rete \n vector-to-sequence\n  prende in input ripetutamente lo stesso vettore per \nuna successione di steps e produce una sequenza in output.  \n•\nSe il vettore in input corrisponde ad una immagine, possiamo addestrare la \nrete per produrre una descrizione testuale associata (sequenza di parole).\n12\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#12": "Architetture RNN (3) \nIn\nﬁ\nne si può combinare una rete \n sequence-to-vector\n  (\nencoder\n ) con una \nvector-to-sequence\n  (\ndecoder\n ) ottenendo una rete \n encoder-decoder\n . \n•\nUn \nencoder\n  può rappresentare una frase in un linguaggio in un singolo \nvettore che viene impiegato poi dal \n decoder\n  per generare la frase in diverso \nlinguaggio.  \n•\nUna rete \n sequence-to-sequence\n  non è adatta poiché l'intera frase. Le ultime \nparole dell'input potrebbero in\n ﬂ\nuenza l'inizio dell'output, mentre la rete \ntraduce ogni parola via via che l'input è reso disponibile. Inoltre le lunghezze \ndell'input e output potrebbero non corrispondere.\n13\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#13": "RNN: Addestramento \nL'addestramento di una RNN richiede l'\n unrolling through time\n  e l'uso della \ntecnica \n backpropagation through time\n  (\nBPTT\n ). \n•\nLa prima passata corrisponde alla \n forward\n  pass tradizionale (frecce \ntratteggiate).  \n•\nL'output è valutato con una \n funzione di costo  \n . Per talune \narchitetture la funzione può ignorare alcuni output.  \n•\nIl gradiente della funzione di costo è propagato \n backward\n  (frecce continue) e \ni parametri aggiornati di conseguenza.\nC\n(\nY\n(\n0\n)\n,\nY\n(\n1\n)\n,\n⋯\n,\nY\n(\nT\n)\n)\n14\nIn questo esempio la funzione  \nè valutata con gli ultimi 3 \noutput, e perciò i gradienti non \ntransitano per Y (0) e Y (1).\nDa notare che i medesimi \nparametri W,b sono impiegati \nad ogni step. La \nbackpropagation considera \ntutti gli steps per fare \nl'aggiornamento.",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#14": "Time series: Predizione (forecasting)\nAnalizziamo le seguenti \n time series\n :  \n•\nnumero orario di utenti attivi su un sito web,  \n•\ntemperatura giornaliera in un certo luogo  \n•\nsituazione \n ﬁ\nnanziare di una società misurata trimestralmente con metriche \nmultiple (es. reddito, debito, etc).  \nLe prime due sono \n univariate  \ntime series\n  perché valutiamo temporalmente \nuna singola metrica, l'ultima è una \n multivariate  \ntime series\n . \nLa predizione di un valore in un tempo futuro è chiamato \n forecasting\n . \nCon \nimputation\n  si intende stimare un valore mancante all'interno della time \nseries.  \nLe RNN si usano spesso per fare \n forecasting\n  e \nimputation\n .\n15",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#15": "RNN e Keras (1)\nGeneriamo \n time series\n  in modo random:  \ndef \ngenerate_time_series\n (\nbatch_size\n , \nn_steps\n)\n:\n    \n# valori random in [0,1); il parametro di rand è lo shape\n    freq1, freq2, offsets1, offsets2 = np.random.rand(\n 4\n, batch_size, \n 1\n)\n    \n# n_steps valori nell'intervallo 0, 1 equamente spaziati\n    time = np.linspace(\n 0\n, \n1\n, n_steps)\n    series = \n 0.5\n * np.sin((time - offsets1) * (freq1 * \n 10\n + \n10\n))  \n#   wave 1\n    series += \n 0.2\n * np.sin((time - offsets2) * (freq2 * \n 20\n + \n20\n)) \n# + wave 2\n    series += \n 0.1\n * (np.random.rand(batch_size, n_steps) - \n 0.5\n)   \n# + noise\n    \nreturn\n series[..., np.newaxis].astype(np.float32)\nDove \nbatch_size\n  sono il numero di \n time series\n  da generare con lunghezza \nn_steps\n . Le serie sono \n univariate\n . La funzione restituisce un array NumPy di \ndimensioni [\n batch_size\n , \nn_steps\n , 1].  \nOgni serie è generata come somma di due funzioni \n seno\n di ampiezza \n ﬁ\nssa ma \nfrequenza e fase random, e con aggiunta di rumore.  \n16",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#16": "RNN e Keras (2)\nCreiamo training set, validation set e test set:  \nn_steps = \n 50\nseries = generate_time_series(\n 10000\n, n_steps + \n 1\n)\nX_train, y_train = series[:\n 7000\n, :n_steps], series[:\n 7000\n, \n-1\n]\nX_valid, y_valid = series[\n 7000\n:\n9000\n, :n_steps], series[\n 7000\n:\n9000\n, \n-1\n]\nX_test, y_test = series[\n 9000\n:, :n_steps], series[\n 9000\n:, \n-1\n]\nX_train\n  contiene 7000 time series di lunghezza 50 steps, e ha dimensioni \n[7000,50,1]  \nX_valid\n  contiene 2000 time series  \nX_test\n  contiene 1000 time series  \nPoiché vogliamo un \n forecast\n  di un singolo valore per time series, il vettore \ncolonna target ha dimensioni [7000,1]  \n17",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#17": "RNN e Keras (3)\nPer valutare il modello introduciamo degli approcci \n baseline\n , con cui \nconfrontarci.  \nUn semplice modello stima il valore futuro facendolo coincidere con l'ultimo \nvalore nella time series (\n naive forecoasting\n ). \n>>> y_pred = X_valid[:, \n -1\n]\n>>> np.mean(keras.losses.mean_squared_error(y_valid, y_pred))\n0.020211367\n•\nSebbene banale, ottiene buone prestazioni: \n Mean squared error (MSE) di 0.02  \nUn altro approccio è impiegare una rete \n fully connected\n . Ad esempio con un \nsingolo layer, perciò si riduce ad una combinazione lineare dei valori della \ntime series in ingresso.  \nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[\n 50\n, \n1\n]),\n    keras.layers.Dense(\n 1\n)\n]\n)\n•\nCon la con\n ﬁ\ngurazione: MSE loss, Adam optimizer, 20 epoche di training; si \nottiene un MSE di 0.004.\n18",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#18": "RNN e Keras (4)\nImplementiamo una semplice RNN, con un layer con un singolo nodo con la \nfunzione Keras \n SimpleRNN\n .  \nmodel = keras.models.Sequential(\n [\n  keras.layers.SimpleRNN(1, input_shape=[None, 1]\n )\n]\n)\nCome dimensione dell'input impostiamo \n None\n  poiché una RNN elabora \n time \nseries\n  di qualsiasi lunghezza e non occorre speci\n ﬁ\ncarla anticipatamente.  \nDi default la \n SimpleRNN\n  usa la attivazione \n tangente iperbolica\n .  \nIl primo output viene elaborato con \n h\n(init)\n=0\n e \nx\n(0)\n pari al valore in input al \nprimo step. Il nodo calcola la somma pesata dei 2 contributi e valuta la \ntangente iperbolica al risultato, ottenendo il primo valore in output \n y\n0\n. Nella \nSimpleRNN\n  tale valore corrisponde al valore per lo stato \n h\n(0)\n.  \nAl successivo step, lo stesso nodo prende in input il successivo input \n x\n(0) \ne lo \nstato \n h\n(0)\n e ripete l'elaborazione.  \nL'unico valore in output corrisponde \n y\n49\n. Se si vogliono ottenere tutti i valori in \noutput impostare \n return_sequences=True\n .\n19",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#19": "RNN e Keras (5)\nUna volta compilato e sottoposto a \n ﬁ\nt (con\n ﬁ\ngurazione: 20 epoche, Adam op.), \ntale modello raggiunge un MSE di 0.014, perciò al di sotto del modello \nlineare.  \n•\nIl modello lineare ha un totale di 51 parametri, cioè un parametro per ogni \ninput e il bias. Nella SimpleRNN abbiamo un singolo parametro per input, \nuno per l'hidden state e per il bias, cioè 3 parametri in totale.  \n•\nUn SimpleRNN è una rete troppo semplice per avere questo task.\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#2": "Motivazioni\nAbbiamo assunto input per i nostri modelli di tipo vettoriale, dove ogni \nelemento \n x\nj\n corrisponde ad un attributo (o feature). Perciò possiamo \nraggruppare facilmente i dati in formato tabellari \n istanze \n x\n attributi\n . \nSuccessivamente abbiamo considerato immagini, dove per ogni coordinata \nabbiamo il valore del pixel. In questo scenario abbiamo introdotto le CNN, \ncapaci di implementare logiche gerarchiche e gestire proprietà di \ninvarianza.  \nCome possiamo trattare input sotto forma di sequenze, come time series \nprediction, video analysis, etc?  \nOppure affrontare task che producono in output sequenze come l'\n image \ncaptioning, speech synthesis, \n e\n music generation.\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#20": "Trend e stagionalità (seasonality)\nCi sono molti altri modelli per il forecast di time series, come il \n weighted \nmoving average\n  e l'\nautoregressive integrated moving average \n (\nARIMA\n ). \nAlcune modelli richiedono di \n rimuovere preliminarmente trend e stagionalità \nnei dati, ad esempio:  \n•\nSe i visitatori di un sito web crescono stabilmente 10% al mese, occorre \nrimuovere questa variazione dai dati in input. Una volta ottenuta la \npredizione si può reintegrare al valore \n ﬁ\nnale.  \n•\nPer predire la vendita di creme solari, occorre preliminarmente rimuovere la \nstagionalità annuale associata ai mesi estivi. Per esempio, rimuovendo al \nvalore attuale il valore nell'anno precedente (\n differencing\n ). Si può reintegrare \nal valore \n ﬁ\nnale ottenuto.  \nLe RNN non richiedono generalmente questi preprocessamenti, anche se \npossono aumentare le prestazioni, poiché la rete non è costretta ad \napprenderli.\n21",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#21": "Deep RNN: considerazioni\nUna DeepRNN raggiunge un MSE di 0.003.  \nUna architettura più adatta avrebbe un ultimo layer con un singolo valore in \noutput per time step. Ma in questo caso avremmo un \n hidden state  \nrappresentato con un solo valore, che non avrebbe molta utilità. Una \nDeepRNN sfrutta tutti gli hidden states dei layer precedente per \"trasportare\" \nl'informazione necessaria per produrre l'ultimo output, e il contributo \ndell'hidden dell'ultimo layer risulta assai limitato.  \nPossiamo sostituire il layer in output con un layer fully connected (o Dense). \nL'accuratezza non è alterata, il tempo di addestramento si riduce leggermente \ne possiamo scegliere qualsiasi funzione di attivazione.  \nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(\n 20\n, return_sequences=\n True\n, input_shape=[\n None\n, \n1\n]),\n    keras.layers.SimpleRNN(\n 20\n),\n    keras.layers.Dense(\n 1\n)\n])\n22",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#22": "Predizione di più dati\nPossiamo tentare di stimare più valori temporalmente futuri, impi  \nElenchiamo alcuni approcci:  \n•\nUsare l'output come input nello step successivo ottenendo un valore alla \nvolta  \n•\nSimile al precedente ma in output prediciamo contemporaneamente più \nvalori ma considerando una unica loss (\n sequence-to-vector\n ) \n•\nSimile al precedente ma con una loss per ogni valore predetto (\n sequence-to-\nsequence\n )\n23",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#23": "Predizione di più dati - uno dato alla volta\nPossiamo tentare di stimare più valori temporalmente futuri.  \nUna possibilità è impiegare il modello attuale, ottenere l'output e \nconcatenarlo al predente input, ottenendo un secondo input e così via.  \n•\nAd esempio, per predire 10 dati:  \nseries = generate_time_series(\n 1\n, n_steps + \n 10\n)\nX_new, Y_new = series[:, :n_steps], series[:, n_steps:]\nX = X_new\nfor\n step_ahead \n in \nrange\n(\n10\n):\n    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n    X = np.concatenate([X, y_pred_one], axis=\n 1\n)\nY_pred = X[:, n_steps:]\nOtteniamo un MSE=0.029. L'approccio \n naive\n  ottiene 0.223, ma il modello \nlineare 0.0188, ed è più accurato e veloce da addestrare.  \nLa Deep RNN rimane valida se limitiamo il numero di valori da predire.\n24",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#24": "Predizione di più dati - sequence-to-vector\nUna seconda opzione è addestrare la RNN per predire 10 valori \ncontemporaneamente.  \n•\nUsiamo la \n sequence-to-vector\n , ma con 10 valori in output invece di 1.  \nIntanto cambiamo i valori target:  \nseries = generate_time_series(\n 10000\n, n_steps + \n 10\n)\nX_train, Y_train = series[:\n 7000\n, :n_steps], series[:\n 7000\n, \n-10\n:, \n0\n]\nX_valid, Y_valid = series[\n 7000\n:\n9000\n, :n_steps], series[\n 7000\n:\n9000\n, \n-10\n:, \n0\n]\nX_test, Y_test = series[\n 9000\n:, :n_steps], series[\n 9000\n:, \n-10\n:, \n0\n]\nL'output layer consisterà di 10 nodi:  \nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(\n 20\n, return_sequences=\n True\n, input_shape=[\n None\n, \n1\n]),\n    keras.layers.SimpleRNN(\n 20\n),\n    keras.layers.Dense(\n 10\n)\n]\n)\nE si potranno predire 10 valori in modo simile:  \nY_pred = model.predict(X_new)\nOra l'MSE è di 0.008, migliore del modello lineare.\n25",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#25": "Predizione di più dati - sequence-to-sequence\nUn ulteriore modello da impiegare è il \n sequence-to-sequence\n , dove le 10 \npredizioni sono comunque ottenute sequenzialmente step-by-step.  \n•\nIl vantaggio è avere una \n loss\n ad ogni step, perciò più gradienti che saranno \nusati per aggiornare il modello, non solo seguendo un approccio \n through-time,  \nma direttamente dagli output generati, come avviene nelle reti non ricorrenti.  \nAl primo step il modello produrrà in output la predizione per gli step da 1 a 10, \nallo step successivo le predizioni da 2 a 11, e così via. Il target avrà la stessa \nlunghezza dell'input.  \n•\nSebbene in output otteniamo una parte dei valori usati in input, l'input \ncorrente consiste sempre in valori apparsi nel passato. Sarebbe scorretto \nimpiegare valori del dataset che temporalmente sono da considerarsi futuri.  \nCreiamo le sequenze target di 10 elementi:  \nY = np.empty((\n 10000\n, n_steps, \n 10\n)) \n# each target is a sequence of 10D vectors\nfor\n step_ahead \n in \nrange\n(\n1\n, \n10\n + \n1\n):\n    Y[:, :, step_ahead - \n 1\n] = series[:, step_ahead:step_ahead + n_steps, \n 0\n]\nY_train = Y[:\n 7000\n]\nY_valid = Y[\n 7000\n:\n9000\n]\nY_test = Y[\n 9000\n:]\n26",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#26": "Predizione di più dati - sequence-to-sequence\nPer avere un modello \n sequence-to-sequence\n  impostiamo \nreturn_sequences=True\n  per ogni layer, compreso l'ultimo. Ad ogni step \nvalutiamo l'output del layer FC.  \n•\nKeras fornisce il \n TimeDistributed\n  layer, adatto ad essere valutato ad ogni step. \nI valori in input vengono automaticamente ridimensionati cosicché ogni step \nè trattato come una istanza separata  \n•\n[\nbatch size, time steps, input dim.\n ] \n [\nbatch size × time steps, input dim.\n ] \n•\nNell'esempio abbiamo 20 nodi nel layer \n SimpleRNN\n . L'output sarà una \nsequenza e non un singolo vettore. Il layer Dense viene applicato in modo \nindipendente ad ogni step.  \nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(\n 20\n, return_sequences=\n True\n, input_shape=[\n None\n, \n1\n]),\n    keras.layers.SimpleRNN(\n 20\n, return_sequences=\n True\n),\n    keras.layers.TimeDistributed(keras.layers.Dense(\n 10\n))\n])\n→\n27",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#27": "Predizione di più dati - sequence-to-sequence\nL'output nell'ultimo step è impiegato per la predizione e valutazione.  \nSebbene usiamo la MSE su tutti gli output, per la valutazione ci limitiamo a \nusare una metrica \n custom\n  che elabora l'MSE sull'ultimo step.  \ndef \nlast_time_step_mse\n (\nY_true\n, \nY_pred\n):\n    \nreturn\n keras.metrics.mean_squared_error(Y_true[:, \n -1\n], Y_pred[:, \n -1\n])\noptimizer = keras.optimizers.Adam(lr=\n 0.01\n)\nmodel.\ncompile\n(loss=\n\"mse\"\n, optimizer=optimizer, metrics=[last_time_step_mse])\nSi ottiene MSE di 0.006, 25% meglio del modello precedente.  \nÈ possibile combinare i due approcci: predire 10 valori, concatenarli ai dati in \ninput e predire i successivi 10, ottenendo sequenze di lunghezza arbitraria.  \nNota: Il \n Montecarlo Dropout\n  (\nMC Dropout\n ) è spesso inclusa in ogni cella per \nomettere in modo random parte degli input e degli hidden state.  \n28",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#28": "Predizione di più dati: limiti (1)\nPer addestrare una RNN su sequenze molto lunghe occorre creare \n reti molto \ndeep\n , coi noti problemi di \n instabilità dei gradienti\n  (es. tempo di \napprendimento troppo lungo, o instabile).  \nInoltre la rete fa più fatica a ricordare le informazioni iniziali della sequenza.  \nAlcune tecniche viste possono essere nuovamente applicate (es. dropout, \noptimizers più adatti alla architettura deep).  \nLe \nReLU  \nnon sono adatte\n  per le \n RNN\n . \n•\nSupponiamo che la discesa del gradiente aggiorni i parametri in modo da \nincrementare leggermente l'output. Siccome ad ogni step sono usati gli stessi \nparametri, anche l'output al successivo step può essere leggermente \nincrementato, e così via, \n ﬁ\nno a valori troppo elevati o instabili. \n Una funzione \nche non satura non può prevenire questo\n .  \n•\nAnche i gradienti possono assumere valori troppo elevati, perciò sono utili \ntecniche quali il \n Gradient clipping.\n29",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#29": "Predizione di più dati: limiti (2)\nLa \nBatch normalization\n  non mostra sperimentalmente la stessa ef\n ﬁ\ncacia \nrispetto alle reti deep tradizionali e non ottiene bene\n ﬁ\nci. \n•\nTeoricamente un layer BN può essere aggiunto ad ogni memory cell, e \ninterverrà dopo ogni step, sia sugli input correnti sia sull'hidden state (dello \nstep precedente).  \n•\nMa il layer BN sarà usato ad ogni step, con gli stessi parametri, senza \nconsiderare la scala di valori e l'offset degli input e dell'hidden state attuali.  \nNota: un tecnica simile ma più adatta è la \n Layer normalization\n , ma invece di \nnormalizzare rispetto al batch, normalizza rispetto alla dimensione delle \nfeatures.\n30",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#3": "Introduzione\nUna \n time series\n  consiste in una serie di misurazioni indicizzate con un \nordine temporale.  \n•\nEs. ultima quotazione giornaliera di un titolo \n ﬁ\nnanziario, situazione oraria del \nmeteo, traiettoria di una automobile  \nLe \nRecurrent Neural Networks\n  (\nRNN\n ) sono architetture di reti neurali \nadatte ad analizzare time series e stimare misure mancanti o future.  \nRispetto alle \n CNN\n  possono elaborare dati in ingresso con lunghezza \narbitraria non pre\n ﬁ\nssata, più adatte in certi contesti.  \n•\nEs. analisi di una frase per fare una traduzione automatica o speech-to-text  \nCiononostante non sono le uniche architetture per \n time series\n . \n•\nReti\n fully-connected \n sono sempre adatte per sequenze di lunghezza \nlimitata, mentre sequenze molto lunghe possono essere elaborate da \nﬁ\nltri convoluzionali.\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#30": "RNN più recenti\nLe celle introdotte \n ﬁ\nnora soffrono del problema del \n vanishing\n  (e exploding) \ngradient. Il gradient clipping o altre tecniche, sebbene risolvano il problema, \nnon permettono alle RNN di analizzare sequenze lunghe.  \nPer tale motivo sono state introdotte le \n memory cells\n , cioè unità di \nelaborazione che mantengono lo stato memorizzato e lo propagano alle celle \nsuccessive evitando che \"svanisca\" a causa dei gradienti troppo bassi.  \nLe \nLong Short-Term Memory (LSTM)\n  sono le prime memory cell introdotte in \nletteratura, le \n Gated Recurrent Unit (GRU)\n  ne sono una versione sempli\n ﬁ\ncata. \nNelle \n Bidirectional Recurrent Neural Networks\n  si sfruttano le informazioni \nraccolte negli step precedenti e successivi per determinare l'output nello step \ncorrente.\n31",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#31": "Celle LSTM - motivazioni\nLe \nLSTM\n  sono memory cells introdotte per dotare la cella di memoria \n a lungo \ntermine \n utile per riconoscere pattern di segnale più estesi.  \nOltre alla rappresentazione \n long-term\n , determinata dai pesi che sono appresi \ndurante il training, le celle hanno una memoria\n  short-term\n  capace di \nrappresentare le attivazioni ef\n ﬁ\nmere. Tale memoria viene condivisa da una \ncella alla successiva.  \nAll'interno delle memory cells, oltre allo stato, esistono una serie di \n gate \ncontrollers\n  che determinano quali input in\n ﬂ\nuenzano lo stato, se lo stato deve \nessere azzerato (o dimenticato) e come lo stato in\n ﬂ\nuenza l'output della cella.  \n•\nPerciò nella LSTM esistono meccanismi dedicati sia per aggiornare lo stato, \nsia per azzerarlo.  \nI gate sono governati da parametri che sono stimati durante l'apprendimento.\n32",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#32": "Celle LSTM e Keras\nKeras ne sempli\n ﬁ\nca l'uso con la funzione \n LSTM\n : \nmodel = keras.models.Sequential([\n    keras.layers.LSTM(\n 20\n, return_sequences=\n True\n, input_shape=[\n None\n, \n1\n]),\n    keras.layers.LSTM(\n 20\n, return_sequences=\n True\n),\n    keras.layers.TimeDistributed(keras.layers.Dense(\n 10\n))\n]\n)\nO impiegando la cella general purpose \n RNN\n  che può assumere come \nargomento \n LSTMCell\n , utile per creare celle \n custom\n , con lo svantaggio di \nperdere parte delle ottimizzazioni GPU:  \nmodel = keras.models.Sequential([\n    keras.layers.RNN(keras.layers.LSTMCell(\n 20\n), return_sequences=\n True\n,\n                     input_shape=[\n None\n, \n1\n]),\n    keras.layers.RNN(keras.layers.LSTMCell(\n 20\n), return_sequences=\n True\n),\n    keras.layers.TimeDistributed(keras.layers.Dense(\n 10\n))\n]\n)\nSuccessivamente vedremo una implementazione con le funzionalità\n  d2l\n.\n33",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#33": "Celle LSTM: Architettura (1)\nL'architettura di una cella \n LSTM\n  è la seguente:  \nEsistono 4 gates (\n FC\n) i cui input sono: l'\n hidden state\n  dello step precedente \n h\n(t-1) \ne l'input corrente \n x\n(t)\n.  \nI gate sono: \n forget gate\n , \ninput gate\n , \noutput gate\n , e \ninput node\n . I relativi output, \nf(t)\n, \ni(t)\n, \no(t)\n e \ng(t)\n; sono determinati da una rete fully connected (FC). Hanno \ntutti funzione di attivazione \n logistic\n , tranne l'\n input node\n  che impiega la \n tanh\n.\n34\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#34": "Celle LSTM: Architettura (2)\nPossiamo interpretare i gate nel seguente modo:  \n•\nL'\ninput gate\n  determina quanto dell'input corrente deve essere aggiunto ad \n c\n(t) \nche assume il ruolo di stato corrente.  \n•\nIl \nforget gate \n in\nﬂ\nuenza quanto tenere e quanto dimenticare dello stato interno \nprecedente \n c\n(t-1)\n.  \n•\nL'\noutput gate\n  quanto la cella corrente in\n ﬂ\nuenzerà l'output \n y\n(t)\n. \n•\nL'\ninput node\n  rappresenta la computazione di una cella ricorrente tradizionale.\n35\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#35": "Celle LSTM: Architettura (3)\nPer esempio, se il \n forget gate\n  fosse sempre 1 e l'\n input gate\n  fosse 0, lo stato \n c\n(t-1) \nrimarrebbe imperturbato negli step futuri. Nella realtà, i gate saranno addestrati \nin modo da perturbare lo stato in funzione degli input analizzati dalla cella.  \n•\nQuesta tecnica basata su gate affronta il \n vanishing gradient problem  \ngarantendo che gli stati possano propagarsi temporalmente per molti step.  \nL'\ninput node\n  produce \n g\n(t)\n e si comporta come una cella \"base\", ma nella LSTM \nuna parte rilevante del output del cella base è memorizzato nello stato interno \nc(t)\n, e il resto scartato. La suddivisione tra output e stato è più netta. \n36\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#36": "Celle LSTM: Architettura (4)\nL'output della cella \n h(t), \n corrispondente al valore y di una cella tradizionale, è \ngenerato prendendo il valore \n tanh\n dello stato interno corrente \n c(t)\n e calcolando \nuna moltiplicazione element-wise con il valore ottenuto dall'\n output gate\n . \n•\nSe l'\noutput gate\n  è 1 lo stato interno in\n ﬂ\nuenzerà i layer successivi (in una \narchitettura multilayer) nello step corrente. Se è 0 lo stato non li in\n ﬂ\nuenzerà.  \n•\nÈ sempre possibile che lo stato interno si propaghi per molti step, e che non \nin\nﬂ\nuenzi l'output a causa dell'\n output gate \n che lo inibisce, \n ﬁ\nno ad un certo \nstep in cui il gate potrà invertire il valore. Per tale motivo \n h(t)\n è visto come \nstato \n short-term,\n  mentre \n c(t) \nlong-term\n .\n37\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#37": "Celle LSTM: Architettura (5)\nIn sintesi, la \n LSTM\n  è in grado di riconoscere sequenze lunghe, per mezzo dell'\n input \ngate\n, memorizzarlo in uno stato long-term, e preservarlo \n ﬁ\nnché è giudicato \nimportante, per mezzo del\n  forget gate\n , e ripescarlo quando necessario.  \n•\nPer tale motivo le LSTM hanno ottenuto buoni risultati nell'analisi di pattern, anche molto estesi, in \ntime series, testi, audio, etc.  \nIn termini analitici, per una singola istanza in input si ha:  \n•\ndove \n W\nxi\n, \nW\nxf\n, \nW\nxo\n,\nW\nxg\n sono le matrici dei pesi dei 4 layer per le connessioni con \nl'input vector \n x\n(t)\n. \n•\nW\nhi\n, \nW\nhf\n, \nW\nho\n,\nW\nhg\n sono le matrici dei pesi dei 4 layer per le connessioni con lo stato \nshort-term precedente \n h\n(t-1)\n. \n•\nb\ni\n, \nb\nf\n, \nb\no\n,\nb\ng\n sono i bias, inizializzati a 1 invece di 0 per evitare di \"dimenticare\" tutto \nall'inizio del training.\n38\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#38": "Celle LSTM, Keras e d2l\n11-memory_cells.ipynb\n39",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#39": "Celle LSTM: Peephole connections\nIn una cella LSTM tradizionale i gate controllers analizzano \n x\n(t)\n e \nh\n(t-1)\n. Può \nessere utile dargli la possibilità di usare le informazioni nello stato long-term.  \nLe \npeephole connections\n  aggiungono il precedente stato long-term \n c\n(t-1) \nall'input dei controllers del \n forget\n  e \ninput\n  gate. Lo stato long-term corrente \n c\n(t)\n è \naggiunto all'input controller dell'output gate.  \nNon sempre ci sono miglioramenti,  \nperciò si può tentare di usarli e valutare.  \nIn Keras non c'è supporto uf\n ﬁ\nciale alla cella con \n peephole connections\n , ma si \npuò creare un layer RNN generico e passargli \n PeepholeLSTMCell\n  al suo \ncostruttore.\n40\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#4": "Alcuni task con dati temporali\nSpeech recognition  \nMusic generation  \nSentiment classi\n ﬁ\ncation  \nDNA sequence analysis  \nMachine translation  \n(Video) Activity recognition\n“It was a bright cold day in \nApril, and the clocks were \nstriking thirteen\n“I loved this so much. I crap out \non books about 40 pages in about \n90% of the time.”\nØ  \no few inputs\nACAAGATGCCATTGTCCCCCGGCCTCCTGCTGC ACAAGATG CCATTGTCCCCCGGCCTCCT GCTGC\n“Ho corso per arrivare in orario.” “I ran to get on time.”\nAlzarsi -> In piedi -> Camminare",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#40": "Celle GRU - motivazioni\nDopo le LSTM sono state studiate altre architetture che potessero mantenere i \nvantaggi ma con meno risorse di calcolo necessarie.  \nLe celle \n Gated Recurrent Unit (GRU)\n , con un numero minore di gate, sono \nstate proposte per tale scopo, \n41",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#41": "Celle GRU\nLe celle \n Gated Recurrent Unit\n  (\nGRU\n ) sono una versione sempli\n ﬁ\ncata, e con \nmeno parametri, delle LSTM. In molti task mostrano prestazioni simili con \ntempi di addestramento ridotti.  \nLe sempli\n ﬁ\ncazioni sono le seguenti:  \n•\nEntrambi i vettori di stato sono fusi in un singolo vettore \n h\n(t)\n. \n•\nUn singolo \n update gate  \nz\n(t)\n rappresenta una fusione del \n forget\n  e \ninput gate\n . \nLa funzione \n keras.layers.GRU\n  è impiegata in modo simile a SimpleRNN e \nLSTM.\n42\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#42": "Celle GRU (2)\nIntuitivamente il \n reset gate\n  r(t)\n controlla quanto dello stato precedente \nvogliamo mantenere nelle successive elaborazioni.  \nL'\nupdate gate  \nz(t)\n controlla quanto il nuovo stato sia copia dello stato \nprecedente.  \nEntrambi i gate sono implementati con una FC e funzione di attivazione \nsigmoid\n , perciò con output in (0,1).\n43\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#43": "Celle GRU (3)\nIl \nhidden state candidato h(t) \n (candidato poiché dobbiamo ancora sommare la \ncomponente del \n update gate\n ) sarà generato combinando insieme \n x(t)\n e \nl'output del reset gate \n r(t)\n, e impiegando una funzione di attivazione \n tanh\n. \nQuando il  \nreset gate  \nha output pari a 1, otteniamo una RNN tradizionale. Se il \ngate\n genera 0, l'hidden state candidato coincide con l'output della FC con \n x(t) \ncome input. Perciò l'hidden state sarà resettato. \n44\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#44": "Celle GRU (4)\nIl \nhidden state h(t)  \ndipende dal \n update gate\n , che determina quanto il nuovo \nstato corrisponde al vecchio oppure al nuovo stato candidato.  \nQuando l'\n update gate\n  è 1, manteniamo lo stato così com'è, e l'informazione \nx(t) non sarà considerata per alterarlo. Perciò ignoriamo lo step corrente nella \ncatena di correlazioni che stiamo rappresentando con lo stato. Se l'output del \ngate è 0, lo stato corrisponde al candidato che abbiamo appena creato.\n45\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#5": "Nodi Ricorrenti\nFinora abbiamo considerato reti neurali \n feedforward\n , dove le attivazioni si \npropagano\n  dall'input all'output layer.   \nLe \nRNN\n  sono simili alle reti feedforward, ma hanno connessioni anche \n verso i \nlayer precedenti\n , creando una specie di ciclo.  \nLa più semplice \n RNN\n  consiste in un \n nodo ricorrente \n (o\n recurrent neuron\n ) \nche \nriceve l'input \n x\n, produce in output \n y,\n e lo stesso output viene \n riproposto\n  in input.  \n•\nAd ogni \n iterazione  \nt\n, (o \nstep\n, o \nframe\n ), il nodo ricorrente riceve l'input \n x\n(t)\n e \nl'output precedente \n y\n(t-1)\n. Il valore \n y\n(t)\n alla prima iterazione si considera pari a 0.  \nLa RNN si può rappresentare esplicitando l'asse temporale (\n unrolling the \nnetwork through time\n ).\n6\n",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#6": "Nodi Ricorrenti e Layers\nSe \nx\n(t) \ne \ny\n(t-1)\n sono vettori, i parametri che de\n ﬁ\nniscono il comportamento di un \nnodo ricorrente consistono in due vettori di pesi: \n w\nx\n e \nw\ny\n.  \nL'\noutput  \ndi un singolo nodo\n  si ricava nel modo usuale:  \n \nUn\n layer di nodi ricorrenti \n comprende più nodi, ed i parametri saranno \nperciò rappresentati da due matrici \n  e \n .\ny\n(\nt\n)\n=\nσ\n(\nw\nT\nx\nx\n(\nt\n)\n+\nw\nT\ny\ny\n(\nt\n−\n1\n)\n+\nb\n)\nW\nx\nW\ny\nlayer di nodi ricorrenti",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#7": "Nodi ricorrenti: calcolo degli output\nNel caso più generale di\n  un layer di nodi ricorrenti \n con un input costituito da \nvettori\n , cioè istanze rappresentate con più features:  \n  dove:  \n•\n  è una matrice \n m\nn\nnodi\n, che contiene gli output del layer di \n n\nnodi\n nodi \nricorrenti, per ognuna delle \n m\n istanze all'interno del mini-batch,  \n•\n  è una matrice \n m\nn\ninputs\n, dove \n n\ninputs\n sono il numero di features in input,  \n•\n  è la matrice \n n\ninputs\n n\nnodi\n dei pesi delle connessioni per le istanze in input,  \n•\n  è la matrice \n n\nnodi\nn\nnodi\n dei pesi delle connessioni per i valori in output \nottenuti nello step precedente.  \n•\nSi può dire che una RNN è una feedforward NN dove i parametri di ogni \nlayer sono condivisi (cioè sono gli stessi) per tutti i time steps.\nY\n(\nt\n)\n=\nσ\n(\nx\n(\nt\n)\nW\nx\n+\ny\n(\nt\n−\n1\n)\nW\ny\n+\nb\n)\ny\n(\nt\n)\n×\nx\n(\nt\n)\n×\nW\nx\n×\nW\ny\n×\n8",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#8": "Nodi ricorrenti: forma compatta\nLe matrici dei pesi \n  e \n   sono spesso \n concatenate\n  verticalmente in una \nsingola matrice (\n n\ninputs\n+n\nnodi\n)\nn\nnodi \nLa notazione \n  rappresenta la concatenazione delle matrici \n  \nIn questo modo si ottiene la seguente \n forma compatta\n : \n   con    \nÈ chiaro che \n  è funzione di \n  e \n , quest'ultimo è funzione di \n  e \n, che è funzione di \n  e \n , e così via.  \nDi conseguenza  \ndipende da tutti i valori in input \n ﬁ\nno a \nt=0\n. \nSi può dire che il nodo contiene memoria di tutti gli input precedenti. In realtà \ni pattern che può riconoscere non sono lunghi tipicamente più di 10 steps.\nW\nx\nW\ny\n×\n[\nX\n(\nt\n)\nY\n(\nt\n−\n1\n)\n]\nX\n(\nt\n)\n e \nY\n(\nt\n−\n1\n)\nσ\n(\n[\nX\n(\nt\n)\nY\n(\nt\n−\n1\n)\n]\nW\n+\nb\n)\n W\n=\n[\nW\nx\nW\ny\n]\nY\n(\nt\n)\n X\n(\nt\n)\nY\n(\nt\n−\n1\n)\n X\n(\nt\n−\n1\n)\nY\n(\nt\n−\n2\n)\n X\n(\nt\n−\n2\n)\nY\n(\nt\n−\n3\n)\nY\n(\nt\n)\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\12-RNN 1-sbloccato.pdf#9": "Memory Cells\nUna rete neurale in grado di tenere traccia degli stati in cui si è trovata nelle \npassate iterazioni si chiama \n memory cell\n  (o \ncell\n). \n•\nUn singolo\n  recurrent node,\n  o un layer di tali nodi, è una \n cella\n elementare, in \ngrado di riconoscere piccoli patterns, tipicamente non più lunghi di 10 steps.  \nIndichiamo \n stato\n  di una cella all'istante \n t\n con la notazione \n , dove \n h\n sta per \nhidden\n . Lo stato dipende dall'input corrente e dallo stato precedente:  \n \n•\nAnche l'\n output  \n dipende dalle stesse quantità.  \nNelle celle elementari \n output\n  e \nstato  \ncoincidono\n , ma nelle celle più \ncomplesse non sempre accade, come nel seguente esempio:\nh\n(\nt\n)\nh\n(\nt\n)\n=\nf\n(\nh\n(\nt\n−\n1\n)\n,\nx\n(\nt\n)\n)\ny\n(\nt\n)\n10\n",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nRecurrent Neural Networks (RNN) - parte #2\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#1": "Sommario\n1d convolution per sequenze  \nWaveNet  \nDeep RNN  \nBidirectional RNN  \nEncoder-decoder e Keras  \nEsercizi",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#10": "Bidirectional RNN - motivazioni\nFinora abbiamo visto scenari di predizione dove un valore in output dipende \ndai valori precedenti (es. predizione di una parola data una frase iniziale).  \nIn altri task è utile considerare il contesto di un valore in entrambe le \ndirezioni, es. Part-of-speech tagging.  \n•\nAd esempio, nel seguente task tentiamo di predire il token mancante dal \ntesto dato come input:\n11\n",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#11": "Bidirectional RNN\nNelle Bidirectional RNN abbiamo 2 layer unidirezionali con direzioni opposte, \nche operano sul medesimo input. Nel primo layer, il primo input è \n x\n1\n e l'ultimo \nx\nT\n, nel secondo il primo input è \n x\nT\n e l'ultimo \n x\n1\n. \nL'output è generato concatenando l'output dei 2 layers.  \n•\nNel caso multilayer, l'output diverrà l'input dei successivi 2 layers \nbidirezionali, e così via \n ﬁ\nno al layer di output.  \nIn Keras sono implementate con il parametro \n bidirectional=\n True\n.\n12\n",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#12": "Architettura Encoder-decoder - Keras\n14-encoder_decoder_interfaces.ipynb\n13\n",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#13": "RNN - Applicazioni\nPuoi immaginare a possibili applicazioni di una RNN sequence-to-sequence?  \nE di una sequence-to-vector, o di una vector-to-sequence?\n14",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#14": "RNN - Applicazioni\nPuoi immaginare a possibili applicazioni di una RNN sequence-to-sequence?  \nE di una sequence-to-vector, o di una vector-to-sequence?  \nSequence-to-sequence: previsioni meteo, machine translation (con Encoder-\ndecoder), video captioning, speech to text, music generation, identi\n ﬁ\ncare \naccordi nella musica.  \nSequence-to-vector: classi\n ﬁ\ncare brani musicali in base al genere, analizzare il \nsentimento di una recensione di un libro, predire quale parola sta pensando un \npaziente afasico in base ai segnali di impianti cerebrali, stimare la probabilità \ndi vedere un certo \n ﬁ\nlm in base ai \n ﬁ\nlm visti in passato.  \nVector-to-sequence: image captioning, creare una playlist di musica in base agli \nembedding dell'artista corrente, generare una melodia in base a dei parametri, \nidenti\n ﬁ\ncare pedoni in una foto.\n15",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#15": "RNN - Dimensioni\nQuante dimensioni deve avere l'input layer di una RNN? Cosa rappresenta ogni \ndimensione? E riguardo gli output?\n16",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#16": "RNN - Dimensioni\nQuante dimensioni deve avere l'input layer di una RNN? Cosa rappresenta ogni \ndimensione? E riguardo gli output?  \nUn layer RNN deve avere input 3 dimensionali: la prima dimensione è la \ndimensione del batch (cioè il numero di time series), la seconda rappresenta il \ndimensione temporale, e la terza indica il numero di features per step.  \nL'output sarà ancora 3 dimensionale, con le stesse 2 dimensioni dell'input, ma \ncon l'ultima dimensione uguale al numero di nodi. \n17",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#17": "RNN - Parametro return_sequence\nSe vuoi costruire una sequence-to-sequence RNN, quali layer devono avere \nreturn_sequence=True? E per quanto riguarda la sequence-to-vector?\n18",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#18": "RNN - Parametro return_sequence\nSe vuoi costruire una sequence-to-sequence RNN, quali layer devono avere \nreturn_sequence=True? E per quanto riguarda la sequence-to-vector?  \nPer una sequence-to-sequence, il parametro è True per tutti i layer.  \nPer una sequence-to-vector, il parametro è True per tutti gli RNN layers eccetto \nl'ultimo layer, impostato a False.\n19",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#19": "RNN - Forecasting\nSupponi di avere una time series univariate con campionamento giornaliero e \nvuoi fare forecasting dei prossimi 7 giorni. Quale architettura RNN impieghi?\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#2": "1d convolution per le sequenze\nSebbene popolari, \n LSTM\n  e \nGRU\n  non sono adatte a sequenze che contengono \npattern signi\n ﬁ\ncativi che si estendo per molti steps (es. 100). Una alternativa è \nridurre\n  la lunghezza delle sequenze in input.  \nImpieghiamo le \n 1d convolution\n  sulle sequenze in input considerando come \nprofondità la dimensione temporale piuttosto che spaziale. Fissiamo segmenti \ndi dimensioni prede\n ﬁ\nnita sulle sequenze in input per creare tali sequenze che \ncorrispondono alle dimensioni del kernel.  \nEstendiamo l'approccio considerando più \n ﬁ\nltri 1d convolution.  Ogni \n ﬁ\nltro \nriconoscerà determinati pattern.  \n•\nAd esempio, con 10 kernels, l'output complessivo del layer consisterà in 10 \nsequenze 1-dimensionali, tutte della stessa lunghezza, o una singola \nsequenza 10-dimensionale.  \nPossiamo avere reti che alternano layer 1d convolution e layer ricorrenti, ed \neventualmente layer di pooling. In questo modo le celle analizzeranno dati \ntemporali più \n compatti\n . Oppure possiamo avere reti costituite interamente da \nmoduli convolutivi (es. WaveNet).\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#20": "RNN - Forecasting\nSupponi di avere una time series univariate con campionamento giornaliero e \nvuoi fare forecasting dei prossimi 7 giorni. Quale architettura RNN impieghi?  \nL'architettura più semplice è una sequence-to-vector, cioè uno stack di RNN \nlayers (tutti con return_sequences=True eccetto il primo), con 7 nodi nel layer \ndi output. Si addestra il modello con \n ﬁ\nnestre random dalle time series (es. \nsequence di 30 giorni consecutivi e un vettore contenente i valori dei successivi \n7 giorni come target).  \nIn alternativa si imposta return_sequences=True per tutti i layer creando una \nsequence-to-sequence. Per l'addestramento si usano random windows con la \nstessa lunghezza del target. \n21",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#21": "RNN - Training\nQuali sono le maggiori dif\n ﬁ\ncoltà nel training di una RNN e come puoi \naffrontarle?\n22",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#22": "RNN - Training\nQuali sono le maggiori dif\n ﬁ\ncoltà nel training di una RNN e come puoi \naffrontarle?  \nLe due maggiori problematiche sono l'instabilità dei gradienti e la short-term \nmemory limitata. I problemi peggiorano in presenza di sequenze molto lunghe.  \nPer affrontarli si usano learning rate più bassi, funzioni di attivazioni che \nsaturano ed eventualmente gradient clipping, layer normalization o dropout ad \nogni step. Per la short-term memory, si impiegano celle LSTM o GRU.\n23",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#23": "RNN - LSTM\nRappresenta l'architettura LSTM gra\n ﬁ\ncamente.\n24",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#24": "RNN - 1d conv\nPerché vorresti impiegare una 1d conv all'interno di una RNN?\n25",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#25": "RNN - 1d conv\nPerché vorresti impiegare una 1d conv all'interno di una RNN?  \nUna RNN opera sequenzialmente: per calcolare l'output al tempo t, deve prima \ncalcolare gli output degli step precedenti. Questo rende impossibile \nparallelizzare l'elaborazione.  \nLa 1d conv non mantiene uno stato tra elaborazioni successive perciò e \nfacilmente parallelizzabile. Non essendo ricorrente, è meno affetta da gradienti \ninstabili.  \nPiù 1d conv possono processare l'input riducendo la risoluzione temporale \n(downsampling) permettendo di analizzare time series molto lunghe.  \nInfatti la WaveNet analizza time series impiegando solo 1d conv.\n26",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#26": "RNN - Scenario\nQuale architettura NN impiegheresti per classi\n ﬁ\ncare video? \n27",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#27": "RNN - Scenario\nQuale architettura NN impiegheresti per classi\n ﬁ\ncare video?  \nPrendiamo un frame per secondo e lo diamo in input a una rete \nconvoluzionale. L'output della CNN è passato in input a una sequence-to-\nvector RNN, il cui output è passato a una layer softmax, ottenendo una \ndistribuzione di probabilità sulle classi.  \nLa funzione di costo può essere una cross entropy.  \nPer usare l'audio si possono impiegare layer 1d conv, per ridurre la risoluzione \nda migliaia di audio frames per secondo a 1 solo per secondo, così da \nsincronizzarsi rispetto ai frame, e concatenare l'output con l'input alla \nsequence-to-vector.\n28",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#28": "RNN - Sketch dataset\nAddestra un modello per la classi\n ﬁ\ncazione per il Sketch-RNN dataset \ndisponibile dentro Tensor\n ﬂ\now.\n29\n",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#29": "RNN - Sketch dataset\nAddestra un modello per la classi\n ﬁ\ncazione per il Sketch-RNN dataset \ndisponibile dentro Tensor\n ﬂ\now. \nDOWNLOAD_ROOT = \n \"http://download.tensorflow.org/data/\"\nFILENAME = \n \"quickdraw_tutorial_dataset_v1.tar.gz\"\nfilepath = keras.utils.get_file(FILENAME,\n                                DOWNLOAD_ROOT + FILENAME,\n                                cache_subdir=\n \"datasets/quickdraw\"\n ,\n                                extract=\n True\n)\nquickdraw_dir = Path(filepath).parent\ntrain_files = \n sorted\n([str(path) \n for\n path \nin\n quickdraw_dir.glob(\n \"training.tfrecord-*\"\n )])\neval_files = \n sorted\n([str(path) \n for\n path \nin\n quickdraw_dir.glob(\n \"eval.tfrecord-*\"\n )])\nwith \nopen\n(quickdraw_dir / \n \"eval.tfrecord.classes\"\n ) \nas\n test_classes_file:\n    test_classes = test_classes_file.readlines()\n    \nwith \nopen\n(quickdraw_dir / \n \"training.tfrecord.classes\"\n ) \nas\n train_classes_file:\n    train_classes = train_classes_file.readlines()\nassert\n train_classes == test_classes\nclass_names = [name.strip().lower() \n for\n name \nin\n train_classes]\nsorted\n(class_names)\n30",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#3": "1d convolution per le sequenze: esempio\nIl modello include una \n 1d conv \n che opera una sorta di \n downsampling\n  delle \nsequenze in input di un fattore 2 impiegano uno stride 2. Il kernel è più grande \ndello stride perciò tutta l'informazione verrà considerata.  \nRiducendo la lunghezza in input sarà più facile per la GRU riconoscere pattern \npiù lunghi.  \nmodel = keras.models.Sequential([\n    keras.layers.Conv1D(filters=\n 20\n, kernel_size=\n 4\n, strides=\n 2\n, \npadding=\n \"valid\"\n,\n                        input_shape=[\n None\n, \n1\n]),\n    keras.layers.GRU(\n 20\n, return_sequences=\n True\n),\n    keras.layers.GRU(\n 20\n, return_sequences=\n True\n),\n    keras.layers.TimeDistributed(keras.layers.Dense(\n 10\n))\n])\nmodel.\ncompile\n(loss=\n\"mse\"\n, optimizer=\n \"adam\"\n, \nmetrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train[:, \n 3\n::\n2\n], epochs=\n 20\n,\n                    validation_data=(X_valid, Y_valid[:, \n 3\n::\n2\n]))\n•\nNota: Avendo kernel di dimensione 4, è opportuno ignorare i primi 3 step nei valori target, e \nfare dowsampling dei target di un fattore 2.\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#30": "RNN - Sketch dataset\nAddestra un modello per la classi\n ﬁ\ncazione per il Sketch-RNN dataset disponibile dentro Tensor\n ﬂ\now. \ndef \nparse\n(\ndata_batch\n ):\n    feature_descriptions = {\n        \n \"ink\"\n: tf.io.VarLenFeature(dtype=tf.float32),\n        \n \"shape\"\n: tf.io.FixedLenFeature([\n 2\n], dtype=tf.int64),\n        \n \"class_index\"\n : tf.io.FixedLenFeature([\n 1\n], dtype=tf.int64)\n    }\n    examples = tf.io.parse_example(data_batch, feature_descriptions)\n    flat_sketches = tf.sparse.to_dense(examples[\n \"ink\"\n])\n    sketches = tf.reshape(flat_sketches, shape=[tf.size(data_batch), \n -1\n, \n3\n])\n    lengths = examples[\n \"shape\"\n][:, \n0\n]\n    labels = examples[\n \"class_index\"\n ][:, \n0\n]\n    \nreturn\n sketches, lengths, label\n s\ndef \nquickdraw_dataset\n (\nfilepaths\n , \nbatch_size\n =\n32\n, \nshuffle_buffer_size\n =\nNone\n,\n                      \n n_parse_threads\n =\n5\n, \nn_read_threads\n =\n5\n, \ncache\n=\nFalse\n)\n:\n    dataset = tf.data.TFRecordDataset(filepaths\n ,\n                                      num_parallel_reads=n_read_threads\n )\n    \nif\n cache\n:\n        dataset = dataset.cache(\n )\n    \nif\n shuffle_buffer_size\n :\n        dataset = dataset.shuffle(shuffle_buffer_size\n )\n    dataset = dataset.batch(batch_size\n )\n    dataset = dataset.\n map\n(parse, num_parallel_calls=n_parse_threads\n )\n    \nreturn\n dataset.prefetch(\n 1\n)\n31",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#31": "RNN - Sketch dataset\nAddestra un modello per la classi\n ﬁ\ncazione per il Sketch-RNN dataset disponibile dentro Tensor\n ﬂ\now. \ntrain_set = quickdraw_dataset(train_files, shuffle_buffer_size=\n 10000\n)\nvalid_set = quickdraw_dataset(eval_files[:\n 5\n]\n)\ntest_set = quickdraw_dataset(eval_files[\n 5\n:]\n)\ndef \ndraw_sketch\n (\nsketch\n, \nlabel\n=\nNone\n)\n:\n    origin = np.array([[\n 0\n., \n0\n., \n0\n.]]\n)\n    sketch = np.r_[origin, sketch\n ]\n    stroke_end_indices = np.argwhere(sketch[:, \n -1\n]==\n1\n.)[:, \n0\n]\n    coordinates = np.cumsum(sketch[:, :\n 2\n], axis=\n 0\n)\n    strokes = np.split(coordinates, stroke_end_indices + \n 1\n)\n    title = class_names[label.numpy()] \n if\n label \nis \nnot \nNone \nelse \n\"Try to guess\"\n    plt.title(title\n )\n    plt.plot(coordinates[:, \n 0\n], -coordinates[:, \n 1\n], \n\"y:\"\n)\n    \nfor\n stroke \n in\n strokes\n :\n        plt.plot(stroke[:, \n 0\n], -stroke[:, \n 1\n], \n\".-\"\n)\n    plt.axis(\n \"off\"\n)\ndef \ndraw_sketches\n (\nsketches\n , \nlengths\n, \nlabels\n)\n:\n    n_sketches = \n len\n(sketches\n )\n    n_cols = \n 4\n    n_rows = (n_sketches - \n 1\n) // n_cols + \n 1\n    plt.figure(figsize=(n_cols * \n 3\n, n_rows * \n 3.5\n)\n)\n    \nfor\n index, sketch, length, label \n in \nzip\n(\nrange\n(n_sketches), sketches, lengths, labels)\n :\n        plt.subplot(n_rows, n_cols, index + \n 1\n)\n        draw_sketch(sketch[:length], label\n )\n    plt.show()\n32",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#32": "RNN - Sketch dataset\nAddestra un modello per la classi\n ﬁ\ncazione per il Sketch-RNN dataset disponibile dentro \nTensor\n ﬂ\now. \nfor\n sketches, lengths, labels \n in\n train_set.take(\n 1\n)\n:\n    draw_sketches(sketches, lengths, labels\n )\nlengths = np.concatenate([lengths \n for\n _, lengths, _ \n in \ntrain_set.take(\n 1000\n)]\n)\nplt.hist(lengths, bins=\n 150\n, density=\n True\n)\nplt.axis([\n 0\n, \n200\n, \n0\n, \n0.03\n]\n)\nplt.xlabel(\n \"length\"\n )\nplt.ylabel(\n \"density\"\n )\nplt.show(\n )\ndef \ncrop_long_sketches\n (\ndataset\n, \nmax_length\n =\n100\n)\n:\n    \nreturn\n dataset.\n map\n(\nlambda\n inks, lengths, labels: \n(inks[:, :max_length], labels)\n )\ncropped_train_set = crop_long_sketches(train_set\n )\ncropped_valid_set = crop_long_sketches(valid_set\n )\ncropped_test_set = crop_long_sketches(test_set\n )\n33",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#33": "RNN - Sketch dataset\nAddestra un modello per la classi\n ﬁ\ncazione per il Sketch-RNN dataset disponibile dentro \nTensor\n ﬂ\now. \nmodel = keras.models.Sequential(\n [\n    keras.layers.Conv1D(\n 32\n, kernel_size=\n 5\n, strides=\n 2\n, activation=\n \"relu\"\n)\n,\n    keras.layers.BatchNormalization()\n ,\n    keras.layers.Conv1D(\n 64\n, kernel_size=\n 5\n, strides=\n 2\n, activation=\n \"relu\"\n)\n,\n    keras.layers.BatchNormalization()\n ,\n    keras.layers.Conv1D(\n 128\n, kernel_size=\n 3\n, strides=\n 2\n, activation=\n \"relu\"\n)\n,\n    keras.layers.BatchNormalization()\n ,\n    keras.layers.LSTM(\n 128\n, return_sequences=\n True\n)\n,\n    keras.layers.LSTM(\n 128\n)\n,\n    keras.layers.Dense(\n len\n(class_names), activation=\n \"softmax\"\n )\n]\n)\noptimizer = keras.optimizers.SGD(lr=\n 1e-2\n, clipnorm=\n 1\n.\n)\nmodel.\ncompile\n(loss=\n\"sparse_categorical_crossentropy\"\n ,\n              optimizer=optimizer\n ,\n              metrics=[\n \"accuracy\"\n , \n\"sparse_top_k_categorical_accuracy\"\n ]\n)\nhistory = model.fit(cropped_train_set, epochs=\n 2\n,\n                    validation_data=cropped_valid_set\n )\ny_test = np.concatenate([labels \n for\n _, _, labels \n in\n test_set]\n )\ny_probas = model.predict(test_set)\n34",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#34": "RNN - Sketch dataset\nAddestra un modello per la classi\n ﬁ\ncazione per il Sketch-RNN dataset disponibile dentro \nTensor\n ﬂ\now. \nnp.mean(keras.metrics.sparse_top_k_categorical_accuracy(y_test, y_probas)\n )\nn_new = \n 10\nY_probas = model.predict(sketches\n )\ntop_k = tf.nn.top_k(Y_probas, k=\n 5\n)\nfor\n index \nin \nrange\n(n_new)\n:\n    plt.figure(figsize=(\n 3\n, \n3.5\n)\n)\n    draw_sketch(sketches[index]\n )\n    plt.show(\n )\n    \nprint\n(\n\"Top-5 predictions:\"\n .\nformat\n(index + \n 1\n)\n)\n    \nfor\n k \nin \nrange\n(\n5\n)\n:\n        class_name = class_names[top_k.indices[index, k]\n ]\n        proba = \n 100\n * top_k.values[index, k\n ]\n        \n print\n(\n\"  {}. {} {:.3f}%\"\n .\nformat\n(k + \n1\n, class_name, proba)\n )\n    \nprint\n(\n\"Answer: {}\"\n .\nformat\n(class_names[labels[index].numpy()])\n )\nmodel.save(\n \"my_sketchrnn\"\n )\n35",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#35": "RNN - Generazione di musica\nScarica il dataset Bach chorales e scompattalo. Consiste in 382 corali composti \nda Johann Sebastian Bach. Ogni corale è lungo da 100 a 640 time steps, e ogni \nstep contiene 4 interi, dove ogni interno corrisponde alla nota su un piano. Lo 0 \nindica che che non si suona alcuna nota.  \nAddestra un modello ricorrente o convoluzionale, o entrambi, che può predire \nil successivo step (4 note), data una sequenza del corale. Usa quello modello \nper generare musica in stile Bach, ad esempio dando in input l'inizio di un \ncorale e ottenendo la predizione che userai come successivo input.  \nIn\nﬁ\nne dai un'occhiata al Google Coconet model.  \n36",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#36": "RNN - Generazione di musica\nDOWNLOAD_ROOT = \n \"https://github.com/ageron/handson-ml2/raw/master/datasets/jsb_chorales/\"\nFILENAME = \n \"jsb_chorales.tgz\"\nfilepath = keras.utils.get_file(FILENAME,\n                                DOWNLOAD_ROOT + FILENAME,\n                                cache_subdir=\n \"datasets/jsb_chorales\"\n ,\n                                extract=\n True\n)\njsb_chorales_dir = Path(filepath).parent\ntrain_files = \n sorted\n(jsb_chorales_dir.glob(\n \"train/chorale_*.csv\"\n ))\nvalid_files = \n sorted\n(jsb_chorales_dir.glob(\n \"valid/chorale_*.csv\"\n ))\ntest_files = \n sorted\n(jsb_chorales_dir.glob(\n \"test/chorale_*.csv\"\n ))\nimport\n pandas \n as\n pd\ndef \nload_chorales\n (\nfilepaths\n ):\n    \nreturn\n [pd.read_csv(filepath).values.tolist() \n for\n filepath \n in\n filepaths]\ntrain_chorales = load_chorales(train_files)\nvalid_chorales = load_chorales(valid_files)\ntest_chorales = load_chorales(test_files)\ntrain_chorales[\n 0\n]\nnotes = set()\nfor\n chorales \n in\n (train_chorales, valid_chorales, test_chorales):\n    \nfor\n chorale \n in\n chorales:\n        \n for\n chord \nin\n chorale:\n            notes |= set(chord)\nn_notes = \n len\n(notes)\nmin_note = \n min\n(notes - {\n 0\n})\nmax_note = \n max\n(notes)\n37",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#37": "RNN - Generazione di musica\nassert\n min_note == \n 36\nassert\n max_note == \n 81\nfrom\n IPython.display \n import\n Audio\ndef \nnotes_to_frequencies\n (\nnotes\n):\n    \n# Frequency doubles when you go up one octave; there are 12 semi-tones\n    \n# per octave; Note A on octave 4 is 440 Hz, and it is note number 69.\n    \nreturn \n2\n ** ((np.array(notes) - \n 69\n) / \n12\n) * \n440\ndef \nfrequencies_to_samples\n (\nfrequencies\n , \ntempo\n, \nsample_rate\n ):\n    note_duration = \n 60\n / tempo \n # the tempo is measured in beats per minutes\n    \n# To reduce click sound at every beat, we round the frequencies to try to\n    \n# get the samples close to zero at the end of each note.\n    frequencies = np.\n round\n(note_duration * frequencies) / note_duration\n    n_samples = int(note_duration * sample_rate)\n    time = np.linspace(\n 0\n, note_duration, n_samples)\n    sine_waves = np.sin(\n 2\n * np.pi * frequencies.reshape(\n -1\n, \n1\n) * time)\n    \n# Removing all notes with frequencies ≤ 9 Hz (includes note 0 = silence)\n    sine_waves *= (frequencies > \n 9\n.).reshape(\n -1\n, \n1\n)\n    \nreturn\n sine_waves.reshape(\n -1\n)\ndef \nchords_to_samples\n (\nchords\n, \ntempo\n, \nsample_rate\n ):\n    freqs = notes_to_frequencies(chords)\n    freqs = np.r_[freqs, freqs[\n -1\n:]] \n# make last note a bit longer\n    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n                     \n for\n melody \n in\n freqs.T], axis=\n 0\n)\n    n_fade_out_samples = sample_rate * \n 60\n // tempo \n # fade out last note\n    fade_out = np.linspace(\n 1\n., \n0\n., n_fade_out_samples)**\n 2\n    merged[-n_fade_out_samples:] *= fade_out\n    \nreturn\n merged\n38",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#38": "RNN - Generazione di musica\ndef \nplay_chords\n (\nchords\n, \ntempo\n=\n160\n, \namplitude\n =\n0.1\n, \nsample_rate\n =\n44100\n, \nfilepath\n =\nNone\n):\n    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n    \nif\n filepath:\n        \n from\n scipy.io \n import\n wavfile\n        samples = (\n 2\n**\n15\n * samples).astype(np.int16)\n        wavfile.write(filepath, sample_rate, samples)\n        \n return\n display(Audio(filepath))\n    \nelse\n:\n        \n return\n display(Audio(samples, rate=sample_rate))\nfor\n index \nin \nrange\n(\n3\n):\n    play_chords(train_chorales[index])\ndef \ncreate_target\n (\nbatch\n):\n    X = batch[:, :\n -1\n]\n    Y = batch[:, \n 1\n:] \n# predict next note in each arpegio, at each step\n    \nreturn\n X, Y\ndef \npreprocess\n (\nwindow\n):\n    window = tf.where(window == \n 0\n, window, window - min_note + \n 1\n) \n# shift values\n    \nreturn\n tf.reshape(window, [\n -1\n]) \n# convert to arpegio\ndef \nbach_dataset\n (\nchorales\n , \nbatch_size\n =\n32\n, \nshuffle_buffer_size\n =\nNone\n,\n                 \n window_size\n =\n32\n, \nwindow_shift\n =\n16\n, \ncache\n=\nTrue\n):\n    \ndef \nbatch_window\n (\nwindow\n):\n        \n return\n window.batch(window_size + \n 1\n)\n    \ndef \nto_windows\n (\nchorale\n):\n        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n        dataset = dataset.window(window_size + \n 1\n, window_shift, drop_remainder=\n True\n)\n        \n return\n dataset.flat_map(batch_window)\n    chorales = tf.ragged.constant(chorales, ragged_rank=\n 1\n)\n    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n    dataset = dataset.flat_map(to_windows).\n map\n(preprocess)\n    \nif\n cache:\n        dataset = dataset.cache()\n    \nif\n shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.\n map\n(create_target)\n    \nreturn\n dataset.prefetch(\n 1\n)\n39",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#39": "RNN - Generazione di musica\ntrain_set = bach_dataset(train_chorales, shuffle_buffer_size=\n 1000\n)\nvalid_set = bach_dataset(valid_chorales)\ntest_set = bach_dataset(test_chorales)\nn_embedding_dims = \n 5\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n                           input_shape=[\n None\n]),\n    keras.layers.Conv1D(\n 32\n, kernel_size=\n 2\n, padding=\n \"causal\"\n , activation=\n \"relu\"\n),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(\n 48\n, kernel_size=\n 2\n, padding=\n \"causal\"\n , activation=\n \"relu\"\n, dilation_rate=\n 2\n),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(\n 64\n, kernel_size=\n 2\n, padding=\n \"causal\"\n , activation=\n \"relu\"\n, dilation_rate=\n 4\n),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv1D(\n 96\n, kernel_size=\n 2\n, padding=\n \"causal\"\n , activation=\n \"relu\"\n, dilation_rate=\n 8\n),\n    keras.layers.BatchNormalization(),\n    keras.layers.LSTM(\n 256\n, return_sequences=\n True\n),\n    keras.layers.Dense(n_notes, activation=\n \"softmax\"\n )\n])\nmodel.summary()\noptimizer = keras.optimizers.Nadam(lr=\n 1e-3\n)\nmodel.\ncompile\n(loss=\n\"sparse_categorical_crossentropy\"\n , optimizer=optimizer,\n              metrics=[\n \"accuracy\"\n ])\nmodel.fit(train_set, epochs=\n 20\n, validation_data=valid_set)\nmodel.save(\n \"my_bach_model.h5\"\n )\nmodel.evaluate(test_set)\n40",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#4": "Architettura WaveNet\nLe rete convolutiva \n WaveNet\n  impiega più layer 1d conv, dove ad ogni layer si \nraddoppia la lunghezza della LRF, chiamato anche \n dilatation rate.\n  In questo \nmodo ogni layer raddoppia i dati analizzati rispetto al layer precedente.  \n•\nI primi layer riconoscono pattern \n short-term\n , gli ultimi i pattern estesi.  \n•\nPer dilatation >1, i layer ignoreranno alcuni campioni all'interno del LRF che \nperò saranno considerati nei layer precedenti.  \nIl training è più rapido rispetto alle RNN non essendoci collegamenti ricorrenti.  \nL'output può essere accodato all'input successivo per fare predizione (es. \ngenerazione voce umana).\n5\nstack of conv layers",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#40": "RNN - Generazione di musica\ndef \ngenerate_chorale\n (\nmodel\n, \nseed_chords\n , \nlength\n):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [\n 1\n, \n-1\n])\n    \nfor\n chord \nin \nrange\n(length):\n        \n for\n note \nin \nrange\n(\n4\n):\n            next_note = model.predict_classes(arpegio)[:\n 1\n, \n-1\n:]\n            arpegio = tf.concat([arpegio, next_note], axis=\n 1\n)\n    arpegio = tf.where(arpegio == \n 0\n, arpegio, arpegio + min_note - \n 1\n)\n    \nreturn\n tf.reshape(arpegio, shape=[\n -1\n, \n4\n])\nseed_chords = test_chorales[\n 2\n][:\n8\n]\nplay_chords(seed_chords, amplitude=\n 0.2\n)\nnew_chorale = generate_chorale(model, seed_chords, \n 56\n)\nplay_chords(new_chorale)\ndef \ngenerate_chorale_v2\n (\nmodel\n, \nseed_chords\n , \nlength\n, \ntemperature\n =\n1\n):\n    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n    arpegio = tf.reshape(arpegio, [\n 1\n, \n-1\n])\n    \nfor\n chord \nin \nrange\n(length):\n        \n for\n note \nin \nrange\n(\n4\n):\n            next_note_probas = model.predict(arpegio)[\n 0\n, \n-1\n:]\n            rescaled_logits = tf.math.log(next_note_probas) / temperature\n            next_note = tf.random.categorical(rescaled_logits, num_samples=\n 1\n)\n            arpegio = tf.concat([arpegio, next_note], axis=\n 1\n)\n    arpegio = tf.where(arpegio == \n 0\n, arpegio, arpegio + min_note - \n 1\n)\n    \nreturn\n tf.reshape(arpegio, shape=[\n -1\n, \n4\n])\n41",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#41": "RNN - Generazione di musica\nnew_chorale_v2_cold = generate_chorale_v2(model, seed_chords, \n 56\n, temperature=\n 0.8\n)\nplay_chords(new_chorale_v2_cold, filepath=\n \"bach_cold.wav\"\n )\nnew_chorale_v2_medium = generate_chorale_v2(model, seed_chords, \n 56\n, temperature=\n 1.0\n)\nplay_chords(new_chorale_v2_medium, filepath=\n \"bach_medium.wav\"\n )\nnew_chorale_v2_hot = generate_chorale_v2(model, seed_chords, \n 56\n, temperature=\n 1.5\n)\nplay_chords(new_chorale_v2_hot, filepath=\n \"bach_hot.wav\"\n )\nplay_chords(test_chorales[\n 2\n][:\n64\n], filepath=\n \"bach_test_4.wav\"\n )\n42",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#5": "6\n1\n2\n3\n4\nhttps://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio\nNella \n WaveNet\n  l'output layer ha la \nstessa dimensionalità temporale \ndell'input.  \nIl singolo valore in output è prodotto \nda una \n softmax\n , perciò con \ndistribuzione sulle categorie \ndisponibili.  \nLe \ndilated convolution\n  sono simili ai \nlayer convolutivi con pooling e \nstride, ma in questo caso l'output ha \nla stessa dimensione dell'input.  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#6": "WaveNet: esempio\nImpostando il \n padding  \ncausal\n  si garantisce che l'input non conterrà dati oltre a \nquello attuale.  \nIl parametro \n dilatation_rate\n  de\nﬁ\nnisce l'architettura WaveNet.  \nL'ultimo layer è convolutivo con 10 \n ﬁ\nltri di dimensione 1 senza funzione di \nattivazione. Ogni conv layer produce una sequenza della stessa lunghezza \ndell'input, cosicché possiamo usare i target senza ridimensionarli.  \nNell'esempio manca il layer softmax impiegato nell'esempio precedente.  \nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=[\n None\n, \n1\n]))\nfor\n rate \nin\n (\n1\n, \n2\n, \n4\n, \n8\n) * \n2\n:\n    model.add(keras.layers.Conv1D(filters=\n 20\n, kernel_size=\n 2\n, padding=\n \"causal\"\n ,\n                                  activation=\n \"relu\"\n, dilation_rate=rate))\nmodel.add(keras.layers.Conv1D(filters=\n 10\n, kernel_size=\n 1\n))\nmodel.\ncompile\n(loss=\n\"mse\"\n, optimizer=\n \"adam\"\n, metrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train, epochs=\n 20\n,\n                    validation_data=(X_valid, Y_valid))\n7",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#7": "Deep RNN\nFinora abbiamo visto RNN con un singolo hidden layer ricorrente, uno strato \ndi input e uno di output. Le memory cell permettono di creare correlazioni \nche in\n ﬂ\nuenza step anche distanti tra loro (direzione temporale).  \nNelle Deep RNN si vogliono identi\n ﬁ\ncare correlazioni anche tra input e output \nnello stesso step (direzione input-output). Per questo si considerano più layer \nstacked \n sequence-to-sequence.  \n•\nIl primo layer produce una sequenza in output di lunghezza T, che sarà \nl'input del successivo layer.  \n•\nOgni cella perciò dipenderà dai valori  \ndel layer negli step precedente, e dai  \nvalori generati dai layer precedenti nello  \nstesso step.  \nArchitetture comuni di RNN hanno  \nlunghezza (\n numero di step\n ) nel range  \n64-2056 e profondità in 1-8.\n8\n",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#8": "Deep RNN - Keras (1)\n12-deep_rnn.ipynb\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\13-RNN 2-sbloccato.pdf#9": "Deep RNN - Keras (2)\nIn alternativa alla classe GRU possiamo operare uno stacking impiegando \nSequential:  \nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(\n 20\n, return_sequences=\n True\n, input_shape=[\n None\n, \n1\n]),\n    keras.layers.SimpleRNN(\n 20\n, return_sequences=\n True\n),\n    keras.layers.SimpleRNN(\n 1\n)\n])”\nNota\n : ricordati di impostare return_sequences=\n True\n per ogni layer (tranne \nl'ultimo se ci interessa in output solo l'ultimo valore), altrimenti l'output del \nlayer sarà 2D (solo l'ultimo valore) e si crea un mismatch con l'input atteso in \n3D dal successivo layer.\n10",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nAttention mechanisms e Transformers\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#1": "Sommario\nBeam search  \nAttention mechanism  \nMulti-head attention  \nSelf-attention  \nTransformers  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#10": "Beam search (2)\nLa complessità è pari a \n O\n(\nk·\n|\nY\n|·\nT'\n)\n, dove \n T'\n è il numero massimo di token \ndella sequenza in output, e \n Y\n è il vocabolario.  \nAl contrario dell'approccio greedy, la beam search permette di scegliere alcuni \ntoken meno probabili, sebbene la frase nel suo complesso generi accuracy \nmigliori.  \nUna ricerca esaustiva di tutte le possibili combinazioni richiederebbe \ncomplessità \n O\n(|\nY\n|\nT'\n).\n11",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#11": "Attention cues\nIl meccanismo si ispira a studi di neuroscienze cognitive, dove si tenta di dare \nattenzione solo ad una parte degli stimoli generati dal sistema di visione.  \n•\nle \nnonvolitional cues\n  (\nkeys\n) sono legate alla visibilità dell'oggetto \nnell'ambiente (es. tazza di caffè rossa su un tavolino grigio),  \n•\nle \nvolitional cue\n  (o \nquery\n ) dipendono dal task che stiamo seguendo (es. \nleggere un libro su un tavolo).  \nSe abbiamo dei \n sensory inputs\n  da analizzare, la \n query\n  interagisce con le \n keys \nper selezionare gli input più corretti.  \nL'\nattention pooling\n  aggrega gli input per generare l'output.\n12\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#12": "Attention pooling e Keras\nCi sono vari modi di implementare l'attention pooling. Qui diamo un esempio \nbasato sul modello di regressione Nadaraya-Watson kernel.  \nGeneriamo un dataset arti\n ﬁ\nciale con questo modello:  \ncon epsilon generato con distribuzione normale media 0 e varianza 0.5, e 50 \nistanze di training e test.  \nclass \nNonlinearData\n (\nd2l\n.\nDataModule\n ):\n    \ndef \n__init__\n (\nself\n, \nn\n, \nbatch_size\n ):\n        \n self\n.save_hyperparameters()\n        f = \n lambda\n x: \n2\n * tf.sin(x) + x**\n 0.8\n        \n self\n.x_train = tf.sort(tf.random.uniform((n,\n 1\n)) * \n5\n, \n0\n)\n        \n self\n.y_train = f(\n self\n.x_train) + tf.random.normal((n,\n 1\n))\n        \n self\n.x_val = tf.\n range\n(\n0\n, \n5\n, \n5.0\n/n)\n        \n self\n.y_val = f(\n self\n.x_val)\n    \ndef \nget_dataloader\n (\nself\n, \ntrain\n):\n        arrays = (\n self\n.x_train, \n self\n.y_train) \n if\n train \nelse\n (\nself\n.x_val, \n self\n.y_val)\n        \n return \nself\n.get_tensorloader(arrays, train)\nn = \n50\ndata = NonlinearData(n, batch_size=\n 10\n)\n13\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#13": "Attention pooling e Keras (2)\nGra\nﬁ\nchiamo gli esempi di training (cerchi), il ground truth senza rumore \n(curva blu) la funzione generata per la prediction (tratteggiata rossa).  \nProviamo con l'estimator più facile: \n average pooling\n  sui dati di input:  \ny_hat = tf.repeat(tf.reduce_mean(data.y_train), n)\nplot_kernel_reg(y_hat)\ndef \nplot_kernel_reg\n (\ny_hat\n):\n    d2l.plot(data.x_val, [data.y_val, y_hat.numpy()], \n 'x'\n, \n'y'\n, legend=[\n 'Truth'\n, \n'Pred'\n],\n             xlim=[\n 0\n, \n5\n], ylim=[\n -1\n, \n5\n])\n    d2l.plt.plot(data.x_train, data.y_train, \n 'o'\n, alpha=\n 0.5\n);\n14",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#14": "Attention pooling e Keras (3)\nNel seguente nonparametric attention pooling chiamato \n Nadaraya-Watson \nkernel regression \n pesiamo gli output in base alla posizione degli input.  \ndove K è il kernel. Generalizzandolo (non serve ora studiare i dettagli del \nregressore) possiamo de\n ﬁ\nnire un qualsiasi \n attention pooling\n  nel seguente \nmodo:  \ndove \n x\n è la \n query\n , e (\nx\ni\n,y\ni\n) e la coppia \n chiave-valore\n . Perciò ogni valore \n y\ni\n è \npesato e si può pensare come una distribuzione di probabilità sull'insieme \nchiavi-valore.  \n15\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#15": "Attention pooling e Keras (4)\nGra\nﬁ\ncando l'output del nuovo modello otteniamo:  \ndef \ndiff\n(\nqueries\n, \nkeys\n):\n    \nreturn\n tf.reshape(queries, (\n -1\n, \n1\n)) - tf.reshape(keys, (\n 1\n, \n-1\n))\ndef \nattention_pool\n (\nquery_key_diffs\n , \nvalues\n):\n    attention_weights = tf.nn.softmax(- query_key_diffs**\n 2\n/\n2\n, axis=\n1\n)\n    \nreturn\n tf.matmul(attention_weights, values), attention_weights\ny_hat, attention_weights = attention_pool(\n    diff(data.x_val, data.x_train), data.y_train)\nplot_kernel_reg(y_hat)\n16\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#16": "Attention pooling e Keras (5)\nAnalizzando i pesi generati dell'attention pooling, con le \n query\n  come \nvalidation inputs\n  e \nkeys\n come \n training inputs\n , notiamo come all'avvicinarsi dei \nvalori tra \n query\n  e \nchiave\n , i pesi sono più signi\n ﬁ\ncativi:  \nd2l.show_heatmaps([[attention_weights]],\n                  xlabel=\n 'Sorted training inputs'\n ,\n                  ylabel=\n 'Sorted validation inputs'\n )\n17\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#17": "Attention Scoring functions\nL'output dell'\n attention mechanism\n  possiamo valutarlo con una \n softmax\n  e \ninterpretarlo come distribuzione di probabilità sui valori che sono in paio con \nle chiavi.  \n18\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#18": "Multi-head attention\nL'attention mechanism potrebbe richiedere di analizzare dipendenze su vari \nintervalli (short e long-range) all'interno delle sequenze. Invece di un singolo \npooling, possiamo generare \n h\n proiezioni lineari indipendenti. Gli \n h\n output \nsono concatenati e passati ad una combinazione lineare \n ﬁ\nnale per produrre \nl'output. Ognuna delle \n h \nattention pooling\n  è chiamato \n head\n .\n19\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#19": "Self-attention e positional encoding\nSupponiamo che in input all'attention mechanism diamo una sequenza di \ntokens. I tokens rappresentano perciò sia le query, le chiavi e i valori. Ogni \nquery è relativa e tutte le coppie chiave-valore e genera un output. Per tale \nmotivo si parla di \n self-attention\n . \nSi può dimostrare facilmente che le CNN e i self-attention possono essere \nimplementati con computazioni parallele, sebbene sequenze molto lunghe \npenalizzano molto i self-attention.  \nPossiamo aggiungere informazioni aggiuntive, come quelle associate alla \nposizione, alla rappresentazione in input del self-attention, poiché durante i \ncalcoli queste informazioni vengono ignorate, a differenza delle RNN.\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#2": "Introduzione\nIn una architettura \n encoder-decoder,\n  durante la traduzione di un testo notiamo \ncome alcune parole generate dal decoder dipendano da poche parole in input \nal encoder. Se le frasi sono molte lunghe, una RNN fa fatica a identi\n ﬁ\ncare tali \ndipendenze.  \nGli \nattention mechanism \n introdotti nel 2014 da Bahdanau et al.\n(1)\n risolvono in \nparte le limitazioni della memoria delle RNN, arrivando a processare frasi di \n30 parole circa.  \nGli attention mechanism sono alla base dell'architettura \n Transformers\n , che è \nlo stato dell'arte nel campo dell'NLP in molti task, es. machine language \ntranslation, conversational chatbots, e per migliorare le performance dei \nsearch engines.  \nL'obiettivo degli \n attention mechanism \n è assegnare un livello di importanza alle \nfeatures e sfruttarlo per agevolare il raggiungimento di un certo task. \n3 (1) https://arxiv.org/abs/1409.0473",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#20": "Attention mechanism e Encoder-decoder (1)\nNell'architettura \n encoder-decoder con \n attention mechanism\n , oltre all'ultimo \nhidden state propagato dal encoder al decoder (non visibile nel disegno), \ninviamo al decoder tutti i valori generati in output dal encoder\n . \n•\nAd ogni step, il \n decoder\n  calcola una combinazione lineare dei valori ottenuti \nin output dal encoder così da determinare su quale parola porre l'attenzione \nal successivo step.  \n•\nIl peso \n  è riferito al \n i\n-esimo output, allo step \n t\n-esimo...\n α\n(\nt\n,\ni\n)\n21\nNell'esempio, se  è \nmaggiore di  e , \nallora il decoder \nassegnerà maggiore \nattenzione al termine \n\"milk\" nello step corrente .\nLa restante elaborazione \ncoincide l'architettura \noriginale.α(3,2)\nα(3,0)α(3,1)Questa con ﬁgurazione \nspeci ﬁca di attention \nmechanism è anche \nchiamata:  \nBahdanau attention .  \n \nDato che concatena gli \noutput del encoder con gli \nhidden state, si chiama \nanche: concatenative \nattention , o additive \nattention .",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#21": "Attention mechanism e Encoder-decoder (2)\nI valori \n  sono generati da una rete neurale chiamata \n alignment model  \n(o \nattention layer)\n  addestrata con il resto del encoder-decoder.  \n•\nConsiste in una \n time-distributed Dense layer\n  con un singolo nodo, che \nriceve tutti gli output dal encoder, concatenati con l'hidden state del decoder \nestratti dallo step precedente.  \n•\nIl layer produce una serie di valori e\n (3,0)\n, e\n(3,1)\n, etc; che indicano \n quanto ogni \noutput è allineato con l'hidden state precedente\n . La \nsoftmax\n  normalizza tali \nvalori.\nα\n(\nt\n,\ni\n)\n22\n rappresenta l'hidden \nstate del decoder .\nLa rete densa richiede il \ncalcolo di n2 parametri, \nsupponendo n la \nlunghezza delle frasi in \ninput e output. Ma se non \nabbiamo frasi lunghissime \nla complessità è ancora \npraticabile. h(2)",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#22": "Image captioning\nIn questo task occorre generare un testo signi\n ﬁ\ncativo in linguaggio naturale \nche descrive una immagine. In altre parole dobbiamo dotare l'algoritmo di \ncapacità di \"\n comprensione\"\n . \nUsiamo un \n encoder-decoder\n , dove il \n decoder\n  sfrutta l'\n attention mechanism\n . \nL'\nencoder\n  è una CNN, e le features sono estratte dal layer convolutivo. \nL'\nattention mechanism\n  avrà perciò informazione posizionale codi\n ﬁ\ncata \nnell'input. Il \n decoder\n  usa lo stato precedente, il token generato in precedenza \ne un context vector per generare il nuovo token.\n23 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#23": "Image caption e Visual Attention\nIl \ncontext vector\n  è generato dall'\n attention mechanism \n a partire dalle features \ndella CNN nell'encoder. Esso rappresenta i pesi per ogni location spaziale \ndell'output dell'encoder (\n visual attention\n ).  \nAd ogni step il decoder usa l'attention model per identi\n ﬁ\ncare le giusta \nporzione dell'immagine da analizzare per poi per generare la frase.\n24\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#24": "Explainability\nNel ML con \n explainability\n  si intende la capacità del modello di descrivere il \nsuo comportamento in termini comprensibili all'uomo.  \n•\nInterpretability\n  è un concetto simile, ma legato alle relazioni causa-effetto \ndel modello, e perciò sulla capacità di stimare un certo output in base a una \ncerta con\n ﬁ\ngurazione di input. Si può avere interpretability senza \nexplainability.  \nUtile quando si vuole comprendere un certo output, eventualmente errato.  \n•\nEs. l'output \"un lupo che si muove sulla neve\" prodotto quando c'è un cane \ncome input può dipendere dal fatto che il modello si focalizza sulla presenza \ndella neve per classi\n ﬁ\ncare l'animale.  \nI \nmodelli DL \n sono molto complessi e spesso \n dif\nﬁ\ncilmente esplorabili\n . \nUn tentativo è creare modelli interpretabili (es. alberi di decisione) a partire \ndagli output di un modello non interpretabile\n(3)\n, e usarli per costruire le \nmotivazioni di un certo output.\n25 (3) https://arxiv.org/abs/1602.04938",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#25": "Architettura Transformer - motivazioni\nComputazionalmente, i modelli di attention e le \nCNN sono più veloci rispetto all RNN. I Transformer \nsono unicamente basati su tali modelli.  \nIn \"Attention is all you need\"\n(4)\n, un team di Google \npropose l'architettura \n Transformer\n , dove il task della \ntraduzione dei testi si affronta senza RNN, ma \nprincipalmente con \n layer embedding\n , \ndense layers\n  e \ndi \nnormalization\n .  \nInizialmente proposti nel task \n seq2seq\n  sul testo, \nsono stati poi usati in svariati altri domini, es. \nvisione, speech, reinforcement learning.  \nL'encoder\n  mappa le sequenze in input in una \nrappresentazione continua latente che incapsula \ntutte le informazioni rilevanti della sequenza.  \nI multi-head attention creano associazioni tra una \nparola e le altre con un sistema di pesi.\n26\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#26": "Architettura Transformer - (1)\nAd alto livello possiamo notare uno stack di layer \nmultipli che si ripetono (\n N\n volte). Ognuno di questi \nmacro-layer ha sotto layer.  \nNell'encoder si ha un \n multi-head self-attention\n  e un \npositionwise feed-forward network\n .  \nNell'\n encoder self-attention\n , queries, keys a values \nsono ottenuti dall'output del layer precedente. In \nmodo simile alla ResNet, abbiamo connessioni \nresidue.  \nLo stack del decoder oltre layer simili all'encoder \nabbiamo un ulteriore layer \n encoder-decoder \nattention\n  nel mezzo, che prende in input l'output \ndel layer precedente nel decoder, e keys e values \nottenuti dall'encoder. \n27\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#27": "Architettura Transformer - (2)\nNel decoder self-attention, queries, keys e values \nsono ottenuti dal layer precedente.  \nOgni posizione nel decoder interessa tutte le \nposizioni viste \n ﬁ\nno alla posizione corrente. Infatti \nnel training i dati sono noti, ma in produzione \npossiamo generare token che dipendono solo dai \ntoken già generati.  \nLa \npositionwise feed-forward network\n  è composta \nda 2 layer FC e prende in input (batch size, number \nof time steps/sequence length in tokens, number of \nhidden units/feature dimension) e produce in output \n(batch size, number of time steps, ffn_num_outputs).  \nI transfomer mantengono la proprietà \n auto-\nregressive\n , cioè l'output dipende linearmente dai \nvalori prodotti in precedenza e da un termine \nstocastico.\n28\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#28": "Architettura Transformer e NLP: dettagli (1)\n29 (4) https://arxiv.org/abs/1706.03762\nL'input si pre-processa come prima, ottenendo \nuna\n rappresentazione in uno spazio a 512 \ndimensioni\n  (cioè uno shape [batch size, max \ninput sentence length, 512]) mediante \n word \nembeddings\n . \nL'output del decoder ha sempre forma di \ndistribuzione di probabilità ([batch size, max \noutput sentence length, vocabulary length])  \nIl \ndecoder\n  prende in input la frase target \ntraslata di 1 posizione.  \nAd ognuno dei \n N\n livelli, l'output del \n encoder  \nè inviato in input al \n encoder-decoder attention  \nnel corrispondete livello del \n decoder\n .EncoderDecoder",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#29": "Architettura Transformer e NLP: dettagli (2)\nIn produzione ancora una volta l'output del decoder (una nuova parola) verrà \naccodata all'input del decoder allo step successivo.\n30 (4) https://arxiv.org/abs/1706.03762\nSia encoder sia decoder, oltre agli layer \nembedding, hanno \n 5\nN skip connections\n , \nognuna seguita da una layer di \nnormalizzazione\n . \nLe \npositionwise feed-forward\n  network sono \nreti MLP\n  tradizionali con 2 Dense layer \nciascuna, il primo layer con attivazione \nReLU, il secondo senza attivazione. Il \n layer \ndi output\n  è una layer denso con softmax.  \nTutti i layer sono \n time-distributed\n , cosicché \nogni parola è trattata indipendentemente \ndalle altre.\n×\nEncoderDecoder",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#3": "Introduzione\nNell'esempio sono evidenziate features ricavate dall'\n attention mechanism \n che \nmettono in correlazione features anche molto distanti tra loro.\n4\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#30": "Architettura Transformer e NLP: dettagli (3)\nMa se la generazione delle parole è indipendente una dall'altra, e non \nabbiamo RNN, come può funzionare?\n31 (4) https://arxiv.org/abs/1706.03762\nIl modulo \n multi-head attention\n  codi\n ﬁ\nca ogni \nrelazione di una parola con tutte le altre nella \nstessa frase, in modo da evidenziale le \nrelazioni più importanti (\n self-attention\n ). \n•\nEs. In “They welcomed the Queen of the \nUnited Kingdom”, il termine queen sarà più \nlegato cone \n united\n  e \nkingdom\n  rispetto a \n they \ne \nwelcomed\n . \nIl modulo \n masked multi-head attention\n  si \ncomporta in modo simile, ma si limita a \nconsiderare la parola precedente.  \nIl \nmulti-head attention\n  del decoder analizza le \nparole nella frase in input e si focalizza sui \ntermini più importanti per la traduzione (es. \n\"Queen\").",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#31": "Architettura Transformer e NLP: dettagli (5)\nMa se la generazione delle parole è indipendente una dall'altra, come può \nfunzionare?\n32 (4) https://arxiv.org/abs/1706.03762\nI \npositional embeddings\n , che sono dati in \ninput al \n encoder\n  e \ndecoder\n  insieme agli \nembedding tradizionali, \n rappresentano la \nposizione di un termine rispetto agli altri in \nuna certa frase\n .  \nSono aggiunti ai \n word embedding\n  poiché il \nmulti-head attention laye\n r ignora posizione \ne ordine delle parole nella frase, come pure \ngli altri layer  essendo tutti \n layer time-\ndistributed\n . \n ",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#32": "Keras: Transformer\nIn Keras una versione di Attention basata su \n scaled dot-product\n  è \nimplementata in keras.layers.Attention.  \nencoder_outputs = Z\nZ = decoder_in\nfor\n N \nin \nrange\n(\n6\n):\n    Z = keras.layers.Attention(use_scale=\n True\n, causal=\n True\n)([Z, Z])\n    Z = keras.layers.Attention(use_scale=\n True\n)([Z, encoder_outputs])\noutputs = keras.layers.TimeDistributed(\n    keras.layers.Dense(vocab_size, activation=\n \"softmax\"\n ))(Z)\n33 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#33": "Demo Transformers e NLP\nhttps://transformer.huggingface.co/  \n34 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#4": "Richiamo: Architetture encoder-decoder\nSi può combinare una rete \n sequence-to-vector\n  (\nencoder\n ) con una \n vector-to-\nsequence\n  (\ndecoder\n ) ottenendo una rete \n encoder-decoder\n . \n•\nUn \nencoder\n  può rappresentare una frase in un linguaggio in un singolo \nvettore che viene impiegato poi dal \n decoder\n  per generare la frase in diverso \nlinguaggio. \n5\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#5": "Encoder/Decoder in Keras (1)\nL'interfaccia \n encoder\n  può prendere in input una sequenza di lunghezza \nvariabile X.  \nclass \nEncoder\n(\ntf\n.\nkeras\n.\nlayers\n.\nLayer\n):\n   \ndef \n__init__\n (\nself\n):\n        super().\n __init__\n ()\n    \n# Later there can be additional arguments (e.g., length excluding padding)\n    \ndef \ncall\n(\nself\n, \nX\n, *\nargs\n):\n        \n raise\n NotImplementedErro\n r\nIl \ndecoder\n  prende l'output dei decoder per tramutarlo nello stato.  \nclass \nDecoder\n(\ntf\n.\nkeras\n.\nlayers\n.\nLayer\n):\n   \ndef \n__init__\n (\nself\n):\n        super().\n __init__\n ()\n    \n# Later there can be additional arguments (e.g., length excluding padding)\n    \ndef \ninit_state\n (\nself\n, \nenc_outputs\n , *\nargs\n):\n        \n raise\n NotImplementedError\n    \ndef \ncall\n(\nself\n, \nX\n, \nstate\n):\n        \n raise\n NotImplementedError\n6",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#6": "Encoder/Decoder in Keras (2)\nL'architettura completa:  \nclass \nEncoderDecoder\n (\nd2l\n.\nClassifier\n ):\n   \ndef \n__init__\n (\nself\n, \nencoder\n, \ndecoder\n):\n        super().\n __init__\n ()\n        \n self\n.encoder = encoder\n        \n self\n.decoder = decoder\n    \ndef \ncall\n(\nself\n, \nenc_X\n, \ndec_X\n, *\nargs\n):\n        enc_outputs = \n self\n.encoder(enc_X, *args, training=\n True\n)\n        dec_state = \n self\n.decoder.init_state(enc_outputs, *args\n )\n        \n # Return decoder output only\n        \n return \nself\n.decoder(dec_X, dec_state, training=\n True\n)[\n0\n]\n7",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#7": "Esempio: Machine Translation (1)\nIn questo caso input e output sono sequenze di lunghezza variabile. La \nsequenza in input è tradotta in una rappresentazione nascosta/latente \n ﬁ\nxed-\nshape.  \nLa sequenza in output è generata \n token-by-token\n : data l'attuale sequenza in \ninput, e i token precedenti generati in output. Durante l'addestramento il \ntoken da generare sarà estratto dai dati ground-truth. L'\n hidden state \ndell'\nencoder\n  viene dato in input ad ogni step di decoding. L'output generato \nsarà il nuovo input del decoder nello step successivo.  \nNota: se ignoriamo \n l'encoder\n , il \ndecoder\n  corrisponde ad un \n language model\n . \nNell'esempio abbiamo frasi tokenizzate, dove <eos> corrisponde a end-of-\nsequence. Ad ogni istanze iniziale, si impiega un tag <bos> per indicare \nl'inizio della sequenza\n8\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#8": "Esempio: Machine Translation (2)\nL'encoder\n  può essere implementato con una RNN (es. GRU) uni o bi-\ndirezionale.  \nL'output layer del \n decoder\n  sarà una FC che genera la distribuzione di \nprobabilità sui token in output.  \nLa \nloss\n sarà una basata sulla \n cross-entropy\n . \nUsualmente si usano sequenze di padding per frasi di lunghezza variabile, in \ninput e output. Tali padding non intervengono nel calcolo della loss.\n9\n",
    "data_test\\rootfolder\\università\\DeepLearning\\14-Attention e Autoencoders 1-sbloccato.pdf#9": "Beam search (1)\nNell'architettura precedente il token generato è quello con più alta \nprobabilità, \n ﬁ\nno a quando non è generato il token <eos>. Seguiamo una \nstrategia \n greedy\n , \nbasata sui passati token generati e la variabile di contesto \n c\n, \nche rappresenta la frase in input:  \nNella \n beam search\n  scegliamo i \n k\n token candidati più probabili. \nSuccessivamente teniamo in considerazioni i \n k\n token per la generazione del \nnuovo token, e così via.\n10\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nAutoencoders\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#1": "Sommario\nLanguage models: recenti sviluppi  \nAutoencoders  \nStacked Autoencoders  \nFashion MNIST dataset  \nVisualizzazione con t-SNE  \nUnsupervised training con Stacked autoencoders  \nConvolutional Autoencoders  \nRecurrent Autoencoders  \nDenoising Autoencoders  \nSparse Autoencoders  \nVariational Autoencoders  \nSemantic interpolation",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#10": "Rappresentazioni latenti\nLatent representation learning\n  (LRL), o \n Latent variable modeling \n (LVM), è una \ntecnica di apprendimento automatico che tenta di inferire le variabili latenti \nda misurazioni empiriche da variabili osservabili. Tali variabili latenti non \npossono essere misurate direttamente e quindi devono essere dedotte.  \nUna o più variabili latenti costituiscono congiuntamente uno \n spazio latente\n  o \nuna\n rappresentazione latente\n . Questa rappresentazione è solitamente una \nforma \n compatta\n  dello spazio impiegato per rappresentare le misurazioni \nempiriche, cioè consiste in un numero di variabili latenti inferiore alla \ndimensionalità delle misurazioni.\n11 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#11": "Autoencoders\nGli \nautoencoders\n  apprendono \n rappresentazioni latenti\n  dei dati senza \nl'impiego di approcci supervisionati.  \nInoltre possono essere impiegati per il \n unsupervised pretraining\n , e per \ngenerare casualmente nuovi dati \n che risultano simili a dati già visti in \nprecedenza (es. visi di persone), sebbene non sempre realistici.  \n•\nCon le più recenti \n Generative adversarial networks\n  (\nGANs\n ), che spesso \nincludono anche moduli di autoencoders, suddividono il processo in 2 parti: \ngenerazione e discrimazione. In questi casi il realismo è maggiore.  \n•\nhttps://thispersondoesnotexist.com/  \nhttps://thisrentaldoesnotexist.com/  \nhttps://github.com/jantic/DeOldify    \nInoltre le GAN possono incrementare la risoluzione delle immagini, \naggiungere colore alle immagini b/w, photo editing come rimpiazzare oggetti, \nconvertire un disegno in una foto realistica, predire il successo frame in un \nvideo, creare/incrementare dataset per l'addestramento, etc.\n12 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#12": "Rappresentazione dei dati ef\n ﬁ\nciente\nUn \nautoencoder\n  può avere una architettura piuttosto comune, es. MLP, dove \ninput e output hanno hanno medesima dimensione.  \n•\nNel seguente esempio l'\n hidden layer\n  consiste in 2 soli nodi, mentre l'\n output \ne\n l'input layer \n di 3 nodi. Lo scopo del \n decoder\n  (\noutput layer\n ) è ricostruire \nl'input a partire dalla rappresentazione creata dall'\n encoder\n .  \n•\nSe la rappresentazione dell'encoder ha meno dimensioni rispetto all'input, \nl'\nautoencoder\n  si chiama \n undercomplete\n . \n•\nLa\n loss di ricostruzione\n  valuta la differenza dell'output generato rispetto \nall'input.\n13 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#13": "Autoencoders\nSe l'\nautoencoder \n usa \nattivazioni lineari\n  e la \n MSE\n come funzione di costo, \nallora otteniamo un modello simile alla \n Principal Component Analysis \n (\nPCA\n). \nNel seguente esempio proiettiamo istanze da un dataset 3d in 2d.  \nfrom\n tensorflow \n import\n keras\nencoder = keras.models.Sequential([keras.layers.Dense(\n 2\n, input_shape=[\n 3\n])])\ndecoder = keras.models.Sequential([keras.layers.Dense(\n 3\n, input_shape=[\n 2\n])])\nautoencoder = keras.models.Sequential([encoder, decoder])\nautoencoder.\n compile\n(loss=\n\"mse\"\n, optimizer=keras.optimizers.SGD(lr=\n 0.1\n))\nPossiamo addestrarlo e impiegarlo per ottenere le rappresentazioni latenti:  \nhistory = autoencoder.fit(X_train, X_train, epochs=\n 20\n)\ncodings = encoder.predict(X_train)\n14 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#14": "Stacked Autoencoders\nGli \nstacked autoencoders\n  (o \ndeep autoencoders\n ) sono autoencoders \norganizzati su più layer, spesso in modo speculare.  \n•\nNon è mai consigliato creare autoencoders troppo complessi per non \npenalizzare il grado di generalizzazione su istanze in input non presenti nel \ndataset di training, su cui il modello non è capace di determinare attivazioni \nsigni\nﬁ\ncative.  \nNel caso \n MNIST\n  possiamo creare una architettura con 784 inputs, 100 nodi \nnel primo hidden layer, 30 nodi in quello centrale, un altro da 100 e l'output \nlayer.\n15 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#15": "Scaled Exponential Linear Units\nLa funzione di attivazione \n Scaled Exponential Linear Units\n  (\nSELU\n ) \nè simile alla \nELU ed è de\n ﬁ\nnita nel seguente modo:  \n \n \nLa SELU non si annulla per valore < 0, a differenza della ReLU.  \nSi può ipotizzare che la SELU implementi un ulteriore tipo di normalizzazione \n\"interna\" che supporti l'invarianza tra media e varianza tra i layer, oltre alle \ndue normalizzazioni già note:  \n•\nInput normalization (\n es. quando scaliamo i dati in ingresso)  \n•\nBatch normalization\nf\n(\nx\n)\n=\nλ\nx\n,\nif \nx\n>\n0\nf\n(\nx\n)\n=\nλ\nα\n(\ne\nx\n−\n1\n)\n altrimenti\n16\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#16": "Keras: Stacked Autoencoders\nL'implementazione è simile a una MLP. Nell'esempio usiamo una funzione di \nattivazione SELU, e invece delle MSE impieghiamo la \n binary cross-entropy\n  loss  \nper accelerare la convergenza.  \nstacked_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[\n 28\n, \n28\n]),\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n),\n    keras.layers.Dense(\n 30\n, activation=\n \"selu\"\n),\n])\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n, input_shape=[\n 30\n]),\n    keras.layers.Dense(\n 28\n * \n28\n, activation=\n \"sigmoid\"\n ),\n    keras.layers.Reshape([\n 28\n, \n28\n])\n])\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\nstacked_ae.\n compile\n(loss=\n\"binary_crossentropy\"\n ,\n                   optimizer=keras.optimizers.SGD(lr=\n 1.5\n))\nhistory = stacked_ae.fit(X_train, X_train, epochs=\n 10\n,\n                         validation_data=[X_valid, X_valid])\n17 (13) https://arxiv.org/pdf/1706.02515.pdf",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#17": "`\nPer comprendere se l'output è corretto è utile visualizzarlo. Nel caso del \ndataset Fashion MNIST si ha:  \ndef \nplot_image\n (\nimage\n):\n    plt.imshow(image, cmap=\n \"binary\"\n )\n    plt.axis(\n \"off\"\n)\ndef \nshow_reconstructions\n (\nmodel\n, \nn_images\n =\n5\n):\n    reconstructions = model.predict(X_valid[:n_images])\n    fig = plt.figure(figsize=(n_images * \n 1.5\n, \n3\n))\n    \nfor\n image_index \n in \nrange\n(n_images):\n        plt.subplot(\n 2\n, n_images, \n 1\n + image_index)\n        plot_image(X_valid[image_index])\n        plt.subplot(\n 2\n, n_images, \n 1\n + n_images + image_index)\n        plot_image(reconstructions[image_index])\nshow_reconstructions(stacked_ae)\n18 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#18": "t-Distributed Stochastic Neighbourh \nt-Distributed Stochastic Neighbourh Embedding (t-SNE)\n  è un algoritmo non \nsupervisionato usato per la visualizzazione dei dati.  \nBasato su una tecnica di riduzione della dimensionalità non lineare che punta \na raggruppare punti simili tra loro in spazi con poche dimensioni preservando \nla struttura dei dati originali.  \nSfrutta la distribuzione t-Student per il calcolo della similarità tra 2 punti nello \nspazio ridotto e sulla Kullback–Leibler divergence.  \nÈ poco affetto dagli outliers.\n19 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#19": "Visualizzare un dataset con Autoencoders\nSebbene esistano tecniche avanzate di riduzione della dimensionalità e la \nvisualizzazione dei dati, gli \n autoencoders\n  sono capaci di ridurre notevolmente \nle dimensioni di un dataset con molti istanze e molte features. Perciò \npossiamo sfruttarlo per generare input verso approcci più tradizionali, anche \ndi visualizzazione.  \nSfruttiamo \n t-distributed stochastic neighbor embedding\n  (\nt\n-\nSNE\n) implementato \nin Scikit-Learn per la visualizzazione 2d.  \nfrom\n sklearn.manifold \n import\n TSNE\nX_valid_compressed = stacked_encoder.predict(X_valid)\ntsne = TSNE()\nX_valid_2D = tsne.fit_transform(X_valid_compressed)\nplt.scatter(X_valid_2D[:, \n 0\n], X_valid_2D[:, \n 1\n], c=y_valid, s=\n 10\n, cmap=\n\"tab10\"\n)\n20 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#2": "Language Models: recenti sviluppi (1)\nELMo\n(6)\n Embeddings from Language Models: rappresentazioni contestuali \nottenute da un approccio \n Deep bidirectional language model (biLM) \naddestrato su larghi dataset testuali. Gli stati generati dalla rete sono associati \nai testi in modo da creare rappresentazioni latenti.   \nULMFiT\n(7)\n Universal Language Model Fine-tuning: basato su \n self-supervised \nlearning\n  con una architettura \n LSTM\n  a 3 layer dove sono richiesti meno dati (es. \n100 istanze) per l'addestramento sfruttando il transfer learning. State-of-the-art \nper la classi\n ﬁ\ncazione NLP.\n3(6) https://arxiv.org/abs/1802.05365  \n(7) https://arxiv.org/abs/1801.06146  \n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#20": "Fashion MNIST: Autoencoders e t-SNE\nFashion MNIST sottoposto a \n autoencoders\n  e \nt-SNE\n :\n21 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#21": "Unsupervised training con Stacked autoencoders\nSeguendo la \n ﬁ\nloso\nﬁ\na del \n transfer learning\n , possiamo addestrare un \nautoencoder\n  su un grande dataset di dati \n unlabeled\n , e \nriutilizzare i parametri \nottenuti nei primi layer \n con un dataset più limitato di dati \n labeled\n  nel task \nprincipale di interesse.  \n•\nDati \n unlabeled\n  si trovano facilmente sul web, es. immagini, testi, mentre i \ndati labeled sono molto preziosi poiché richiedono molte ore-uomo per \ncreare valori target.\n22 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#22": "Autoencoders: Tying weights (1)\nPer \nautoencoders\n  simmetrici è possibile creare \n vincoli\n  tra i valori dei parametri \nnei layer speculari (\n tying weights\n ) in modo da dimezzare i parametri da \nstimare durante l'apprendimento.  \nIn Keras costruiamo un Dense layer per impiegare i pesi di un layer \nprecedente; attenzione: occorre trasporli prima di impiegarli nella decodi\n ﬁ\nca. \nclass \nDenseTranspose\n (\nkeras\n.\nlayers\n.\nLayer\n):\n    \ndef \n__init__\n (\nself\n, \ndense\n, \nactivation\n =\nNone\n, **\nkwargs\n):\n        \n self\n.dense = dense\n        \n self\n.activation = keras.activations.get(activation)\n        super().\n __init__\n (**kwargs\n )\n    \ndef \nbuild\n(\nself\n, \nbatch_input_shape\n ):\n        \n self\n.biases = \n self\n.add_weight(name=\n \"bias\"\n, initializer=\n \"zeros\"\n,\n                                      shape=[\n self\n.dense.input_shape[\n -1\n]])\n        super().build(batch_input_shape\n )\n    \ndef \ncall\n(\nself\n, \ninputs\n):\n        z = tf.matmul(inputs, \n self\n.dense.weights[\n 0\n], \ntranspose_b\n =\nTrue\n)\n        \n return \nself\n.activation(z + \n self\n.biases)\n23 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#23": "Autoencoders: Tying weights (2)\nLa costruzione della rete impiega i nuovi layer per legarli con i precedenti:  \ndense_1 = keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n)\ndense_2 = keras.layers.Dense(\n 30\n, activation=\n \"selu\"\n)\ntied_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[\n 28\n, \n28\n]),\n    dense_1,\n    dense_2\n])\ntied_decoder = keras.models.Sequential([\n    DenseTranspose(dense_2, activation=\n \"selu\"\n),\n    DenseTranspose(dense_1, activation=\n \"sigmoid\"\n ),\n    keras.layers.Reshape([\n 28\n, \n28\n])\n])\ntied_ae = keras.models.Sequential([tied_encoder, tied_decoder])\n24 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#24": "Greedy layerwise training\nInvece di addestrare tutti i layer contemporaneamente possiamo farlo uno step \nalla volta, aggiungendo un layer dopo aver addestrato il precedente (\n greedy \nlayerwise training\n ). \nDopo il primo step codi\n ﬁ\nco tutto il dataset per mezzo del primo autoencoder \n(\nphase 1\n ). Uso il nuovo dataset per addestrare un secondo autoencoder (\n phase \n2\n). In\nﬁ\nne metto tutti i layer insieme (\n phase 3\n ) \n•\nTale tecnica è attualmente poco popolare a causa di tecniche più recenti.\n25 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#25": "Convolutional Autoencoders\nLe architetture di \n autoencoders\n  viste non sono adatte per le immagini, cioè \ninput con grandi dimensionalità.  \nLe \nconvolutional autoencoders\n  impieghiamo dei \n layer convoluzionali \n per \nridurre la dimensionalità\n  incrementando la profondità del modello durante la \ncodi\nﬁ\nca. \nIl \ndecoder\n  deve fare l'inverso: ridurre la profondità ed aumentare la risoluzione \n(\nupsampling\n ). Si impiegano \n transpose convolutional layers\n  che operano in \nmodo inverso alle convolution layer tradizionali.  \nEcco un esempio per Fashion MNIST:  \nconv_encoder = keras.models.Sequential([\n    keras.layers.Reshape([\n 28\n, \n28\n, \n1\n], input_shape=[\n 28\n, \n28\n]),\n    keras.layers.Conv2D(\n 16\n, kernel_size=\n 3\n, padding=\n \"same\"\n, activation=\n \"selu\"\n),\n    keras.layers.MaxPool2D(pool_size=\n 2\n),\n    keras.layers.Conv2D(\n 32\n, kernel_size=\n 3\n, padding=\n \"same\"\n, activation=\n \"selu\"\n),\n    keras.layers.MaxPool2D(pool_size=\n 2\n),\n    keras.layers.Conv2D(\n 64\n, kernel_size=\n 3\n, padding=\n \"same\"\n, activation=\n \"selu\"\n),\n    keras.layers.MaxPool2D(pool_size=\n 2\n)\n]\n)\n...\n26 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#26": "Convolutional Autoencoders\nconv_decoder = keras.models.Sequential([\n    keras.layers.Conv2DTranspose(\n 32\n, kernel_size=\n 3\n, strides=\n 2\n, padding=\n \"valid\"\n,\n                                 activation=\n \"selu\"\n,\n                                 input_shape=[\n 3\n, \n3\n, \n64\n]),\n    keras.layers.Conv2DTranspose(\n 16\n, kernel_size=\n 3\n, strides=\n 2\n, padding=\n \"same\"\n,\n                                 activation=\n \"selu\"\n),\n    keras.layers.Conv2DTranspose(\n 1\n, kernel_size=\n 3\n, strides=\n 2\n, padding=\n \"same\"\n,\n                                 activation=\n \"sigmoid\"\n ),\n    keras.layers.Reshape([\n 28\n, \n28\n])\n])\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n27 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#27": "Recurrent Autoencoders\nPer dati \n time series\n  abbiamo visto come le RNN sono una valida alternativa alle \nFC. \nI \nrecurrent autoencoder \n hanno un \n encoder\n  tipicamente \n sequence-to-vector \n che \n\"comprime\" l'input in una rappresentazione vettoriale, mentre il decoder è \nvector-to-sequence\n . \nIl seguente \n autoencoder\n  processa sequenze di qualsiasi lunghezza, con 28 \ndimensioni considerate per singolo step. L'input possono essere immagini \nFashion MNIST che saranno processate una riga di pixel alla volta.  \n•\nIl\n RepeatVector layer \n del decoder garantisce che l'input vector al decoder sarà \ninviato per intero ad ogni step.  \nrecurrent_encoder = keras.models.Sequential([\n    keras.layers.LSTM(\n 100\n, return_sequences=\n True\n, input_shape=[\n None\n, \n28\n]),\n    keras.layers.LSTM(\n 30\n)\n])\nrecurrent_decoder = keras.models.Sequential([\n    keras.layers.RepeatVector(\n 28\n, input_shape=[\n 30\n]),\n    keras.layers.LSTM(\n 100\n, return_sequences=\n True\n),\n    keras.layers.TimeDistributed(keras.layers.Dense(\n 28\n, activation=\n \"sigmoid\"\n ))\n])\nrecurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])\n28 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#28": "Denoising Autoencoders (1)\nNelle \n denoising autoencoders\n(14)\n in input abbiamo immagini a cui \naggiungiamo del rumore, e in output ci sono le versioni originali: stiamo \naddestrando la rete a \n rimuovere il rumore\n .  \n•\nIl rumore può essere gaussiano, oppure con un random switch-off degli input \n(es. tramite tecniche simili al Dropout).\n29 (14) https://jmlr.csail.mit.edu/papers/v11/vincent10a\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#29": "Denoising Autoencoders (2)\nCodi\nﬁ\nca e output con Fashion MNIST  \ndropout_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[\n 28\n, \n28\n]),\n    \nkeras.layers.Dropout(\n 0.5\n),\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n),\n    keras.layers.Dense(\n 30\n, activ\n                       “    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n),\n    keras.layers.Dense(\n 30\n, activation=\n \"selu\"\n)\n])\ndropout_decoder = keras.models.Sequential([\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n, input_shape=[\n 30\n]),\n    keras.layers.Dense(\n 28\n * \n28\n, activation=\n \"sigmoid\"\n ),\n    keras.layers.Reshape([\n 28\n, \n28\n])\n])\ndropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])\n30 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#3": "Self-supervised learning\nCon \nself-supervised learning  \nsi intende l'addestramento di una modello in assenza di \nun dataset suf\n ﬁ\ncientemente grande in termini di valori target riguardo il task di \ninteresse principale.  \nIn tali casi si sfruttano le stessi istanze presentate in input per creare dei nuovi target \nsecondari, distinti da quello iniziale, ma comunque af\n ﬁ\nni. In tal modo la rete può \nidenti\n ﬁ\ncare alcune features salienti impiegabili nel task principale (\n knowledge transfer \nprocess\n ).  \n•\nEsempi di task secondari sono\n : identi\n ﬁ\ncare relazioni sostantivo-verbo o frase-\naggettivo (dominio NLP); due immagini, una ruotata, ed aspettarsi l'angolo di \nrotazione in output alla rete (dominio immagini).  \nSebbene i dataset secondari siano più facili da costruire, non sono suf\n ﬁ\ncienti per \nrisolvere il problema principale. ma riducono il tempo totale necessario per la fase di \ntraining del task principale. Una ulteriore fase di addestramento su un dataset ridotto \n(es. quello disponibile inizialmente) rendono la rete ef\n ﬁ\ncace anche sul task di interesse \nprimario.  \nÈ fondamentale scegliere un task secondario che, durante l'addestramento, generi un \nsottoinsieme di features utili anche per il task principale.  \nAttenzione: è un approccio distinto dal \n unsupervised pretraining\n .\n4",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#30": "Sparse Autoencoders (1)\nNegli \n sparse autoencoders \n introduciamo un termine nella funzione di costo che \nfavorisce un numero limitato di nodi \"attivi\" nel layer di coding\n , cioè quello che \nè associato allo spazio che stiamo costruendo.  \n•\nIn altre parole, forziamo la rete a rappresentare ogni input con poche attivazioni, \ne perciò \n ogni nodo attivo rappresenterà un numero limitato di feature molto \nsigni\nﬁ\ncative\n . \nCon la funzione di attivazione \n sigmoid\n , che pone una sorta di vincolo sulle \ncodi\nﬁ\nche in [0,1], e con la \n ℓ\n1\n regularization\n  al layer di coding, otteniamo il \nseguente codice Keras:  \nsparse_l1_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[\n 28\n, \n28\n]),\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n),\n    keras.layers.Dense(\n 300\n, activation=\n \"sigmoid\"\n ),\n    \nkeras.layers.ActivityRegularization(l1=\n 1e-3\n)\n  # vedi lucido seguente\n])\nsparse_l1_decoder = keras.models.Sequential([\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n, input_shape=[\n 300\n]),\n    keras.layers.Dense(\n 28\n * \n28\n, activation=\n \"sigmoid\"\n ),\n    keras.layers.Reshape([\n 28\n, \n28\n])\n])\nsparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n31 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#31": "Sparse Autoencoders (2)\nIl layer \n ActivityRegularization\n  restituisce gli stessi input, ma ha l'effetto \ncollaterale di aggiungere una\n  training loss che coincide con la somma dei valori \nassoluti dei suoi input\n . In questo modo forziamo la rete, sia a produrre \ncodi\nﬁ\nche vicine allo 0\n , sia a ricostruire l'output corretto; perciò avremo \ncodi\nﬁ\nche con pochi valori, ma molto signi\n ﬁ\ncativi, diversi da 0.  \nUn modo alternativo è \n misurare una sorta di \"sparsity\" calcolata durante \nl'apprendimento\n  e, se si discosta da un valore target, \n penalizziamo la rete\n . La \nricaviamo con l'\n attivazione media\n  per ogni nodo nel coding layer nell'intero \ntraining batch, che avrà una dimensione suf\n ﬁ\nciente per stimare correttamente \ntali valori.  \nSuccessivamente introduciamo la \n sparsity loss\n  che penalizza i nodi troppo \nattivi, o i nodi non suf\n ﬁ\ncientemente attivi.  \n•\nEs. Se l'average per un nodo è 0.3, e la target sparsity è 0.1, penalizziamo in \nmodo da ridurre l'attivazione aggiungendo ad esempio l'\n errore quadratico  \n(0.3-0.1)2  alla funzione di cost.  \n•\nUna alternativa migliore è usare \n Kullback–Leibler (KL)\n , che deriva dei gradienti \npiù signi\n ﬁ\ncativi interpretando le 2 attivazioni come distribuzioni di probabilità.\n32 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#32": "Sparsity loss\nSparsity loss\n  ricavata con diverse metriche:\n33 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#33": "Keras: Sparse Autoencoders\nSparse autoencoders con la KL-divergence:  \nK = keras.backend\nkl_divergence = keras.losses.kullback_leibler_divergence\nclass \nKLDivergenceRegularizer\n (\nkeras\n.\nregularizers\n .\nRegularizer\n ):\n    \ndef \n__init__\n (\nself\n, \nweight\n, \ntarget\n=\n0.1\n):\n        \n self\n.weight = weight\n        \n self\n.target = target\n    \ndef \n__call__\n (\nself\n, \ninputs\n):\n        mean_activities = K.mean(inputs, axis=\n 0\n)\n        \n return \nself\n.weight * (\n            kl_divergence(\n self\n.target, mean_activities) +\n            kl_divergence(\n 1\n. - \nself\n.target, \n 1\n. - mean_activities))\n        \n..\n.\nkld_reg = KLDivergenceRegularizer(weight=\n 0.05\n, target=\n 0.1\n)\nsparse_kl_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[\n 28\n, \n28\n]),\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n),\n    keras.layers.Dense(\n 300\n, activation=\n \"sigmoid\"\n ,”\n                       activity_regularizer=kld_reg)\n])\nsparse_kl_decoder = keras.models.Sequential([\n    keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n, input_shape=[\n 300\n]),\n    keras.layers.Dense(\n 28\n * \n28\n, activation=\n \"sigmoid\"\n ),\n    keras.layers.Reshape([\n 28\n, \n28\n])\n])\nsparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n34 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#34": "Sparse Autoencoders e attivazioni\nDopo la fase di training su Fashion MNIST notiamo come circa il 70% delle \nattivazioni è prossima a 0, e che gran parte dei neuroni (90%) ha attivazione \nmedia tra 0.1 e 0.2.\n35 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#35": "Variational Autoencoders (1)\nI \nvariational autoencoders \n sono molto diffusi. Sono distinti dai precedenti \npoiché sono in parte \n probabilistici\n , cioè una parte dell'output è generato in \nmodo random, e \n generativi\n , cioè possono generare nuove istanze che \nsembrano campionate dal training set. \n36 (15) https://arxiv.org/abs/1312.6114\nHanno una architettura simile agli altri \nautoencoders, ma all'interno \nl'encoder produce un vettore \n codi\nﬁ\nca \nmedia  \n e un vettore \n deviazione \nstandard  \n.  \nL'\neffettiva codi\n ﬁ\nca\n della istanza in \ninput sarà generata per mezzo di una \ndistribuzione gaussiana con tali \nparametri.  \nDurante il training la loss tende a \nraggruppare le codi\n ﬁ\nche in modo da \ngenerare una \"nuvola gaussiana\" di \npunti.\nμ\nσ",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#36": "Variational Autoencoders (2)\nLa \nfunzione di costo\n  consiste in due loss: la \n reconstruction loss \n che impone \nche l'output sia fedele all'input, e una \n latent loss \n che spinge ad avere \ncodi\nﬁ\nche come se fossero campionate da una distribuzione gaussiana, de\n ﬁ\nnita \nper mezzo della KL-divergence.  \n37 (15) https://arxiv.org/abs/1312.6114\nLa forma analitica contiene dettagli \nper limitare l'informazione trasmessa \nal coding layer, ma sempli\n ﬁ\ncando si \nottiene la seguente \n latent loss\n : \nK \nè la dimensione delle codi\n ﬁ\nche e \n i \nindica l'i-esima componente della \ncodi\nﬁ\nca.  \n•\nImpiegando la log(\n ) invece di \n  si \nottiene \n più stabilità\n .\nσ\n σ\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#37": "Keras: Variational Autoencoders (1)\nIn Keras creiamo un \n Sampling\n  layer che campiona una istanza dalla distribuzione \ngaussiana  \nclass \nSampling\n (\nkeras\n.\nlayers\n.\nLayer\n):\n    \ndef \ncall\n(\nself\n, \ninputs\n):\n        mean, log_var = inputs\n        \n return\n K.random_normal(tf.shape(log_var)) * K.exp(log_var / \n 2\n) + mean\nNon avendo un modello strettamente sequenziale usiamo le \n Functional API\n  di Keras, \nadatte per architetture particolari, ad esempio con topologie non lineari, pesi condivisi \ntra layer, o input e output multipli.  \nNel codice del \n encoder\n  con Functional API notiamo la rappresentazione esplicita dei \ndati ricavati dalla rete (es. \n z, codings_mean, codings_log_var\n  etc):  \ncodings_size = 1\n 0\ninputs = keras.layers.Input(shape=[\n 28\n, \n28\n])\nz = keras.layers.Flatten()(inputs)\nz = keras.layers.Dense(\n 150\n, activation=\n \"selu\"\n)(z)\nz = keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n)(z)\ncodings_mean = keras.layers.Dense(codings_size)(z)  \n # \nμ\ncodings_log_var = keras.layers.Dense(codings_size)(z)  \n # \nγ\ncodings = Sampling()([codings_mean, codings_log_var])\nvariational_encoder = keras.Model(\n    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n38 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#38": "Keras: Variational Autoencoders (2)\nCreiamo il \n decoder\n , in questo caso potevamo usare anche l'approccio Keras \nSequential.  \ndecoder_inputs = keras.layers.Input(shape=[codings_size])\nx = keras.layers.Dense(\n 100\n, activation=\n \"selu\"\n)(decoder_inputs)\nx = keras.layers.Dense(\n 150\n, activation=\n \"selu\"\n)(x)\nx = keras.layers.Dense(\n 28\n * \n28\n, activation=\n \"sigmoid\"\n )(x)\noutputs = keras.layers.Reshape([\n 28\n, \n28\n])(x)\nvariational_decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs])\nCreiamo il modello e de\n ﬁ\nniamo la \n latent_loss\n  e \nreconstruction_loss\n , e \naddestriamo con \n RMSprop\n  optimizer, adatto in questa con\n ﬁ\ngurazione:  \n_, _, codings = variational_encoder(inputs)\nreconstructions = variational_decoder(codings)\nvariational_ae = keras.Model(inputs=[inputs], outputs=[reconstructions])\nlatent_loss = \n -0.5\n * K.\nsum\n(\n    \n1\n + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n    axis=\n -1\n)\nvariational_ae.add_loss(K.mean(latent_loss) / \n 784\n.)\nvariational_ae.\n compile\n(loss=\n\"binary_crossentropy\"\n , optimizer=\n \"rmsprop\"\n )\nhistory = variational_ae.fit(X_train, X_train, epochs=\n 50\n, batch_size=\n 128\n,\n                             validation_data=[X_valid, X_valid])\n39 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#39": "Generare immagini stile Fashion MNIST\nGeneriamo nuove istanze campionando in modo causale dalla distribuzione \ngaussiana:  \ncodings = tf.random.normal(shape=[\n 12\n, codings_size])\nimages = variational_decoder(codings).numpy(\n )\nLe istanze in output, un po' fuzzy, sono abbastanza verosimili:\n40 ...\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#4": "Self-supervised learning - esempio\nNel dominio delle immagini possiamo considerare come task secondari quelli \nil cui obiettivo è ricostruire una immagine manipolata, es. distorta, ruotata.  \nNel task \n patches\n , estraiamo più segmenti dall'immagine e cerchiamo di \ndeterminare la relazione tra essi (es. posizione relativa tra 2 segmenti).  \n•\nEs\n. prendiamo una patch \n X\n a caso dall'immagine, ed otteniamo le 8 patches \nche la circondano \n Y\ni\n. Usiamo coppie di patches (\n X,Y\ni\n) come input per \ndeterminare la posizione della patch \n Y\ni\n. Oppure diamo in input tutte le 9 \npatches in ordine random e chiediamo alla rete di \"risolvere il puzzle\".\n5\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#40": "Variational Autoencoders e semantic interpolation\nInvece di interpolare due immagini nel \"dominio dei pixel\", possiamo \ninterpolare nel dominio latente \n costruito dall'\n autoencoder\n . \n•\nDeriviamo la rappresentazione dal coding layer di due immagini in input, \ninterpoliamole e decodi\n ﬁ\nchiamole.  \ncodings_grid = tf.reshape(codings, [\n 1\n, \n3\n, \n4\n, codings_size])\nlarger_grid = tf.image.resize(codings_grid, size=[\n 5\n, \n7\n])\ninterpolated_codings = tf.reshape(larger_grid, [\n -1\n, codings_size])\nimages = variational_decoder(interpolated_codings).numpy()\n41 ...\nLe immagini nel box sono \nquelle originali, quelle \nsenza sono l'interpolazione \ndi quelle adiacenti.",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#5": "Language Models: recenti sviluppi (2)\nGPT\n(8)\n sfrutta il \n unsupervised pretraining\n  con una  architettura \n Transform\n  con \nstack di 12 moduli, addestrata su un dataset esteso con tecniche \n self-\nsupervised\n .  \n•\nNecessita di un \n ﬁ\nne-tuned training per impiegarla su task speci\n ﬁ\nci (es. \nclassi\n ﬁ\ncazione, misura di similarità, question answering).  \n•\nGPT-2\n(9)\n è una versione estesa con 1.5 miliardi di parametri, con modelli \ndisponibili online.  \n•\nGPT-3\n  è estesa a 175 miliardi di parametri. Le APIs sono disponibili ma \nl'accesso è previa veri\n ﬁ\nca.\n6(8) https://bit.ly/38FfBQ7  \n(9) https://bit.ly/3oGLu0i   https://github.com/openai/gpt-2",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#6": "Language Models: recenti sviluppi (3)\nGPT-2 output\n7(8) https://bit.ly/38FfBQ7  \n(9) https://bit.ly/3oGLu0i   https://github.com/openai/gpt-2\n",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#7": "Language Models: recenti sviluppi (4)\nBERT\n(10)\n Bidirectional Encoder Representations from Transformers: sviluppata \nda Google, simile alla GPT, impiega \n self-supervised pretraining\n  con approccio \nbidirezionale. È stata pre-addestrata su due task:  \n•\nOgni parola in una frase ha il 15% di probabilità di essere \n mascherata\n . Il \ncompito della rete è di indovinarla.  \n•\nDate due frasi, predire se sono consecutive.  \nAltri approcci recenti usano CNN con \n masked 2d-conv\n  per il task di \ntrasformazione sequence-to-sequence\n(11)\n, o RNN dove ogni nodo risulta \nindipendente dall'altro, garantendo apprendimenti su sequenze molto più \nlunghe\n(12)\n.\n8(10) https://arxiv.org/abs/1810.04805  \n(11) https://arxiv.org/abs/1808.03867(12) https://arxiv.org/abs/1803.04831  ",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#8": "Rappresentazione dei dati ef\n ﬁ\nciente\nRiusciresti a memorizzare queste due sequenze?  \n•\n40, 27, 25, 36, 81, 57, 10, 73, 19, 68  \n•\n50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14\n9 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\15-Attention e Autoencoders 2-sbloccato.pdf#9": "Rappresentazione dei dati ef\n ﬁ\nciente\nRiusciresti a memorizzare queste due sequenze?  \n•\n40, 27, 25, 36, 81, 57, 10, 73, 19, 68  \n•\n50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14  \nSe riconosci il pattern \"tutti i numeri pari dal 50 al 14\" ti sarà più facile \nricordare la seconda.\n10 ...",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nGenerative Adversarial Networks\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#1": "Sommario\nApprendimento discriminativo e generativo  \nGeneratie adversarial networks (GANs)  \nDeep Convolutional Generative Adversarial Networks",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#10": "GAN e Keras: Generatore e Discriminatore\n   \nfor\n epoch \nin \nrange\n(num_epochs):\n        timer = d2l.Timer()\n        metric = d2l.Accumulator(\n 3\n)  \n# loss_D, loss_G, num_examples\n        \n for\n (X,) \nin\n data_iter:\n            batch_size = X.shape[\n 0\n]\n            Z = tf.random.normal(\n                mean=\n 0\n, stddev=\n 1\n, shape=(batch_size, latent_dim))\n            metric.add(update_D(X, Z, net_D, net_G, loss, optimizer_D),\n                       update_G(Z, net_D, net_G, loss, optimizer_G),\n                       batch_size)\n        \n # Visualizzazione dei dati generati\n        Z = tf.random.normal(mean=\n 0\n, stddev=\n 1\n, shape=(\n 100\n, latent_dim))\n        fake_X = net_G(Z)\n        animator.axes[\n 1\n].cla()\n        animator.axes[\n 1\n].scatter(data[:, \n 0\n], data[:, \n 1\n])\n        animator.axes[\n 1\n].scatter(fake_X[:, \n 0\n], fake_X[:, \n 1\n])\n        animator.axes[\n 1\n].legend([\n \"real\"\n, \n\"generated\"\n ])\n        \n # Visualizzazione delle loss\n        loss_D, loss_G = metric[\n 0\n] / metric[\n 2\n], metric[\n 1\n] / metric[\n 2\n]\n        animator.add(epoch + \n 1\n, (loss_D, loss_G))\n    \nprint\n(\nf\n'loss_D \n {loss_D\n:.3f\n}\n, loss_G \n {loss_G\n:.3f\n}\n, '\n          \n f\n'\n{metric[\n 2\n] / timer.stop()\n :.1f\n}\n examples/sec'\n )\n11",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#11": "GAN e Keras: Generatore e Discriminatore\nSpeci\n ﬁ\nchiamo gli iperparametri per fare \n ﬁ\ntting di una distribuzione gaussiana:  \nlr_D, lr_G, latent_dim, num_epochs = \n 0.05\n, \n0.005\n, \n2\n, \n20\ntrain(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\n      latent_dim, data[:\n 100\n].numpy())\n> \nloss_D \n0.693\n, loss_G \n 0.693\n, \n333.2\n examples/sec\n12\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#12": " Deep Convolutional Generative Adversarial \nNelle GAN abbiamo visto come i campioni generati provengano perloppiù da \ndistribuzioni uniformi o normali, che sono poi trasformati in istanze che \ncorrispondono alle distribuzioni di un certo dataset di dati reali.  \nPer generare campioni più complessi (es. immagini fotorealistiche) sono \nnecessarie architetture più complesse come le \n Deep Convolutional GANs \n(DCGAN)\n . \n13",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#13": "DCGAN - Keras (1)\nIl dataset è la collezione di Pokemon sprites, ottenuto da pokemondb.  \nimport\n tensorflow \n as\n tf\n!\npip install d2l==\n 1.0.0\na1.post0\nfrom\n d2l \nimport\n tensorflow \n as\n d2\nl\nd2l.DATA_HUB[\n 'pokemon'\n ] = (d2l.DATA_URL + \n 'pokemon.zip'\n ,\n                           \n 'c065c0e2593b8b161a2d7873e42418bf6a21106c'\n )\ndata_dir = d2l.download_extract(\n 'pokemon'\n )\nbatch_size = \n 256\npokemon = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir, batch_size=batch_size, image_size=(\n 64\n, \n64\n)\n)\nRidimensiono ogni immagini in 64x64. I valori sono in [0,1], mentre il generatore usa la \n tanh\n e \ngenera campioni con pixel in [-1,1]. Normalizziamo i dati con media e deviazione standard pari \na 0.5.  \ndef \ntransform_func\n (\nX\n):\n    X = X / \n 255\n.\n    X = (X - \n 0.5\n) / (\n0.5\n)\n    \nreturn\n X\ndata_iter = pokemon.\n map\n(\nlambda\n x, y: (transform_func(x), y),\n                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndata_iter = data_iter.cache().shuffle(buffer_size=\n 1000\n).prefetch(\n    buffer_size=tf.data.experimental.AUTOTUNE)\n14",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#14": "DCGAN - Keras (2)\nd2l.set_figsize(figsize=(\n 4\n, \n4\n))\nfor\n X, y \nin\n data_iter.take(\n 1\n):\n    imgs = X[:\n 20\n, :, :, :] / \n 2\n + \n0.5\n    d2l.show_images(imgs, num_rows=\n 4\n, num_cols=\n 5\n)\nIl generatore deve mappare una \n noise variabile  \n e un vettore di dimensione \n d\n ad una  \nimmagine 64x64.  \nUsiamo la \n Conv2DTranspose\n  per incrementare  \nla risoluzione in input, seguita da una \n batch normalization  \ne attivazione ReLU.  \nclass \nG_block\n(\ntf\n.\nkeras\n.\nlayers\n.\nLayer\n):\n    \ndef \n__init__\n (\nself\n, \nout_channels\n , \nkernel_size\n =\n4\n, \nstrides\n=\n2\n, \npadding\n=\n\"same\"\n,\n                 **\n kwargs\n):\n        super().\n __init__\n (**kwargs)\n        \n self\n.conv2d_trans = tf.keras.layers.Conv2DTranspose(\n            out_channels, kernel_size, strides, padding, use_bias=\n False\n)\n        \n self\n.batch_norm = tf.keras.layers.BatchNormalization()\n        \n self\n.activation = tf.keras.layers.ReLU()\n    \ndef \ncall\n(\nself\n, \nX\n):\n        \n return \nself\n.activation(\n self\n.batch_norm(\n self\n.conv2d_trans(X)))\nz\n∈\nℝ\nd\n15\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#15": "DCGAN - Keras (3)\nDi default abbiamo\n  kernel 4x4, stride 2x2 e same padding.  \nCon un input 16x16, il generatore raddoppia larghezza e altezza dell'input.  \nx = tf.zeros((\n 2\n, \n16\n, \n16\n, \n3\n))  \n# creiamo un dato sintetico\ng_blk = G_block(\n 20\n)\ng_blk(x).shape\n> TensorShape([2, 32, 32, 20]\n )\nSe usiamo un kernel 4x4, stride 1x1 e zero padding, con un input 1x1, l'output avrà \nlarghezza e altezza incrementati di 3.  \nx = tf.zeros((2, 1, 1, 3)\n )\n# padding=\"valid\" corresponds to no padding\ng_blk = G_block(\n 20\n, strides=\n 1\n, padding=\n \"valid\"\n)\ng_blk(x).shape\n> TensorShape([2, 4, 4, 20])\n16\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#16": "DCGAN - Keras (4)\nIl \ngeneratore\n  consiste in 4 blocchi base che incrementano ampiezza e altezza \nda 1 a 32. Inizialmente mappa le variabili latenti in 64x8 canali, e poi \ndimezza i canali ogni volta. In\n ﬁ\nne, un\n  transposed convolution layer \n genera \nl'output raddoppiando la dimensione a 64x64 e riducendo i canali a 3 per \nrispettare l'output desiderato. La funzione di attivazione \n tanh\n è usata per \ngenerare output in (-1,1).  \nn_G = \n64\nnet_G = tf.keras.Sequential([\n    \n# Output: (4, 4, 64 * 8)\n    G_block(out_channels=n_G*\n 8\n, strides=\n 1\n, padding=\n \"valid\"\n),\n    G_block(out_channels=n_G*\n 4\n), \n# Output: (8, 8, 64 * 4)\n    G_block(out_channels=n_G*\n 2\n), \n# Output: (16, 16, 64 * 2)\n    G_block(out_channels=n_G), \n # Output: (32, 32, 64)\n    \n# Output: (64, 64, 3)\n    tf.keras.layers.Conv2DTranspose(\n        \n 3\n, kernel_size=\n 4\n, strides=\n 2\n, padding=\n \"same\"\n, use_bias=\n False\n,\n        activation=\n \"tanh\"\n)\n])\n17",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#17": "Richiami: Leaky ReLU\nLa leaky ReLU è  utile per affrontare il dying ReLU.  \nalphas = [\n 0\n, \n.2\n, \n.4\n, \n.6\n, \n.8\n, \n1\n]\nx = tf.\nrange\n(\n-2\n, \n1\n, \n0.1\n)\nY = [tf.keras.layers.LeakyReLU(alpha)(x).numpy() \n for\n alpha \nin\n alphas]\nd2l.plot(x.numpy(), Y, \n 'x'\n, \n'y'\n, alphas)\n18\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#18": "DCGAN - Keras (5)\nIl blocco base del discriminatore è un convolution layer seguito da \n batch \nnormalization\n  (tranne per l'input layer) e leaky ReLU activation. Gli \niperparametri sono simili a quelli impiegati al blocco generatore.  \nclass \nD_block\n(\ntf\n.\nkeras\n.\nlayers\n.\nLayer\n):\n    \ndef \n__init__\n (\nself\n, \nout_channels\n , \nkernel_size\n =\n4\n, \n                 \n strides\n=\n2\n, \npadding\n=\n\"same\"\n, \nalpha=\n0.2\n, **kwargs):\n        super().\n __init__\n (**kwargs)\n        \n self\n.conv2d = tf.keras.layers.Conv2D(out_channels, kernel_size,\n                                             strides, padding,  \n                                      use_bias=\n False\n)\n        \n self\n.batch_norm = tf.keras.layers.BatchNormalization()\n        \n self\n.activation = tf.keras.layers.LeakyReLU(alpha)\n    \ndef \ncall\n(\nself\n, \nX\n):\n        \n return \nself\n.activation(\n self\n.batch_norm(\n self\n.conv2d(X))\n )\nx = tf.zeros((\n 2\n, \n16\n, \n16\n, \n3\n))\nd_blk = D_block(\n 20\n)\nd_blk(x).shape\n> TensorShape([2, 8, 8, 20])\n19",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#19": "DCGAN - Keras (6)\nPer un input shape 16x16 e un kernel 4x4 e stride 2 e same padding:  \nn_D = \n64\nnet_D = tf.keras.Sequential([\n    D_block(n_D), \n # Output: (32, 32, 64)\n    D_block(out_channels=n_D*\n 2\n), \n# Output: (16, 16, 64 * 2)\n    D_block(out_channels=n_D*\n 4\n), \n# Output: (8, 8, 64 * 4)\n    D_block(out_channels=n_D*\n 8\n), \n# Outupt: (4, 4, 64 * 64)\n    \n# Output: (1, 1, 1)\n    tf.keras.layers.Conv2D(\n 1\n, kernel_size=\n 4\n, use_bias=\n False\n)\n]\n)\nx = tf.zeros((\n 1\n, \n64\n, \n64\n, \n3\n))\nnet_D(x).shape\n> TensorShape([1, 1, 1, 1]) # Predizione: Singolo valore\n20\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#2": "Apprendimento discriminativo e generativo\nFinora abbiamo trattato il \n discriminative learning\n , cioè un apprendimento \ncapace di distinguere le differenti istanze, cioè le relative caratteristiche, e \nclassi\n ﬁ\ncarle, o fare predizione. In molti task arriviamo a livelli di accuratezza \ncomparabili a quelli umani.  \nCi sono altri casi in cui vogliamo analizzare grossi dataset senza labels per \ncreare un modello che ne descrive le caratteristiche. Con tale modello \npossiamo creare esempi di dato \n sintetici\n  che assomigliano a quelli reali. Tale \napproccio si chiama \n generative learning\n . \n•\nEs. le reti ricorrenti sono un esempio di un modello discriminativo che può \nessere impiegato anche per generare nuove istanze.\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#20": "Richiami: Adam Optimization\nAdam (Adaptive Moment Estimation)  \nè una combinazione di Momentum \noptimization e RMSProp\n . \n \n \n \n \n \ndove \n T\n indica l'iterazione corrente  \nRispetto al Momentum, nella prima espressione si introduce il decay dei gradienti \ncon  \nLa 3\na\n e 4\na\n espressione sono utili per incrementare il valore di \n m\n ed \ns\n all'inizio del \ntraining, essendo i valori iniziali pari a 0.\nm\n←\nβ\n1\nm\n+\n(\n1\n−\nβ\n1\n)\n∇\nΘ\nJ\n(\nΘ\n)\ns\n←\nβ\n2\ns\n+\n(\n1\n−\nβ\n2\n)\n∇\nΘ\nJ\n(\nΘ\n)\n⊗\n∇\nΘ\nJ\n(\nΘ\n)\nm\n←\nm\n1\n−\nβ\nT\n1\ns\n←\ns\n1\n−\nβ\nT\n2\nΘ\n←\nΘ\n−\nη\n⋅\nm\n⊘\ns\n+\nϵ\nβ\n1\n21Momentum :  \n \nRMSProp :  \n \n \n \n \n \nRMSProp : m←β⋅m+η∇ΘJ(Θ)\ns←βs+(1−β)∇ΘJ(Θ)⊗∇ΘJ(Θ)\nΘ←Θ−η∇ΘJ(Θ)⊘s+ϵ",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#21": "DCGAN - Keras (7)\nRispetto alle GAN usiamo lo stesso \n learning rate\n  per il generatore e \ndiscriminatore essendo architetture simili. Portiamo \n  in Adam da 0.9 (spesso \nusato come default) a 0.5.  \nDecrementiamo lo \n smooth\n  del momentum, la weighted moving average \nesponenziale dei passati gradienti, per tenere traccia più puntuale delle \nvariazioni rapide dei gradienti a causa dell'instabilità creata dal discriminatore-\ngeneratore.  \ndef \ntrain\n(\nnet_D\n, \nnet_G\n, \ndata_iter\n , \nnum_epochs\n , \nlr\n, \nlatent_dim\n ,\n          \n device\n=d2l.try_gpu()):\n    loss = tf.keras.losses.BinaryCrossentropy(\n        from_logits=\n True\n, reduction=tf.keras.losses.Reduction.SUM)\n    \nfor\n w \nin\n net_D.trainable_variables:\n        w.assign(tf.random.normal(mean=\n 0\n, stddev=\n 0.02\n, shape=w.shape))\n    \nfor\n w \nin\n net_G.trainable_variables:\n        w.assign(tf.random.normal(mean=\n 0\n, stddev=\n 0.02\n, shape=w.shape))\n    optimizer_hp = {\n \"lr\"\n: lr, \n\"beta_1\"\n : \n0.5\n, \n\"beta_2\"\n : \n0.999\n}\n    optimizer_D = tf.keras.optimizers.Adam(**optimizer_hp)\n    optimizer_G = tf.keras.optimizers.Adam(**optimizer_hp)\nβ\n1\n22",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#22": "DCGAN - Keras (8)\n    animator = d2l.Animator(xlabel=\n 'epoch'\n, ylabel=\n 'loss'\n,\n                          xlim=[\n 1\n, num_epochs], nrows=\n 2\n, figsize=(\n 5\n, \n5\n),\n                          legend=[\n 'discriminator'\n , \n'generator'\n ]\n)\n    animator.fig.subplots_adjust(hspace=\n 0.3\n)\n    \nfor\n epoch \nin \nrange\n(\n1\n, num_epochs + \n 1\n):\n \n       timer = d2l.Timer()\n        metric = d2l.Accumulator(\n 3\n) \n# loss_D, loss_G, num_examples\n        \n for\n X, _ \nin\n data_iter:\n            batch_size = X.shape[\n 0\n]\n            Z = tf.random.normal(mean=\n 0\n, stddev=\n 1\n,\n                                 shape=(batch_size, \n 1\n, \n1\n, latent_dim))\n            metric.add(d2l.update_D(X, Z, net_D, net_G, loss,  \n                                                           optimizer_D),\n                       d2l.update_G(Z, net_D, net_G, loss, optimizer_G),\n                       batch_size)\n23",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#23": "DCGAN - Keras (9)\n        \n # Visualizziamo i dati generti\n        Z = tf.random.normal(mean=\n 0\n, stddev=\n 1\n, \n                                        shape=(\n 21\n, \n1\n, \n1\n, latent_dim))\n        \n # Normalizziamo i dati generati in (0,1)\n        fake_x = net_G(Z) / \n 2\n + \n0.5\n        imgs = tf.concat([tf.concat([fake_x[i*\n 7\n+j] \nfor\n j \nin \nrange\n(\n7\n)],\n                                    axis=\n 1\n)\n                          \n for\n i \nin \nrange\n(\nlen\n(fake_x) // \n 7\n)], axis=\n 0\n)\n        animator.axes[\n 1\n].cla()\n        animator.axes[\n 1\n].imshow(imgs)\n        \n # Visualizziamo le loss\n        loss_D, loss_G = metric[\n 0\n] / metric[\n 2\n], metric[\n 1\n] / metric[\n 2\n]\n        animator.add(epoch, (loss_D, loss_G)\n )\n    \nprint\n(\nf\n'loss_D \n {loss_D\n:.3f\n}\n, loss_G \n {loss_G\n:.3f\n}\n, '\n          \n f\n'\n{metric[\n 2\n] / timer.stop()\n :.1f\n}\n examples/sec on  \n                                      \n {\nstr\n(device._device_name)}\n '\n)\n24",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#24": "DCGAN - Keras (10)\nlatent_dim, lr, num_epochs = \n 100\n, \n0.0005\n, \n40\ntrain(net_D, net_G, data_iter, num_epochs, lr, latent_dim)\n> loss_D 0.217, loss_G 3.687, 2310.5 examples/sec on /GPU:\n 0\n25\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#3": "Generative adversarial networks\nNel 2014 sono state introdotte le \n Generative adversarial networks (GAN)\n , un \nmodello che sfrutta un approccio \n discriminativo\n  per generare modelli \ngenerativi:  \n•\nL'idea è che un modello generativo è buono se non riusciamo a distinguere i \ndati generati da quelli reali.  \nDal punto di vista statistico corrisponde ad un \n 2-sample test\n : misurare se due \nsequenze di istanze \n X\n= {\nx\n1\n, ..., \nx\nn\n} e \nX'\n= {\nx'\n1\n, ..., \nx'\nn\n} sono ottenute dalla stessa \ndistribuzione.  \nNelle GAN tale test è usato dal modello generativo per adattarsi a creare \nistanze sempre più simili ai casi reali.  \n•\nIn pratica, cerchiamo di \"bidonare\" il classi\n ﬁ\ncatore reale/fake. \n4",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#4": "Generative adversarial networks (GANs)\nL'architettura include un \n generative network\n , nel nostro caso una deep \nnetwork, che ha lo scopo di generare istanze simili a quelle reali (es. segnale \ndi voce umana, immagini di visi). Il \n discriminative network\n  cerca di \ndistinguere dati reali da quelli generate (o fake).  \nIl \ndiscriminator\n  è implementato come un classi\n ﬁ\ncatore binario che produce \nuno scalare per ogni input \n x \n(es. una FC con 1 layer e funzione di attivazione \nsigmoid\n  per convertire lo scalare in probabilità). Assumiamo che la label \ncorrispondente sia 1 per una istanza da dati reali, 0 per una fake creata dal \ngeneratore.  \nIl generatore mira a generare una immagine più  \nvicina possibile alle immagini reali, e per ottenere  \ndal discriminatore il relativo output corrispondente  \na \"il dato e' real\".\n5\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#5": "Generative adversarial networks (GANs)\nIl \ndiscriminatore\n  mira a distinguere immagini generate da immagini reali minimizzando \nla \ncross-entropy loss:  \n \ndove \n D\n(x) = 1/(1 + \n e\n-o\n) è la probabilità ottenuta con la \n sigmoid\n  a partire dallo scalare \n o.  \nCollezionando in modo casuale \n  (es. con distribuzione normale), dove \n  riveste il \ncompito di \n variabile latente,\n  il compito del \n generatore\n  è di bidonare il discriminatore per \nclassi\n ﬁ\ncare \n x'=G(z)\n  come \"dato reale\", cioè vogliano D(G(z)) ≈ 1.  \nIn altre parole, dato un discriminatore D, aggiorniamo i parametri del generatore per \nmassimizzare la \n cross-entropy loss \n quando y=0, cioè \n dato fake\n : \n \nSe il generatore si comporta in modo ottimale, D(x') è circa 1, cosicché la loss è vicina \nallo 0, e perciò i gradienti sono assai ridotti per generare progressi per il discriminatore. \nMinimizzeremo perciò la seguente loss:  \n \nche è il feed x'=G(z) nel discriminatore dando il label 1.  \nmin\n D\n{\n−\ny\n log \nD\n(\nx\n)\n−\n(\n1\n−\ny\n)\n log\n(\n1\n−\nD\n(\nx\n)\n)\n}\nz\n∈\nℝ\nD\nz\nmax\n G\n{\n−\n(\n1\n−\ny\n)\n log \n(\n1\n−\nD\n(\nG\n(\nz\n)\n)\n)\n}\n=\nmax\n G\n{\n−\n log \n(\n1\n−\nD\n(\nG\n(\nz\n)\n)\n)\n}\nmin\n G\n{\n−\ny\n log \n(\nD\n(\nG\n(\nz\n)\n)\n)\n}\n=\nmin\n G\n{\n−\n log \n(\nD\n(\nG\n(\nz\n)\n)\n)\n}\n6",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#6": "GAN e Keras (1)\nGeneriamo dati con un modello gaussiano:  \nX = tf.random.normal((1000, 2), 0.0, 1\n )\nA = tf.constant([[1, 2], [-0.1, 0.5]]\n )\nb = tf.constant([1, 2], tf.float32\n )\ndata = tf.matmul(X, A) + \n b\nOtteniamo una gaussiana traslata in modo arbitrario con media \n b\n e covarianza \nA\nT\nA\n. \nd2l.set_figsize(\n )\nd2l.plt.scatter(data[:100, 0].numpy(), data[:100, 1].numpy())\n ;\nprint(f'The covariance matrix is\\n{tf.matmul(A, A, transpose_a=True)}'\n )\n> The covariance matrix i\n s\n> [[1.01 1.95\n ]\n> [1.95 4.25]\n ]\nbatch_size = \n 8\ndata_iter = d2l.load_array((data,), batch_size)\n7\n",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#7": "GAN e Keras: Generatore e Discriminatore\nIl generatore è un semplice layer lineare.  \nnet_G = tf.keras.layers.Dense(2\n )\nPer il discriminatore usiamo una MLP a 3 layer:  \nnet_D = tf.keras.models.Sequential(\n [\n    tf.keras.layers.Dense(5, activation=\"tanh\", input_shape=(2,))\n ,\n    tf.keras.layers.Dense(3, activation=\"tanh\")\n ,\n    tf.keras.layers.Dense(1\n )\n]\n)\nDe\nﬁ\nniamo una funzione per aggiornare il \n discriminatore\n : \ndef \nupdate_D\n (\nX\n, \nZ\n, \nnet_D\n, \nnet_G\n, \nloss\n, \noptimizer_D\n ):\n    batch_size = X.shape[\n 0\n]\n    ones = tf.ones((batch_size,)) \n # Labels corrispondenti ai dati reali\n    zeros = tf.zeros((batch_size,)) \n # Labels corrispondenti ai dati fake\n    \n# Ignoriamo i gradienti per `net_G` all'interno di GradientTape\n    fake_X = net_G(Z)\n    \nwith\n tf.GradientTape() \n as\n tape:\n        real_Y = net_D(X)\n        fake_Y = net_D(fake_X)\n        loss_D = (loss(ones, tf.squeeze(real_Y)) + loss(\n            zeros, tf.squeeze(fake_Y))) * batch_size / \n 2\n    grads_D = tape.gradient(loss_D, net_D.trainable_variables)\n    optimizer_D.apply_gradients(\n zip\n(grads_D, net_D.trainable_variables))\n    \nreturn\n loss_D\n8",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#8": "GAN e Keras: Generatore e Discriminatore\nIl generatore sarà aggiornato in modo simile. Riutilizziamo la cross-entropy \nloss ma cambiamo la label dei dati fake da 0 a 1  \ndef \nupdate_G\n (\nZ\n, \nnet_D\n, \nnet_G\n, \nloss\n, \noptimizer_G\n ):\n    batch_size = Z.shape[\n 0\n]\n    ones = tf.ones((batch_size,))\n    \nwith\n tf.GradientTape() \n as\n tape:\n        fake_X = net_G(Z)\n \n       fake_Y = net_D(fake_X)\n       loss_G = loss(ones, tf.squeeze(fake_Y)) * batch_size\n    grads_G = tape.gradient(loss_G, net_G.trainable_variables)\n    optimizer_G.apply_gradients(\n zip\n(grads_G, net_G.trainable_variables))\n    \nreturn\n loss_G\nSia il discriminatore sia il generatore operano una\n  logistic regression binaria \ncon cross-entropy loss. Usiamo \n Adam\n  per rendere smooth il processo di \ntraining. Ad ogni iterazione, prima aggiorniamo il discriminatore e poi il \ngeneratore. \n9",
    "data_test\\rootfolder\\università\\DeepLearning\\16-GAN-sbloccato.pdf#9": "GAN e Keras: Generatore e Discriminatore\ndef \ntrain\n(\nnet_D\n, \nnet_G\n, \ndata_iter\n , \nnum_epochs\n , \nlr_D\n, \nlr_G\n, \nlatent_dim\n , \ndata\n):\n    loss = tf.keras.losses.BinaryCrossentropy(\n        from_logits=\n True\n, reduction=tf.keras.losses.Reduction.SUM)\n    \nfor\n w \nin\n net_D.trainable_variables:\n        w.assign(tf.random.normal(mean=\n 0\n, stddev=\n 0.02\n, shape=w.shape))\n    \nfor\n w \nin\n net_G.trainable_variables:\n        w.assign(tf.random.normal(mean=\n 0\n, stddev=\n 0.02\n, shape=w.shape))\n    optimizer_D = tf.keras.optimizers.Adam(learning_rate=lr_D)\n    optimizer_G = tf.keras.optimizers.Adam(learning_rate=lr_G)\n    animator = d2l.Animator(\n        xlabel=\n \"epoch\"\n, ylabel=\n \"loss\"\n, xlim=[\n 1\n, num_epochs], nrows=\n 2\n,\n        figsize=(\n 5\n, \n5\n), legend=[\n \"discriminator\"\n , \n\"generator\"\n ])\n    animator.fig.subplots_adjust(hspace=\n 0.3\n)\n   ...\n10",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nRecommender Systems\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#1": "Sommario\nMovieLens dataset  \nAutoRec  \nImplicit feedback (richiami)ù  \nNeural Collaborative Filtering (NCF)  \nCaser e Sequence-aware Recsys  \nFactorization Machines e Deep FM",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#10": "Neural Collaborative Filtering for Personalized Ranking\nL'output del penultimo layer di entrambe le reti è concatenato è dato in input \nal NeuMF layer:\n11\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#11": "Sequence-aware recommender systems\nSpesso gli utenti operano una sequenza di azioni nei servizi online il cui \nordine temporale può essere signi\n ﬁ\ncativo nel processo di raccomandazione.  \nIl modello \n Caser \n (Convolutional sequence embedding recommendation \nmodel) sfrutta le CNN, in particolare \n horizontal\n  e \nvertical\n  convolutional \nnetworks, per identi\n ﬁ\ncare rispettivamente pattern sequenziali \n union-level\n  e \npoint-level\n  di tipo short-term.  \nI pattern \n point-level\n  identi\n ﬁ\ncano l'in\n ﬂ\nuenza di un item all'interno di una \nsequenza verso un certo item target. L'union-level analizza l'in\n ﬂ\nuenza di varie \nazioni fatte sul valore target (es. l'acquisto di latte e burro può implicare \nl'acquisto di farina).  \nI bisogni di lungo termine sono rappresentati nei layer FC \n ﬁ\nnali.\n12",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#12": "Sequence-aware recommender systems\nSupponiamo che ogni utente u sia associato ad una sequenza di items \n. Se prendiamo i passati \n L\n items, possiamo costruire una \nmatrice che rappresenta le interazioni passati con tali items rispetto al time \nstep \nt\n. \nDove \n  rappresenta sempre lo spazio di embedding e \n q\ni\n indica la \n i\n-\nma riga. \n  è usata per ottenere i bisogni transienti dell'utente \n u\n per \nil time step \n t\n, ed è l'input per i successivi convolutional layers:  \n•\nL'horizontal layer ha \n d \nﬁ\nltri orizzontali \n . \n•\nIl vertical layer ha d' \n ﬁ\nltri verticali  \nDopo una serie di operatori convoluzionali e di pooling otteniamo gli output \n e \n :\nS\nu\n=\n(\nS\nu\n1\n,\n⋯\n,\nS\nu\n|\nS\nu\n|\n)\nQ\n∈\nℝ\nn\n×\nk\nE\n(\nu\n,\nt\n)\n∈\nℝ\nL\n×\nk\nF\nj\n∈\nℝ\nh\n×\nk\n,\ni\n≤\nj\n≤\nd\n,\nh\n=\n{\n1,\n⋯\n,\nL\n}\nG\nj\n∈\nℝ\nL\n×\n1\n,\ni\n≤\nj\n≤\nd\n′ \no\n∈\nℝ\nd\no\n′ \n∈\nℝ\nd\n′ \n13\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#13": "Sequence-aware recommender systems\nGli output sono concatenati e dati in input ad una MLP per ricavarne \nrappresentazioni ad alto livello:  \nL'output \n  indica i bisogni a breve termine dell'utente.  \nI bisogni a lungo termine dell'utente sono ricavati:  \ndove \n  è una ulteriore embedding matrix, e  \n  è la \n user \nembedding matrix\n  per i bisogni a lungo termine. \n  e \n  sono la \nu\n-ma riga e \n i\n-ma riga rispettivamente di \n P\n e \nV\n.\nz\n∈\nℝ\nk\nV\n∈\nℝ\nn\n×\n2\nk\nP\n∈\nℝ\nm\n×\nk\np\nu\n∈\nℝ\nk\nv\ni\n∈\nℝ\n2\nk\n14\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#14": "Sequence-aware recommender systems\nL'architettura generale di Caser è così rappresentata:\n15\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#15": "Addestramento e architetture sequenziali\nIn modo simile all'addestramento RNN, in presenza di dati con timestamp che \nrappresentano azioni tra utenti e items (es. l'utente ha lasciato un rating su un \nﬁ\nlm), possiamo costruire un dataset di addestramento creando sequenze di \ndimensione prede\n ﬁ\nnita, ad esempio:\n16\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#16": "Factorization Machines\nLe FM sono algoritmi supervisionati che possono essere impiegati in contesti \ndi classi\n ﬁ\ncazione, regressione e ranking. Si ispirano a modelli di regressione \nlineare, modelli di MF e Support vector machines con kernel polinomiali.  \nMostrano vantaggi in caso di dataset sparsi riducendo notevolmente il tempo \ndi addestramento. Inoltre individuano più facilmente correlazioni signi\n ﬁ\ncative \ntra i dati.  \nSe indichiamo con \n  il vettore delle feature per una certa istanza, e con \ny la label numerica associata (es. 4.5 oppure click/no-click), formalizziamo il \nmodello in questo modo:  \ndove \n  è il bias globale, \n  sono i pesi per la i-ma variabile, \n sono i feature embeddings, e \n v\ni\n è la i-ma riga di \n V\n, <v\n i\n,v\nj\n> è il \nprodotto vettoriale tra i 2 vettori è modella l'interazione tra la \n i\n-ma e \n j\n-ma \nfeature.\nx\n∈\nℝ\nd\nw\n0\n∈\nℝ\n w\n∈\nℝ\nd\nV\n∈\nℝ\nd\n×\nk\n17\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#17": "Factorization Machines\nNell'espressione precedente si può notare una prima componente che \nrappresenta il modello di regressione lineare, mentre il secondo estende un \nmodello di MF:  \nSe la feature \n i\n rappresenta un item, e la feature \n j\n un utente, il terzo termine è il \nprodotto scalare tra i due embedding, uno dell'utente ed uno dell'item. \n18\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#18": "Deep Factorization Machines\nIn alcuni scenari l'approccio lineare delle FM non è suf\n ﬁ\nciente per \nrappresentare correlazioni e patterns complessi. Ma è possibile integrare \napprocci \"deep\" nel FM per rappresentare interazioni tra features nei dati.  \nNel \nDeepFM\n  la componente FM e deep sono combinate in modo \n parallelo\n . La \nFM è simile all'architettura originale. La deep è implementata con una MLP. \nL'input/embeddings delle 2 componenti è il medesimo, l'output è \n sommato  \nper creare la predizione \n ﬁ\nnale.  \nLa DeepFM si ispira allearchitetture RecSys chiamate \n wide & deep\n . In tali \narchitetture la predizione combina due pipeline:  \n•\nla \nmemorizzazione\n  mira a rappresentare co-occorrenze frequenti tra items o \nfeatures nei dati storici. Si implementa con un modello lineare (es. logistic \nregression)  \n•\nla \ngeneralizzazione\n  punta a implementare la \"transitività\" delle correlazioni, \ncioè esplorare combinazioni signi\n ﬁ\ncative tra features che non sono state mai \nincontrate nel passato. La pipeline è generalmente basata su una MLP.\n19",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#19": "Deep Factorization Machines\nSupponiamo che l'output della FM sia \n . Indichiamo con \n  il vettore \ndelle feature latenti del campo \n i\n-mo.  \nL'input della componente deep è la concatenazione degli embeddings di tutti \ni campi associati alle \n f\n feature categoriche date in input:  \nLa rete neurale è cosi de\n ﬁ\nnita: \nL'output \n  è combinato con il precedente per generare l'output \n ﬁ\nnale:  \n \n̂\ny\n(\nF\nM\n)\ne\ni\n∈\nℝ\nk\n̂\ny\n(\nD\nN\nN\n)\n̂\ny\n=\nσ\n(\nh\na\nt\ny\n(\nD\nN\nN\n)\n+\n̂\ny\n(\nD\nN\nN\n)\n)\n20\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#2": "Motivazioni\nI \nsistemi di raccomandazione\n  sono strumenti chiave in molti scenari: e-\ncommerce, siti per la fruizione di musica e video (es. Spotify, Net\n ﬂ\nix), app \nstores, pubblicità, etc.  \n•\nAlcune conferenze internazionali speci\n ﬁ\nche nel settore (es. RecSys) \nattraggono i maggiori players che fanno a gara per aggiudicarsi i migliori \nricercatori.  \nSupponiamo nel resto dei lucidi che alcuni argomenti sia già noti dai \nprecedenti corsi:  \n•\nCollaborative \n ﬁ\nltering  \n•\nExplicit e Implicit feedback  \n•\nRecommendation tasks (es. rating vs top-n vs sequence aware \nrecommendation\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#20": "Deep Factorization Machines\nL'architettura DeepFM è la seguente:\n21\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#3": "MovieLens dataset\nIl dataset più popolare nel mondo accademico \n RecSys\n . Ne esistono diverse \nversioni in base alla quantità di dati contenuti.  \n•\nIn \nMovieLens 100K\n  sono contenuti 100.000 ratings espressi in una scala da 1 \na 5, da 943 utenti su 1682 \n ﬁ\nlm. Ogni utente ha espresso rating su almeno 20 \nﬁ\nlm. Il formato del dataset è \n csv\n. \n•\nhttp://\n ﬁ\nles.grouplens.org/datasets/movielens/ml-100k.zip  \nTecniche quali \n Matrix Factorization\n  sono state in in grado di individuare \npatterns chiave per ottenere migliori risultati rispetto ad approcci più \ntradizionali. Ma si limitano a catturare patterns e correlazioni di tipo lineare. \n4",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#4": "MovieLens dataset\nMovieLens\n  100K\n  vs \n1M\n5\n",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#5": "AutoRec\nIn \nAutoRec\n  si impiega un paradigma di Collaborative Filtering basato su \nautoencoders. Invece di rappresentare la matrice user-ratings in un spazio \nlatente, o di impiegare le stesse istanze di input anche per l'output come nel \ncaso degli autoencoders visti in precedenza, si segue un approccio alternativo:  \n•\nin \ninput\n  si hanno un le interazioni utente-item osservate  \n•\nin \noutput\n  ci si aspetta l'intera matrice delle interazioni utente-item  \nEsistono architetture AutoRec \n user-based \n e \nitem-based\n . Qui vedremo le \nseconde ma è facile immaginare le prime per analogia.\n6",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#6": "(Item-based) AutoRec\nIndichiamo con \n  la i-ma colonna della matrice dei ratings. I valori di rating \nsconosciuti saranno 0.  \nLa rete AutoRec la possiamo de\n ﬁ\nnire formalmente:  \n \ndove f e g sono funzioni di attivazione, W e V matrici di pesi, \n  e \nb\n sono \nbiases, \n  la ricostruzione della i-ma colonna.  \nLa funzione da ottimizzare è la seguente:  \n \ndove il primo modulo considera solo i \n ratings\n  noti.\nR\n*\ni\nh\n(\nR\n*\ni\n)\n=\nf\n(\nW\n⋅\ng\n(\nV\n⋅\nR\n*\ni\n+\nμ\n)\n+\nb\n)\nμ\nh\n(\nR\n*\ni\n)\nargmin\nW\n,\nV\n,\nμ\n,\nb\nM\n∑\ni\n=\n1\n|\n|\nR\n*\ni\n−\nh\n(\nR\n*\ni\n)\n|\n|\n2\n+\nλ\n(\n|\n|\nW\n|\n|\n2\nF\n+\n|\n|\nV\n|\n|\n2\nF\n)\n)\n7",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#7": "Richiami: Implicit Feedback\nI \nrating espliciti\n  (es. valutazioni da 1 a 5) sono generalmente scarsi nei servizi \ndi raccomandazione. Inoltre valori mancanti di rating possono essere \nerroneamente interpretati, es.: forme di feedback negativi invece di rating che \ndevono essere ancora speci\n ﬁ\ncati. \nSi tendono a sfruttare altre fonti che possono essere interpretate come forme di \nimplicit feedback\n . \n•\nEs. clicks, acquisti, visite, wish lists.\n8",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#8": "Neural Collaborative Filtering for Personalized Ranking\nIl \nNeural Collaborative Filtering (NCF)  \nframework con implicit feedback \nsfrutta la capacità di \n non-linearità \n delle reti neurali.  \nÈ composto da 2 reti, una basata su Generalized Matrix Factorization, l'altra \nconsiste in una MLP.  \nL'output delle 2 reti è concatenato per generale la predizione \n ﬁ\nnale. Se in \nAutoRec puntavamo a predire i rating, NCF produce una lista di \nraccomandazioni con score associato ad ogni item della lista.\n9",
    "data_test\\rootfolder\\università\\DeepLearning\\17-RecSys-sbloccato.pdf#9": "Neural Collaborative Filtering for Personalized Ranking\nLa \nGMF\n  è un approccio di MF implementato con reti neurali. L'input è il \nprodotto element-wise (Hadamard product) tra le rappresentazioni latenti degli \nutenti e degli item:  \nDove \n p\nu\n e \nq\ni\n sono rispettivamente la u-ma riga di \n P\n e q-ma riga di \n Q\n, dove \n e \n . L'output è la previsione di score dell'utente \n u\n per \nl'item \n i\n. \nLa \nMLP\n prende in input le rappresentazioni degli utenti e item, ignorando lo \nspazio latente della GMF. Lo scopo è individuare correlazioni aggiuntive con \noperazioni non lineari.\nP\n∈\nℝ\nm\n×\nk\nQ\n∈\nℝ\nn\n×\nk\n10\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#0": "Deep Learning  \nUniversità Roma Tre  \nDipartimento di Ingegneria  \nAnno Accademico 2022 - 2023  \nDeep Learning e Natural Language Processing\n1",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#1": "Sommario\nDL e NLP: motivazioni  \nword2vec  \nskip-gram  \nCBOW  \nGloVe  \nfastText  \nBERT",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#10": "word2vec: skip-gram (training)\nCi poniamo l'obiettivo di massimizzare la probabilità, cioè minimizzare la \nfunzione:  \nSe impieghiamo lo SGD, usiamo sequenze brevi per stimare il gradiente \nstocastico e aggiornare il modello. La stima è basta sul gradiente del logaritmo \ndella probabilità condizionata data una coppia \n w\no\n e \nw\nc\n. \nUna volta terminato l'apprendimento, i vettori \n v\ni\n sono tipicamente impiegati \ncome \n embedding\n  associati ad un termine.\n11\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#11": "word2vec: Continuous Bag of Words (CBOW)\nSimile allo skip-gram ma assume che le parole contestuali generico la parola \ncentrale.  \nEssendoci più parole contestuali, i vettori sono mediati. La probabilità \ncondizionata di generare un termine \n  dati i termini contestuali \n  è \nla seguente:  \nSe consideriamo una sequenza di lunghezza T abbiamo:\nw\nc\n w\no\n1\n,\n⋯\n,\nw\no\n2\nm\n12\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#12": "word2vec: recap\nDiagramma riassuntivo dei 2 modelli:\n13\nCBOW model Skip-gram model\nt-word embedding t-word embeddingmatrix  \nVxdmatrix\ndxV",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#13": "Word Embedding with Global Vectors (GloVe)\nLe co-occorrenze tra termini rappresentano informazioni importanti per \ncostruire gli embeddings. Indichiamo con \n q\nij\n la probabilità condizionata \n P(w\n j\n|\nw\ni\n)\n nel modello \n skip-gram\n : \nLa parola \n w\ni\n può presentarsi molte volte in un corpus. Tutte le parole \ncontestuali che co-occorrono con \n w\ni\n creano un multiset, dove \n x\nij\n indica il \nnumero di volte che la parola \n w\nj \nco-occorre con \n w\ni\n. La loss function è:  \nIndichiamo con \n x\ni\n il numero di parole contestuali dove compare wi come \nparola centrale, e avendo \n p\nij\n=x\nij\n/x\ni\n, otteniamo:\n14\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#14": "Word Embedding with Global Vectors (GloVe)\nLa sommatoria interna è la cross-entropy tra la probabilità condizionata \n q\nij \nrelativa alla predizione generata dal modello, e \n p\nij\n ottenuta analizzando le \nstatistica dell'intero corpus.  \nPer ridurre la complessità computazionale (soprattutto per generare \n q\nij\n) e per \nmitigare gli effetti generate dai termini che compaiono di rado nel corpus ma \nche possono assumere importanza elevata dalla cross entropy, il modello \nGloVe introduce alcune varianti.  \nLa nuova \n loss function\n  è la seguente:  \ndove si introducono ad ogni parola sono associati 2 bias, \n b\ni\n per le parole \ncentrali e \n c\ni\n per le parole impiegate nel contesto; il primo e ultimo termine nel \ntermine a quadrato sono il termine di loss, e \n h(x\nij\n)\n genera un peso associato al \ntermine di loss.\n15\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#15": "fastText model\nCi sono relazioni morfologiche comuni tra molti vocaboli, es., tra \n help\n e \nhelps, \nhelped, helping\n ; tra \ndog\n e \ndogs\n e tra \n cat\n e \ncats\n; tra \nboy\n e \nboyfriend\n  e tra \n girl\n e \ngirlfriend\n . Modelli come skip-gram ignorano queste relazioni, poiché ognuno \ndi questi termini è rappresentato da un vettore distinto.  \nIl modello \n fastText\n  usa \nsubword embeddings\n , dove ogni \n subword\n  è un \n n-gram \ndi caratteri. Ad ogni subword è associato un vettore.  \n•\nPer esempio, la parola \"where\" genera le subwords “<wh”, “whe”, “her”, \n“ere”, “re>” impiegando una \n ﬁ\nnestra di lunghezza 3.  \nLa rappresentazione di un termine \n v\nw\n sarà la somma delle sue subwords \n z\ng\n: \nIl resto del modello è basato su skip-gram.\n16\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#16": "Modelli context-indipendent e context-sensitive\nNei modelli precedenti, data una parola, il vettore generato non dipende dal \ncontesto attuale (approccio \n context-independent\n ). Termini polisemici o \nrelazioni semantiche del linguaggio naturale saranno perciò ignorate.  \nModelli come \n ELMo\n  combinano le rappresentazioni intermedie ottenute da \nLSTM bidirezionali per ottenere una rappresentazione che dipende dalla \nsequenza in input (approccio \n context-sensitive\n ).  \n•\nLa rappresentazione così ottenuta è solitamente combinata con quella \nottenuta in modo context-independent (es. tramite GloVe) nei task successivi. \nIl modello impiegato da ELMo deve essere speci\n ﬁ\nco per il task che si andrà \nad affrontare, e perciò rimarrà costante.  \nPer evitare di avere diversi modelli per ogni task, GPT pre-addestra un \nlanguage model che sarà usato per rappresentare sequenze testuali. I \nparametri saranno poi \n ﬁ\nne-tuned\n  in base all'ouput del task successivo. GPT è \nbasato su Transformers. Il contesto analizzato da GPT sarà limitato alla parte \nantecedente al termine attuale, perciò non si analizza il contesto a destra del \ntermine. \n17",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#17": "Bidirectional Encoder Representations from Transformers (BERT)\nBERT combina i due approcci appena descritti, rappresentando l'intero \ncontesto mediante un approccio bidirezionale.  \nÈ basato su Transformer encoders pre-addestrati. Un output layer speci\n ﬁ\nco per \nil task da affrontare sarà di volta in volta addestrato da zero. \n18\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#18": "Bidirectional Encoder Representations from Transformers (BERT)\nL'input di BERT può essere una singolo testo o coppie di testi.  \nOltre agli \n positional\n  embedding, si impiegano anche \n segment\n  e \ntoken  \nembeddings. Infatti, a differenza delle RNN, i Transformer richiedono tecniche \nspeci\n ﬁ\nche per rappresentare internamente l'ordine relativo in cui i termini \ncompaiono tra loro. Tali embedding sono ricavati durante la fase di training.\n19\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#19": "Bidirectional Encoder Representations from Transformers (BERT)\nCome pretraining (auxiliary) task si impiega il \n Masked Language modeling\n . \nDato un corpus testuale, il 15% dei tokens saranno selezionati in modo \nrandom per il task di predizione. Al loro posto sarà presente un tag <mask>, \nes: \n•\n“this movie is great” becomes “this movie is <mask>”  \nUn ulteriore auxiliary task speci\n ﬁ\nco nello scenario in cui si hanno 2 testi in \ninput è il \n Next sentence prediction\n . Dal corpus si estraggono coppie di frasi \nconsecutive, e altrettante coppie di frasi che non sono consecutive. Il task è di \nclassi\n ﬁ\ncazione binaria.\n20",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#2": "DL e NLP \nDurante l'apprendimento di tecniche ML abbiamo spesso bisogno di grosse \nmoli di dati \n labelled\n  per implementare approcci supervisionati.  \nAlcune architetture DL hanno la capacità di riconoscere pattern e \ncaratteristiche anche complesse in tali dati, ma dataset adeguati per \nl'addestramento non sono disponibili.  \nPer tale motivo sono stati proposti vari approcci come il \n self-supervised \nlearning\n , per analizzare dati un modo non supervisionato (\n auxiliary task, es. \npredire una parte mancante del testo) e costruire rappresentazioni utili per \nsupportare l'apprendimento (tipicamente supervisionato) in task più speci\n ﬁ\nci.  \nAvendo un dataset di testo, l'input può essere costruito impiegando singole \nparole o n-grams formati da lettere, utili per catturare informazioni \nmorfologiche delle parole. L'output è tipicamente una rappresentazione \nvettoriale associata ad ogni parola (\n embedding\n ), indipendente dal contesto in \ncui sarà presente in seguito.\n3",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#20": "Bidirectional Encoder Representations from Transformers (BERT)\nBERT raggiunge prestazioni elevate su numero categorie di tasks, es.:  \n•\nsingle text classi\n ﬁ\ncation \n (e.g., sentiment analysis),  \n•\ntext pair classi\n ﬁ\ncation\n  (e.g., date due domande di Quora determinare se sono \nsimili o no),  \n•\nquestion answering\n ,  \n•\ntext tagging\n  (e.g., named entity recognition)\n21",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#3": "Esempio di auxiliary task\n4\ninputsliding-window\noutput (training set)\n  ",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#4": "Esempio di embeddings\nEmbeddings relativi a termini che identi\n ﬁ\ncano 115 nazioni estratti da un \ncorpus testuale, rappresentati su un piano 2d.\n5   \n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#5": "Esempio di operazioni su embeddings\nEssendo vettori, possiamo fare operazioni sugli embeddings, es:  \n•\nvector(“paris”)−vector(“france”)+vector(\"germany\")  \nImpiegando il modello di embeddings \n GloVe\n  addestrato sul testa estatto da \nWikipedia otteniamo:  \n•\nberlin: 0.8015347  \n•\nparis: 0.7623165     \n•\nmunich: 0.7013252     \n•\nleipzig: 0.6616945    \n•\ngermany: 0.6540700 \n6   ",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#6": "DL e NLP \nLe rappresentazioni pretrained ottenute con approcci non supervisionati sono \nsuccessivamente impiegate su architetture di DL in base al task da risolvere.\n7\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#7": "word2vec\nIl modello impiega 2 reti: \n skip-gram\n  e \ncontinuos bag of words\n  (CBOW).  \nIl training è basato sulla stima delle probabilità condizionate di predire una \ncerta parola in base a termini che occorrono nel suo intorno. Si segue sempre \nun approccio non supervisionato.\n8",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#8": "word2vec: skip-gram\nAssume che un termine può generare il testo circostante in una sequenza.  \nSupponiamo di considerare la sequenza \n “the”, “man”, “loves”, “his”, “son”\n ; e \nconsiderare il termine \n loves\n  con parola centrale, e una \n ﬁ\nnestra di 2 termini \nintorno al termine centrale.  \nIl modello skip-gram valuta la seguente probabilità condizionata:  \nSe assumiamo che i termini siano generati in modo indipendente tra loro, \npossiamo riscriverla:\n9\n",
    "data_test\\rootfolder\\università\\DeepLearning\\18-NLP-sbloccato.pdf#9": "word2vec: skip-gram\nOgni parola con indice \n i\n ha associati 2 vettori d-dimensionali, \n  e \n. Il primo impiegato quando la parola è usata centralmente, l'altro \nquando la parola appare nel contesto.  \nLa probabilità di generare un certo termine contestuale con indice \n o\n dato il \ntermine centrale con indice \n c\n è de\n ﬁ\nnita mediante una operazione softmax nel \nseguente modo:  \nData una sequenza di testo lunga \n T\n, dove \n  indica la parola posizionata allo \nstep \nt\n, e assumendo che le parole contestuali siano generate in modo \nindipendente tra loro, per una \n ﬁ\nnestra di lunghezza \n m\n, la probabilità di \ngenerare tutti i termini contestuali è de\n ﬁ\nnita nel seguente modo:\nv\ni\n∈\nℝ\nd\nu\ni\n∈\nℝ\nd\nw\n(\nt\n)\n10\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#0": "!1Introduzione",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#1": "Cos’è la Fisica?La Fisica è la disciplina che si propone di fornire una spiegazione a tutti i fenomeni naturali\n!2\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#10": "Misura delle grandezze fisicheEsempio: per misura della larghezza L di una lavagna occorre confrontare la larghezza della lavagna con uno standard di misura delle lunghezze: \n!11L{}=LL⎡⎣⎤⎦Risultato del confrontoGrandezza fisica da misurareLunghezza standard• Se [L]=metro            → {L} = 3,5    [L]=m   →   L = 3,5 m • Se [L]=centimetro    → {L} = 350     [L]=cm   →   L = 350 cm • Se [L]=piede             → {L} = 11,5    [L]=ft     →   L = 11,5 ft • Se [L]=pollice           → {L} = 138    [L]=in     →  L = 138 in La grandezza è sempre la stessa, ma cambiano sia la parte numerica che quella relativa allo standard di misura utilizzato",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#11": "Misura delle grandezze fisicheLa misura è identificata da due elementi: • La parte numerica (numero)  {L} • Lo standard usato (l’unità di misura) [L] \n!12L={L}[L]Devono essere specificati entrambi!!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#12": "Dimensioni delle grandezze fisicheGrandezze principali (che useremo nel corso) • Lunghezza L, Tempo T, Massa M, Intensità di corrente I\n!13e ( alcune) grandezze derivate: • Superficie S=[L2], V olume V=[L3] • Frequenza F=[1/T]=[T-1]  • Velocità V=[L/T]=[LT-1], accelerazione A=[L/T2]=[LT-2] • Tensione elettrica V=[ML2I-1T-3]• Grandezza generica  [X]=[MαLβTγI𝛿] 𝛼,𝛽,𝛾,𝛿 sono dette dimensioni della grandezza fisica",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#13": "Dimensioni nelle formuleOgni formula ﬁsica è una relazione tra grandezze ﬁsiche → sono due relazioni, una sui numeri e una sulle unità di misura\n!14",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#14": "Unità di misura (standard di misura)Gli standard devono soddisfare i criteri: • Essere stabili nel tempo • Essere precisi • Essere “facilmente” riproducibili in ogni parte del mondo (universo)\n!15Dal 20 maggio 2019 si utilizzano nuove definizioni",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#15": "Unità di tempo: il secondoScelta di un fenomeno periodico:  •Giorno solare medio. Diviso in  - 24 ore, 60 minuti primi, 60 minuti secondo - 1 giorno = 86400 secondi  (minuti secondi) • 1967: un secondo corrisponde a 9.192.631.770 oscillazioni dell’isotopo di Cesio 133 tra lo stato fondamentale e il suo primo stato eccitato (invariato al 20/5/2019)!16\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#16": "Unità di lunghezza: il metroProdotto della rivoluzione francese (1795)•Deﬁnizione originale– 1 metro = 1/10 000 000 della distanza tra polo nord ed equatore •Deﬁnizione successiva (1889): distanza tra due tacche di una sbarra di platino-iridio (campione di Sèvres)•1983: Lo standard di tempo è ben deﬁnito; la velocità della luce è una costante universale:– 1 metro = distanza percorsa dalla luce in 1/299 792 458 secondi(invariato al 20/5/2019)!17\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#17": "Unità di corrente: l’ampereFino al 20/5/2019 L’intensità di corrente che, se mantenuta in due conduttori lineari paralleli di lunghezza infinita e sezione trascurabile, posti a un metro di distanza l’uno dall’altro nel vuoto, produce tra questi una forza pari a 2×10-7 newton per ogni metro di lunghezza. Oggi:  L’ampere sarà definito dal valore numerico della carica elementare fissato a 1,602176634×10-19 coulomb e sarà realizzato attraverso speciali circuiti che contano gli elettroni.!18\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#18": "Unità di massa: il kilogrammoProdotto della rivoluzione francese (1795) Intenzione: 1 kg = massa di 1 dm3 di acqua a 4 gradi centigradi Fino al 20/5/2019 Definizione: 1kg =  massa di un cilindro campione di platino iridio di 39 mm di altezza e 39 mm di diametro!19\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#19": "Unità di massa: il kilogrammoOggi:  Sarà ridefinito in termini della costante di Planck, sarà realizzato attraverso una speciale bilancia elettromagnetica (detta bilancia di Kibble) e non sarà più necessario riferirsi al campione di Sèvres. il chilogrammo diventa la massa controbilanciata da un certa quantità di corrente, dove entra in gioco la costante di Planck.\n!20\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#2": "Cos’è la Fisica?La Fisica è la disciplina che si propone di fornire una spiegazione a tutti i fenomeni naturali\n!3Fino al XVII secolo la Fisica era considerata come filosofia della natura (spinta più da considerazioni filosofiche)Il senso moderno del termine è stato introdotto da Galileo Galilei, partendo dalla definizione di Metodo Scientifico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#20": "Sistema Metrico DecimaleUn insieme di unità di misura costituisce un sistema: Sistema MKS: metro, kilogrammo, secondo (rinominato in SI nel 1970: Sistema Internazionale) Sistema cgs: centimetro, grammo, secondo Sistema metrico decimale: i multipli ed i sottomultipli sono potenze di 10: Multipli prefisso  sottomultipli prefisso 10 deca (da)  10-1 deci   (d) 102 etto (h)  10-2 centi  (c) 103 kilo  (k)                 10-3 milli  (m) 106 mega (M)  10-6 micro  (µ) 109 giga (G)  10-9 nano (n) Esempi: 1 mm, 2 µm, 5 ns, 20 km, 4 hg!21",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#21": "Esempio\n!22Una macchina percorre una curva semicircolare di raggio R=50 m con una velocità di v = 20 m/s. Calcolare l’accelerazione della macchina.\nRisultato: l’accelerazione vale: Analisi dimensionale: [a]=[LT-2]Suggerimento: l’accelerazione (a) si misura in m/s2. La formula da usare è una delle seguenti:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#22": "Esercizio\n!23Determinare quanti secondi ci sono in un anno solare;Quanto pesa un metro cubo di acqua?Enrico Fermi amava dire che faceva lezioni che duravano tipicamente un microsecolo. A quanti minuti corrisponde un microsecolo?365.25×24×60×60=31,556,7361dm3→1kg1m3=1000dm3→1000kg100×365.25×24×60=52,594,560 minuti in un secoloUn microsecolo corrisponde a 52.59456 minuti ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#23": "Lorenzo RinaldiDipartimento di Fisica e Astronomialorenzo.rinaldi@unibo.ithttps://www.unibo.it/sitoweb/lorenzo.rinaldi/\n!24",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#3": "Il Metodo Scientifico-SperimentaleAlla base del metodo Scientifico c’è l’Esperimento: i processi della Natura sono schematizzati in Modelli da verificare sperimentalmente\n!4\n...tra le sicure maniere di conseguire la verità è l’anteporre l’esperienza a qualsivoglia discorso, non sendo possibile che una sensata esperienza sia contraria al vero... ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#4": "Il Metodo Scientifico-SperimentaleNessun modello teorico risulta essere valido universalmente Le teorie risultano essere valide entro ben determinati limiti esempio:  •piccole distanze: serve la “teoria dei quanti” •elevate velocità: serve la “teoria della relatività” \n!5",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#5": "Il Metodo Scientifico-SperimentaleLe teorie fisiche sono validate tramite osservazioni sperimentali Gli esperimenti devono essere realizzati per determinare con precisione (MISURARE) in maniera RIPRODUCIBILE le grandezze fisiche.\n!6Le grandezze Fisiche sono quantità che servono per descrivere i fenomeni naturali in maniera oggettiva (esempio: tempo, spazio, massa,…) \n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#6": "Grandezze Fisiche\n!7Grandezza fisica: proprietà o caratteristica di un oggetto o di un fenomeno che può essere quantificata (→ misurata)\nEsempi: \nlunghezze, durate, velocità, forza, temperatura, pressioneodori, intelligenza, bello, brutto…\nControesempi: \n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#7": "Grandezze principali e derivate Lunghezza e V olume • Il volume V di un cubo di lato L: V=L3\n!8\nL In un viaggio di T=1 h, ho percorso L=100 km spostandomi ad una velocità v=100 km/h • 3 grandezze: durata, distanza, velocità •1 relazione tra le grandezze v=L/Tlunghezza e tempo sono grandezze principali V olume e velocità sono grandezze derivate",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#8": "Grandezze principali e derivate\n!9In Fisica ci sono 7 grandezze principali, tutte le altre sono derivabili da esselunghezza tempo massa temperatura intensità di corrente elettrica intensità luminosa quantità di sostanzaMECCANICAELETTROMAGNETISMO",
    "data_test\\rootfolder\\università\\FisicaGenerale\\01-introduzione.pdf#9": "Misura delle grandezze fisicheMisura: processo di determinazione di una grandezza fisica Operativamente: misura=confronto della grandezza che ci interessa con uno standard (una misura campione di quel tipo di grandezza)Le grandezze Fisiche sono definite in Modo Operativo: il modo di misurare la grandezza ne fissa la definizione Le grandezze Fisiche sono definite da tutte le possibili operazioni di misurazione\n!10",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#0": "VETTORI  CdS Ingegneria Informatica A.A. 2019/20",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#1": "!2Grandezze fisiche•Grandezze scalari: completamente definite da un numero ed una unità di misura –Esempi: distanza, lunghezza, periodo, pressione, temperatura •Grandezze vettoriali: completamente definite da 3 numeri e da una unità di misura o da un numero, una unità di misura, una direzione ed un verso –Esempi: spostamenti, forze, velocità, accelerazione, campi elettrici e magnetici, … •Grandezze tensoriali: definite da più di 3 numeri ed una unità di misura –Esempi: momento d’inerzia, matrice di rotazione, …",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#10": "!11Proprietà associativa della somma\n⃗a+⃗c",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#11": "!12Differenza tra vettori•Definizione:OABOAB−⃗bvettore opposto (stesso modulo e direzione, ma con verso opposto)⃗d=⃗a−⃗b=⃗a+(−⃗b)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#12": "Somma e differenza\n!13⃗a⃗d=⃗a−⃗b⃗b⃗c=⃗a+⃗b",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#13": "!14Moltiplicazione per uno scalare•Si può definire come moltiplicazione tra un numero naturale n ed un vettore     come una somma ripetuta:•Generalizzando, si può definire come  moltiplicazione tra un numero reale λ ed un vettore      come vettore di direzione pari ad      modulo pari a            e verso concorde con     se             , verso opposto se    È un vettore!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#14": "!15Moltiplicazione per scalare: proprietà•La moltiplicazione per uno scalare gode delle proprietà commutative, associative e distributive sia rispetto agli scalari che ai vettori:•Inoltre:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#15": "!16Prodotto scalareAssocia a due vettori arbitrari uno scalare:θa⋅b=abcosϑ\na⋅b=abb\na⋅b=aba",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#16": "!17                   : casi notevoli•Vettori in direzione opposta:a⋅b=abcosϑ•Vettori paralleli:a⋅b=ab>0•Vettori ortogonali:a⋅b=0•La componente: –Sia      un versorea⋅ˆu=aˆucosϑ==acosϑ=au",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#17": "!18Prodotto scalare: proprietà•Il prodotto scalare gode della proprietà commutativa, distributiva sulla somma:•Quadrato di un vettore:DEFINIZIONE  DI MODULO!  a⋅b=b⋅aa⋅b+c()=a⋅b+a⋅cλa⋅b()=λa()⋅b=a⋅λb()   a⋅a=a2=a2=a2  ⇒a=a⋅a",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#18": "!19Moduli di somme e di differenze\n   a+b=a+b()2=a+b()⋅a+b()=  =a⋅a+a⋅b+b⋅a+b⋅b   a+b=a2+b2+2abcosϑ   a−b=a−b()2=a−b()⋅a−b()=  =a⋅a−a⋅b−b⋅a+b⋅b   a−b=a2+b2−2abcosϑ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#19": "!20Prodotto vettore•Associa a due vettori un terzo vettore:\nModulo Direzione ⊥ piano dei vettori Verso: regola della mano destraModulo: area del parallelogrammaVerso convenzionale",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#2": "!3Stazione: 2.2 km in direzione nord-est",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#20": "!21Prodotto vettore•Associa a due vettori un terzo vettore:Modulo Direzione ⊥ piano dei vettori Verso: regola della mano destra\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#21": "!22Prodotto vettore: proprietà•Il prodotto vettore (o vettoriale) gode della proprietà anticommutativa e distributiva sulla somma: •Il prodotto vettore non gode della proprietà associativa: •Caso notevole:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#22": "!23Doppio prodotto misto\nProprietà:V=a∧b⋅c=a∧b()⋅ch=c⋅versa∧b() a∧b⋅c=b∧c⋅a=c∧a⋅b a∧b⋅c=a⋅b∧c",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#23": "!24Sistemi di riferimento•I vettori sono entità astratte, indipendenti da come sono rappresentate.AB•Per convenienza pratica i vettori si descrivono bene utilizzando il concetto di SISTEMA DI RIFERIMENTO, costituito in estrema sintesi da un punto privilegiato detto origine e da un insieme di vettori campione (vettori di base)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#24": "!25Spazio unidimensionale•Ogni vettore può essere scritto come:O In uno spazio unidimensionale ogni vettore può essere espresso come uno scalare (la componente) moltiplicato il versore dell’asse (sempre lo stesso).a+b=auˆu+buˆu=au+bu()ˆua=a⋅a=auˆu⋅auˆu=au2(ˆu⋅ˆu)=au",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#25": "!26Spazio bidimensionale: vettori nel piano•Scelgo 2 assi ortogonali x,y: \nOXYˆi⋅ˆj=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#26": "Esempio di spazio bi-dimensionale\n!27\n(C; 4)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#27": "!28Spazio tridimensionale\n•Scelgo 3 assi ortogonali x,y,z: ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#28": "!29Vettori di base nello spazio\n1ˆˆˆˆˆˆ=⋅=⋅=⋅kkjjii0ˆˆˆˆˆˆ=⋅=⋅=⋅ikkjjiˆi∧ˆi=ˆj∧ˆj=ˆk∧ˆk=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#29": "!30Operazioni nella rappresentazione cartesiana\na⋅b=ca⋅b=(axˆi+ayˆj+azˆk)⋅(bxˆi+byˆj+bzˆk)==axbxˆi⋅ˆi+axbyˆi⋅ˆj+axbzˆi⋅ˆk+aybxˆj⋅ˆi+aybyˆj⋅ˆj+aybzˆj⋅ˆk++azbxˆk⋅ˆi+azbyˆk⋅ˆj+azbzˆk⋅ˆk==axbx+ayby+azbz=c",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#3": "!4Vettore nel piano\nABModuloDirezioneVersoVettore libero 1 direzione nello spazio 1 verso 1 modulo (intensità)Prototipo: vettore spostamento",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#30": "!31Prodotto vettoriale\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#31": "Esercizio A•Sianoˆˆˆˆˆˆ21322aijkbijk=−+=−+−!!1.Trovare i moduli 2.Trovare il vettore somma ed il vettore differenza 3.Calcolare  4.Calcolare  5.Trovare l’angolo compreso!3232,23cabdab=+=−!!!!!!abλ=⋅!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#32": "Esercizio Nel piano XY, la componente x di un vettore v vale -25, quella y +40. Quanto vale il modulo del vettore? Quanto vale l’angolo compreso fra v e l’asse delle ascisse? \n!33",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#33": "!34Esercizio 1•Sia1.Trovare i moduli 2.Trovare l’angolo compreso 3.Trovare il vettore somma ed il vettore differenza 4.Trovare un vettore perpendicolare ad entrambi",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#34": "!35Esercizio 3•Una barca naviga in direzione Nord-Est per 15 km, successivamente vira in direzione Sud e prosegue per 10 km, quindi vira nuovamente in direzione Ovest e percorre altri 5 km. Trovare la distanza percorsa e la distanza dal punto di partenza.NESO",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#35": "Esercizio Nella somma a+b = c il vettore a ha modulo 12 e forma un angolo di 40° rispetto al semiasse positivo delle ascisse, mentre il vettore c ha modulo 15 ed è diretto con un angolo di 20° in senso antiorario rispetto al semiasse negativo delle ascisse. Calcolare il modulo e la direzione (rispetto al semiasse positivo delle ascisse) di b. \n!36",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#36": "Esercizio Dati nel piano cartesiano i punti A = (1, 1), B = (3, 4) e  C = (5, 2), determinare il valore dell’angolo formato dai segmenti CA e CB e l’area del triangolo ABC. \n!37",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#37": "EsercizioDeterminare il volume del parallelepipedo individuato dai vettori   ˆˆˆˆˆˆˆ2,3,32ajkbjcjkιι=−+=−=−+−!!!\n!38",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#38": "Lorenzo Rinaldi Dipartimento di Fisica e Astronomia lorenzo.rinaldi@unibo.it www.unibo.it/docenti/lorenzo.rinaldi\n!39",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#4": "!5Versore•Versore: vettore di modulo unitario, adimensionaleUn versore individua un asse orientatoAB",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#5": "I versori dove non te li aspetti\n!6",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#6": "!7LA componente ed IL componente\nvu = v cosθla componente\n(vu = v cosθ u) il componenteLA componente è una grandezza scalare!IL componente è un vettore (il vettore componente lungo una direzione)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#7": "!8Algebra dei vettori•I vettori liberi costituiscono un’algebra –È definita l’operazione somma tra due vettori –È definita l’operazione di moltiplicazione tra un vettore ed uno scalare •Inoltre: sono definiti un prodotto esterno ed uno interno –Prodotto scalare: –Prodotto vettoriale:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#8": "!9Somma tra due vettori•Prototipo: somma tra due vettori spostamentoABCD\nRegola del parallelogramma: Il vettore somma è dato dalla diagonale (C-A) del  Parallelogramma costruito  con i vettori (B-A) e (C-B)B≡C⃗c=⃗a+⃗b=(D−A)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\02-vettori.pdf#9": "!10Proprietà commutativa della somma•La somma gode della proprietà commutativa: \n•È un risultato sperimentale, non teorico, valido nel nostro ambiente. Ci sta dicendo che lo spazio fisico in cui viviamo è in buona approssimazione uno spazio euclideo.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#0": "CINEMATICA CdS Ingegneria InformaticaA.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#1": "\u00002Modello base•Punto materiale: –Punto geometrico –Dotato di una proprietà chiamata massa •Valido per la descrizione del moto di ogni oggetto, quando le dimensioni dello stesso non sono importanti e possono essere trascurate",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#10": "\u000011Velocità scalare, vettoriale, versore tangente\nNella rappresentazione intrinseca: v(t)=limΔt→0P(t+Δt)−P(t)Δt=dPdt v(t)=s(t)=limΔt→0s(t+Δt)−s(t)Δt⎯⎯→⎯→Δ0t)()()()(lim0tvtsttsttst==Δ−Δ+=→Δ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#11": "\u000012Velocità: derivata del vettore posizione\nxyzP(t)\nNella rappresentazione cartesiana:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#12": "\u000013Derivata di un vettore⃗a=⃗a(t)=B(t)−A(t)d⃗adt=limΔt→0⃗a(t+Δt)−⃗a(t)Δtd⃗adt è un vettored⃗adt=limΔt→0⃗a(t+Δt)−⃗a(t)Δt⃗a(t)⃗a(t+Δt)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#13": "\u000014Regole di derivazione\nDimostrabili tramite la rappresentazione cartesiana dei vettori",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#14": "se a(t)=λ costante   ⇒da2dt=0 per ipotesida2dt=da⋅a()dt=dadt⋅a+a⋅dadt=2a⋅dadt≡0⇒a⊥dadt\n\u000015Esempi e dimostrazioni•Tutte le relazioni si dimostrano facilmente nella rappresentazione cartesiana:•Caso notevole: vettore di modulo costante.\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#15": "\u000016Cinematica: Riassunto•Il moto è sempre un fenomeno relativo\nMoto di un punto (P) rispetto ad un sistema di riferimento (SR)TraiettoriaDescrizione cartesianaEquazioni  parametricheVettore posizionexyz",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#16": "\u000017Velocità e descrizione intrinseca\nxyzOΩss: ascissa/coordinata curvilinea\nVelocità scalare istantanea\nVelocità vettoriale  istantanea\nRappresentazione cartesianaRappresentazione intrinsecaversore tangente alla traiettoria",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#17": "\u000018Accelerazione scalare media ed istantanea•Analogamente a quanto fatto per la variazione della posizione, si introduce il concetto di accelerazione per descrivere le variazioni di velocità •Data la velocità scalare istantanea   Accelerazione scalare mediaAccelerazione istantanea   a(t)=limΔt→0am(t,t+Δt)=limΔt→0v(t+Δt)−v(t)Δt=dvdt=!v   a(t)=dvdt=!v=d2sdt2=!!s  am⎡⎣⎤⎦=a⎡⎣⎤⎦=ΔvΔt⎡⎣⎢⎤⎦⎥=LT−1/T⎡⎣⎤⎦=LT−2⎡⎣⎤⎦→(m/s2)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#18": "\u000019Accelerazione vettorialeAccelerazione mediaAcc. Istantanea !a=limΔt→0!am=limΔt→0!v(t+Δt)−!v(t)Δt",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#19": "\u000020Esercizio 1•Il movimento di un punto dello spazio è descritto dal vettore posizione: •Trovare la velocità vettoriale e scalare •Trovare l’accelerazione vettoriale •Calcolare l’angolo tra il vettore velocità e quello accelerazione e spiegare il risultato ottenuto.\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#2": "•La terra non ruota solo su se stessa: ruota intorno al Sole a una velocità superiore a 110.000 km/h.\nQual è la nostra VELOCITÀ?Vi sembra di essere seduti immobili mentre ascoltate la lezione?\n\u00003•Il pianeta ruota su se stesso, il che significa che in realtà viaggiamo verso est a una velocità che può raggiungere i 1600 km/h .\n•Il Sole e il Sistema Solare viaggiano nello spazio alla folle velocità di 2 milioni di km/hQual è la nostra vera VELOCITÀ? ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#20": "\u000021Esercizio 2•In un piano xy, un punto materiale si muove seguendo le leggi: •Trovare il vettore posizione •Trovare la velocità scalare al tempo t=10s •Mostrare che l’accelerazione è costante\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#21": "\u000022Rapp. Intrinseca: versore normale\nTraiettoria circolare\nΔ̂utΔs⟶α,Δs→0d̂utds=d̂utdŝun",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#22": "\u000023Componenti intrinseche dell’accelerazione\nCirconferenza osculatrice (nel piano osculatore)ρ = raggio di curvatura (funzione di s)Espressione intrinseca",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#23": "\u000024Rappresentazione intrinseca del moto•Traiettoria:•Legge oraria:•Versore tangente: •Versore normale: •Versore binormale: Terna intrinseca  di versori  Ortonormali  •Raggio di curvatura: •Velocità: •Accelerazione: •Legge oraria:   →!v=\"s=vx2+vy2+vz2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#24": "\u000025Classificazione dei moti - I1. Moto a velocità costante:\nxyzOMoto rettilineo  uniforme",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#25": "\u000026Classificazione dei moti - II2. Velocità costante in direzione\nxyzOEsempio: Moto uniformemente  accelerato",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#26": "\u000027Classificazione dei moti - III3. Moto a modulo di velocità costanteEsempio: Moto circolare uniformeMoto uniformemente curvo\nxyzOΩs",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#27": "\u000028Classificazione dei moti - IV4. Moto senza vincoli sulla velocitàMoto curvo varioxyzOΩs",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#28": "\u000029Osserviamo la realtà che ci circonda …Perché le curve in autostrada sono sempre molto dolci (→ hanno un raggio di curvatura di centinaia di metri), mentre in campagna mi trovo anche curve molto secche (a 90° in pochi metri)?Perché NON ESISTONO curve in autostrada con raggio inferiore a diverse centinaia di metri, ad eccezione dei raccordi per i caselli, dove, se non si rallenta opportunamente, è facile finire fuori strada?Perché i progettisti delle tratte ferroviarie dell’alta velocità considerano sempre traiettorie con raggi di curvatura di qualche km, quando in stazione ci sono curve con raggi di curvatura di qualche decina di metri?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#29": "\u000030Espressioni cartesiane\n an=!s2ρ→ρ=!s2an",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#3": "\u00004Cos’è il moto?•Il moto è sempre un fenomeno relativo•Moto di una macchina rispetto ad una strada•Moto di un aereo rispetto alle nuvole, al terreno•Moto di un pianeta rispetto alle stelle ﬁsse•Moto di un sistema osservato rispetto all’osservatoreESEMPIMoto di un punto (P) rispetto ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#30": "\u000031Problemi di cinematica•Problema DIRETTO della cinematica: dato il vettore posizione, trovare velocità ed accelerazione: •Problema INVERSO della cinematica: data l’accelerazione (o la velocità), trovare velocità e vettore posizione !at() noto ⇒ !vt()=?!rt()=?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#31": "\u000032Problema inverso•Tipico problema: nota la velocità:\nL’analisi illustra che esistono ∞3 soluzioni (moti diversi)La richiesta che il sistema di equazioni abbia  una sola soluzione richiede l’introduzione di altri dati:  le condizioni iniziali",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#32": "\u000033Esempio di problema inversoCostanti arbitrarieCondizioni iniziali: Soluzioneunivoca",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#33": "\u000034Esempio: problema inverso unidimensionale•ES:Sia                  , trovare il moto x(t).Soluzione: txk=-2k=0k=-1k=+1Moltitudine di moti diversi!Condizioni iniziali:  x(0)=0Un moto particolareSoluzione: k=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#34": "\u000035Esempio•Un punto materiale si muove lungo l’asse x con una accelerazione data da •Sapendo che le condizioni iniziali sono •Trovare velocità e posizione ad ogni istante di tempo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#35": "\u000036Soluzione•Velocità:• Posizione:dv adt→=→dv∫=adt∫",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#36": "\u000037Esercizio 1 La posizione di un punto materiale è individuata dal vettore posizione con t  espresso in secondi ed r in metri. Determinare la velocità e l’accelerazione ad ogni istante di tempo, la terna di versori intrinseca ed il raggio di curvatura della traiettoria per t=0 s.\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#37": "\u000038Esercizio 2In un certo istante, un punto materiale è in moto lungo un arco di circonferenza. Sapendo che rispetto ad un certo SR, la velocità e l’accelerazione valgono  e determinare la velocità scalare, l’accelerazione tangenziale, il raggio di curvatura della traiettoria e la normale al piano in cui avviene il moto. \n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#38": "\u000039Moti nel piano\nXYP1y(t)Ox(t)Equazioni parametriche\nEquazione della traiettoria",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#39": "\u000040Moto rettilineo uniforme\nXYP1y(t)Ox(t)Equazioni parametricheVelocità ed accelerazioneda x=x0+v0xt→t=x−x0v0xintercettapendenza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#4": "\u00005Scelta del sistema di riferimento•Lo stesso oggetto può essere descritto in modo diverso a seconda del SR:Moto della lavagna (e di tutti noi):•Ferma rispetto a noi (SR della stanza)•Moto circolare per chi ci osserva dalla luna (SR lunare; velocità circa 1200 km/h)•Moto + complesso per chi ci osserva dal Sole (SR eliocentrico; velocità circa 108000 km/h)•Moto ancora più complesso per chi ci osserva dal centro della Galassia!Scegliamo un SR a seconda di cosa si muove e di come vogliamo descrivere il moto",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#40": "\u000041Moto dei gravi•Caduta di un corpo sulla superficie terrestre\nXYy(t)Ox(t)costantealtoAsse orizzontaleCondizioni iniziali",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#41": "\u000042Moto dei gravi: soluzione\n•Traiettoria:→y=ax2+bx+c",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#42": "\u000043Caduta dei graviOggetto che cade da fermo da un’altezza hCaso particolare con condizioni iniziali (con t0=0): {⃗r0=ĥ𝚥⃗v0=⃗0{x0=0y0=hv0x=0v0y=0{vx(t)=0vy(t)=−gt{x(t)=0y(t)=h−g2t2traiettoria: linea retta (asse y)XYO",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#43": "\u000044Caduta dei gravi{vx(t)=0vy(t)=−gt{x(t)=0y(t)=h−g2t2\nXYOTempo di caduta (da altezza h):y(tc)=0h−g2t2c=0tc=2hgVelocità d’impatto al suolo⃗v(tc)=−gtĉ𝚥⃗v(tc)=−g2hĝ𝚥=−2gĥ𝚥",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#44": "\u000045Moto dei gravi:  generale•La traiettoria è una parabolaXYy(t)Ox(t)altoAsse orizzontaleCoordinate del punto di massimo della parabola?In quale punto dell’asse x atterra ? (y=0)A che istante e con che velocità ci arriva?Rappresentazione intrinseca:velocità e accelerazione?terna di versori intrinseca?raggio di curvatura?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#45": "\u000046Coordinate Polari piane\nxyPvettore posizione del punto Pin coordinate cartesiane⃗r=x̂ı+ŷ𝚥\n{x=rcosφy=rsinφpotremmo usare un’altra coppia di scalari: + distanza r dall’origine e angolo 𝜑 rispetto ad asse x̂ı̂𝚥φrtrasformazioni delle coordinater=x2+y2φ=arctanyx",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#46": "\u000047Coordinate Polari piane\nxyP{x=rcosφy=rsinφ̂ı̂𝚥φrr=x2+y2φ=arctanyxversori in coordinate polari pianêur=(̂ur⋅̂ı)̂ı+(̂ur⋅̂𝚥)̂𝚥=cosφ̂ı+sinφ̂𝚥⃗r=x̂ı+ŷ𝚥̂uφ=(̂uφ⋅̂ı)̂ı+(̂uφ⋅̂𝚥)̂𝚥=−sinφ̂ı+cosφ̂𝚥̂ur̂uφsi veriﬁca facilmented̂urdφ=̂uφd̂uφdφ=−̂urcambiano durante il moto!α=φ+π2cosα=−sinφsinα=cosφ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#47": "\u000048Coordinate polari cilindriche\nxyzPϕzrP’̂ur̂uφ̂uz\n̂ur̂uz{x=rcosφy=rsinφz=zr=x2+y2φ=arctanyxz=ẑur=cosφ̂ı+sinφ̂𝚥uφ=−sinφ̂ı+cosφ̂𝚥̂uz=̂k̂uφterna ortogonalêur=̂uφ∧̂k",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#48": "\u000049Moto circolare\nxyP(t)ascissa curvilinea s (con origine sull’asse x)s=RφTraiettoria circolare di raggio R centrata nell’origine di un SdR cartesianoRarco di circonferenzaφΩs[0≤s≤2πR]{x(φ)=Rcosφy(φ)=RsinφNel SdR cartesiano⃗r(φ)=Rcosφ̂ı+Rsinφ̂𝚥⃗r(s)=Rcos(sR)̂ı+Rsin(sR)̂𝚥|⃗r|=R\nla posizione dipende solo dall’angolo 𝜑⃗r(φ)=R̂urs(t)=Rφ(t)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#49": "\u000050Cinematica del moto circolare⃗r(φ(t))=Rcosφ(t)̂ı+Rsinφ(t)̂𝚥=R̂ur⃗v(φ(t))=d⃗rdt=−R·φsinφ̂ı+R·φcosφ̂𝚥=R·φ̂uφ⃗a(φ(t))=d⃗vdt==(−R··φsinφ−R·φ2cosφ)̂ı+(R··φcosφ−R·φ2sinφ)̂𝚥==R··φ(−sinφ̂ı+cosφ̂𝚥)−R·φ2(cosφ̂ı+sinφ̂𝚥)==R··φ̂uφ+R·φ2(−̂ur)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#5": "\u00006Sistema di riferimento cartesianoTraiettoria Luogo dei punti dello spazio per cui passa un corpo P=P(t)\nxyz\n= Descrizione cartesianaEquazioni  parametricheOVettore posizione\nr⎡⎣⎤⎦=x⎡⎣⎤⎦=y⎡⎣⎤⎦=z⎡⎣⎤⎦=L⎡⎣⎤⎦→(m)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#50": "\u000051Cinematica del moto circolareGrandezze angolari:velocità angolareω=·φaccelerazione angolareα=·ω=··φ⃗v=R·φ̂uφ=Rω̂uφ=Rω̂k∧̂ur=(ω̂k)∧(R̂ur)=⃗ω∧⃗rvettore velocità angolarêuφ=̂k∧̂ur⃗v=⃗ω∧⃗r⃗ω=ω̂k⃗a=Rα̂uφ+Rω2(−̂ur)=⃗α∧⃗r−ω2⃗rvettore accelerazione angolare⃗α=α̂k⃗a=ddt(⃗ω∧⃗r)=d⃗ωdt∧⃗r+⃗ω∧d⃗rdt=ur=̂uφ∧̂k\n⃗at=⃗α∧⃗r⃗an=⃗ω∧⃗v=⃗ω∧(⃗ω∧⃗r)=⃗α∧⃗r+⃗ω∧⃗v=⃗α∧⃗r+⃗ω∧(⃗ω∧⃗r)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#51": "\u000052Cinematica del moto circolareGrandezze angolari:velocità angolareω=·φ=ddt(sR)=·sR=vRaccelerazione angolareα=·ω=ddt(·sR)=··sRangoloφ=sR⃗v=Rω̂uφ=·ŝuφ⃗a=Rα̂uφ+Rω2(−̂ur)=··suφ+·s2R(−̂ur)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#52": "\u000053Cinematica del moto circolare⃗r=R̂ur⃗v=R·φ̂uφ=Rω̂uφ⃗a=R··φ̂uφ+R·φ2(−̂ur)=xyφR̂ur̂uφ=Rα̂uφ+Rω2(−̂ur)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#53": "\u000054Moto circolare uniforme⃗r=R̂ur⃗v=Rω̂uφ=⃗ω∧⃗rx̂uryφR̂uφ⃗a=−Rω2(̂ur)=−ω2⃗r=⃗ω∧(⃗ω∧⃗r)velocità costante in modulo ma varia continuamente in direzioneω=·φ=costanteα=·ω=0\naccelerazione (centripeta) costante in modulo ma varia continuamente in direzione",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#54": "Moti periodici•Un fenomeno è detto periodico se, a partire da un istante qualsiasi t, le sue caratteristiche si ripresentano inalterate dopo un certo intervallo di tempo T, detto periodo. •Quantità caratteristiche: –       ,  periodo fondamentale; –         , frequenza (numero di T contenuti nell’unità di tempo); –          , pulsazione (numero di giri compiuti nell’unità di tempo sulla traiettoria chiusa) υ=1Tω0=2πTTmin\n\u000055 !r(t+nT)=!r(t)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#55": "Esempio di moto periodicoMoto circolare uniforme:→v=costantexy !r•In coordinate cartesiane:Tutte le equazioni:hanno la stessa famiglia di soluzioni:\u000056\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#56": "\u000057Soluzione generale•Problema di base:Ipotesi di soluzione:f+ω2f=0→",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#57": "\u000058Moto oscillatorio armonicoTraiettoria Equazione oraria \nx+l-l",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#58": "\u000059Moto armonico: condizioni inizialiSia dato un moto armonico di pulsazione ω e condizioni iniziali x(0)=x0 e v(0)=v0. Trovare la legge oraria.Soluzione:  Equazione del moto armonico:Soluzione generale:Velocità:Accelerazione:Impongo le condizioni iniziali:x(0)=Acos(φ0)=x0x(0)=−ωAsin(φ0)=v0→⎧⎨⎪⎩⎪",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#59": "\u000060Esercizio•Dati due moti armonici di pulsazione ω nel piano con condizioni iniziali:\nTrovare leggi orarie e traiettoriaMoto A xA0()=x0!xA0()=0⎧⎨⎪⎩⎪yA0()=y0!yA0()=0⎧⎨⎪⎩⎪Moto B xB0()=x0!xB0()=0⎧⎨⎪⎩⎪yB0()=0!yB0()=ωx0⎧⎨⎪⎩⎪",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#6": "\u00007TraiettoriaLuogo dei punti dello spazio per cui passa un corpo (entità unidimensionale)P=P(t)xyzOP1=P(t1)P2=P(t2)Equazione parametrica della traiettoria:ParametriVettore spostamentor⎡⎣⎤⎦=L⎡⎣⎤⎦→(m)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#60": "Moti relativi\n\u000061\nP(')(')POOOPO−=−+−\n()rrOOʹʹ=+−!!Posizioner!rʹ!\n''PPOrrr=+!!\"'Or!‘‘‘‘‘‘‘\nSS’S: sistema di riferimento fermoS’: sistema di riferimento mobile\nSS’",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#61": "Trasformazione della velocità (opz)\n\u000062Velocità''Orrr=+!!!ˆˆˆˆˆˆ(')(')''''''(')rPOxiyjzkPOOOxiyjzkOO=−=++==−+−=+++−!ˆˆˆ'(')''''''ˆˆˆ()(')(')rPOxiyjzkPOOOxiyjzkOO=−=++==−+−=+++−!\nPr!rʹ!\n'Or!’’’’’’’ˆˆˆ'''''''xiyjzk=++v!\"\"\"ˆˆˆxiyjzk=++v!\"\"\"ˆˆˆˆˆˆ'('''''')'''ˆˆˆ'''''''''drdxiyjzkddjdkxiyjzkxyzdtdtdtdtdtι++==+++++!\"\"\"ˆˆˆ'''ddjdkdtdtdtιQuanto valgono                          ?\n!v=d!rdt=d!r'dt+d!rO'dt=d!r'dt+!vO'",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#62": "Formule di Poisson (opz)•Quindi:\n\u000063ˆ'ddtιDerivata di un vettore di modulo costanteˆˆ''ˆˆˆ'''ddajbkdtdtιιι⊥=+drrdtω==×v!!!!Vista nel moto circolare uniforme. E’ generalizzabile !Formule di Poisson: esiste un unico       per cui:ˆ'ˆ'ˆ'ˆ'ˆ'ˆ'ddtdjjdtdkkdtιωιωω⎧=×⎪⎪⎪=×⎨⎪⎪=×⎪⎩!!!ω!ˆˆˆ''''''ˆˆˆ'(')'(')'(')ˆˆˆ('''''')'ddjdkxyzdtdtdtxyjzkxyjzkrιωιωωωιω++==×+×+×==×++=×!!!!!!ω!Descrive la rotazione di S’ rispetto ad S: asse + velocità angolare",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#63": "Trasformazione della velocità (opz)•Posizione:\n\u000064''Orrr=+!!!ˆˆˆ'''''''xiyjzk=++v!\"\"\"ˆˆˆxiyjzk=++v!\"\"\"•Velocità:•Trasformazione:•Velocità di trascinamento:•Un punto fermo in S’ si muove di velocità     in STv!Addizione delle velocità!v=!v'+!ω×!r'+!vO'!v=d!rdt=d!r'dt+d!rO'dt=d!r'dt+!vO'=!v'+!ω×!r'+!vO'!vT=!ω×!r'+!vO'!v=!v'+!vT",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#64": "Trasformazione dell’accelerazione (opz)\n\u000065Accelerazioneˆˆˆˆˆˆ'''''''axiyjzkaxiyjzk=++=++!!\"\"\"\"\"\"\"\"\"\"\"\"'ˆˆˆˆˆˆˆˆˆˆˆˆ('''''''''''')('''''''''''')Oxiyjzkxiyjzkxiyjzkxiyjzka=++++++++++++=!\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"ˆˆˆˆˆˆˆˆˆ('''''')'(')''''('''''')'xiyjzkxiyjzkxiyjzkωωωωω++=×+×+×=×++=×v!!!!!!\"\"\"\"\"\"\"\"\"\"\"\"NB:()ˆˆˆ'(')'ˆˆˆˆ''''diddiidtdtdtωιωιωωιωωι×===×+×=×+××!\"!!!!!\"\"\"\"\n!v=\"xˆi+\"yˆj+\"zˆk=\"x'ˆi'+\"y'ˆj'+\"z'ˆk'+x'ˆ\"i'+y'ˆ\"j'+z'ˆ\"k'+!vO'!v'=\"x'ˆi'+\"y'ˆj'+\"z'ˆk'!a=d!vdt=ddt(\"x'ˆi'+\"y'ˆj'+\"z'ˆk'+x'ˆ\"i'+y'ˆ\"j'+z'ˆ\"k'+!vO')=\nx'ˆ!!i'+y'ˆ!!j'+z'ˆ!!k'==x'(!\"ω×ˆι'+!ω×!ω×ˆι'())+y'(!\"ω×ˆj'+!ω×!ω×ˆj'())+z'(!\"ω×ˆk'+!ω×!ω×ˆk'())==!\"ω×(x'ˆi'+y'ˆj'+z'ˆk')+!ω×!ω×(x'ˆi'+y'ˆj'+z'ˆk')()=!\"ω×!r'+!ω×!ω×!r'()",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#65": "Trasformazione dell’accelerazione\n\u000066Accelerazioneˆˆˆˆˆˆ'''''''axiyjzkaxiyjzk=++=++!!\"\"\"\"\"\"\"\"\"\"\"\"()''2'''odaarradtωωωω==+×+×+××+vv!!!!!!!!!!!\"()'''Toarraωωω=×+××+!!!!!!!\"Accelerazione di trascinamento:Un punto fermo in S’ si muove di accelerazione     in STa!'2''TCOTaaaaaaω=+×+=++v!!!!!!!!2'COaω=×v!!!Accelerazione di Coriolis:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#66": "SR in moto rettilineo uniforme\n\u000067\n''Orrr=+!!!\nxyzx’y’z’'Ov!\n'aa=!!P(t)O’O00COTaa==!!!!\n()''2'''COTCOTOaaaaaarraωωωω=++=×=×+××+v!!!!!!!!!!!!!!\"\nTrasformazioni di Galileo!vO'=costante,!ω=!0x(t)=x'(t)+vO'ty(t)=y'(t)z(t)=z'(t)!v(t)=!v'(t)+!vO'=vx(t)=vx'(t)+vO'vy(t)=vy'(t)vz(t)=vz'(t)⎧⎨⎪⎩⎪!vT=!vO'\n!v=!v'+!vT!vT=!vO'+!ω×!r'",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#67": "Lorenzo RinaldiDipartimento di Fisica e Astronomialorenzo.rinaldi@unibo.itwww.unibo.it/docenti/lorenzo.rinaldi\n\u000068",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#68": "Argomenti opzionali\n\u000069",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#69": "\u000070Velocità areolare\nA(t)=limΔt→0ΔSΔtαΔt→0⎯→⎯⎯π−β[A(t)]=[rv]=[L2T−1]→(m2/s) !A=12P−O()∧!v=12!r∧!v",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#7": "\u00008Rappresentazione intrinseca\nxyzOΩss: ascissa curvilineaDescrizione intrinseca: -Geometria della traiettoria -Origine Ω, verso della traiettoria  -Coordinata curvilinea s(t)s(t)⎡⎣⎤⎦=L⎡⎣⎤⎦→(m)origine della traiettoriaverso",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#70": "\u000071Accelerazione Areolare•Velocità areolare•Accelerazione areolare   costante in direzione à il moto si svolge su un piano   costante à                    MOTO CENTRALE à                    MOTO RETTILINEOClassiﬁcazione dei moti",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#71": "SR in rotazione uniforme\n\u000072\nxy  x’y’O=O’Pr\n'Trrωω=×=×v!!!!!\n'rr=!!\n()'2'COTCOTaaaaaarωωω=++=×=××v!!!!!!!!!!!Un punto fermo in S’ ha accelerazione                                  in S()Tarωω=××!!!!\n''Orrr=+!!!\n()''2'''COTCOTOaaaaaarraωωωω=++=×=×+××+v!!!!!!!!!!!!!!\"\n!v=!v'+!vT!vT=!vO'+!ω×!r'\n!v=!v'+!ω×!r!rO'=!0,!ω=costante",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#72": "Esercizio•Nei pressi di un incrocio a 90° tra due strade si urtano due macchine che viaggiavano rispettivamente a 40 km/h e 50 km/h. Determinare la velocità relativa dell’urto (velocità di una macchina come misurata da un osservatore solidale con l’altra) quando: –L’urto è frontale –L’urto è un tamponamento –L’urto avviene tra due macchine che viaggiavano su due strade a 90° tra loro.\n\u000073",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#73": "Caso 1: tamponamento\n\u000074\n150/kmh=v\n240/kmh=vx’y’z’xyz!v2=!v'2+!vO'=40km/h ˆι!vO'=!v1=50km/h ˆι!v'2=!v2−!vO'=!v2−!v1=−10km/hˆι",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#74": "Caso 2: urto frontale\n\u000075\n150/vkmh=\n240/vkmh=x’y’z’xyz!v2=!v'2+!vO'=−40km/h ˆι!vO'=!v1=50km/h ˆι!v'2=!v2−!vO'=!v2−!v1=−90km/hˆι",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#75": "Caso 3: urto a 90 gradi\n\u000076\n150/vkmh=\n240/vkmh=x’y’z’xyz!v2=!v'2+!vO'=40km/h ˆk!vO'=!v1=50km/h ˆι!v'2=!v2−!vO'=!v2−!v1=−50ˆι+40ˆk()km/h!v'2=502+402=64km/h",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#76": "Esercizio•Su una giostra costituita da una piattaforma rotante si trova un bambino con in mano una pallina. La giostra ruota con una velocità angolare pari a 0.5 s-1 quando, il bambino, che si trova a 4 m dall’asse di rotazione, lancia la pallina con una velocità iniziale pari a 3 m/s. Calcolare la velocità che ha la pallina per un osservatore solidale con il terreno quando il bambino lancia la pallina: –Verso l’asse di rotazione della giostra –In direzione radiale –In orizzontale a 90° dalla direzione radiale.\u000077",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#77": "Impostazione della soluzione\n\u000078\nxy  x’y’O=O’r\nvʹ!\n'vvrω=+×!!!!\nCaso 2:Caso 3:5(/)vms=!!rO'=!0,!ω=costante=0.5ˆk!r=!ʹr=4ˆι(m)\n!v=−3ˆι+0.5⋅4ˆk∧ˆι=−3ˆι+2ˆjCaso 1:!ʹv=3ˆk(m/s)\n!v=3ˆk+0.5⋅4ˆk∧ˆι=3ˆk+2ˆj!ʹv=−3ˆι(m/s)\n!ʹv=3ˆj(m/s)\n!v=3ˆj+0.5⋅4ˆk∧ˆι=5ˆj!v=13 (m/s)!v=13 (m/s)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#78": "Esercizio •Calcolare i moduli delle velocità e dell’accelerazione di un corpo fermo sulla superficie terreste a 45° di latitudine rispetto ad un SR con origine nel centro della terra e assi rivolti verso le stelle fisse.\n\u000079",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#8": "\u00009Velocità scalare media ed istantanea\nVelocità scalare istantanea\nVelocità scalare mediavm⎡⎣⎤⎦=v⎡⎣⎤⎦=ΔsΔt⎡⎣⎢⎤⎦⎥=L/T⎡⎣⎤⎦=LT−1⎡⎣⎤⎦→(m/s)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\03-cinematica.pdf#9": "\u000010Velocità vettoriale media ed istantanea\nVelocità vettoriale media Velocità vettoriale Istantanea",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#0": "Moti in SR NON INERZIALI CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#1": "LAURA FABBRI  -\nMoti in SR in moto relativo: cinematica\n!7Cinematica: movimento è un concetto relativo, legato al SR scelto.ES: oggetto lasciato cadere sul vagone di un treno in moto:  - Cade lungo la verticale per un osservatore sul treno,  - Compie una traiettoria parabolica per osservatore a terra.Entrambi i moti sono veri rispetto al loro SR.  Cambia la descrizione di posizione, velocità…. Un SR può essere solo più “conveniente” computazionalmente\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#10": "LAURA FABBRI  -\nCosa misura una bilancia  in ascensore?\n!50\nAscensore fermo:Ascensore in salita con      costante. Se l’ascensore sale, un corpo fermo in esso sentirà una forza fittizia diretta come  Ascensore in discesa con      costante. Se l’ascensore scende, un corpo fermo in esso sentirà una forza fittizia diretta come  Caso limite: caduta libera→P→/u1D445/u1D463’’",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#2": "LAURA FABBRI  -\nMoti in SR in moto relativo: dinamica\n!11Due principi fondamentali:•Tutti i sistemi di riferimento inerziali sono equivalenti;•Nei SRI le leggi della fisica sono le stesse e per il secondo principioCosa succede se studio il moto in un SR NON INERZIALE?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#3": "LAURA FABBRI  -\nMoti in SR Non inerziali\n!14\nEs: treno in moto con velocità costante. Ragazzo seduto sul treno con un cubetto di ghiaccio sul vassoio. Due SR: S solidale a un osservatore a terra    S’ solidale con l’osservatore sul trenoIn S: sul cubetto non agisce nessuna forza -> si muove con la stessa velocità del SRIn S’: sul cubetto non agisce nessuna forza -> è fermo nel SR ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#4": "LAURA FABBRI  -\nMoti in SR Non inerziali\n!19\nSe treno in moto decelera improvvisamente….In S’ il cubetto vola fuori dal vassoio come se fosse sottoposto a una forza in avanti. È reale?Forza reale: attrito esercitato tra treno e binari, che ha coinvolto tutti gli oggetti solidali al treno in moto. Forza non ha agito sugli oggetti sul treno non solidali ad esso. Moto improvviso non dovuto a forze sull’oggetto ma a forze sul SR che non è più inerziale",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#5": "LAURA FABBRI  -\nSistemi di riferimento non inerziali\n!23•Esiste una classe di sistemi di riferimento in cui NON vale il secondo principio della dinamica: sistemi di riferimento non inerziali •I SR non inerziali sono tutti quelli in moto accelerato rispetto ad un SRI. Es: veicolo in partenza, veicolo in frenata, piattaforma rotante….•Nel SR non inerziale compaiono forze dette fittizie dovute all’accelerazione del nuovo SR.•In un SR non inerziale possiamo scrivere il secondo principio a patto di usare come risultante delle forze sia quelle reali che quelle fittizie",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#6": "LAURA FABBRI  -\nForze fittizie\n!30\n‘‘‘‘‘‘‘SS’\nForza di CoriolisForza centrifuga→/u1D439′\u0000=/u1D45A→/u1D44E′\u0000(/u1D461)=/u1D45A(→/u1D44E−→/u1D44E/u1D450/u1D45C−→/u1D44E/u1D461)S’: sistema di riferimento mobileS: sistema di riferimento fermoUn osservatore in S’ direbbe che sul corpo agisce una forza        tale che:Corpo in S sente una forza reale \nForza fittizia",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#7": "LAURA FABBRI  -\nForza centrifuga\n!35\n-In S (SRI) solidale alla strada agisce una forza reale: forza centripeta;-Oggetti dentro la macchina non sentono l’azione dell’attrito fra ruote e strade e tendono a mantenere il moto rettilineo per il primo principio.  -In S’ (SR NON I) solidale alla macchina: osservatore dentro la macchina si sente spinto verso l’esterno dalla forza centrifuga: •Forza fittizia; •Stesso modulo e direzione della forza centripeta ma verso oppostoForza a cui è soggetto un corpo in moto curvilineo: es macchina in curva",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#8": "LAURA FABBRI  -\nForza di Coriolis\n!40Piattaforma che ruota con velocità angolare costante e corpo lanciato radialmente con velocità iniziale non nulla.In S SRI solidale al terreno moto della pallina rettilineo uniforme poichè non agiscono forze (primo principio).In S’ SR non I solidale alla piattaforma: pallina risente della forza fittizia di CoriolisyxˆjˆiForza a cui è soggetto un corpo in moto in un sistema di riferimento in rotazione\nTraiettoria circolare verso destra rispetto osservatore in S’→/u1D439/u1D450/u1D45C=−2/u1D45A→/u1D714×→/u1D463′\u0000\n→/u1D439/u1D450/u1D45C=−2/u1D45A→/u1D714×→/u1D463′\u0000=−2/u1D45A(/u1D714^/u1D458)×(/u1D463^/u1D456)=−2/u1D45A/u1D714/u1D463^/u1D457",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-MotiNonIneriziali.pdf#9": "LAURA FABBRI  -\nForza di Coriolis\n!45\n•Le masse d’aria si spostano dall’alta pressione H alla bassa pressione L: vento di ciclone; •H e L separate di 1000 km; •Emisfero settentrionale: correnti d’aria tendono verso L deviando a destra; •Vortice ciclonico che ruota in senso antiorario nell’emisfero nord e in senso orario nell’emisfero sudResponsabile di molti fenomeni visibili: •Sbilanciata usura dei binari dei treni orientati secondo i meridiani: treno da nord a sud usura maggiormente il binario di destra; •Moto dei gravi su lunghe distanze (missili…); •Circolazione dei venti→/u1D439/u1D450/u1D45C=−2/u1D45A→/u1D714×→/u1D463′\u0000",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#0": "Forza: dinamometro•Definizione operativa della forza. •Dinamometro: strumento graduato contente una molla ideale, elastica, deformabile.\n!1\n0\n01\n02\n03Calibrazione del dinamometro tramite peso campione: unità kg-forza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#1": "Modello del filo inestensibile•Filo ideale senza massa, in grado di trasportare una forza (TENSIONE) senza allungarsi.\n!2\n01\n01\n01\n01\nIl filo è in grado di trasportare una tensione.  Il dinamometro si allinea al filo.Ciò che misura un dinamometro è un vettore: -Direzione (del filo) -Verso (il filo tira) -Intensità (scala graduata)La forza è un vettore",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#10": "Esercizio 2•Un acrobata, stando nel punto di mezzo di una fune lunga 18 m, esercita una forza di 700 N e fa abbassare la fune di 1.5 m rispetto alle estremità. Determinare la tensione T della fune.\n!11\nP!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#11": "Esercizio 3•Una sfera di peso 4 kg-f si ferma tra due piani inclinati di 30° e 60°. Determinare le reazioni vincolari delle superfici.\n!12\nP!\nP!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#2": "03\nCome funziona un dinamometro?•Il dinamometro misura una forza       esterna generando una forza         tale che\n!3\n03\nF!F−!dinFkl=−Δ!!\"\"Legge di HookeestF!dinF!\n0estdinFF+=!!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#3": "Natura vettoriale delle forze\n!4\n02\n03\n021F!2F!3F!23FF+!!1230FFF++=!!!!In condizioni statiche! (Macchina di Atwood)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#4": "Quiete, equilibrio e statica\n!5Equilibrio: se un sistema (insieme di punti o di corpi) inizialmente quiete in un dato SdR, pur soggetto a forze rimane in quiete, allora esso si trova in uno stato di equilibrio.Quiete: un punto (o un corpo) è in quiete in un dato SdR, se il punto (o ogni punto del corpo) ha una velocità nulla in ogni istante di tempo (è e rimane fermo). Assenza di velocità!\nStatica: studio delle forze nei sistemi in stato di equilibrioEquilibrio stabile: piccole variazioni nel sistema portano a piccoli spostamenti dalla posizione di equilibrio Equilibrio instabile: piccole variazioni nel sistema portano a grandi spostamenti\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#5": "Statica del punto materiale•Risultato sperimentale: il punto è in quiete se:\n!61F!2F!\n3F!1234RFFFF=+++!!!!!\n4F!\n12340RFFFF=+++=!!!!!!Risultante delle forze applicate al puntoCondizione necessaria per l’equilibrio di un punto materiale è che si annulli la risultante        di tutte le forze ad esso applicate.R!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#6": "Studio statico delle forzeFORZA PESO\n!7\n01!P!FdinIdea: applicare ad un corpo una forza tramite il dinamometro, adattando verso, direzione e modulo fino a raggiungere l’equilibrio!Fdin+ EQ ⇒!R=0!R=!Fdin+ ?\nAd ogni punto materiale posto in prossimità della superficie terrestre risulta applicata una forza diretta lungo la verticale, verso il basso, con intensità dipendente dal corpo materiale.!P=−!Fdin",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#7": "REAZIONE VINCOLARE\n!8\n!P!RV!P+ EQ ⇒!R=!P+!RV!RV=−!P\n!F\nQuando la superficie di un corpo materiale C, giungendo a contatto con la superficie di un corpo materiale V (vincolo) esercita su tale superficie una forza perpendicolare F, determina una deformazione di V che esercita a sua volta su C una forza RV uguale e contraria ad F   㱺  RV: normale alla superficie, uscente e di modulo dipendente dalla forza applicata F  CV!RV",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#8": "!9Un vincolo impedisce alcuni movimenti del corpo considerato e ne consente altri (es.: rotaia treno, cardine porta, piano su cui è appoggiato un oggetto, ecc.). Per impedire i movimenti vietati dei corpi, i vincoli debbono esercitare sui corpi delle forze, dette forze vincolari o reazioni vincolari.Le forze vincolari sono a priori sconosciute, in quanto debbono adeguarsi alle circostanze per neutralizzare le forze attive che potrebbero causare movimenti vietati.\nVincolo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica-integrazione.pdf#9": "Vincoli ideali o lisci•Vincolo ideale o liscio: vincoli che non offrono resistenza apprezzabile quando le forze tendono a produrre degli spostamenti tangenziali rispetto alla loro superficie\n!10\nPF!VR!\nPF!VR!•In caso contrario, se c’è resistenza ai movimenti tangenziali, parleremo di vincolo scabro (forze d’attrito)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#0": "DINAMICA CdS Ingegneria InformaticaA.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#1": "Concetto di forza\n\u00002Grandezza ﬁsica vettoriale (intensità, direzione, verso e punto di applicazione): vettore applicatoLe forze possono produrre variazioni dello stato di moto degli oggetti su cui agisconoLe forze possono deformare i corpi su cui agisconoLe forze possono compensarsi, determinando situazioni di equilibrioLe forze si presentano sempre in coppia: derivano sempre da interazioni tre i corpi, che esercitano forze l’uno sull’altro",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#10": "Formulazione esplicita del 2° principioNewton: “La forza è uguale alla massa per l’accelerazione.\" \n\u000011\nIn un sistema di riferimento inerziale, la forza complessiva (totale, risultante) che agisce su un corpo materiale di massa m è tale che:Fma=!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#11": "Unità di misura della forza\n\u0000122o Principio Unità di misura della massa nel sistema internazionale: chilogrammo (kg) Prototipo: cilindro di platino-iridio c/o Bureau International des Poids et Mèsures a Sèvres dal 22/5/2019 nuova definizione basata sulla costante di PlankUnità di misura della forza nel sistema internazionale: Newton (N) Corrisponde alla forza che, agendo su una massa di 1 kg le imprime un’accelerazione di 1 m/s2Unità di misura della forza nel sistema tecnico: chilogrammo-forza (kgf) È il peso del cilindro di cui sopra,nei luoghi in cui g=9,80665 m/s2, detto valore “standard”Per convenzione\nDerivata per definizione\n1kgf9.80665N;1N0.101972kgf==Per convenzione\n!F=m!a\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#12": "Forza peso: misura della massa inerzialeN corpi con pesi\n\u000013\na!!P29,8 m/sag==!12,,,NPPP!!!…12Naaag====!!!…12,,,Naaa!!!…cadono con accelerazionig=!Pmg=!!Osservazione sperimentale valida (con opportune approssimazioni) in ogni punto della Terra.Approssimazioni: ignoro gli attriti, la forma non sferica della terra, considero di essere in un SRI",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#13": "Misura della massa inerziale\n\u000014Presa una massa campione      ho anche un peso campionecmccPmg=!!\nPmg=!!cP!cRPλ=!!ccPRPPλ==!!!!cccPmgmmgmPλ===!!!!Condizione di equilibrio:cPRPλ==!!!\nccPmmP=!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#14": "Principi fondamentali•Il secondo principio NON è una definizione di forza\n\u000015•SdR: Inerziale•La forza è ciò che indica il dinamometro•La massa è una proprietà dei corpi•L’accelerazione è una caratteristica cinematicaFma=!!•In ogni SRI vale:Fma=!!\n''SSaa=!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#15": "Indipendenza delle azioni simultanee\n\u000016\nTTFma=!!11Fma=!!22Fma=!!12FF=+=!!()12Tmaama=+=!!!2a!1a!\nOgni forza produce un effetto indipendentemente dalla presenza di altre forze.L’effetto complessivo è dato dalla risultante di tutte le forze applicate.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#16": "Relazione tra moto e cause•Tutti i problemi di determinazione del moto di un corpo a partire dalle forze che agiscono sono problemi inversi di cinematica\n\u000017Fma=!!Fam=!!nototrovare (),  ()atrt=v!!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#17": "Moto circolare uniforme\nnF!Rappresentazione intrinseca della forza\nMoto rettilineo\n\u000018\nˆtuˆnuF!tF!\nˆˆtnttnnFFFfufu=+=+!!!ˆˆ()ttnnFmamauau==+!!2forza tangenteforza centripetattnnfmafmamR===v00tnffR≠=→=+∞200/tnnffRmf=≠→=vˆˆttnnaauau=+!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#18": "Moto circolare uniforme\n\u000019\nF!r!v!drrdtω==×v!!!!costanterRωω===v!!!22costanteaRRωω====vv!!!??2nnFmafmamRω===!!\nv!T!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#19": "Caduta dei graviGrave: punto materiale o oggetto in moto a causa del suo peso\n\u000020\nz\nhPmg=!!ˆkProblema: studiare la caduta di un grave di massa m che parte da fermo da una quota hˆˆPmgmgkmzk==−=!!\"\"zg→=−!!0()(0)()tztzgdtʹ→=+−∫!!()ztgt→=−!200()(0)(')'''2ttgztzztdthgtdtht→=+=−=−∫∫!Legge oraria:2()2gztht=−Velocità:()ztgt=−!Tempo di caduta:2()0hhhzttg=→=vmax=v(th)=2gh",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#2": "Concetto di forza\n\u00003Forze di contattomacroscopicamente sono associate ad un contatto tra corpi interagenti (es. spinta di un oggetto, forze elastiche)Forze a distanzaLe interazioni avvengono senza contatto (forza peso, forze elettriche e magnetiche)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#20": "Tutti i corpi cadono nello stesso modo\n\u000021Tempo di caduta:2()0hhhzttg=→=\n11Pmg=!!22Pmg=!!33Pmg=!!1ag=!!2ag=!!3ag=!!vmax=v(th)=2gh",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#21": "Un punto materiale inizialmente (per t=0 s)  fermo in y=0  è soggetto ad una accelerazione pari a                         nell’intervallo di tempo 0 < t < 𝜏 ,                              nell’intervallo di tempo 𝜏 < t < 2𝜏  e a(t) = 0 per t >2𝜏, con a0 = 0.5 m/s2 e 𝜏 = 2 s.                                      Determinare la velocità e la posizione nell’istante t = 4𝜏.             \nEsercizio\n\u000022()()otatyaτ==!!()(2)otatyaτ==−!!v(4τ)=a0τ=1m/sy(4τ)=3a0τ2=6m",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#22": "Piano Inclinato liscio\n\u000023\nP!\nVR!tP!nP!sintPPα=!\nαcosnPPα=!hcosVnNRPPα===!!\n\u000023|⃗PT|=Psinα=mgsinα=maa=gsinαcostante:moto unif. accel.Lunghezza del pianoL=hsinαs(t)=s0+vot+12at2L=hsinα=12gsinαt2v(t)=v0+att=2Lgsinα=2hgsin2αv(t)=gsinα2hgsin2α=2ghs0=0v0=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#23": "Forza elastica / Legge di Hooke•k costante elastica della molla. [k]=[MT-2]\n\u000024\n03\n0\nXF!\nF!0F=!!ˆ()Ffxι=!(0)0f=0,()0xfx><0,()0xfx<>ˆFkxι=−!()fxx\nFkr=−!!0()Fkrr=−−!!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#24": "Oscillatore armonico unidimensionale\n\u000025\n0\nmˆFkxι=−!ˆaxι=!\"\"Fmakxmx=→−=!!\"\"0mxkx→+=!!0kxxm+=!!2pongo  0kmω=>20xxω+=!!\nx+l-l\n0(0)cos()xlφ=\n0()cos()xtltωφ=+kmω=\n000arctanxφω=−v()0220lxω=+v22mTkππω==SRIEquazione oraria ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#25": "Regime delle piccole oscillazioni•Osservazione: ogni sistema in prossimità di un punto di equilibrio stabile si comporta come un oscillatore armonico\n\u000026ˆ()Ffxι=!()fxx\n()0fx=Punti di equilibrioAB()0Bfxʹ>xB punto instabile\n()0Afxʹ<xA punto stabile\n()2()()()()()AAAAfxfxfxxxOxxʹ+−+−!()()AAxxfxkxx→⎯⎯⎯→−−Moto oscillatorio attorno al punto di equilibrio stabile xA2():;2()AAkfxmTmmfxωπʹ−===ʹ−",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#26": "Pendolo sempliceGalileo: osservazione del moto del lampadario nel Duomo di Pisa\n\u0000271.Ampiezza max a DX = SX2.Il periodo del pendolo (T) è  indipendente dall’ampiezza massima3.Il periodo non dipende dalla massa ma solo dalla lunghezza del ﬁlo\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#27": "Pendolo semplice\n\u000028\nMetodo: 1.“Inquadrare” il problema 2.Scrivere la F = ma  3.Individuare il SdR migliore  4.Scrivere le equazioni parametriche 5.Integrare 6.Calcolare le costanti arbitrarie\n0v!Ol\nC\nθslθ=\ngmP!!=VR!RgmF!!!+=\nˆtuˆnu21ˆˆtnasusuρ=+!\"\"\"\n0(0)(0)0(0)(0)lslslρθθ=====v!!ˆˆsinttmsumguθ=−!!sin0glθθ+=!!()2ˆˆcosnVnsmumgRulθ=−+!2cosVmlmgRθθ+=!\nam!=\nPiccole oscillazioni (sinθ ≅ θ )0glθθ+=!!\n()0()singlttglαθαωω⎧=⎪=⎨=⎪⎩v\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#28": "Pendolo semplice II\n\u0000292cosVRmlmgθθ=+!()0()singlttglαθαωω⎧=⎪=⎨=⎪⎩v()2221122()cos,cos11sintttθαωωθθαω=−=−!\"()()222222223122cos1sin1sinVRmltmgtmgtαωωαωααω=+−=+−Durante il moto, la reazione vincolare cambia. ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#29": "Nota storica•Le oscillazioni di un pendolo hanno costituito un primo sistema meccanico per misurare il tempo\n\u000030Esercizio pratico per casa:Costruire un pendolo e misurare l’accelerazione di gravitàElementi: punto ﬁsso, ﬁlo, massa, cronometro, metro\nL22/TLgππω==\n224LgTπ→=22/TLgππω==\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#3": "Forze fondamentali\n\u00004\nLa maggior parte delle forze sono riconducibili alla forza elettromagnetica •Forze di contatto (attrito, viscosità, reazioni vincolari) •Forze elastiche •Forze chimiche (molecolari e biologiche) Ad oggi sappiamo che esistono 4 forze fondamentali della natura",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#30": "Forza di attrito statico\n\u000031\nP!VR!\n03\nF!Se il corpo NON si sposta nonostante la forza orizzontale introduciamo una nuova forza di attrito: l’attrito staticoASF!Caratteristiche: E’ una forza di contatto; di entità NON nota a priori.E’ una caratteristica delle superﬁci (secche, non lubriﬁcate) Ma non dipende all’area di contatto !Dipende da tutte le forze agenti sui corpi.Quando esiste in condizioni statiche, annulla sempre la risultante.Per ogni situazione esiste un valore massimo della forza. \nVincolo ruvido!vincoli o superﬁciNON ideali",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#31": "Forza di attrito statico\n\u000032\nP!VR!\n03\nF!ASF!0ASFF+=!!!maxmax:ASASASFFF∃≤!maxASSFNµ=N: forza perpendicolare alle superfici      forza di carico µS: coefficiente di attrito staticoVNRP==!!Tipicamente:0.011Sµ<<",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#32": "Piano inclinato ruvidoProblema: sapendo che                , qual è il valore massimo di α per cui il corpo NON si muove?\n\u000033\nP!\nVR!tP!nP!sintPPα=!\nαcosnPPα=!hSe il corpo non si muove nonostante la presenzadi una forza non bilanciata                     allora esiste una forza di attrito staticosintPPα=!ASF!0tASPF+=!!!cosVnNRPPα===!!maxcosASSSFNPµµα==maxtASASPFF==!!maxsincosASASSPFFPαµα→===!sintancosSαµαα→==0,5Sµ=arctan0,463646...0,46Sradαµα→==→=26,565...27α→==°\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#33": "Importanza dell’attrito statico\n\u000034\nASF!,av!!\nPF!ASF!\nASF!VR!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#34": "Importanza dell’attrito statico\n(/)(/)1SSrocciametallogommaasfaltoµµ≈≪Indipendente dall’area di contattoASF!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#35": "EsercizioUn mattone è appoggiato su una scanalatura rettilinea ruvida inclinata di un angolo α=35° rispetto ad un piano orizzontale e raccordato nel punto B con un pavimento orizzontale. Le due superfici hanno lo stesso coefficiente di attrito cinetico µc = 0,4. All’istante t = 0 il mattone viene lasciato in quiete da una altezza h = 3 m (punto A). Studiare il moto del mattone calcolando la velocità massima e la lunghezza totale del percorso.\n\u000036\nαhABC\nmax?vLABBC=+\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#36": "Esercizio – Passo 1\n\u000037\nαhAB\nP!\nVR!tP!nP!ADF!ˆιa!iADtFFPma=+=∑!!!!ˆsintPmgαι=!ˆˆcosADCCFNmgµιµαι=−=−!ˆaxι=!\"\"(sincos)ADtCFPmamxmgmgαµα+=→→=−!!!\"\"2(sincos)2,41 m/sCxgaαµα=−==!!()(0)(')'xtxxtdtat=+=∫!!!!2()(0)(')'2txtxxtdta=+=∫!x0,/sinABxxhα==\n222:()2,08 s 2sinsinBBBBthxhtxtataaαα==→===max2:()25,02 m/sBBBBxxtataaxa====v!/sin5,23 mBxhα==",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#37": "Esercizio – Passo 2\nDistanza percorsa: \u000038C\nˆιADF!\nP!VR!v!BiADtFFPma=+=∑!!!!0tP=!!ˆˆADCCFNmgµιµι=−=−!ADtCFPmamxmgµ+=→→=−!!!\"\"23,92 m/sCxgaµ=−=−=−!!()(0)(')'Bxtxxtdtat=+=−∫v!!!!2()(0)(')'2Btxtxxtdtta=+=−∫v!()0Cxt=→!/1,28 sCBta==v8,45 mBCDxx=+=xxC=vBtC−atC22=vB22a=3,22 m",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#38": "Esercizio•  Una pallina di massa M=0,2 kg è ferma tra una superficie verticale ed un piano inclinato di un angolo α=15°, come mostrato in figura. Determinare le reazioni vincolari della superficie verticale e del piano inclinato.\n\u000039\nα2VR!1VR!1tan0,52550,526VRmgNNα===…2/cos2,03..2,03VRmgNNα===",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#39": "Un punto materiale inizialmente (per t=0 s)  fermo in y=0  è soggetto ad una accelerazione pari a                         nell’intervallo di tempo 0 < t < 𝜏 ,                              nell’intervallo di tempo 𝜏 < t < 2𝜏  e a(t) = 0 per t >2𝜏, con a0 = 0.5 m/s2 e 𝜏 = 2 s.                                      Determinare la velocità e la posizione nell’istante t = 4𝜏.             \nEsercizio\n\u000040()()otatyaτ==!!()(2)otatyaτ==−!!v(4τ)=a0τ=1m/sy(4τ)=3a0τ2=6m",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#4": "Galileo Galilei (1564-1642)Qual è il moto di un corpo non soggetto ad alcuna forza?\n\u00005\n!P!RV!v=costSe riusciamo ad eliminare tutti gli attriti PRINCIPIO DI INERZIA",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#40": "EsercizioUn proiettile di massa M viene sparato orizzontalmente da un cannone fermo posto su una altura che si eleva di h=50 m rispetto alla pianura circostante. Determinare: 1)il modulo v  della velocità con cui si deve sparare il proiettile affinché colpisca un bersaglio nella pianura e che dista orizzontalmente dal cannone di D=250 m; 2)l’angolo con cui il proiettile colpisce il bersaglio;  3)la velocità scalare del proiettile quando colpisce il bersaglio.\n\u000041vf=D2g2h+2gh=84m/stanα=vyvx=2hD=0,4⇒α=21,8°v0=Dg2h=78,3m/s",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#41": "EsercizioUna molla ideale, di costante elastica k, è sospesa in verticale tramite un aggancio in alto. Ad un certo istante (t=0) si applica una massa m all’altro estremo della molla, che viene quindi lasciata libera. Trovare il moto del punto nella direzione verticale. In presenza di un piccolo attrito, dove si fermerà il punto?\n\u000042\nxzBOPbz\neF!\nPF!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#42": "EsercizioIl sistema meccanico in ﬁgura è costituito da tre corpi uguali di massa m, un ﬁlo inestensibile ed una superﬁcie ruvida con coefﬁciente di attrito statico 0.6 e coefﬁciente di attrito cinetico 0.4. Determinare: 1) se il sistema è in condizioni di staticità; 2) la tensione nel ﬁlo; 3) cosa succede se il corpo 3 è eliminato dal sistema.\n\u000043\n3\n2\n1\nˆTmgι=!ˆPmgj=−!ASF!max2ASASSSFFNmgµµ<==!Statica ?0ASTF+=!!!maxASASTFF=<!!?21.2cmgmgmgµ→<=1. Si, il sistema è in equilibrio statico. max1.2ASASFmgFmg=<=!ˆTmgι=!3. maxASASTFF=<!!?0.6cmgmgmgµ→<=No: il sistema non è statico e si muove",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#5": "Primo Principio della Dinamica\n➡criterio cinematico per stabilire quando su di un punto materiale non agiscono forze. \n\u00006\nSe in un dato sistema di riferimento la risultante delle forze applicate ad un punto materiale è nulla allora il punto materiale o rimane in quiete o si muove di moto rettilineo uniforme.Il moto è relativo:Si muove rispetto a cosa? \nSistema di riferimento inerziale \nFormulazione moderna Esiste almeno un sistema di riferimento, detto “inerziale” (SRI), rispetto al quale un qualunque punto materiale che sia sufficientemente lontano da tutti gli altri corpi, o rimane in quiete o si muove di moto rettilineo uniforme.“Principia”  di Newton:\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#6": "Classe di Sistemi di Riferimento Inerziali\n\u00007Sia dato un punto su cui non agiscono forze in un SRI   S:  il punto si muove di moto rettilineo uniforme in S:      = costante\nxyzx’y’z’\n''aaSS=!!P(t)O’Ov!v!SdR S’ in moto rettilineo uniforme\nS’ è un sistema di riferimento inerzialeTrasformazioni di Galileo!vO'=costante,!ω=!0!vO'x(t)=x'(t)+vO'xty(t)=y'(t)+vO'ytz(t)=z'(t)+vO'zt!v(t)=!v'(t)+!vO'vx(t)=vx'(t)+vO'xvy(t)=vy'(t)+vO'yvz(t)=vz'(t)+vO'z⎧⎨⎪⎩⎪!v'=!v−!vO'=costante",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#7": "Esiste un SRI privilegiato?S ed S’ sono due SRI: come posso distinguerli sperimentalmente operando solo all’interno di un SRI ?\n\u00008\n''aaSS=!!\nR: No, non esiste un SRI privilegiato\nPrincipio di relatività galileiano Tutte le leggi della fisica si scrivono nello stesso modo in ogni sistema di riferimento inerziale.\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#8": "Approssimazioni di SRI1.Sistema solidale alla terra (SR principale in statica) 2.Sistema con origine nel centro della terra e assi rivolti verso le “stelle fisse”  3.Sistema con origine nel centro del Sole e assi rivolti verso le stelle fisse 4.Sistema con origine nel centro della nostra galassia e assi rivolti verso le galassie più lontane \u00009()'2oCOTaarraaaaωωωωʹʹʹʹʹ=+×+×+××+=++v!!!!!!!!!!!!!\"22221 d,6370 km,0,035 m/sTTRarRTπω⎛⎞=====⎜⎟⎝⎠622356 d1 y,15010 km,0,0059 m/sTTRarω=====i22200 Ma,26000 al,0,00000000025 m/sTTRarω====\n29.8 m/sg=",
    "data_test\\rootfolder\\università\\FisicaGenerale\\04-dinamica.pdf#9": "Principi fondamentali\n\u000010\n2o Principio Un qualunque punto materiale che sia sottoposto ad una o più forze ha un’accelerazione vettorialmente proporzionale alla risultante di tali forze.\namF!!=CausaEffettoRisulta essere: -Positiva -Indipendente da posizione e velocità -Proprietà additiva dei corpi12Tmmm=+Massa inerziale ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#0": "LAVORO ed ENERGIA CdS Ingegneria Informatica A.A. 2019/20\n1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#1": "Lavoro ed Energia: definizioni intuitive•Lavoro: caratteristica di una forza di operare uno spostamento •L’energia è la capacità di produrre lavoro •Il lavoro è il processo attraverso il quale una certa quantità di energia si trasferisce da un corpo a un altro. •Ingredienti per una definizione più rigorosa di Lavoro: forza e movimento",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#10": "Lavoro di una forza elastica\n11\nEsempio: lavoro compiuto da una molla compressa da l1 fino all’ espansione l2δℒ=⃗F⋅d⃗l=(−kx̂ı)⋅(̂ıdx)=−kxdx(̂ı⋅̂ı)=−kxdxℒ1,2=∫x2x1⃗F⋅d⃗l=−k∫x2x1xdx−k[x22]x2x1=−k2x22+k2x21=−k2(l2−l0)2+k2(l1−l0)2ℒ1,2=−k2(x22−x21)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#11": "Lavoro della forza peso\n12\nSARA VALENTINETTI  -\nStessa conclusione se al posto del piano inclinato abbiamo un profilo curvo.Lavoro infinitesimoLavoro totale\n𝑧1𝑧2 ℒ1,2=mghh = differenza di quota a cui si porta il punto\n𝑑𝑙𝑧1𝑧2Esempio: punto materiale di massa m scivola su un piano liscio inclinato di un angolo  da un punto P1 a un punto P2 a differenza di quota hδℒ=⃗F⋅d⃗l=(−mĝk)⋅(̂ıdx+̂kdz)==−mg(̂k⋅̂ı)−mg(̂k⋅̂k)=−mgdzℒ1,2=∫P2P1⃗F⋅d⃗l=−mg∫z2z1dz=−mg(z2−z1)=mgh>0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#12": "Proprietà additiva dei lavori\n13Punto materiale soggetto a N forze, equivale a un punto soggetto alla risultante delle forzeLavoro infinitesimo di ciascuna forza:Lavoro infinitesimo totale:Date n forze Fi , applicate allo stesso punto P, che si muove lungo una propria curva  dal punto A al punto B,  il lavoro complessivo è dato da….ℓLo spostamento è lo stesso per ogni forza perché agiscono tutte sulla stessa particella che compie un tratto di traiettoria.\nSomma dei lavori delle singole forze.δℒ=⃗F⋅d⃗lδℒtot=δℒ1+δℒ2+...=⃗F1⋅d⃗l+⃗F2⋅d⃗l+...=(⃗F1+⃗F2+...)⋅d⃗l=(∑i⃗Fi)⋅d⃗l=⃗R⋅d⃗lℒℓtot=∫BAℓ⃗R⋅d⃗l=∫BAℓ(∑i⃗Fi)⋅d⃗l=∫BAℓ(∑i⃗Fi⋅d⃗l)=∑i∫BAℓ⃗Fi⋅d⃗l=∑iℒℓi",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#13": "Potenza di una forzaDef: capacità di produrre lavoro per unità di tempo\n14•Per un punto materiale:Nuova definizione di lavoroLavoro compiuto da una forza per unità di tempo durante un intervallo di tempo infinitesimo P=δℒdtP=δℒdt=⃗F⋅d⃗ldt=⃗F⋅d⃗ldt=⃗F⋅⃗vP=⃗F⋅⃗vδℒ=Pdt⟹ℒ=∫Pdt",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#14": "Analisi dimensionale\n15LavoroL⎡⎣⎤⎦=F⋅ds⎡⎣⎤⎦=N⋅m⎡⎣⎤⎦=kg⋅ms2⋅m⎡⎣⎢⎤⎦⎥=MLT−2L⎡⎣⎤⎦⇒L⎡⎣⎤⎦=ML2T−2⎡⎣⎤⎦Unitá di misura: SI-MKS  Joule(J)=N⋅mPotenzaP⎡⎣⎤⎦=LΔt⎡⎣⎢⎤⎦⎥=F⋅dsΔt⎡⎣⎢⎤⎦⎥=N⋅ms⎡⎣⎢⎤⎦⎥=kg⋅ms2⋅m⋅1s⎡⎣⎢⎤⎦⎥=MLT−2LT−1⎡⎣⎤⎦⇒P⎡⎣⎤⎦=ML2T−3⎡⎣⎤⎦Unitá di misura: SI-MKS  Watt(W)=Joule/s",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#15": "Esercizio\n16Determinare la potenza istantanea sviluppata durante la caduta su un piano inclinato di un angolo  alto h da un punto A in cima al piano ad un punto B in fondo al piano da un punto materiale di massa m.α",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#16": "Teorema delle Forze Vive\n17\nEs: Corpo in moto con velocità  su piano orizzontale liscio contro molla. Corpo comprime molla perdendo velocità. Molla esercita forza tale che  e  opposti e       v⃗Fdxℒ<0•Lavoro: scalare legato alla forza e allo spostamento •Maxwell: “il lavoro è l’atto con cui viene realizzata una modificazione (deformazione o spostamento) nella configurazione di un sistema materiale contro le forze che a questa modificazione si oppongono” •Il lavoro è una forma di energia. Es: macchina che compie lavoro trasferisce energia da un corpo all’altro.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#17": "Teorema delle Forze Vive\n18•Lavoro: scalare legato alla forza e allo spostamento •Maxwell: “il lavoro è l’atto con cui viene realizzata una modificazione (deformazione o spostamento) nella configurazione di un sistema materiale contro le forze che a questa modificazione si oppongono” •Il lavoro è una forma di energia. Es: macchina che compie lavoro trasferisce energia da un corpo all’altro.\nAlla fine del moto il corpo è fermo e molla compressa esercita forza che tende a ridistenderla, in grado di rimettere in moto il corpo. In questo caso  e  hanno stesso verso e     ⃗Fdxℒ>0Relazione fra velocità e lavoroSperimentalmente  ⃗vfinale=−⃗viniziale",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#18": "Teorema delle Forze Vive\n19Descrive il lavoro compiuto da un sistema di forze qualunque (attive, vincolari, interne, esterne, di interazione o apparenti), su un sistema meccanico qualunque (puntiforme, esteso, rigido, non rigido, vincolato, ecc.). Teorema delle forze vive per il punto materiale:Lavoro compiuto da tutte le forze per spostare corpo da A a B lungo un tratto di traiettoriaSostituisco la definizione di d⃗lProprietà delle derivate principio: risultante di tutte le forzeℒA,B=∫BA⃗F⋅d⃗l=∫BAmd⃗vdt⋅d⃗ld⃗l=⃗vdt⟹ℒA,B=∫BA⃗F⋅d⃗l=∫BAmd⃗vdt⋅⃗vdtℒA,B=∫BA⃗F⋅d⃗l=∫BAmd⃗vdt⋅⃗vdt=∫BAm12dv2dtdt=12m∫BAdv2=12m[v2]BA=12mv2B−12mv2A",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#19": "Teorema delle Forze Vive\n20Il lavoro compiuto dalla risultante delle forze che agiscono su un sistema meccanico qualunque, nel passaggio da una configurazione A ad un’altra B, è uguale alla corrispondente variazione dell’energia cinetica di tale sistema.Se  forza ha accelerato il corpo compiendo un lavoro positivoℒ>0⟹vB>vA→Se forza ha decelerato il corpo compiendo un lavoro negativoℒ<0⟹vB<vA→Energia Cinetica: 1. Ha le dimensioni del lavoro       2. Non è  mai negativa  (T>=0)[𝑇]=[12𝑚𝑣2]=[𝑀𝐿2𝑇−2]→JouleT=12mv2ℒA,B=12mv2B−12mv2A=TB−TA",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#2": "Lavoro ed Energia: MacchineUna macchina è un dispositivo vincolato capace di spostare il punto di applicazione di una forza, chiamata “resistente”, sfruttando un’altra forza chiamata “motrice”.\nUna macchina “vantaggiosa” sposta il punto di applicazione di una forza resistente utilizzando una forza motrice di modulo più piccolo.carrucolapiano inclinatoleva",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#20": "Energia Potenziale\n21\nLavoro sul corpo (teorema delle forze vive):Corpo in moto su un piano orizzontale liscio contro una molla ideale •velocità iniziale del carrello v0,  •velocità finale nulla poiché comprime una molla che esercita una forza opposta allo spostamento del corpo provocandone l’arresto.ℒcorpo=Tfin−Tin=12mv2fin−12mv2in=0−12mv20<0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#21": "Energia Potenziale\n22•Molla inizialmente in quiete si comprime per effetto di una forza premente.  •Chi produce la forza compressiva è il carrello e l’entità della compressione è una misura della forza agente.  •Forza reagente della molla è uguale e opposta alla forza premente.Lavoro sulla molla (teorema delle forze vive):   \nDal punto di vista della molla…\nℒmolla=Tfin−Tin=12mv2fin−12mv2in=12mv20−0=−ℒcorpo>0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#22": "Energia Potenziale\n23•Corpo in ragione della velocità compie un lavoro positivo sulla molla comprimendola; •Energia di un corpo: possibilità di un corpo di compiere un lavoro positivo; •Energia del carrello in moto si trasferisce gradualmente alla molla; •Quando il carrello si ferma, tutta la sua energia cinetica iniziale è trasferita alla molla compiendo un lavoro positivo su di essa; •Molla immagazzina l‘energia del corpo comprimendosi; •Molla poi rilascia gradualmente l’energia immagazzinata distendensosi e imprimendo al corpo una velocità uguale e contraria: restituisce energia cinetica al carrello; •energia totale immagazzinata dalla molla ridiventa interamente energia cinetica",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#23": "Energia Potenziale\n24\n0Lavoro forza elastica:Molla compressa di una certa quantità a ad un certo tempo t>0Teorema forze vive:Uguagliando:Energia iniziale: cineticaEnergia totale istantanea durante la compressioneLavoro che sarebbe in grado di fornire la molla ridistendendosi -> misura dell’energia propria immagazzinata dalla molla: Energia potenzialeLa somma dell’energia cinetica e di quella potenziale si conserva IN QUESTO moto.ℒel=Tfin−Tin=12mv2−12mv20ℒel=∫finin⃗Fel⋅d⃗l=∫finin(−kx̂ı)⋅(̂ıdx)=∫finin−kxdx=−k[x22]finin=−kx2212mv2−12mv20=−kx22⟹12mv2+kx22=12mv20kx22",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#24": "Esercizio\n25Un blocco di ghiaccio è in moto su una salita inclinata di  Sapendo che all’inizio la sua velocità è pari a v = 9 m/s e che il coefficiente di attrito dinamico è pari a , di quanto si sposta lungo il piano inclinato il blocco di ghiaccio prima di fermarsi?α=6∘μc=0.07",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#25": "Esercizio\n26Un blocco P di massa m = 3 kg si muove di moto rettilineo su un piano orizzontale scabro nella direzione dell’asse di una molla non deformata, di cui va a colpire uno degli estremi, mentre l’altro è bloccato a un supporto verticale fisso. La molla, di costante elastica k = 300 N/m, viene compressa di  Sapendo che il coefficiente di attrito dinamico fra P e il piano è , determinare: 1) I lavori compiuti durante tale compressione dalla forza elastica e dalla forza di attrito; 2) Il modulo della velocità di P nel momento in cui colpisce la molla. δ=8cmμc=0.25",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#26": "Esercizio\n27Un cubetto P di massa m scivola lungo il segmento AB disposto lungo un piano inclinato di un angolo  rispetto alla direzione orizzontale. Il coefficiente di attrito dinamico passa dal valore massimo di ½ alla sommità A al valore 0 alla base B secondo una legge del tipo  dove e k sono costanti positive e s è la distanza da A di un generico punto di AB. Sapendo che  e che P è partito da fermo in A, calcolare il modulo v della sua velocità all’istante in cui arriva in B in termini di  e della variazione di quota h fra A e B.Ah𝛼B",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#27": "Alcuni concetti matematici\n28•Derivate parziali: primo e secondo ordine, miste… •Differenziali; •Campi: scalari, vettoriali…",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#28": "Derivate parziali\n29Se una funzione esiste per ogni valore della variabile nel dominio, derivate parziali al primo ordine:Se le derivate parziale al primo ordine esistono per ogni valore della variabile nel dominio, derivate parziali al secondo ordine:\nDerivate parziali miste (l’ordine non influenza)𝜕𝑧2Es: ∂f(x,y,z)∂xx0,y0,z0=limΔx→0f(x0+Δx,y0,z0)Δx∂f(x,y,z)∂yx0,y0,z0=limΔy→0f(x0,y0+Δy,z0)Δy∂f(x,y,z)∂zx0,y0,z0=limΔy→0f(x0,y0,z0+Δz)Δz∂2f(x,y,z)∂x∂y,∂2f(x,y,z)∂y∂z,∂2f(x,y,z)∂x∂zf(x,y,z)=x3−y2+3z",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#29": "DifferenzialiSia  una funzione a 3 variabili. Quanto varia il valore della funzione se ci spostiamo da un punto  a un punto infinitamente vicino  ?f(x0+dx,y0+dy,z0+dz)=f(x0,y0,z0)+∂f∂xP0dx+∂f∂yP0dy+∂f∂zP0dzDifferenzialedF=f(x0+dx,y0+dy,z0+dz)−f(x0,y0,z0)=∂f∂xdx+∂f∂ydy+∂f∂zdzPiù ordini successivi\n30",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#3": "Definizione di lavoro infinitesimo\n4\nP1P2\nxyzOLavoro infinitesimo compiuto da  durante uno spostamento infinitesimo   la quantità scalare:⃗Fd⃗lPer definire lavoro infinitesimo compiuto da una forza:In un intervallo di tempo   i, punto si sposta da P1 a P2 :    ΔtΔ⃗r=⃗r2−⃗r1-Regione di spazio in cui agisce  -Punto P si muove lungo linea curva   ⃗FℓSe  piccolo,             tangente ΔtΔ⃗r→d⃗l=̂utdlLavoro infinitesimo  perché non è un differenziale esatto (in generale non dipende solo dagli estremi in cui si integra).δLδℒ=⃗F⋅d⃗lΔ⃗rℓ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#30": "CampiCampo scalare: una grandezza scalare funzione delle coordinate spaziali U(x,y,z) definita ovunque dentro una certa regione di spazio. Superficie di livello: luogo geometrico dei punti dove la funzione scalare assume un valore costante prefissato  U(x,y,z) = costante -> infinite superficiCampo vettoriale: un vettore applicato funzione delle coordinate spaziali               definito ovunque dentro una certa regione di spazioLinee di forza: una o più linee sempre tangenti al vettore del campo. Le linee di forza sono più fitte dove il modulo del vettore è maggiore.31",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#31": "Campi di forze conservativi\n32In generale il lavoro di una forza per spostare un punto materiale su un tratto AB di traiettoria dipende dalla traiettoriayzx0AB..12Per il teorema delle forze vive:Le velocità agli estremi sono diverse a seconda che si percorra la curva 1 o la 2.ℒ1(A,B)=∫BA1⃗F⋅d⃗lℒ2(A,B)=∫BA2⃗F⋅d⃗l⟹ℒ1(A,B)≠ℒ2(A,B)ℒ1(A,B)=12m(vB)2−12m(vA)2ℒ2(A,B)=12m(v′\u0000B)2−12m(v′\u0000A)2⟹vB≠v′\u0000B⟹vA≠v′\u0000A",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#32": "Campi di forze conservativi\n33Campo di forza conservativo: forza posizionale che, spostando il suo punto di applicazione da A a B, punti qualunque del dominio di esistenza, compie un lavoro che è indipendente dalla particolare traiettoria seguita, ma dipendente soltanto dagli estremi A e B. \nℒA,B=∫BA⃗F⋅d⃗lℒ1(A,B)=ℒ2(A,B)=ℒ3(A,B)=...=ℒA,B",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#33": "Campi di forze conservativi: 1a proprietà\n34\nAB12Campo conservativo:\n1a Proprietà\nLavoro su una curva chiusa di una forza conservativa (circuitazione) è nulloCondizione necessaria (se il campo è conservative allora la circuitazione è nulla) e sufficiente (se la circuitazione è nulla allora il campo è conservativo)ℒ1(A,B)=∫BA1⃗F⋅d⃗lℒ2(A,B)=∫BA2⃗F⋅d⃗lℒA,B=∫BA1⃗F⋅d⃗l=∫BA2⃗F⋅d⃗l⟹∫BA1⃗F⋅d⃗l−∫BA2⃗F⋅d⃗l=0∫BA1⃗F⋅d⃗l+∫AB2⃗F⋅d⃗l=0⟹ℒ=∮⃗F⋅d⃗l\nℒ=∮⃗F⋅d⃗l",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#34": "Campi di forze conservativi: 2a proprietà \n35 funzione scalare definita in ogni punto dello spazio legata al valore della forza in quell punto. Fissata a meno di una costante additiva arbitraria: se  soddisfa la relazione anche  lo faU=U(x,y,z)U(x,y,z)U′\u0000=U(x,y,z)+kDifferenziale esatto di una funzione scalare  è un campo scalare Analisi: integrale fra A e B di un differenziale esatto è la differenza della primitiva fra B e A.Scelgo arbitrariamente un punto in cui   (origine)U=0yzx0PU funzione solo del punto non della traiettoria∫BA1⃗F⋅d⃗l=∫BA2⃗F⋅d⃗l⟹ℒA,B=∫BA⃗F⋅d⃗lℒA,B=∫BA⃗F⋅d⃗l=∫BAdU=U(B)−U(A)ℒA,B=U(B)−U(A)=[U′\u0000(B)−k]−[U′\u0000(A)−k]=U′\u0000(B)−U′\u0000(A)U(x,y,z)=∫P(x,y,z)0⃗F⋅d⃗l=U(P)−U(0)=U(P)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#35": "Campi di forze conservativi: 2a proprietà \nEsiste una  funzione scalare U(P), dipendente solo dalla posizione e dalla forza, detta potenziale tale che2a Proprietàyzx0BA..123Percorso AB: A->O->B\nℒA,B=∫BA1⃗F⋅d⃗l=∫0A2⃗F⋅d⃗l+∫03B⃗F⋅d⃗l=−∫A02⃗F⋅d⃗l+∫03B⃗F⋅d⃗l==−[U(A)−U(0)]+[U(B)−U(0)]=U(B)−U(A)ℒA,B=U(B)−U(A)36",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#36": "Energia potenzialeAl posto di U(P) si preferisce introdurre V(P)=−U(P)\n37•V(P) misura la capacità che ha il punto P di produrre lavoro quando il punto materiale ritorna all’origine.2a ProprietàEsiste una funzione scalare V(P), dipendente solo dalla posizione e dalla forza, detta energia potenziale tale cheLe prime due proprietà devono essere mutualmente dimostrabili.ℒA,B=U(B)−U(A)=V(A)−V(B)V(P)=−U(P)=−ℒ0,P=−∫P0⃗F⋅d⃗l=∫0P⃗F⋅d⃗l",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#37": "Forze conservative 3a proprietà\n38Campo conservativo è un differenziale esattoPer definizioneFxdx+Fydy+Fzdz=−∂V∂xdx−∂V∂ydy−∂V∂zdzdV=∂V∂xdx+∂V∂ydy+∂V∂zdzUguagliandoδℒ=⃗F⋅d⃗l=Fxdx+Fydy+Fzdz=dU=−dV",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#38": "Forze conservative 3a proprietà\n39Operatore vettoriale (operatore simbolico)  nabla\n3a ProprietàLa forza è col segno meno il gradiente dell’energia potenziale NB: l’operatore nabla non è un vettore, è un operatore che agisce sulle funzioni, come la derivata o l’integrale. Il risultato dell’operazione dipende al tipo di funzione a cui è applicato. ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#39": "Operatore Nabla\n40\nNabla:Gradiente di una funzione scalare\nDivergenza di un campo vettoriale\nRotore di un campo vettoriale\nÈ un vettoreÈ uno scalareÈ un vettoreÈ un operatore",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#4": "Se chiamo   la componente della forza lungo la tangente allo spostamento allora  Ft=Fcosθ\nDefinizione di lavoro infinitesimo\n5Lavoro infinitesimo compiuto da una forza:Il lavoro infinitesimo è il prodotto dello spostamento per la componente della forza lungo lo spostamentoSe la forza è  allo spostamento:        ⊥δℒ=0Es:\nNB:definizioni valgono per qualsiasi forza, non è detto sia quella che causa il moto!δℒ=⃗F⋅d⃗l=|⃗F||d⃗l|cosθ=Fdlcosθδℒ=Ftdlδℒpeso=0δℒforza⃗F≠0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#40": "Significato dell’operatore gradienteL’operatore gradiente restituisce un vettore diretto lungo la direzione in cui aumenta più velocemente la funzione scalare.\n41Campo scalare\nxy\nSuperfici (linee) di livello\nVettore applicato!\nDiretto dove f(x,y) aumenta e       alle superfici di livello⊥",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#41": "Forze conservative 4a proprietà\n423a proprietàRotoreSostituisco la prima nella seconda:Proprietà delle derivate parziali seconde per funzioni continue e derivabili:∂2V∂x∂y=∂2V∂y∂x∂2V∂x∂z=∂2V∂z∂x∂2V∂y∂z=∂2V∂z∂y𝜕𝑧",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#42": "Forze conservative 4a proprietà\n4a pr oprietà:  se il campo è conservativo il suo rotore é nulloCondizione necessaria e sufficiente affinché un campo di forze sia conservativo è che il rotore si annulli in tutti i punti del campo.Modo facile e pratico per verificare se un campo è conservativo:𝜕𝑧\n43",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#43": "ℒ=∮⃗F⋅d⃗lRiepilogo: Forze conservative\n44\n1a Proprietà\n2a ProprietàEsiste una funzione scalare V(P) (=-U(P)), detta energia potenziale, tale che\nℒA,B=V(A)−V(B)\n3a Proprietà\n4a Proprietà\nTutte le proprietà sono simultaneamente necessarie e sufficienti (la verifica di una implica tutte le altre)Forze posizionali il cui lavoro non dipende mai dal percorso ma solo dal punto di partenza e dal punto di arrivo.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#44": "Calcoli di energia potenziale\n45yz\nxP(t)O1Data una forza conservativa, trovare l’energia potenziale\n2AB\nV(P)=−U(P)=−ℒ0,P=−∫P0⃗F⋅d⃗l=∫0P⃗F⋅d⃗lℒ0,P=ℒ1(0,P)=ℒ2(0,P)=ℒ(0,A,B,P)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#45": "Calcoli di energia potenziale\n46\n1. Scelgo un percorso arbitrario su una spezzata2. Applico la definizione di energia potenziale sul percorso sceltoIn coordinate cartesiane:V(P)=−U(P)=−ℒ0,P=−∫P0⃗F⋅d⃗l=∫0P⃗F⋅d⃗l",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#46": "47Esercizio1.15Calcolare l’energia elettrostatica di una sfera di raggioRin cui ` e distribuita uniformemente una carica condensit` a⇢costante. (R:4⇡⇢2R515\"0)1.16In una certa regione di spazio sono presenti i due campi vettoriali~E1=K1xˆı+K2y2ˆ|+K1zˆke~E2=K2xyˆı+K2x2ˆ|. Determinarea) il gradiente della grandezza~E1·~E2(R: (2K1K2+2K21)xyˆı+(K1K2x2+1K22X2y)ˆ|);b) quale dei due campi pu essere considerato elettrostatico (R:~E1);1.17Si consideri il campo~F(x, y, z)=\u00002xˆı\u0000z2ˆ|\u0000ayzˆk. Determinare:a) per quali valori diail campo risulta conservativo (R:a=2);b) il potenziale'generato dal campo~F(R:'=x2+yz2);c) la densit` a di carica⇢che genera il campo~F(R:⇢=\u00002✏0(y+ 1))1.18Si consideri il campo~F(x, y, z)=2xˆı\u0000zˆ|\u0000ayˆk. Determinare:a) per quali valori diail campo risulta conservativo (R:a=1);b) il potenziale'generato dal campo~F(R:'=yz\u0000x2);c) la densit` a di carica⇢che genera il campo~F(R:⇢=\u00002✏0)1.19Sia dato il campo~E(x, y, z)=↵(4xˆı+zˆ|+yˆk).a) Veriﬁcare che~E` e conservativo; (R: veriﬁcare che~r⇥~E= 0)b) calcolare il ﬂusso di~Eattraverso un cubo di spigoloLcon un vertice nell’origine del sistema diriferimento e tre spigoli posizionati sui tre semiassi positivi; (R:\u0000=4↵L3)c) calcolare la carica totale contenuta nel cubo, utilizzando il teorema di Gauss sia in forma integrale chedi↵erenziale (R:Q=4↵\"0L3)2 Elettrostatica dei conduttori2.1Una sfera conduttrice di raggior1=5 cm porta una caricaQ1=+10\u00006C. Un guscio sferico di materialeconduttore, concentrico alla prima sfera, di raggio internor2=10cm e raggio esternor3=12cm ` e caricato conuna caricaQ2=10Q1. Nell’ipotesi che il sistema sia nel vuoto, calcolare:a) la densit` a di carica superﬁciale\u00002sulla superﬁcie interna del guscio sferico (R:\u00002\u0000Q14⇡r22=\u00008·10\u00006C/m);b) la di↵erenza di potenziale tra i due conduttori. (R:\u0000V=Q4⇡\"0r2\u0000r1r1r2= 15kV)V",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#47": "Esercizio   Sia dato un punto materiale di massa M su cui agisce una forza conservativa di energia potenziale:                                Sapendo che le costanti α e β sono positive, determinare:   1) l’espressione della forza;   2) le dimensioni e le unità di misura delle costanti α e β;    3) l’accelerazione del corpo quando passa per il punto P(0,L,L).\n48\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#48": "Conservazione dell’energia meccanica\n49Sistema meccanico con: -Vincoli ideali -Forze attive conservativeTeo. forze viveCampi conservativiUguagliando:Energia meccanica:E=T+VTeorema della conservazione dell’energia meccanica: Per un sistema meccanico sottoposto a vincoli tutti ideali ed a forze non vincolari tutte conservative, l’energia meccanica E si conserva, ossia la somma fra l’energia cinetica T e  l’energia potenziale totale V, rimane costante durante il moto. \nDimensionalmente:                    JouleE⎡⎣⎤⎦=T⎡⎣⎤⎦=V⎡⎣⎤⎦NB: A e B sono punti sulla traiettoria, l’energia si conserva lungo la traiettoria\nℒA,B=V(A)−V(B)\nℒA,B=T(B)−T(A)ℒA,B=T(B)−T(A)=V(A)−V(B)⟹T(A)+V(A)=T(B)+V(B)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#49": "Conservazione dell’energia meccanica\n50\nx\nx\nE1E2Corpo 1: Stato liberox1x≥x1x2x3Corpo 2: Stato legatox2≤x≤x3Curva nera:  V(x) energia potenzialeIn una certa regione di spazio E è costante Se V aumenta  T diminuisce  e viceversaMoto possibile solo nelle regioni di spazio in cui                        per def di TE≥V",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#5": "Definizione di lavoro\n6\nAB\nxyzOIl lavoro totale compiuto dalla forza su un punto materiale in un intervallo di tempo in cui il punto si sposta da A a B è la somma di tutti i lavori infinitesimi:d⃗l1d⃗l2d⃗l3\nIl lavoro compiuto da una generica forza, il cui punto di applicazione P si sposta da A a B lungo una linea , è l’integrale esteso a tale linea del prodotto scalare fra la forza  e lo spostamento infinitesimo :⃗Fℓ⃗Fd⃗lℓℒ=∑iδℒ=∑i⃗Fi⋅d⃗liℒℓ(A,B)=∫BAℓ⃗F⋅d⃗l",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#50": "Esempi: forza elastica\n51Verifichiamo se è conservativa:  ?⃗∇∧⃗F=0\nForza elastica:\nLa forza elastica è conservativa⃗∇∧⃗F=det̂ı̂𝚥̂k∂∂x∂∂y∂∂zFxFyFz=̂ı(∂Fy∂z−∂Fz∂y)+̂𝚥(∂Fz∂x−∂Fx∂z)+̂k(∂Fx∂y−∂Fy∂x)==̂ı(∂(ky)∂z−∂(kz)∂y)+̂𝚥(∂(kz)∂x−∂(kx)∂z)+̂k(∂(kx)∂y−∂(ky)∂x)=⃗0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#51": "Potenziale elastico\n52yzxP(t)OABV(P) calcolata sul percorso OP: O(0,0,0)→A(x,0,0)→B(x,y,0)→P(x,y,z)\nV(P)=−∫P0⃗F⋅d⃗l=−∫(x,0,0)0Fxdx−∫(x,y,0)(x,0,0)Fydy−∫(x,y,z)(x,y,0)Fzdz==∫(x,0,0)0kxdx+∫(x,y,0)(x,0,0)kydy+∫(x,y,z)(x,y,0)kzdz==kx22+ky22+kz22=k2(x2+y2+z2)=k|⃗r2|2V(P)=k2(x2+y2+z2)=k|⃗r2|2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#52": "Esempi: forza pesoForza peso:\n53\nLa forza peso è conservativaVerifichiamo se è conservativa:  ?⃗∇∧⃗F=0⃗∇∧⃗F=det̂ı̂𝚥̂k∂∂x∂∂y∂∂zFxFyFz=̂ı(∂Fy∂z−∂Fz∂y)+̂𝚥(∂Fz∂x−∂Fx∂z)+̂k(∂Fx∂y−∂Fy∂x)==̂ı(−∂(−mg)∂y)+̂𝚥(∂(−mg)∂x)+0̂k=⃗0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#53": "Potenziale gravitazionale\n54yzxP(t)OAB𝑉(𝑃)=𝑚𝑔𝑧\nV(P) calcolata sul percorso OP: O(0,0,0)→A(x,0,0)→B(x,y,0)→P(x,y,z)V(P)=−∫P0⃗F⋅d⃗l=−∫(x,0,0)0Fxdx−∫(x,y,0)(x,0,0)Fydy−∫(x,y,z)(x,y,0)Fzdz==∫(x,y,z)(x,y,0)mgdz=mgz",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#54": "Esempi: forza costanteForza costante:\n55\nUna forza costante è conservativaVerifichiamo se è conservativa:  ?⃗∇∧⃗F=0⃗∇∧⃗F=det̂ı̂𝚥̂k∂∂x∂∂y∂∂zFxFyFz=̂ı(∂fy∂z−∂fz∂y)+̂𝚥(∂fz∂x−∂fx∂z)+̂k(∂fx∂y−∂fy∂x)=⃗0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#55": "Potenziale di una forza costante\n56yzxP(t)OAB𝑉(𝑃)=−𝑓𝑥𝑥−𝑓𝑦𝑦−𝑓𝑧𝑧\nV(P)=−∫P0⃗F⋅d⃗l=−∫(x,0,0)0fxdx−∫(x,y,0)(x,0,0)fydy−∫(x,y,z)(x,y,0)fzdz==−fx∫(x,0,0)0dx−fy∫(x,y,0)(x,0,0)dy−fz∫(x,y,z)(x,y,0)dz=−fxx−fyy−fzz",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#56": "Esempi: campo centrale  a simmetria sfericaForza centrale a simmetria sferica:\n57Verifichiamo se è conservativa:\nTutti i campi centrali a simmetria sferica sono conservativixyz\nO funzione scalare della sola posizione   è un’energia potenziale se è possibile trovare una funzione scalare t.c  (forza è conservativa).F(r)drF(r)dr=−dVF(r)dr=−dV⟹F(r)=−dV/drd⃗l=̂u⊥(dl⊥)+̂ur(dr)δℒ=⃗F⋅d⃗l=F̂ur⋅(̂u⊥dl⊥+̂urdr)=F(ur⋅̂u⊥dl⊥+ur⋅̂urdr)=Fdrd⃗l=(dl⊥)̂u⊥+(dlr)̂ur=0=1⃗F(⃗r)=F(r)̂ur\nℒA,B=∫BAF(r)dr=−∫BAdV=V(rB)−V(rA)AB",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#57": "•sono arrivato qua •(mancano le animazioni)\n58",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#58": "Lavoro di una forza di attrito dinamico\n59Lavoro lungo la traiettoria 2:\nBLavoro lungo la traiettoria 1:Lavoro da A a B della forza di attrito dinamico:1\nATutte le forze di attrito (dinamico, viscoso) NON sono mai conservative⃗F=−μcN̂ut=−μcN⃗vvℒ1(A,B)=∫BA1⃗F⋅d⃗l=∫BA1(−μcN̂ut)⋅(̂utdl)2=−μcN∫BA1dl=−μcNLA1B<0ℒ2(A,B)=∫BA2⃗F⋅d⃗l=∫BA2(−μcN̂ut)⋅(̂utdl)=−μcN∫BA2dl=−μcNLA2B<0LA1B≠LA2B⟹ℒ1(A,B)≠ℒ2(A,B)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#59": "Lavoro di forze conservative e non\n60\nTeo. Forze vive ℒtot=T(B)−T(A)Lavoro totale ℒtot=ℒcons+ℒncCampi conservativi ℒcons=V(A)−V(B)Il lavoro delle forze non conservative è dato dalla variazione dell’energia meccanica totaleIn presenza di forze d’attrito, generalmente, si ha ℒnc<0⟹E(B)<E(A)Forze dissipativePunto materiale soggetto ad una forza totale: somma di 2 contributi ℒA,B=∫BA⃗F⋅d⃗l=∫BA(⃗Fcons+⃗Fnc)⋅d⃗l=∫BA⃗Fcons⋅d⃗l+∫BA⃗Fnc⋅d⃗lT(B)−T(A)=V(A)−V(B)+ℒncℒnc=[T(B)+V(B)]−[T(A)+V(A)]=E(B)−E(A)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#6": "Graficamente\n7\nAB\nOIn un piano in cui rappresentiamo   in funzione dell’ascissa curvilinea , sia  che  sono funzioni della particolare traiettoria:   e  variano cambiando il percorso da A a B. Fissata una certa traiettoria…FtsFtssAsBFtssAsBIl lavoro infinitesimo in un tratto di s è pari all’area tratteggiatadlδℒ=Ftdlℓ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#60": "Esercizio•Un carrello viene lanciato con una velocità iniziale v lungo un binario orizzontale che poi presenta un avvolgimento circolare verticale di raggio R = 4 m. Calcolare, nell’ipotesi di assenza di attriti,  il minimo valore vmin che deve essere dato alla velocità v affinché il carrello compia il “giro della morte” senza staccarsi dai binari.\n61\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#61": "EsercizioUn punto materiale di massa m=30 g è inizialmente fermo su di un profilo circolare liscio di raggio R=20 cm ad una altezza H=R/2 rispetto al piano orizzontale. Scendendo lungo il profilo il punto incontra in A un piano orizzontale liscio su cui è vincolata in B una molla di costante elastica  k =0,1 kg/s2, inizialmente a riposo. Determinare: a)le componenti tangenziale (aT) e centripeta (aN)  dell’accelerazione del punto nel punto iniziale; b)la reazione vincolare nel punto A; c)la compressione massima della molla. \n62\nmRKA\nR\nB",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#62": "EsercizioUna cassa, di massa M=7 kg è  inizialmente in moto su un piano orizzontale liscio con una velocità di v =8 m/s ad una distanza D =6 m da un piano ruvido inclinato di  α=15° rispetto alla direzione orizzontale. Sapendo che la cassa si ferma dopo aver percorso L = 8 m sul piano inclinato, determinare  a)il coefficiente di attrito dinamico del piano inclinato,  b)il lavoro fatto dalla forza di attrito sul piano inclinato,  c)indicare (motivando la risposta) se, raggiunta la quota massima la cassa ridiscende il piano o si ferma.\n63\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#63": "EsercizioUn corpo di massa M = 12 kg scende da un piano inclinato di α=30° rispetto ad una direzione orizzontale. Sapendo che il corpo parte da fermo, che si abbassa di una quota h = 2 m, che il piano è ruvido e con un coefficiente di attrito dinamico µd=0,2, determinare la sua velocità finale.\n64",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#7": "Graficamente\n8\nAB\nOIn un piano in cui rappresentiamo   in funzione dell’ascissa curvilinea , sia  che  sono funzioni della particolare traiettoria:   e  variano cambiando il percorso da A a B. Fissata una certa traiettoria…FtsFtssAsBFtssAsB\nIl lavoro totale è pari all’area sotto la curva compresa fra i due estremi fra cui si sposta il punto materialeℒ=∫BAFtdlℓ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#8": "Esempi\n91.\nAB\ncostante in modulo, direzione e verso ℓℒℓ(A,B)=∫BAℓ⃗F⋅d⃗l=∫BAℓFtdl=Ft∫BAℓdl=Ftℓ2. costante in moduloFt=|⃗F|cosθ=Fcosθℒℓ(A,B)=∫BAℓ⃗F⋅d⃗l=∫BAℓFcosθdl=Fcosθ∫BAℓdl=FℓcosθIn generale in coordinate cartesiane: forza posizionale⃗F(x,y,z)=Fx(x,y,z)̂ı+Fy(x,y,z)̂𝚥+Fz(x,y,z)̂kd⃗l=dx̂ı+dŷ𝚥+dẑkIntegrale generalmente non scomponibile!\nAB\n𝜗𝜗𝜗𝜗ℓℒ=∫BAℓ⃗F(x,y,z)⋅d⃗l=∫BAℓ[Fx(x,y,z)dx+Fy(x,y,z)dy+Fz(x,y,z)dz]",
    "data_test\\rootfolder\\università\\FisicaGenerale\\05-lavoro-e-energia.pdf#9": "Esercizio\n10Calcolare il lavoro della forzacon k e h costanti che agisce sul piano (x,y) sulle traiettorie: 1)Lungo un segmento rettilineo che congiunge l’origine con un punto A=(a,b); 2)Lungo l’arco di parabola OA avente vertice nell’origine e per asse l’asse x.xyOA",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#0": "Terzo Principio della Dinamica CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#1": "Sviluppo1)Modello del punto materiale troppo povero per descrivere tutta la realtà; 2)Dinamica dei sistemi di punti materiali; 3)Riscrittura  della  in modo opportuno; 4)Terzo principio della dinamica⃗F=m⃗a\n2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#10": "Derivate rispetto al tempo\nRappresenta TUTTE le forze REALI che agiscono sul punto i-esimoPossiamo distinguere tra le forze dovute agli altri punti del sistema  (forze INTERNE al sistema) e forze dovute a tutto ciò che non è il sistema (forze ESTERNE al sistema, dovute all’ambiente). Analogamente per i momenti→𝑄=𝑁∑𝑖=1→𝑞𝑖=𝑁∑𝑖=1𝑚𝑖→𝑣𝑖𝑑→𝑄𝑑𝑡=𝑁∑𝑖=1𝑑→𝑞𝑖𝑑𝑡=𝑁∑𝑖=1→𝐹𝑖→𝑃𝑜=𝑁∑𝑖=1→𝑝𝑖=𝑁∑𝑖=1𝑚𝑖→𝑟𝑖∧→𝑣𝑖𝑑→𝑃𝑜𝑑𝑡=𝑁∑𝑖=1𝑑→𝑝𝑖𝑑𝑡=𝑁∑𝑖=1→𝑀𝑖=𝑁∑𝑖=1→𝑟𝑖∧→𝐹𝑖\n11",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#11": "Forze interne ed esterneTipiche forze interne: vincoli tra punti materiali, fili, sbarre interne al sistema, molle o sistemi di attrazione/repulsione tra punti del sistema Tipiche forze esterne: forze peso, vincoli tra il sistema e l’esterno, tensioni tra il sistema e l’esterno\nPiano verticaleInterneEsterne\nPiano orizzontale\nPiano verticale\n12",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#12": "Separazione tra forze interne ed esterne\nPer un sistema isolato si ha:Attenzione:  risultato parziale→𝐹𝐸𝑆𝑇=0, →𝑀𝐸𝑆𝑇=0𝑑→𝑄𝑑𝑡=→𝐹𝐼𝑁𝑇𝑑→𝑃𝑜𝑑𝑡=→𝑀𝐼𝑁𝑇𝑑→𝑄𝑑𝑡=𝑁∑𝑖=1→𝐹𝑖=𝑁∑𝑖=1(→𝐹𝐼𝑁𝑇𝑖+→𝐹𝐸𝑆𝑇𝑖)=𝑁∑𝑖=1→𝐹𝐼𝑁𝑇𝑖+𝑁∑𝑖=1→𝐹𝐸𝑆𝑇𝑖=→𝐹𝐼𝑁𝑇+→𝐹𝐸𝑆𝑇𝑑→𝑃𝑜𝑑𝑡=𝑁∑𝑖=1→𝑀𝑖=𝑁∑𝑖=1(→𝑀𝐼𝑁𝑇𝑖+→𝑀𝐸𝑆𝑇𝑖)=𝑁∑𝑖=1→𝑀𝐼𝑁𝑇𝑖+𝑁∑𝑖=1→𝑀𝐸𝑆𝑇𝑖=→𝑀𝐼𝑁𝑇+→𝑀𝐸𝑆𝑇\n13",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#13": "Verifiche sperimentaliQuanto valgono nei sistemi isolati?Studio il sistema Terra-Luna o Giove-suoi satelliti o altri sistemi:\nRisultato sperimentale nuovo: nei sistemi isolati si osserva sempre: Nei sistemi isolati la quantità di moto e il momento angolare del sistema sono costanti nel tempo.𝑑→𝑄𝑑𝑡=→𝐹𝐼𝑁𝑇𝑑→𝑃𝑜𝑑𝑡=→𝑀𝐼𝑁𝑇\n𝑑→𝑄𝑑𝑡=→0𝑑→𝑃𝑜𝑑𝑡=→0⟹→𝐹𝐼𝑁𝑇=→0, →𝑀𝐼𝑁𝑇=014",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#14": "Sistema isolato semplice\nLe due forze interne agiscono su una retta d’azione che passa per i due punti materiali\n e : forze interne al sistema di due punti⃗F1⃗F2\nO\n→𝐹𝐼𝑁𝑇=→0→→𝐹1+→𝐹2=→0⟹→𝑀𝐼𝑁𝑇=→𝑟1∧→𝐹1+→𝑟2∧→𝐹2=→0→→𝑟1∧→𝐹1+→𝑟2∧(−→𝐹1)=→0→→𝑟1∧→𝐹1−→𝑟2∧→𝐹1=(→𝑟1−→𝑟2)∧→𝐹1=→0(→𝑟1−→𝑟2)∥→𝐹115→𝐹2=−→𝐹1⃗F1⃗F2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#15": "Terzo principio della dinamicaOgni volta che un corpo (A) esercita una forza su un altro corpo (B), il secondo esercita sul primo una forza vettorialmente opposta e con la stessa retta d’azione.Formulazione storica\n16",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#16": "Terzo principio della dinamica•Col secondo principio prevediamo il moto di un punto materiale sulla base della forza agente su di esso:  •Per avere una forza  occorre almeno un altro corpo che agisca sul punto materiale •Il secondo principio dice come si muove il punto materiale soggetto ad una forza ma non cosa succede al corpo che tale forza la provoca  serve il terzo principio→𝐹=𝑚→𝑎⃗F→17",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#17": "Terzo principio della dinamica  per sistemi di punti materiali•Per ogni punto si suppone di poter distinguere fra le forze agenti sul punto i-esimo quella dovuta al punto j-esimo •Si soppone valere sempre la sovrapposizione degli effetti cioè se  è la forza che 2 esercita su 1 e  è la forza che 3 esercita su 1 allora  •Se valgono queste condizioni allora il terzo principio è estendibile a N corpi applicandolo ad ogni possibile coppia di punti→𝐹1,2→𝐹1,3→𝐹1=→𝐹1,2+→𝐹1,3→𝐹𝑖=𝑁−1∑𝑗=1→𝐹𝐼𝑁𝑇𝑖,𝑗+→𝐹𝐸𝑆𝑇𝑖Sul punto i-esimo agiscono tutte le forze interne dovute agli altri N-1 punti e le forze esterneSe agiscono solo forze interne: il sistema è isolato  →𝐹𝐸𝑆𝑇𝑖=→0⟹𝑑→𝑄𝑑𝑡=018",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#18": "Terzo principio della dinamicaOgni volta che un corpo (A) esercita una forza su un altro corpo (B), il secondo esercita sul primo una forza vettorialmente opposta e con la stessa retta d’azione.Formulazione storicaFormulazione alternativaSe in un SRI, osserviamo che su un corpo (A) si esercita una forza allora esisterà almeno un altro corpo (B) responsabile di tale forza. Su questo corpo B agirà una forza vettorialmente opposta a quella su A e con la stessa retta d’azione.NB: le due forze sono applicate in due punti di applicazione diversi ovvero I due corpi!19",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#19": "Terzo principio, formulazione modernaSemplice, diretto, modernoSu sistemi isolati\nNulli per il terzo principio (sperimentale)Nulli in un sistema isolatoQuantità di moto e momento angolare si conservano per sistemi isolati.In un Sistema di Riferimento Inerziale,  e calcolato rispetto ad un polo O qualunque si conservano per sistemi isolati.→𝑄→𝑃0 𝑑→𝑄𝑑𝑡=→0𝑑→𝑃𝑜𝑑𝑡=→0⟹→𝐹𝐼𝑁𝑇=→0, →𝑀𝐼𝑁𝑇=0\n20",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#2": "Quantità di moto di un punto materialeSi definisce la quantità di moto di un punto come:Se la massa è costante:Secondo principio:Se la massa è variabile: quale delle due è corretta?oppure[→𝑞]=[𝑚→𝑣]=[𝑀𝐿𝑇−1]→𝑘𝑔∙𝑚𝑠→𝐹=𝑚→𝑎=𝑚𝑑→𝑣𝑑𝑡=𝑑(𝑚→𝑣)𝑑𝑡\n→𝐹=𝑑→𝑞𝑑𝑡→𝐹=𝑑→𝑞𝑑𝑡=𝑑(𝑚→𝑣)𝑑𝑡=𝑑𝑚𝑑𝑡→𝑣+𝑚𝑑→𝑣𝑑𝑡3",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#20": "Conseguenze del terzo principio2 corpi su un piano orizzontale tenuti insieme da una molla compressa tramite un filo ideale\n21\nTerzo principio: poiché il sistema è isolatoIn diverse circostanze è possibile ottenere dei risultati di dinamica SENZA conoscere le forze in gioco\n→𝑄𝑖𝑛𝑖𝑧=→0Tagliando il filo i corpi si muovono per effetto della di moto rettilineo uniforme in direzione opposta→𝐹=𝑚→𝑎 →𝑄𝑓𝑖𝑛=𝑚1→𝑣1+𝑚2→𝑣2→𝑄𝑖𝑛𝑖𝑧=→𝑄𝑓𝑖𝑛→→0=𝑚1→𝑣1+𝑚2→𝑣2→→𝑣2=−𝑚1𝑚2→𝑣1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#21": "Urti CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#22": "UrtiSi ha un urto quando due corpi, che si muovono a velocità diverse, interagiscono (p.es. vengono a contatto) e, in un intervallo di tempo molto breve (rispetto al contesto), modificano sostanzialmente le proprie velocità.\n23\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#23": "Forze d’urto – forze impulsive\n24\nForze impulsive",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#24": "Urti collineari di punti materiali\n25\ne = 0 : urto perfettamente anelastico e = 1 : urto perfettamente elasticoEmpiricamente Prima dell’urtoDopo l’urto\nNB.: Trattasi di relazioni tra le componenti dei vettori lungo l’asse x, le quali includono il segno.v0,1x−v0,2x>0→𝑄𝑖𝑛𝑖𝑧=→𝑄𝑓𝑖𝑛𝑣1,𝑥−𝑣2,𝑥=−𝑒(𝑣01,𝑥−𝑣02,𝑥)0≤𝑒≤1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#25": "Urti collineari di punti materiali\n26\nConservazione della quantità di moto (e del momento angolare).\nRelazione fra le velocità\nm1 = m2\ne = 0\nUrto anelastico\nm1 = m2\ne = 1\nUrto elastico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#26": "Energia cineticaFacendo un po’ di conti … :\ne = 1\nE = costante T=12m1v1x2+12m2v2x2T0=12m1v01x2+12m2v02x2\nIn un urto perfettamente elastico l’energia cinetica si conserva\ne = 0\nΔE ≤  0 In un urto perfettamente anelastico l’energia cinetica diminuisce27",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#27": "Urti in sistemi non isolatiSe gli urti avvengono in sistemi non isolati a causa della presenza di forze esterne o vincoli esterni, il terzo principio (formulazione conservativa) non è sempre applicabileSe l’urto è quasi istantaneo, sono molto più importanti le forze impulsive e si può trascurare l’effetto della forza peso. Si ha una quasi conservazione di quantità di moto e momento angolare tra prima e dopo l’urto. Vale in generale per forze esterne LIMITATE.Se le forze esterne hanno una direzione definita, si ha la conservazione della quantità di moto nelle direzioni perpendicolari.\nEsempio: urto di due palloni che si scontrano in aria. E’ presente una forza esterna: quella peso.\n28",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#28": "Urti: riassunto\n29",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#29": "EsercizioUn corpo di massa m=1 kg è in moto rettilineo uniforme ad una velocità v=10 m/s, su un piano liscio, quando entra in una regione permanendovi per t=0.1 s in cui perde velocità scalare. All’uscita della regione il corpo ha una velocità di v=9 m/s. Determinare:  1)la forza media che ha frenato il corpo,  2)il lavoro della forza frenante e  3)il coefficiente di attrito se si tratta di una forza di attrito cinetico. \n30",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#3": "Situazioni di massa variabile•Moto di una goccia d’acqua che cade in presenza di vapor d’acqua saturo  à la massa aumenta •Moto di un aereo in condizioni di tempo brutto con formazione di ghiaccio sulle ali à la massa aumenta •Moto di un razzo che si muove bruciando carburante à la massa diminuisce •Relatività: la massa dipende dalla velocità: •Dato sperimentale: la forza varia con la massa!È più generale della \nm(v)=m01−vc()2→𝐹=𝑑→𝑞𝑑𝑡4",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#30": "EsercizioDue punti materiali di massa m1=1 kg e m2=3 kg sono uniti da un filo inestensibile che risulta sempre in tensione. Sapendo che i due punti si muovono su un piano ideale senza attrito, che costituiscono un sistema isolato e che il punto 1 ha equazioni del moto date da :    (nelle unità del SI) trovare la tensione del filo e l’accelerazione del punto 2. \n31",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#31": "EsercizioDa una pistola con canna lunga L=15 cm esce un proiettile di massa m=5 g con velocità v=180 m/s. Trovare la forza media che ha spinto il proiettile dentro la canna e il tempo che impiega il proiettile a percorrere la canna della pistola dal momento dello sparo.\n32",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#32": "EsercizioDue corpi A e B di massa 2 kg si scontrano fra loro. Le velocità prima dell’urto sono   Dopo l’urto Tutte le velocità sono date in metri al secondo. Qual è la velocità finale di B? Quanta energia cinetica guadagna o perde nell’urto il corpo B? L’urto è elastico? →𝑣𝐴,𝑖=15^𝑖+30^𝑗→𝑣𝐵,𝑖=−10^𝑖+5^𝑗→𝑣𝐴,𝑓=−5^𝑖+20^𝑗\n33",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#33": "EsercizioUna palla di stucco con una massa di 5 g ed una velocità v1 = 4 m/s compie una collisione diretta e perfettamente anelastica con una palla da biliardo inizialmente ferma e che ha una massa di 500 g. Determinare la velocità comune delle due palle dopo l’urto e le energie cinetiche prima e dopo l’urto dei diversi corpi.  - trascurare gli effetti di rotolamento -\n34",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#34": "EsercizioUna pallina di gomma, di massa m=20 g, viene lasciata cadere in verticale da una altezza h=100 cm misurata rispetto ad un pavimento orizzontale. La pallina rimbalza esattamente in verticale e raggiunge una altezza di h' = 90 cm.  Qual è il coefficiente di restituzione del pavimento? A che altezza arriverà il successivo rimbalzo?\n35",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#35": "EsercizioDue carrelli, di massa rispettivamente M=50 kg e 2M si muovono uniti su un binario orizzontale rettilineo ad una velocità costante v=10 m/s. Tra i due carrelli, tenuti uniti da un gancio, vi è un respingente (molla) compresso di 25 cm e di costante elastica k=80000 N/m. Se ad un certo punto il gancio si rompe, trovare le velocità finali dei due carrelli.\n36",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#36": "EsercizioUn proiettile di massa mP = 4 kg viene sparato in orizzontale da un cannone posto su un carrello e avente una massa complessiva di MC = 3 000 kg. Sapendo che la velocità di uscita del proiettile è di vP = 350 m/s, determinare la velocità iniziale di rinculo del cannone.\n37",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#37": "Esercizio (pendolo balistico)Un proiettile, di massa m e velocità v diretta in orizzontale, colpisce in modo totalmente anelastico un peso di massa M appeso al soffitto tramite un filo inestensibile. A seguito dell’urto il peso inizia una oscillazione. Trovare la relazione tra la velocità del proiettile e la massima altezza del peso rispetto alla sua posizione di riposo.\n38",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#38": "Esercizio Un corpo di 2 kg viene spinto contro una molla di costante elastica pari a 200 N/m fino a comprimerla di 15 cm. Lasciato andare, la molla lo spinge su una superficie orizzontale fino a che non si arresta dopo un percorso di 75 cm. Qual e’ il coefficiente di attrito dinamico tra blocco e superficie?\n3939",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#39": "EsercizioDue sferette di masse m1 e m2, vincolate a muoversi su un piano verticale, sono collegate ad uno stesso punto fisso O attraverso due fili flessibili inestensibili, entrambi di lunghezza l e massa trascurabile (vincoli ideali). Inizialmente la sferetta m2 è in posizione di equilibrio stabile, mentre la sferetta m1 con il filo teso è trattenuta ad una quota h rispetto alla posizione di m2. In seguito, m1 viene lasciata libera di muoversi e va a urtare m2. Nell’ipotesi che l’urto sia istantaneo e completamente anelastico, calcolare:   1) il modulo v1 della velocità con cui m1 urta m2;   2) la quota massima h’ raggiunta dal sistema    dopo l’urto e   3) la perdita di energia cinetica.\n40\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#4": "ImpulsoL’azione di una forza in un intervallo di tempo dt provoca una variazione infinitesima della quantità di moto\nImpulso:Viceversa: da una variazione infinitesima della quantità di moto si può risalire alla forza agente.⃗ℐ=∫t2t1⃗Fdt=∫t2t1d⃗q=⃗q(t2)−⃗q(t2)=Δ⃗q[⃗ℐ]=[Δ⃗q]=[MLT−1]5",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#40": "EsercizioUn sistema meccanico, che si trova inizialmente fermo ad una altezza h = 1,2 m dal pavimento, è costituito da una pallina di massa m1 = 10 g collocata in equilibrio (instabile) sopra una pallina di massa m2 = 5m1. A un certo istante, il sistema viene lasciato libero di cadere. Assumendo che ogni urto sia perfettamente elastico e trascurando le dimensioni delle palline, determinare:  1)l’altezza a cui rimbalza la pallina più leggera;  2)la velocità con cui arriva a terra la seconda pallina dopo l’urto.\n41",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#41": "EsercizioUna pallina di massa 2m viene lanciata verso l’alto da una quota z=0 ad una velocità v=10 m/s esattamente nello stesso istante in cui un’altra pallina di massa m, posta ad una quota h=5 m viene lasciata cadere sulla verticale della prima pallina.  1) Se l’urto tra le palline e’ elastico, quanto tempo impiega la prima pallina ad arrivare a terra? 2) Se l’urto e’ completamente anelastico, quanto tempo ci mettono le palline ad arrivare a terra?    (considerare g=10 m/s2)\n42",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#5": "Teorema dell’impulsoForze impulsive: forze che agiscono per un periodo di tempo limitato\nPrimo principio  con la quantità di motoSe m è costante    costante    Se m costante e     costante    Teorema dell’impulso: l’impulso di una forza applicata ad un punto materiale provoca la variazione della sua quantità di moto.Forma integrale del secondo principio della dinamica:  nota la forza anche la variazione della quantità di moto è nota; nota la variazione della quantità di moto è nota la forza media che ha agito nell’intervallo di tempo dt.⃗ℐ=∫t2t1⃗Fdt=Δ⃗q\n⃗ℐ=∫t2t1⃗Fdt=Δ⃗q=m(⃗v2−⃗v1)6",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#6": "⃗p0=⃗r∧⃗q=m(⃗r∧⃗v)Momento angolare\nxyz\nO\nP\nPunto materiale di massa m con velocità  ⃗vMomento angolare o  momento della quantità di moto rispetto al polo O: \nRispetto al polo F: P\nF\nOsservazione: il vettore quantità di moto è un vettore applicato nel punto P\n7⃗pF=(⃗r−⃗rF)∧⃗q",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#7": "Momento angolare nel piano\nY\nO\nVelocità: componente radiale più tangenziale\nIl momento angolare: - Dipende dalla velocità trasversa, non da quella radiale - È un vettore  al piano definito da  e  - È diverso da zero solo quando c’è una rotazione - È costante in un moto circolare uniforme⊥→𝑟→𝑣velocità angolare\n8⃗v=⃗vr+⃗vt=vr̂ur+vt̂ut=·r̂ur+r·φ̂ut̂ur̂utφ⃗p0=⃗r∧⃗q=m(⃗r∧⃗v)⃗p0=⃗r∧⃗q=m(r̂ur)∧(·r̂ur+r·φ̂ut)=mr2·φ(̂ur∧̂ut)=mr2·φ̂k·φ=dφdt=ω",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#8": "d⃗p0dt=⃗r∧⃗F=⃗M0Derivata del momento angolareDerivando:La derivata del momento angolare é uguale al momento della forza agente sul punto materiale rispetto allo stesso polo.Il momento delle forze è nullo se: -La forza agente è nulla (punto isolato da altri corpi) -Vettore posizione e forza sono paralleli  Se il momento delle forze è nullo, il momento angolare è costante in modulo direzione e verso: la traiettoria giace su un piano. 9⃗p0=⃗r∧⃗q=m(⃗r∧⃗v)d⃗p0dt=d(⃗r∧⃗q)dt=d⃗rdt∧⃗q+⃗r∧d⃗qdt=⃗v∧⃗q+⃗r∧⃗F",
    "data_test\\rootfolder\\università\\FisicaGenerale\\06-terzo-principio-e-urti.pdf#9": "Sistemi di punti materialiUn insieme di N punti materiali di masse mi costituisce un sistema di punti materiali \nxyz\nOSRI\n12iDef: massa del sistema di punti materiali:\nNM=mii=1N∑Def: Quantità di moto del sistema di punti materiali: \nDef: Momento della quantità di moto (o momento angolare): (momento risultante del sistema)10⃗P0=N∑i=1⃗pi=N∑i=1⃗ri∧⃗qi=N∑i=1mi⃗ri∧⃗vi",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#0": "Dinamica dei Sistemi CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#1": "Equazioni cardinali \n2Dal principio di indipendenza delle azioni simultanee: le forze ed i momenti interni rimangono nulli anche in presenza di forze e momenti delle forze esterni.6 equazioni scalari! Descrivono esattamente: 1.Il moto di un punto 2.Il moto di 2 punti 3.Il moto di un corpo rigido𝑑→𝑄𝑑𝑡=𝑁∑𝑖=1→𝐹𝑖=𝑁∑𝑖=1→𝐹𝐼𝑁𝑇𝑖+𝑁∑𝑖=1→𝐹𝐸𝑆𝑇𝑖=→𝐹𝐼𝑁𝑇+→𝐹𝐸𝑆𝑇𝑑→𝑃𝑜𝑑𝑡=𝑁∑𝑖=1→𝑀𝑖=𝑁∑𝑖=1→𝑀𝐼𝑁𝑇𝑖+𝑁∑𝑖=1→𝑀𝐸𝑆𝑇𝑖=→𝑀𝐼𝑁𝑇+→𝑀𝐸𝑆𝑇Equazioni cardinali della dinamica dei sistemiChe cosa descrivono per un sistema generico di N punti materiali?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#10": "Teorema del momento angolare\n11A cosa sono dovute le variazioni del momento angolare totale di un sistema di punti?Consideriamo un polo O’ mobile (non necessariamente il CM)\nO\nO’\nDerivando:=𝑁∑𝑖=1𝑚𝑖→𝑣𝑖∧→𝑣𝑖−𝑁∑𝑖=1𝑚𝑖→𝑣𝑂′\u0000∧→𝑣𝑖+𝑁∑𝑖=1𝑚𝑖(→𝑟𝑖−→𝑟𝑂′\u0000)∧→𝑎𝑖==𝑁∑𝑖=1𝑚𝑖→𝑣𝑖∧→𝑣𝑖−→𝑣𝑂′\u0000∧𝑁∑𝑖=1𝑚𝑖→𝑣𝑖+𝑁∑𝑖=1𝑚𝑖(→𝑟𝑖−→𝑟𝑂′\u0000)∧→𝑎𝑖Quantità di moto totale del sistemaMomento delle forze agenti sul sistema rispetto polo O’Nullo perché prodotto vettoriale fra vettori paralleli⃗PO′\u0000=N∑i=1⃗pi=N∑i=1mi⃗r′\u0000i∧⃗vi=N∑i=1mi(⃗ri−⃗rO′\u0000)∧⃗vid⃗PO′\u0000dt=N∑i=1mid[(⃗ri−⃗rO′\u0000)∧⃗vi]dt==N∑i=1mi(⃗vi−⃗vO′\u0000)∧⃗vi+N∑i=1mi(⃗ri−⃗rO′\u0000)∧⃗ai=−→𝑣𝑂′\u0000∧→𝑄+→𝑀𝑂′\u0000",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#11": "Teorema del momento angolare\n12O\nO’\nEstensione della seconda equazione cardinale al caso di un polo mobile:  seconda equazione cardinale generalizzataUsando il primo teorema del centro di massa−→𝑣𝑂′\u0000×𝑀→𝑣𝐶𝑀+→𝑀𝑂′\u0000=−𝑀→𝑣𝑂′\u0000×→𝑣𝐶𝑀+→𝑀𝑂′\u0000𝑑→𝑃𝑂′\u0000𝑑𝑡=−→𝑣𝑂′\u0000×→𝑄+→𝑀𝑂′\u0000=",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#12": "Teorema del momento angolare: se il polo O’ è fisso nel SRI o coincide con il CM, l’evoluzione nel tempo del momento angolare è determinata dal momento risultante delle forze esterne.Teorema del momento angolare\n13Il secondo termine è nullo quando: 1- O’ è fermo nel SRI 2- il CM è fermo nel SRI 3- il polo O’ coincide con il CM 4- \nSempre, anche  quando il CM si muove!\nIn uno di questi casi𝑑→𝑃𝑂′\u0000𝑑𝑡=→𝑀𝑂′\u0000−𝑀→𝑣𝑂′\u0000×→𝑣𝐶𝑀\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#13": "Equazioni cardinali\n14\nDescrivono il moto di un sistema di N punti materiali attraverso il suo CMSistema di equazioni non completo: non si può descrivere il moto di N punti con sole 2 equazioni vettoriali, ma queste forniscono una indicazione globale su come si muove il CM e l’evoluzione del momento angolare calcolato rispetto al CM.Per un sistema di punti materialiEquazioni cardinali:  moto di 1 punto, 2 punti, corpo rigido",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#14": "Energie di un sistema di punti\n15Osservazione: la forza ed il lavoro sono grandezze additive Anche l’energia (cinetica, potenziale, meccanica) è una grandezza additiva.Definiamo energia cinetica di un sistema:\nDefiniamo energia potenziale di un sistema soggetto solo a forze conservative interne o esterne:\nAnalogamente definiremo Energia Meccanica di un sistema:\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#15": "Teorema di König\n16\nUsiamo coordinate intrinseche: \nEnergia cinetica come calcolata nel SR del CM (SR intrinseco)Energia cinetica nel SRI di un punto di massa M  con velocità pari a quella del CM\nTCM=\n⃗Q′\u0000=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#16": "T=T′\u0000+TCMTeorema di König\n17Energia cinetica nel SR del CM (SR intrinseco)Energia cinetica di un punto di massa M con velocità pari a quella del CM\nTeorema di König: l’energia cinetica di un sistema di punti materiali in moto rispetto ad un punto O è, istante per istante, uguale alla somma dell’energia cinetica del sistema rispetto al CM (T’) più l’energia che possiederebbe in quell’istante rispetto ad O il CM se in esso fosse concentrata tutta la massa (TCM).\n()T′\u0000=0TCM=12Mv2CMT′\u0000=N∑i=112miv′\u00002i",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#17": "Sistema di punti soggetti a forza peso\n18O\nCM\nRicordando la definizione di CM:\n⟹𝑀→𝑟𝐶𝑀=∑𝑁𝑖=1𝑚𝑖→𝑟𝑖",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#18": "Sistema di punti soggetti a forza peso\n19O\nCM\nUn sistema di punti soggetto alla forza peso si comporta come un unico punto materiale coincidente con il CM avente massa M\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#19": "Esercizio\n20\nUn proiettile è lanciato con una velocità di 20 m/s ad un angolo di 30° rispetto l’orizzontale. Nel corso della sua traiettoria esplode suddividendosi in due frammenti, uno dei quali ha massa doppia rispetto quell’altro. I due frammenti colpiscono il suolo nello stesso istante. Il frammento di massa minore colpisce il suolo a 20 m dal punto di lancio. In quale punto colpisce il suolo il secondo frammento?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#2": "Equazioni cardinali -Per N punti materiali servono N equazioni vettoriali del tipo  quindi 3N equazioni scalari; -Le equazioni cardinali sono al massimo 6 equazioni scalari  mancano 3N-6 equazioni scalari che diano informazioni mancanti (es: vincoli che legano fra loro i punti come nel caso dei corpi rigidi)→𝐹=𝑚→𝑎⟹Per N punti materiali le equazioni cardinali forniscono solo informazioni parziali:  non forniscono informazioni sul singolo punto ma danno informazioni collettive.  Che cosa descrivono le equazioni cardinali per un sistema di N punti materiali?  È possibile trovare un elemento del sistema che sia descritto dalle equazioni cardinali? 3",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#20": "Sistemi continui\n21Sistemi non puntiformi: insiemi “densi” di punti àsistemi continui (estesi). Per tali sistemi la grandezza (dimensione lineare) è importante.  Possiamo caratterizzarli attraverso una nuova quantità geometrica:      Volume V     [L3] àm3Anche un sistema continuo è dotato della proprietà Massa del Sistema:          Massa M     [M] àkg\nM=∫dm=∫Vρ(⃗r)dτDef: densità volumetrica puntuale (locale) di massa: \ndm,dτρ(⃗r)=dmdτdm=ρ(⃗r)dτDef: densità volumetrica media di massa: \nM,τ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#21": "Legame tra sistemi di punti e sistemi estesi\n22\nSistema di punti\nSistema esteso\nUn sistema continuo può essere pensato come un sistema di N punti materiali, con Nà+∞ \nM=∫dm=∫Vρ(⃗r)dτρ(⃗r)=dmdτ\nM=∫dm=∫Vρ(⃗r)dτ∫f(⃗r,⃗v)dm=∫Vf(⃗r,⃗v)ρ(⃗r)dτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#22": "Esempio: centro di massa\n23\nSistema di punti\nSistema continuo\nEsempio: cubo omogeneo di lato L\nxzy⃗rCM=∫V⃗rρ(⃗r)dτ∫Vρ(⃗(r)dτ=∫V⃗rρ(⃗r)dτMxCM=∫Vxρ(⃗r)dτ∫Vρ(⃗(r)dτ=∫Vxρ(⃗r)dτMyCM=∫Vyρ(⃗r)dτ∫Vρ(⃗(r)dτ=∫Vyρ(⃗r)dτMzCM=∫Vzρ(⃗r)dτ∫Vρ(⃗(r)dτ=∫Vzρ(⃗r)dτM",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#23": "SISTEMI PIANI\n24Se un sistema continuo volumetrico ha una dimensione sempre costante allora può essere trattato come un sistema continuo pianoEsempi: foglio di carta, lastra metallica, tavola di legno, muro\nDef: densità superficiale media di massa: \nDef: densità superficiale puntuale (locale): \nDimensionalmente:\nSe il sistema esteso ha uno spessore costante pari a D:\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#24": "¯λ=MLSistemi lineari\n25Se un sistema continuo volumetrico ha due dimensioni sempre costanti allora può essere trattato come un sistema continuo lineareEsempi: corda, filo, sbarra, palo, colonna, pilastroDef: densità lineare media: \nDef: densità lineare puntuale (locale): \n[¯λ]=[ML−1]kg/m",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#25": "Esercizio\n26Una sbarra di lunghezza L è collocata lungo l’asse x con un estremo nell’origine (0<x<L). Determinare la coordinata x del CM sapendo che la sbarra ha una densità lineare pari a:\nxL0Caso 1:\nCaso 2:\nCaso 3:\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#26": "Esercizio\n27Trovare le coordinate del CM di un triangolo rettangolo isoscele di lato L e densità superficiale σ costante.\nyxO\ndx\nVerificare che \n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#27": "Corpi rigidi\n28\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#28": "Moto del corpo rigido\n29Sistema di N=1 punto \n3 variabili dinamicheSistema rigido di N=2 punti \n6 variabili dinamiche dei punti - 1 vincolo = 5 variabili dinamiche indipendenti\nSistema rigido di N=3 punti 3x3=9 variabili dei punti - 3 vincoli = 6 variabili indipendenti\nSistema rigido di N>3 punti Ogni punto in più introduce 3 variabili dinamiche del punto e 3 vincoli\nNel caso del corpo rigido il moto è descrivibile da sole 6 equazioni.\n6 variabili dinamiche indipendenti",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#29": "Quali variabili dinamiche?\n30La descrizione completa ne richiede 6. Ho la libertà di scegliere quali.\nABCon le 6 variabili così definite posso descrivere la posizione del corpo rigido nello spazio. Sono convenienti dal punto di vista della dinamica?\nSuggerimento: 3 coordinate del CM + 3 variabili angolari6 eq. in 6 incogniteEsempio:  •3 coordinate del punto A + 3 coordinate del punto B – 1 vincolo sulla distanza AB = 5 variabili. •1 angolo di rotazione attorno all’asse AB\nEquazioni  cardinali per sistema di punti",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#3": "Centro di Massa\n4O\nCM\nSia dato un sistema di N punti materiali descritti in un sistema di riferimento inerziale, si definisceCentro di massa:\n3mmL\nL/43L/4CM\nPunto geometrico definito dal vettore posizione   “interno al sistema”, ma non necessariamente appartenente al sistema. E’ una caratteristica del sistema di punti e non del sistema di riferimento usato.→𝒓𝑪𝑴",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#30": "Moto generico di un corpo rigido\n31Due possibili moti indipendenti:\n2.Moto rotatorio puro attorno ad un asse passante per il CM che è fisso    Tutti i punti compiono un moto circolare attorno all’asse istantaneo di rotazione\nVale ancora:Due possibili moti indipendenti:1.Moto traslatorio puro.   - Tutti i punti hanno la stessa velocità (del CM)  - Traiettorie dei punti sono tutte parallele  - Moto descritto dalla prima equazione cardinale\nCM\n Il moto più generico possibile è un moto roto-traslatorio, dove il CM si muove seguendo la prima equazione cardinale ed il corpo fa un moto rotatorio attorno ad un asse istantaneo di rotazione e solo se l’asse di rotazione passa per il CM il moto è descritto dalla seconda equazione cardinale. ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#31": "Moto di un corpo rigido\n32\nEquazioni cardinali per i sistemi sono separate per il moto del CM e per l’evoluzione del momento angolare.Prima equazione cardinale analoga a  per il punto materiale. Ben nota.→𝐹=𝑚→𝑎Seconda equazione cardinale è la vera novità per il moto dei corpi estesi. La studiamo in dettaglio nel caso più semplice: il CM non ha moto traslatorio e utilizzo il sistema di riferimento intrinseco (moto rotatorio puro)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#32": "Moto rotatorio puro\n33\nScelgo un SRI dove il CM è l’origine (Sistema intrinseco) e suppongo il CM fermo.\nUnica equazione cardinale utile:                        per il 3o teorema del CM\nCM\nCon il CM fisso, l’unico moto possibile è una rotazione attorno a un asse fisso passante per il CM con velocità angolare  ⃗ωIl sistema intrinseco S’ ha origine nel CM ed è solidale  con il corpo rigido: tutti i punti del corpo rigido sono fermi in S’Legge di trasformazione delle velocità tra due sistemi di riferimento in moto relativo⃗vi=⃗v′\u0000i+⃗vO′\u0000+ω∧⃗r′\u0000i",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#33": "Moto rotatorio puro\n34\nScelgo un SRI dove il CM è l’origine (Sistema intrinseco) e suppongo il CM fermo.\nUnica equazione cardinale utile:                        per il 3o teorema del CM\nCM\nCon il CM fisso, l’unico moto possibile è una rotazione attorno a un asse fisso passante per il CM con velocità angolare \nIl sistema intrinseco S’ ha origine nel CM ed è solidale  con il corpo rigido: tutti i punti del corpo rigido sono fermi in S’Nulli perché i due sistemi di riferimento si muovono reciprocamente di solo moto rotatorio.⃗vi=⃗v′\u0000i+⃗vO′\u0000+ω∧⃗r′\u0000i",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#34": "Moto rotatorio puro\n35\nScelgo un SRI dove il CM è l’origine (Sistema intrinseco) e suppongo il CM fermo.\nUnica equazione cardinale utile:                        per il 3o teorema del CM\nCM\nCon il CM fisso, l’unico moto possibile è una rotazione attorno a un asse fisso passante per il CM con velocità angolare \nIl sistema intrinseco S’ ha origine nel CM ed è solidale  con il corpo rigido: tutti i punti del corpo rigido sono fermi in S’ perché  per la scelta del sistema S’ →𝑟𝑖=→𝑟′\u0000𝑖→𝑟𝑂′\u0000=→𝑟𝐶𝑀=0⃗vi=⃗v′\u0000i+⃗vO′\u0000+ω∧⃗r′\u0000i=ω∧⃗ri",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#35": "Moto rotatorio puro\n36\nScelgo un SRI dove il CM è l’origine (Sistema intrinseco) e suppongo il CM fermo.\nUnica equazione cardinale utile:                        per il 3o teorema del CM\nCM\nIn generale  è un vettore la cui direzione NON coincide con →𝑷𝑪𝑴→𝝎Con il CM fisso, l’unico moto possibile è una rotazione attorno a un asse fisso passante per il CM con velocità angolare \nIl sistema intrinseco S’ ha origine nel CM ed è solidale  con il corpo rigido: tutti i punti del corpo rigido sono fermi in S’Momento angolare per sistemi di punti materiali.Momento angolare per sistemi estesi.⃗vi=⃗v′\u0000i+⃗vO′\u0000+ω∧⃗r′\u0000i=ω∧⃗ri⃗PCM=∫V⃗r∧⃗vρdτ=∫V⃗r∧(⃗ω∧⃗r)ρdτ⃗PCM=N∑i=1⃗ri∧mi⃗vi=N∑i=1mi⃗ri∧(ω∧⃗ri)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#36": "Momento angolare di un sistema rigido\n37\nProprietà del doppio prodotto vettoriale:2)  ⃗ri(⃗ω⋅⃗ri)=(xîı+yî𝚥+zîk)⋅(xiωx+yiωy+ziωz)=1) ⃗ωr2i=ωxr2îı+ωyr2î𝚥+ωzr2îk⃗ri=(xîı+yî𝚥+zîk)\n=̂ı(ωxx2i+ωyxiyi+ωzxizi)+̂𝚥(ωxxiyi+ωyy2i+ωzyizi)+̂k(ωxxizi+ωyyizi+ωzz2i)⃗PCM=N∑i=1⃗ri∧mi⃗vi=N∑i=1mi⃗ri∧(ω∧⃗ri)⃗PCM=N∑i=1mi⃗ri∧(ω∧⃗ri)=N∑i=1mi⃗ωr2i−N∑i=1mi⃗ri(⃗ω⋅⃗ri)⃗a∧⃗b∧⃗c=⃗b(⃗a⋅⃗c)−⃗c(⃗a⋅⃗b)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#37": "Momento angolare di un sistema rigido\n38Sostituisco le espressioni trovate in precedenza=N∑i=1mi(ωxr2îı+ωyr2î𝚥+ωzr2îk)+−N∑i=1mi[̂ı(ωxx2i+ωyxiyi+ωzxizi)++̂𝚥(ωxxiyi+ωyy2i+ωzyizi)++̂k(ωxxizi+ωyyizi+ωzz2i)]⃗PCM=N∑i=1⃗ri∧mi⃗vi=N∑i=1mi⃗ri∧(ω∧⃗ri)=",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#38": "Momento angolare di un sistema rigido\n39            =̂ı𝑁∑𝑖=1𝑚𝑖[(𝜔𝑥𝑟2𝑖−𝜔𝑥𝑥2𝑖−𝜔𝑦𝑥𝑖𝑦𝑖−𝜔𝑧𝑥𝑖𝑧𝑖)]++̂𝚥𝑁∑𝑖=1𝑚𝑖[(𝜔𝑦𝑟2𝑖−𝜔𝑥𝑥𝑖𝑦𝑖−𝜔𝑦𝑦2𝑖−𝜔𝑧𝑦𝑖𝑧𝑖)]+̂k𝑁∑𝑖=1𝑚𝑖[(𝜔𝑧𝑟2𝑖−𝜔𝑥𝑥𝑖𝑧𝑖−𝜔𝑦𝑦𝑖𝑧𝑖−𝜔𝑧𝑧2𝑖)]Raccolgo tutti i termini diretti lungo lo stesso versore⃗PCM=N∑i=1⃗ri∧mi⃗vi=N∑i=1mi⃗ri∧(ω∧⃗ri)=",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#39": "Momento angolare di un sistema rigido\n40\nSi è sostituito: 𝑟2𝑖−𝑥2𝑖=(𝑥2𝑖+𝑦2𝑖+𝑧2𝑖)−𝑥2𝑖=𝑦2𝑖+𝑧2𝑖𝑟2𝑖−𝑦2𝑖=(𝑥2𝑖+𝑦2𝑖+𝑧2𝑖)−𝑦2𝑖=𝑥2𝑖+𝑧2𝑖𝑟2𝑖−𝑧2𝑖=(𝑥2𝑖+𝑦2𝑖+𝑧2𝑖)−𝑧2𝑖=𝑥2𝑖+𝑦2𝑖E ordino secondo le componenti di →𝜔⃗PCM=N∑i=1⃗ri∧mi⃗vi=N∑i=1mi⃗ri∧(ω∧⃗ri)=",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#4": "Centro di Massa\n5Centro di massa:\nEquazione vettoriale  corrisponde a 3 equazioni scalari⇒𝑥𝐶𝑀=∑𝑁𝑖=1𝑚𝑖𝑥𝑖∑𝑁𝑖=1𝑚𝑖=1𝑀𝑁∑𝑖=1𝑚𝑖𝑥𝑖𝑦𝐶𝑀=∑𝑁𝑖=1𝑚𝑖𝑦𝑖∑𝑁𝑖=1𝑚𝑖=1𝑀𝑁∑𝑖=1𝑚𝑖𝑦𝑖𝑧𝐶𝑀=∑𝑁𝑖=1𝑚𝑖𝑧𝑖∑𝑁𝑖=1𝑚𝑖=1𝑀𝑁∑𝑖=1𝑚𝑖𝑧𝑖",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#40": "Momento angolare di un sistema rigido\n41\nCompaiono dei termini uguali nelle posizioni simmetriche. ⃗PCM=N∑i=1⃗ri∧mi⃗vi=N∑i=1mi⃗ri∧(ω∧⃗ri)=",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#41": "Momento angolare di un sistema rigido\n42\nIn generale  è un vettore la cui direzione NON coincide con →𝑷𝑪𝑴→𝝎I: tensore d’inerzia Matrice 3x3 simmetrica\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#42": "Momento d’inerzia\n43\nin generale non sono paralleli.\nproblema agli autovalori/autovettoriI=I0100010001⎛⎝⎜⎜⎜⎞⎠⎟⎟⎟In casi particolari il tensore d’inerzia è diagonale e proporzionale al tensore unitario\nIndipendentemente dalla sua forma, per un corpo rigido generico esistono tre direzioni (detti assi principali d’inerzia) per cui  : →𝑷∥→𝝎→𝑷=𝝀→𝝎Esempi: - Sfera di raggio R e massa M:    \n              - Cubo di lato L e massa M:    \n⃗P=I⃗ω=λ⃗ω(I−1λ)⃗ω=0→(I−1λ)=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#43": "Momento d’inerzia\n44problema agli autovalori/autovettoriè la matrice d’inerzia simmetrica  esistono sempre 3 autovalori e 3 autovettori che diagonalizzano la matrice𝐼 →𝐼=𝐼𝑥𝑥000𝐼𝑦𝑦000𝐼𝑧𝑧,= momenti principali d’inerzia Per tale matrice  sono gli assi principali d’inerzia.Ixx,Iyy,Izẑı,̂𝚥,̂k(I−1λ)⃗ω=0→(I−1λ)=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#44": "Momento d’inerzia\n45Se l’asse z è un asse principale d’inerzia allora quando l’asse di rotazione coincide con l’asse z (velocità angolare solo lungo l’asse z) si ha:\nPer sistemi di punti materialiPer sistemi estesi⃗P=I⃗ω=Ixx000Izz000Izz(00ω)=Izzω̂k⃗P=Izzω̂kIzz=∫V(x2+y2)ρdτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#45": "Rotazioni attorno all’asse z:⃗P=Iω̂k\n46z\nxy\nPrendo un sistema di N punti materiali simmetrico rispetto all’asse z in moto rotatorio con velocità  attorno ad un asse coincidente con l’asse di simmetria. →𝜔Sistemi simmetrici rispetto ad un asse hanno nell’asse di simmetria un asse principale\nSe l’asse z è un asse principale d’inerzia allora quando l’asse di rotazione coincide con l’asse z si ha con →𝑃=𝐼𝑧𝑧→𝜔=𝐼𝑧𝑧𝜔^𝑘 𝐼𝑧𝑧=∑𝑁𝑖=1𝑚𝑖(𝑥2𝑖+𝑦2𝑖)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#46": "Rotazioni attorno all’asse ⃗P=Iω̂k\n47Per un sistema di N punti materiali simmetrico rispetto all’asse z\nI    (cvd)→𝑃=𝐼→𝜔=𝐼𝑥𝑥𝐼𝑥𝑦0𝐼𝑥𝑦𝐼𝑦𝑦000𝐼𝑧𝑧00𝜔^𝑘=𝐼𝑧𝑧𝜔^𝑘z\nxy\nPer un sistema di N puntiPer un sistema esteso\nIzz=∫V(x2+y2)ρdτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#47": "Dinamica rotazionale\n48Dinamica rotazionale:\nAnaloga a :\nStessa struttura, stesse soluzioni. in generale è una variabile dinamica vettoriale descritta da 3 variabili scalari di tipo angolare che possono essere:  •2 variabili per indicare la direzione del vettore •1 variabile per l’intensità della velocità angolare.→𝜔𝑑→𝑃𝐶𝑀𝑑𝑡=→𝑀𝐸𝑆𝑇𝐶𝑀→𝑃=𝐼→𝜔𝑑→𝑣𝐶𝑀𝑑𝑡=→𝐹𝐸𝑆𝑇𝑀⟹𝑑→𝑣𝐶𝑀=→𝐹𝐸𝑆𝑇𝑀𝑑𝑡⇒→𝑣𝐶𝑀=𝑡∫0→𝐹𝐸𝑆𝑇𝑀𝑑𝑡+→𝑣0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#48": "Dinamica rotazionale\n49\nCaso più semplice: moti con asse di rotazione fisso (z): →𝜔=𝜔^𝑘\n diretto lungo l’asse z→𝑀𝐸𝑆𝑇𝐶𝑀=𝑀𝐸𝑆𝑇𝐶𝑀^𝑘𝐼𝑧𝑧𝑑𝜔𝑑𝑡=𝑀𝐸𝑆𝑇𝐶𝑀→𝑑𝜔𝑑𝑡=𝑀𝐸𝑆𝑇𝐶𝑀𝐼𝑧𝑧⟹𝑑𝜔=𝑀𝐸𝑆𝑇𝐶𝑀𝐼𝑧𝑧𝑑𝑡⇒Il momento d’inerzia rappresenta per la dinamica rotazionale ciò che la massa rappresenta nel moto traslatorio: fornisce l’inerzia del corpo rigido ad essere messo in moto rotatorio.𝜗(𝑡)=𝑡∫0𝜔(𝑡)𝑑𝑡+𝜗0𝜔(𝑡)=𝑡∫0𝑀𝐸𝑆𝑇𝐶𝑀𝐼𝑧𝑧𝑑𝑡+𝜔0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#49": "Esempio\n50Due punti, di uguale massa m, legati da un filo lungo 2R, ruotano nel piano orizzontale e sono soggetti ad un momento delle forze MEST costante diretto in modo perpendicolare al piano del moto.\nθ(t)\nyx =2𝑚(𝑅𝑐𝑜𝑠𝜃^𝑖)∧(−˙𝜃𝑅𝑠𝑒𝑛𝜃^𝑖)+(𝑅𝑐𝑜𝑠𝜃^𝑖)∧(˙𝜃𝑅𝑐𝑜𝑠𝜃^𝑗)+(𝑅𝑠𝑒𝑛𝜃^𝑗) ∧(−˙𝜃𝑅𝑠𝑒𝑛𝜃^𝑖)+(𝑅𝑠𝑒𝑛𝜃^𝑗) ∧(˙𝜃𝑅𝑐𝑜𝑠𝜃^𝑗)=⃗v1=d⃗r1dt=−·θRsinθ̂ı+·θRcosθ̂𝚥⃗v2=d⃗r2dt=−d⃗r1dt=−⃗v1⃗PCM=m⃗r1∧⃗v1+m⃗r2∧⃗v2=m⃗r1∧⃗v1+m(−⃗r1)∧⃗(−v1)=2m⃗r1∧⃗v1=2m⃗r1∧⃗v1=2m(Rcosθ̂ı+Rsinθ̂𝚥)∧(−R·θsinθ̂ı+R·θcosθ̂𝚥)==2𝑚˙𝜃𝑅2(𝑐𝑜𝑠2𝜃+𝑠𝑒𝑛2𝜃)^𝑘=2𝑚˙𝜃𝑅2^𝑘",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#5": "Primo teorema del Centro di Massa\n6Velocità del Centro di massa:\nCentro di massa:\nDerivando:\nRicordando:⟹⟹",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#50": "Esempio\n51Due punti, di uguale massa m, legati da un filo lungo 2R, ruotano nel piano orizzontale e sono soggetti ad un momento delle forze MEST costante diretto in modo perpendicolare al piano del moto.\nθ(t)\nyx→𝑃𝐶𝑀=𝐼→𝜔⟹2𝑚˙𝜃𝑅2^𝑘=𝐼→𝜔⟹{𝐼=2𝑚𝑅2→𝜔=˙𝜃^𝑘Caso  MEST =0  ⟹\n𝑑𝑃𝐶𝑀𝑑𝑡=𝑀𝐸𝑆𝑇→𝑃𝐶𝑀=2𝑚˙𝜃𝑅2^𝑘Seconda equazione cardinaleCaso  MEST=cost  ⟹\n  ˙𝜃(𝑡)=˙𝜃0+𝛼𝑡𝜃(𝑡)=𝜃0+˙𝜃0𝑡+12𝛼𝑡2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#51": "Momenti d’inerzia di solidi\n52Sbarra omogenea di massa M e lunghezza L: \nI=λx2dx−L2L2∫=λx33⎡⎣⎢⎤⎦⎥−L2L2=λ2L33⋅8=MLL33⋅4=ML212Disco omogeneo di raggio R e massa M: \nσ=MS=MπR2,dS=2πrdrI=σr22πrdr0R∫=2πσr44⎡⎣⎢⎤⎦⎥0R=2πMπR2R44=MR22Piastra rettangolare omogenea di lati a e b:\nσ=MS=Mab,dS=dxdy\n-a/2a/2b/2-b/2-L/2L/2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#52": "Teorema di Huygens-Steiner\n53Calcoliamo il momento d’inerzia rispetto ad un asse passante a distanza D                            dal CM (asse lungo z) per un sistema rigido di N punti materiali.\nCMxyy’x’z’z\nPer definizioneIntroduco il SR intrinseco\nIl momento d’inerzia rispetto ad un asse qualunque è sempre pari al momento d’inerzia calcolato rispetto ad un asse parallelo a quello dato ma passante per il CM aumentato della quantità MD2 con D distanza tra i due assi.\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#53": "Teorema di König per corpi rigidi\n54Energia cinetica di un sistema di punti materialiVelocità dei punti nel sistema del CM\nαd\nT=T'+TCM=12mivi'2i∑+12MvCM2\nEnergia cinetica di rotazioneEnergia cinetica di traslazioneCome si calcola l’energia cinetica totale di un corpo rigido?Ipotesi semplificata: corpo in rotazione istantanea attorno a un asse passante per il CM diretto lungo z.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#54": "Statica dei corpi rigidi•Quando un corpo rigido è in condizioni statiche? (ànon si muove)\n55\nEquazioni cardinali della Dinamica dei sistemiR: Quando ogni punto del corpo è e rimane fermo!                      il corpo non trasla e non ruota!\nControllo forze e momenti esterni e questo garantisce che il corpo resti fermo per il 1°, 2° e 3° principio!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#55": "Macchina di AtwoodDiscutere il moto dei due oggetti 1 e 2 appesi tra loro su una carrucola tramite un filo ideale nel caso: 1) la carrucola sia ideale; 2) la carrucola sia reale. \n56\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#56": "EsercizioUn’asta omogenea di sezione trascurabile, di massa m e lunghezza l giace su un piano orizzontale liscio inizialmente ferma ed incernierata in uno degli estremi. Ad un certo istante un punto materiale di massa 2m che si muove con velocità v0 urta in modo totalmente anelastico e perpendicolarmente l’asta nel suo centro. Calcolare le espressioni:  1)Della velocità angolare del sistema dopo l’urto; 2)Della reazione vincolare che agisce sul sistema dopo l’urto.57",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#57": "EsercizioUna forza costante di 1960 N applicata tangenzialmente al bordo di un disco di raggio R=100 cm ne fa variare la velocità angolare da 4 s-1 a 2 s-1 in 30 s. Determinare: -il momento d’inerzia della ruota attorno al suo asse;  -il modulo della variazione del momento angolare nei 30 sec considerati;  -l’angolo descritto dalla ruota in questo intervallo di tempo -l’energia cinetica persa.\n58",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#58": "EsercizioUn disco omogeneo di massa M = 0.4 kg e raggio R = 10 cm viene appoggiato in verticale su un piano inclinato di 30° rispetto l’orizzontale. Sapendo che il disco scende rotolando senza strisciare, determinare la velocità di traslazione del disco dopo aver compiuto 2 m sul piano inclinato.\n59",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#59": "EsercizioDue volani assimilabili a due dischi aventi massa e raggio rispettivamente di M1 = 187,5 kg, R1 = 80 cm, M2 = 120 kg e R2 = 50 cm ruotano attorno allo stesso asse fisso orizzontale coincidente con il loro asse di simmetria con velocità angolari di  e . Ad un certo istante I due volani vengono messi a contatto. Calcolare la velocità angolare finale trascurando gli effetti transienti.𝜔1=33 𝑟𝑎𝑑/𝑠𝜔2=2𝜔1\n60",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#6": "Secondo teorema del Centro di Massa\n7Accelerazione del Centro di massa:\nPrima equazione cardinale→𝑎𝐶𝑀=𝑑→𝑣𝐶𝑀𝑑𝑡=𝑑(→𝑄𝑀)𝑑𝑡=1𝑀𝑑→𝑄𝑑𝑡=1𝑀→𝐹𝐸𝑆𝑇\nAnalogia formale  (e sostanziale) con: \nLa 1a equazione cardinale descrive il moto di un punto fittizio che è il CM. Se il sistema non è soggetto a forze esterne, il CM si muove con velocità costante. ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#60": "EsercizioUna Colonna di marmo di massa M = 600 kg ha la forma di un parallelepipedo a base quadrata di lato L = 30 cm e altezza h = 2.5 m ed è appoggiata in vertical su un piano ruvido inclinato di un angolo  rispetto l’orizzontale. Schematizzando la colonna come una figura piana che appoggia sul piano inclinator nei punti A e B distanti L determinare: 1)Il valore Massimo dell’angolo che permette la stabilità 2)La forza di attrito statica necessaria alla stabilità 3)Il minimo valore del coefficiente di attrito statico necessario per tenere ferma la colonna se 𝛼𝛼=5°\n61\nAB",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#61": "EsercizioDue punti materiali di massa M ruotano nel piano (x,y) attorno all’origine seguendo le equazioni del moto: Determinare le forze esterne ed i momenti delle forze esterne che agiscono sul sistema al tempo t=0.  \n62θ(t)=α2t2+ϖ0t",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#62": "EsercizioUn disco di massa M = 0.5 kg, raggio R = 0.2 m e spessore trascurabile ha densità superficiale . Supponendo che il disco sia disposto orizzontalmente e ruoti attorno ad un asse verticale passante per il suo centro con velocità angolare  = 4 rad/s, calcolare l’energia cinetica del sistema.σ=kr⃗ω\n63",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#63": "Esercizio\n64Una sbarra omogenea di massa M e lunghezza L è appesa al soffitto tramite un filo collegato al suo centro di massa. La sbarra si muove in un piano orizzontale (x,y) e il filo esercita un debole momento delle forze dato da                          .       Calcolare il periodo del movimento.\nxy\nθ\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#7": "Sistema di riferimento del CM\n8\nPosizione del CM rispetto al CMVelocità del CM rispetto al CM\nCalcoliamo la posizione del CM nel sistema intrinsecoIl CM definisce un punto importante per capire la dinamica del sistema. La prima equazione cardinale riguarda il moto di questo punto.  Che cosa descrive la seconda equazione cardinale?E’ conveniente introdurre un nuovo sistema di riferimento S’ (in generale NON inerziale) che esalti il ruolo del CM: SR Intrinseco con origine coincidente col CM.\nO\nCM=O’\n→𝑟𝑖\n→𝑟𝐶𝑀",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#8": "Terzo teorema del centro di massa\n9Riscriviamo il momento angolare nel SRI usando il sistema intrinseco:\n=𝑁∑𝑖=1𝑚𝑖(→𝑟′\u0000𝑖+→𝑟𝐶𝑀)∧(→𝑣′\u0000𝑖+→𝑣𝐶𝑀)Sostituendo:=𝑁∑𝑖=1𝑚𝑖→𝑟′\u0000𝑖∧→𝑣′\u0000𝑖+𝑁∑𝑖=1𝑚𝑖→𝑟′\u0000𝑖∧→𝑣𝐶𝑀+𝑁∑𝑖=1𝑚𝑖→𝑟𝐶𝑀∧→𝑣′\u0000𝑖+𝑁∑𝑖=1𝑚𝑖→𝑟𝐶𝑀∧→𝑣𝐶𝑀=𝑁∑𝑖=1𝑚𝑖→𝑟′\u0000𝑖∧→𝑣′\u0000𝑖+(𝑁∑𝑖=1𝑚𝑖→𝑟′\u0000𝑖)∧→𝑣𝐶𝑀+→𝑟𝐶𝑀∧(𝑁∑𝑖=1𝑚𝑖→𝑣′\u0000𝑖)+(𝑁∑𝑖=1𝑚𝑖)→𝑟𝐶𝑀∧→𝑣𝐶𝑀=momento angolare del sistema calcolato rispetto al SR intrinseco⃗P′\u0000CM=𝑀⟹𝑀→𝑟𝐶𝑀∧→𝑣𝐶𝑀Momento angolare di un punto di massa M situato nel CM Nulli per dimostrazione precedente⃗P0=N∑i=1⃗pi=N∑i=1mi⃗ri∧⃗vi",
    "data_test\\rootfolder\\università\\FisicaGenerale\\07-dinamica-sistemi.pdf#9": "Terzo teorema del centro di massa\n10\nSpin o momento angolare intrinseco\nTerzo teorema del centro di massa: il momento angolare rispetto ad un polo O di un sistema di punti materiali è in ogni istante uguale alla somma del momento angolare del sistema calcolato nel sistema di riferimento del centro di massa e del momento angolare rispetto allo stesso polo O di un punto materiale di massa pari alla massa totale M del sistema collocato nel CM.⃗P0=⃗P′\u0000CM+M⃗rCM∧⃗vCM=⃗P′\u0000CM+⃗rCM∧⃗Q",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#0": "Campo Gravitazionale CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#1": "Moto dei pianeti\n2Meccanica diventa disciplina coerente dopo un’accurata e attendibile descrizione del moto dei pianeti: moto in assenza di attriti studiabile per lungo tempo -> metodo scientifico facilmente applicabile: Comprensione del moto -> previsione del moto -> verifica sperimentale.Principali risultati grazie a : -Tycho Brahe (1546-1601): misura di precisione delle posizioni dei pianeti -Johannes Kepler (1571-1630): formulazione leggi empiriche sui moti dei pianeti a partire dai dati di Brahe\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#2": "Leggi di Keplero1.I pianeti descrivono orbite piane, ellittiche, di cui il Sole occupa uno dei due fuochi. 2.Il raggio vettore che unisce il centro del Sole con il centro del pianeta descrive aree uguali in tempi uguali. 3.I quadrati dei tempi che i pianeti impiegano a percorrere le loro orbite sono proporzionali al cubo del semiasse maggiore dell’orbita.3\na2T3=costante",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#3": "Gravitazione universale\n4•Cosa fa girare i pianeti? •Moto può avvenire anche in assenza di forza (principio di inerzia), ma serve una “spinta” centripeta per mantenere il corpo in traiettoria curva. •Newton: pianeti si muovono sottoposti alla forza di gravità che è la stessa che fa cadere i corpi a Terra. •Che forma ha questa forza?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#4": "5Velocità areolare\nA(t)=limΔt→0ΔSΔtαΔt→0⎯→⎯⎯π−β[A(t)]=[rv]=[L2T−1]→(m2/s) !A=12P−O()∧!v=12!r∧!v",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#5": "Gravitazione universale\n61a legge di Keplero: il moto avviene su un piano.Velocità areolare2a legge di Keplero: la velocità areolare è costante in modulo.\n2o principio dinamica ⃗A=12(P−O)∧⃗v=12⃗r∧⃗vd⃗Adt=12ddt(⃗r∧⃗v)=12(d⃗rdt∧⃗v+⃗r∧d⃗vdt)=12(⃗v∧⃗v+⃗r∧⃗a)=12⃗r∧⃗a=⃗0Campo centrale a simmetria sferica",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#6": "Gravitazione universale\n73a legge di Keplero:Moto dei pianeti può essere schematizzato come moto circolare uniforme \nStessa struttura di quanto ipotizzato dalla 2a leggeDallo studio dei moti celesti:  con M=massa attorno a cui ruota m     GM=4π2kCostante di gravitazione universaleG=6,672⋅10−11N⋅m2kg2a2T3=costanteT=2πω→ω=2πT⃗a(t)=⃗at+⃗an=··ŝut+v2ρ̂un=v2ρ̂unT2=kR3⃗Fcentripeta=m⃗ac=mv2R̂un=mω2R̂un=m4π2T2R̂un⟹⃗Fcentripeta=m4π2T2R̂un=m4π2kR3R̂un=−m4π2kR2̂ur",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#7": "Legge di gravitazione universale\n8\nUn qualsiasi punto materiale P1 di massa m1 esercita su un qualunque altro punto materiale P2 di massa m2 una forza gravitazionale F12 diretta secondo la congiungente di P1 con P2, sempre attrattiva, in modulo direttamente proporzionale al prodotto delle due masse e inversamente proporzionale al quadrato della distanza fra P1 e P2. •Per il terzo principio della dinamica se P1 esercita una forza su P2 allora P2 esercita una forza  su P1 uguale e contraria •Sul sistema agiscono due forze di risultante nulla ma applicate in punti di applicazione diversi -> il moto è uno solo •La forza gravitazionale è conservativa poiché è un campo centrale a simmetria sferica -> esiste un potenziale gravitazionale ⃗F12⃗F21conˆr=P2−P1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#8": "Energia potenziale gravitazionale\n9Campo conservativo\nCostante arbitrariaScelgo r0→∞⇒V(∞)=−Gm1m2r0=0V(A)=−Gm1m2rAEnergia potenziale gravitazionale",
    "data_test\\rootfolder\\università\\FisicaGenerale\\08-gravitazione.pdf#9": "Velocità di fuga\n10\nVelocità di fuga: velocità minima che occorre imprimere ad un corpo per far si che si allontani da un altro corpo senza ricadervi.Corpo in R si allontana in modo che arrivi all’infinito con velocità nullaConservazione dell’energia meccanica\n12mvfuga2−GmMR=0⇒vfuga=2GMRG",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#0": "1 Elettrostatica CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#1": "2Fenomeni elettriciFenomeni elettrici (e magnetici) noti dall’antichità\nTeoria completa dei fenomeni elettrici (e magnetici) nella seconda metà XIX secolo: Volta, Ampère, Faraday, Maxwell, Ørsted \n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#10": "11Elettrizzazione per contatto\nAC++++++++12Q+AB++++++++12Q+12Q+++++++++Fino a che punto possiamo suddividere (separare) la carica elettrica?\nLimite della Natura la più piccola carica elettrica osservata fino ad ora in natura è quella dell’elettrone (-) e del protone (+)Cariche elettriche frazionar ie della carica elementari sono s tate ipotizzate  (quark confinati all’interno di protoni e neutroni) ma MAI osservate fino ad ora",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#11": "12La carica elettricaUnità di misura della carica elettrica nel Sistema Internazionale:  C  (Coulomb) Carica dell’elettrone:   qe= −1.6 × 10-19 C   Carica del protone:      qp= +1.6 × 10-19 C Limite sperimentale:                                                               \u0000\u0000\u0000\u0000|qe|\u0000|qp||qp|\u0000\u0000\u0000\u0000<⇡10\u000021",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#12": "13La carica elettrica - esempiCarica degli elettroni in una goccia d’acqua (1g)  Ne=(Np)=3×1023     |Qe|=(|Qp|)=5×104 C  Forza tra due cariche da 1C ad 1m di distanza 9×109 N (equivalente a 100 Titanic!!)Carica in processi triboelettrici  |Q|=10-7C (1011 elettroni)1m1C1C×100",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#13": "14Proprietà carica elettricaEsistono due tipi di cariche elettriche  •convenzionalmente positive e negative La carica elettrica è quantizzata •in natura le cariche sono multiple della carica elettrica elementare  |qe|= 1.6 × 10-19−19 C In un sistema isolato, la carica elettrica si conserva •il numero totale di cariche (negative e positive) rimane invariato ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#14": "15Interazioni tra cariche elettriche~F=m~aIpotesi iniziali (per il momento…) •Scegliamo un sistema di riferimento inerziale: -un corpo non soggetto a forze si muove con velocità costante -  •Supponiamo che ci sia il vuoto nello spazio interposto tra le cariche che interagiscono •Consideriamo solo cariche ferme (ELETTROSTATICA)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#15": "16Forza elettrostaticaBilancia di torsione (Coulomb, fine XVIII sec)\n𝜃Avvicinando la cariche q2 e q1, si arriva ad una situazione di equilibrio in cui la forza elettrica è bilanciata dalla forza di torsione del pendolo Felettrica=Ftorsione∝𝜃  Dalla misura dell’angolo 𝜃 si ricava l'intensità della forza elettrica.q1, q2 cariche  r distanza tra le cariche 𝜃 angolo di torsione|Fel|/|q1||q2|r2Sperimentalmente si osserva: ricordiamoci che la forza è un vettore…",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#16": "17Legge di Coulomb\nz\ny\nx\nq1q2SdR cartesiano ortogonaleForza esercitata dalla carica q1 sulla carica q2\nCostante dielettrica del vuoto:q1 e q2  puntiformi~F12=14⇡\"0q1q2r3~r~F12ha stessa direzione di~re verso che dipende dal segno delle cariche⃗F12=14πε0q1q2r2̂ur\n(Farad verrà introdotto in seguito)ε0=8.85×10−12C2Nm2=8.85×10−12Fm14πε0=8.99×109Nm2C2⃗r=⃗r2−⃗r1⃗r1⃗r2⃗F12=q1q24πε0⃗r2−⃗r1|⃗r2−⃗r1|3⃗F12=14πε0q1q2r3⃗r",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#17": "18Legge di Coulomb~F21=\u0000~F12III principio della dinamica (sistema isolato)Il vettore forza è applicato sulla caricaStesso segnoq1q2~r=~r2\u0000~r1~F12=14⇡\"0q1q2r3~r~F21q1q2~r=~r2\u0000~r1~F12=14⇡\"0q1q2r3~r~F21Segni opposti",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#18": "19Esercizio\n𝜃 l m,q m,q l Esercizi di Fisica Generale T2Lorenzo Rinaldi18/9/20181 Elettrostatica nel vuoto1.1Due piccole palline di sughero identiche di massamhanno ugual caricaq. Esse sono appese a due ﬁli dilunghezzal, a loro volta vincolati in un medesimo punto. In condizioni di equilibrio, le due palline formanoun angolo✓con la verticale. Determinare quanto vale la carica sulle due palline (nell’approssimazione dipiccoli angoli).(R:q=p16⇡\"0mgl2✓3)1.2Tre cariche positive puntiformi identicheq1=q2=q3=4 mC sono disposte su un piano cartesiano ortogonalerispettivamente nei punti di coordinate (0;3m), (0;-1m) e (-1m;1m). Una quarta carica positivaq4=2 mC ` eposta nel punto di coordinate (1m;1m). Determinare:a) la forza a cui ` e sottoposta la caricaq4;( R :~F=q1q44⇡✏08p5+25100ˆı=3.09ˆıJ)b) l’energia necessaria a spostare la caricaq4dalla posizione iniziale (1m;1m) all’origine del sistema diriferimento. (R:L=q1q44⇡✏025+15p2\u000012p330)1.3Tre cariche puntiformi sono poste ai vertici di un triangolo equilatero di latoa=10cm. Sapendo cheq1=\u00004·10\u00007C,q2=+ 2·10\u00007Ceq3=+ 1·10\u00007C, determinare l’energia elettrostatica del sistema.(R:U=\u000010q24⇡✏0a=\u00009·10\u00003J)1.4Quattro particelle con la stessa caricaQ=-1nC si trovano ai vertici di un quadrato di latol=12cm. Si calcoli:a) il modulo dell’intensit` a del campo elettrico nel centroOdel quadrato e nel punto medioMdi ciascunlato (R:EO= 0,EM=4p525q⇡✏0l2=8.93⇥102V/m);b) la di↵erenza di potenziale tra i puntiOeM;(R:\u0000V=q⇡✏0l(1 +p5/5\u0000p2)=1.12 V)c) il lavoro che si deve compiere per avvicinare le cariche e disporle ai vertici di un quadrato di latol/2.(R:L=q24⇡✏0⌃i,j>i1rij=4.06⇥10\u00007J)1.5Si consideri una carica elettrica distribuita uniformemente con densit` a di carica lineare\u0000=10\u00005C/m su diun ﬁlo di lunghezzaL=10 cm. Si calcoli il campo elettrico in un puntoAposto a distanzah=3 cm daun’estremit` a del ﬁlo, perpendicolarmente ad esso.(Ex=\u00004⇡✏0hLph2+L2,Ey=\u00004⇡✏0h(hph2+L2\u00001))1Esercizi di Fisica Generale T2Lorenzo Rinaldi18/9/20181 Elettrostatica nel vuoto1.1Due piccole palline di sughero identiche di massamhanno ugual caricaq. Esse sono appese a due ﬁli dilunghezzal, a loro volta vincolati in un medesimo punto. In condizioni di equilibrio, le due palline formanoun angolo✓con la verticale. Determinare quanto vale la carica sulle due palline (nell’approssimazione dipiccoli angoli).(R:q=p16⇡\"0mgl2✓3)1.2Tre cariche positive puntiformi identicheq1=q2=q3=4 mC sono disposte su un piano cartesiano ortogonalerispettivamente nei punti di coordinate (0;3m), (0;-1m) e (-1m;1m). Una quarta carica positivaq4=2 mC ` eposta nel punto di coordinate (1m;1m). Determinare:a) la forza a cui ` e sottoposta la caricaq4;( R :~F=q1q44⇡✏08p5+25100ˆı=3.09ˆıJ)b) l’energia necessaria a spostare la caricaq4dalla posizione iniziale (1m;1m) all’origine del sistema diriferimento. (R:L=q1q44⇡✏025+15p2\u000012p330)1.3Tre cariche puntiformi sono poste ai vertici di un triangolo equilatero di latoa=10cm. Sapendo cheq1=\u00004·10\u00007C,q2=+ 2·10\u00007Ceq3=+ 1·10\u00007C, determinare l’energia elettrostatica del sistema.(R:U=\u000010q24⇡✏0a=\u00009·10\u00003J)1.4Quattro particelle con la stessa caricaQ=-1nC si trovano ai vertici di un quadrato di latol=12cm. Si calcoli:a) il modulo dell’intensit` a del campo elettrico nel centroOdel quadrato e nel punto medioMdi ciascunlato (R:EO= 0,EM=4p525q⇡✏0l2=8.93⇥102V/m);b) la di↵erenza di potenziale tra i puntiOeM;(R:\u0000V=q⇡✏0l(1 +p5/5\u0000p2)=1.12 V)c) il lavoro che si deve compiere per avvicinare le cariche e disporle ai vertici di un quadrato di latol/2.(R:L=q24⇡✏0⌃i,j>i1rij=4.06⇥10\u00007J)1.5Si consideri una carica elettrica distribuita uniformemente con densit` a di carica lineare\u0000=10\u00005C/m su diun ﬁlo di lunghezzaL=10 cm. Si calcoli il campo elettrico in un puntoAposto a distanzah=3 cm daun’estremit` a del ﬁlo, perpendicolarmente ad esso.(Ex=\u00004⇡✏0hLph2+L2,Ey=\u00004⇡✏0h(hph2+L2\u00001))1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#19": "20Forza elettrica vs forza gravitazionaleForza ElettrostaticaForza GravitazionaleHanno stessa forma (entrambe dipendono dall’inverso del quadrato), ma… A. la forza elettrica è molto più intensa della forza gravitazionale B.la massa è sempre positiva (forza gravitazionale sempre attrattiva)14πε0=8.99×109 Nm2C−2G=6.67×10−11kg−1m3s−2⃗FCoulomb=14πε0q1q2r2̂ur⃗FGravitazionale=−Gm1m2r2̂ur",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#2": "3Fenomeni elettriciOsservazioni sperimentali (note da VI secolo A.C): oggetti di diversi materiali (es. vetro, plastica, ambra), dopo strofinio su panno di lana, se posti in vicinanza: •oggetti della medesima sostanza, si respingono •oggetti di sostanze diverse possono respingersi o attrarsi \nplastica\nvetro\nambra\nvetro\nplastica\nambra\nevidenza sperimentale esistenza di una forza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#20": "21EsempioCalcolare il rapporto tra le intensità della forza elettrica e di quella gravitazionale fra un elettrone ed un protone qe= −1.6 × 10-19 C       me= 1.9 × 10-31 kg  qp= +1.6 × 10-19 C       mp= 1.7 × 10-27 kg  G= 6.77 × 10-11 Nm-2kg-2 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#21": "22Forza elettrostatica e principio di sovrapposizione\nq3q2q1Sistema di N=3 cariche puntiformiForza totale sulla carica q1 è la somma vettoriale della forza        che la carica q2  eserciterebbe su q1 se q3 fosse assente e della forza          che la carica q3  eserciterebbe su q1 se q2 fosse assente⃗F1=⃗F21+⃗F31⃗F21⃗F31⃗F31⃗F1⃗F21",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#22": "23Forza elettrostatica e principio di sovrapposizione\nq\nsistema di N cariche puntiformiForza totale sulla carica q è la somma vettoriale delle forze che le cariche qi eserciterebbero singolarmente su q se qj≠i fossero assentiqi⃗F=N∑i=1⃗Fi⃗ri⃗rivettore posizione da qi a qqj=N∑i=114πε0qqir3i⃗ri",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#23": "24Il campo elettrostaticoLa forza di Coulomb può essere riformulata utilizzando il concetto di campo di forza •una carica Q altera le proprietà dello spazio, introducendo un campo elettrico    di cui Q è la sorgente •una seconda carica q (carica esploratrice) sentirà una forza dovuta alla presenza della carica Q⃗E⃗F\n  !=lim\"V#0\"q\"V=dqdVDistribuzioni Continue di Carica (II) •!Infine se la carica è distribuita in un volume conviene descrivere la distribuzione della carica utilizzando la densità volumetrica di carica (misurata in C/m3): •!Vogliamo ora calcolare la forza esercitata da una distribuzione di carica descritta dalla densità volumetrica & su di una carica puntiforme q posta a una certa distanza. •!Un volumetto elementare dV situato nel punto P) di vettore posizionale    conterrà la carica elettrica: !r!!r!r!!\"r  dVVqr!!   dq=!!\"r()dV25!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nDistribuzioni Continue di Carica (III) •!Il volumetto dV può essere considerato come una carica puntiforme e dunque possiamo applicare a esso la legge di Coulomb: •!Per il principio di sovrapposizione, la forza totale prodotta su q dalla carica contenuta nel volume V sarà la somma dei contribuiti di tutti i volumetti infinitesimi dV: d!F=14!\"0#!$r()dVdq\"#$%$q!r%!$r3!r%!$r()!F=14!\"0q#!$r()!r%!$r3!r%!$r()dVV&'((&'((&'((26!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!r!!r!r!!\"r  dVVq!F12=14!\"0q1q2r3!r\nCampo Elettrico •!La forza di Coulomb può essere riformulata utilizzando il concetto di campo di forza. •!Possiamo pensare che la presenza di una carica elettrica q1 posta nel punto P1 alteri le proprietà dello spazio, introducendo in esso un campo elettrico. •!Poniamo una carica puntiforme Q nell’origine di una terna cartesiana di riferimento e una seconda carica puntiforme q a una certa distanza r. La forza agente su q si può scrivere: !rqQ!F!r()=14!\"0Qqr2ˆr=q14!\"0Qr2ˆr#$%&'(!E!r()\"#$%$=q!E!r()27!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!F12=14!\"0q1q2r2ˆr\nCampo Elettrico (II) •!Possiamo allora definire campo elettrico di una carica puntiforme Q il campo vettoriale: e scrivere la forza agente su di una carica q situata nel punto di raggio vettore    come:   !F!r()=q!E!r()   !E!r()=14!\"0Qr2ˆrr!\n28!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!rqQQ⃗Eq⃗E=limq→0⃗Fq⃗F=q⃗Elimite va inteso in senso “fisico”: • q è quantizzata  •possiamo trascurare i fenomeni di induzione dovuti a q",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#24": "25Campo elettrostatico di una carica puntiforme\nz\ny\nxQq⃗rIn un SdR cartesiano poniamo: •carica sorgente Q nell’origine •carica esploratrice q in posizione  •q≪Q (trascuriamo il campo elettrico generato da q)⃗r⃗F(⃗r)=14πε0qQr2̂ur⃗E(⃗r)=14πε0Qr2̂ur\ncampo elettrostatico di una carica puntiforme Qla carica q è soggetta alla forza:⃗E=q14πε0Qr2̂ur=q⃗E(⃗r)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#25": "26Campo elettrostatico di una carica puntiformeL’azione della carica Q sulla carica q viene separata in due fasi distinte: • La creazione, da parte della carica Q, di un campo elettrico          in ogni punto dello spazio; •L’accoppiamento nel punto     del campo elettrico          con la carica q. La forza osservata sulla carica q si stabilisce per effetto dell’accoppiamento locale carica-campo elettrico.  Nel Sistema Internazionale il campo elettrico si misura in N/C (Newton/Coulomb) o V/m (V olt/metro)⃗E(⃗r)⃗E(⃗r)⃗r",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#26": "27Campo elettrico di una carica puntiforme\nCampo centrale: •diretto come versore -uscente da Q positiva -entrante in Q negativa •modulo dipende solo da r ̂rIl campo elettrico è un campo vettoriale: per ogni punto dello spazio è associato un vettore\nCampo Elettrico (III) •!In questo modo l’azione della carica Q sulla carica q viene separata in due fasi distinte: –!La creazione, da parte della carica Q, di un campo elettrico in ogni punto dello spazio; –!L’accoppiamento nel punto    del campo elettrico          con la carica q. La forza osservata sulla carica q si stabilisce per effetto dell’accoppiamento locale carica-campo elettrico. •!Nel Sistema Internazionale il campo elettrico si misura in N/C (Newton/Coulomb) o V/m (Volt/metro) e le sue dimensioni sono:   !E!r()  !E!r()r!\n  E!\"#$=F!\"#$Q!\"#$=MLT%3I%1!\"#$29!!rqQDomenico Galli – Fisica Generale B – 1. Elettrostatica!\nCampo Elettrico (IV) •!Il campo elettrico è un campo vettoriale: –!A ogni punto dello spazio è associato un vettore, il vettore campo elettrico:                                   . \n30!Domenico Galli – Fisica Generale B – 1. Elettrostatica!P!!3()\"E\"#\"\"EP()!V\nIntegrale di Superficie di una Funzione Vettoriale •!Sia data una funzione vettoriale   definita in R3 e sia data una superficie % !R3; •!Suddividiamo la superficie % in un certo numero n di superfici infini-tesime &%, prendiamo su di esse i punti: e consideriamo la somma: •!Nel limite in cui le superfici &% diventano infinitesime, la somma diventa l’integrale di superficie: \nDomenico Galli – Fisica Generale B – 1. Elettrostatica!!vP1x1,y1,z1(),P2x2,y2,z2(),…,Pnxn,yn,zn()!vPi()iˆnPi()!\"i=1n#!vPi()iˆnPi()!\"i=1n#n$%!\"$0&$&&I=!vP()iˆnd\"\"''31!\nIntegrale di Superficie di una Funzione Vettoriale (II) •!La superficie % !R3, può essere definita utilizzando i parametri $ e %: •!L’integrale di superficie si calcola quindi come: \nDomenico Galli – Fisica Generale B – 1. Elettrostatica!!=P\"!3;P=P#,$(),#\"#1,#2%&'(,$\"$1,$2%&'({}!vP()iˆnd!!\"\"=d#!vP#,$()()i%P\"!\"%#&%P\"!\"%$'()*+,d$$1$2\"#1#2\"\n32!\nCampo Elettrico (III) •!In questo modo l’azione della carica Q sulla carica q viene separata in due fasi distinte: –!La creazione, da parte della carica Q, di un campo elettrico in ogni punto dello spazio; –!L’accoppiamento nel punto    del campo elettrico          con la carica q. La forza osservata sulla carica q si stabilisce per effetto dell’accoppiamento locale carica-campo elettrico. •!Nel Sistema Internazionale il campo elettrico si misura in N/C (Newton/Coulomb) o V/m (Volt/metro) e le sue dimensioni sono:   !E!r()  !E!r()r!\n  E!\"#$=F!\"#$Q!\"#$=MLT%3I%1!\"#$29!!rqQDomenico Galli – Fisica Generale B – 1. Elettrostatica!\nCampo Elettrico (IV) •!Il campo elettrico è un campo vettoriale: –!A ogni punto dello spazio è associato un vettore, il vettore campo elettrico:                                   . \n30!Domenico Galli – Fisica Generale B – 1. Elettrostatica!P!!3()\"E\"#\"\"EP()!V\nIntegrale di Superficie di una Funzione Vettoriale •!Sia data una funzione vettoriale   definita in R3 e sia data una superficie % !R3; •!Suddividiamo la superficie % in un certo numero n di superfici infini-tesime &%, prendiamo su di esse i punti: e consideriamo la somma: •!Nel limite in cui le superfici &% diventano infinitesime, la somma diventa l’integrale di superficie: \nDomenico Galli – Fisica Generale B – 1. Elettrostatica!!vP1x1,y1,z1(),P2x2,y2,z2(),…,Pnxn,yn,zn()!vPi()iˆnPi()!\"i=1n#!vPi()iˆnPi()!\"i=1n#n$%!\"$0&$&&I=!vP()iˆnd\"\"''31!\nIntegrale di Superficie di una Funzione Vettoriale (II) •!La superficie % !R3, può essere definita utilizzando i parametri $ e %: •!L’integrale di superficie si calcola quindi come: \nDomenico Galli – Fisica Generale B – 1. Elettrostatica!!=P\"!3;P=P#,$(),#\"#1,#2%&'(,$\"$1,$2%&'({}!vP()iˆnd!!\"\"=d#!vP#,$()()i%P\"!\"%#&%P\"!\"%$'()*+,d$$1$2\"#1#2\"\n32!⃗E(⃗r)=14πε0Qr2̂ur",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#27": "28Principio di sovrapposizione del campo elettricosistema di N cariche puntiformiDimostriamo cheq\nqi⃗ri⃗E⃗F=q⃗E⃗F=N∑i=1⃗Fi=qN∑i=1[14πε0qir3i⃗ri]\nprincipio di sovrapposizione del campo elettrico ⃗E=N∑i=1⃗Ei⃗Ei=14πε0qir3i⃗ri=N∑i=114πε0qqir3i⃗ri=N∑i=1q[14πε0qir3i⃗ri]==qN∑i=1⃗Ei=q⃗E",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#28": "29Distribuzioni continue di caricaSpesso la carica elettrica non è puntiforme ma può essere distribuita in un volume nello spazio, su di una superficie o lungo una linea⇢=l i m\u0000⌧!0\u0000q\u0000⌧=dqd⌧\u0000=l i m\u0000l!0\u0000q\u0000x=dqdx\u0000=l i m\u0000S!0\u0000q\u0000S=dqdSDensità volumetrica di caricaDensità superficiale di caricaDensità lineare di carica\ndS\ndl\nd𝜏dq=ρdτcarica contenuta nel volumetto d𝜏  dq=σdS(C/m3)(C/m2)(C/m)\nqτ=∭τρdτcarica contenuta nel volume 𝜏  carica contenuta sulla sup. dS  carica contenuta sulla sup S  carica contenuta sulla linea dl  carica contenuta sulla linea l   qS=∬SσdSql=∫lλdldq=λdl",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#29": "30Campo elettrostatico da distribuzioni continue\nz\ny\nx\nP⃗r−⃗r′\u0000⃗r⃗r′\u0000Campo elettrico nel punto P generato da una carica infinitesima dq (contenuta in d𝜏):dq=ρ(⃗r′\u0000)dτPer il principio di sovrapposizione  (e sostituendo al limite la somma con l’integrale)volume 𝜏d⃗E(⃗r)=dq4πε0(⃗r−⃗r′\u0000)|⃗r−⃗r′\u0000|3=ρ(⃗r′\u0000)dτ4πε0(⃗r−⃗r′\u0000)|⃗r−⃗r′\u0000|3⃗E(⃗r)=14πε0∭τρ(⃗r′\u0000)(⃗r−⃗r′\u0000)|⃗r−⃗r′\u0000|3dτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#3": "4Elettrizzazione per strofinio (triboelettricità)In natura esistono due tipi di elettrizzazione a cui possiamo associare due tipologie di cariche elettriche Convenzionalmente: •elettrizzazione vetrosa     à carica elettrica positiva •elettrizzazione resinosa   à carica elettrica negativa • cariche dello stesso segno: forza repulsiva • cariche di segno opposto: forza attrattiva\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#30": "31Campo elettrostatico da distribuzioni continue\ndl\ndS\nd𝜏Carica distribuita  in volume 𝜏Carica distribuita  su superficie SCarica distribuita  su linea l\nWarning! formule generali da usare con attenzione: il calcolo degli integrali può risultare complesso, non fare confusione tra r (posizione del punto in cui si vuole calcolare il campo) e r’ (variabile di integrazione, relativa alla posizione delle cariche)⃗E(⃗r)=14πε0∫lλ(⃗r′\u0000)(⃗r−⃗r′\u0000)|⃗r−⃗r′\u0000|3dl⃗E(⃗r)=14πε0∭τρ(⃗r′\u0000)(⃗r−⃗r′\u0000)|⃗r−⃗r′\u0000|3dτ⃗E(⃗r)=14πε0∬Sσ(⃗r′\u0000)(⃗r−⃗r′\u0000)|⃗r−⃗r′\u0000|3dS",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#31": "32Il campo elettrico è un campo conservativo?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#32": "33PremessaFino ad ora ci siamo posti in condizioni statiche: le cariche sono ferme → ELETTROSTATICA Non abbiamo ancora studiato gli effetti delle cariche in moto   (esistono forze associate ai movimenti delle cariche?) Per il momento continuiamo la trattazione statica: il campo elettrostatico è conservativo?Andiamo a verificare una delle condizioni di conservatività dei campi ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#33": "34Circuitazione del campo elettrostaticoCalcoliamo la circuitazione del campo lungo la linea chiusa 𝛤 : circonferenza di raggio R centrata in Q \nTeorema della Divergenza (II) •!Per comprendere il significato del Teorema della Divergenza: immaginiamo di suddividere il volume V in tanti cubetti infinitesimi, di volume: •!Per ogni cubetto si ha, per quanto abbiamo visto: per cui si ha, per ogni cubetto (che ha 6 facce): !viˆndSS\"!!=!\"i!vdVV!!!\n57!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!V1,!V2,!V3,…!!i!v()P()=limV\"P{}!vSV()\"##iˆndSdVV###=$tot!v()%V!!i!v()Pi()\"Vi=#toti()!v()=#ki()!v()k=16$!V1!V2!V3!V4!V5!V6!V8!V7!V10!V9V\nTeorema della Divergenza (III) •!Distinguiamo ora, tra le facce dei cubetti, le facce interne e le facce esterne: –!Le facce interne separano un cubetto da un cubetto adiacente; –!Le facce esterne fanno parte della frontiera del volume totale V. •!Sommando le divergenze dei cubetti, i contributi dei flussi delle facce interne si cancellano tra loro: –!Il flusso uscente dal cubetto i verso il cubetto j adiacente è opposto al flusso dal cubetto j al cubetto i. •!Si ha pertanto: \n58!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!i!v()Pi()\"Vii=1N#=$ki()!v()k=16#i=1N#=$ki()!v()facceesterne#=!viˆn\"Sfacceesterne#!!i!vdVV%%%=!viˆndSS\"%%!V1!V2!V3!V4!V5!V6!V8!V7!V10!V9V\nLinee di Flusso del Campo Elettrico •!Come tutti i campi vettoriali, anche il campo elettrico si può rappresentare graficamente con le linee di flusso (o linee di campo), ovvero con linee: –!Tangenti in ogni punto al vettore campo elettrico         ; –!Orientate col verso del campo elettrico         ; –!In numero, per unità di superficie trasversale, proporzionale al modulo del campo elettrico            .   !E!r()  !E!r()\n59!  !E!r()\nDomenico Galli – Fisica Generale B – 1. Elettrostatica!\nAngolo Solido •!Come è noto, l’angolo piano (in radianti) è definito come il rapporto tra un arco di circonferenza l centrata nel vertice e il raggio r: •!L’angolo solido \" si definisce in maniera analoga come il rapporto tra la parte di superficie sferica S (centrata nel vertice), intercettata dal cono centrato nel vertice e il quadrato del raggio della sfera: •!L’angolo solido si misura in steradianti (sr). lr  !=lr\"0,2#$%$%!=Sr2\"0,4#$%&'S!r\n60!Domenico Galli – Fisica Generale B – 1. Elettrostatica!Qd⃗l=̂utdl\n∮Γ⃗E⋅d⃗l=∮ΓQ4πε0r2̂ur⋅̂utdl`=0\nIl campo elettrico (elettrostatico) generato da una carica puntiforme ferma è conservativo𝛤Circuitazione nullâr",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#34": "35Proprietà del campo elettrostaticoIl campo elettrostatico eredita tutte le proprietà dei campi conservativi∮Γ⃗E⋅d⃗l=0⃗∇∧⃗E=⃗0Il campo elettrostatico ha sempre circuitazione nullaIl campo elettrostatico è  irrotazionale  (non esistono linee di campo chiuse su loro stesse)Equazioni fondamentali dell’elettromagnetismo, applicate al caso statico (cariche ferme)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#35": "36Proprietà del campo elettrostaticoIl campo elettrostatico eredita tutte le proprietà dei campi conservativi\n∃ una funzione scalare V(x,y,z):⃗E=−⃗∇VV(x,y,z) è il potenziale elettrostatico ⃗E⋅d⃗l=−dVè un differenziale esattoV(A)−V(B)=∫BA⃗E⋅d⃗lL’integrale non dipende dal percorsometodo per calcolare  il potenziale, partendo dal campo elettrostaticometodo per calcolare  il campo elettrico, partendo dal potenziale ∃ una funzione scalare V(x,y,z):⃗E=−⃗∇V",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#36": "37Il potenziale elettrostaticoIl potenziale elettrostatico V(x,y,z) è una funzione scalare in ℝ3   Dato un campo elettrostatico, operativamente il potenziale si calcola integrando il differenziale esattodV=−⃗E⋅d⃗lIl potenziale è definito a meno di una costante additiva arbitraria La differenza di potenziale tra due punti è indipendente dalla costante arbitraria (è una grandezza misurabile → circuiti) integrale indefinitointegrale definitoV(A)−V(B)=∫BA⃗E⋅d⃗lV(x,y,z)=−∫⃗E⋅d⃗l+costL’unità di misura del potenziale nel S.I. è il Volt=Joule/Coulomb  (V)=(J)/(C)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#37": "38Il potenziale elettrostatico: carica puntiformeV(⃗r)=−∫⃗E⋅d⃗l+cost\nQd⃗l=̂rdr+̂u⊥dl⊥̂u⊥dl⊥d⃗l̂urdr⃗E=Q4πε01r2̂urV(⃗r)=[−∫Q4πε01r2̂ur⋅(̂urdr+̂u⊥dl⊥)]+cost=[−∫Q4πε01r2(̂ur⋅̂urdr+̂ur⋅̂u⊥dl⊥]+costCalcoliamo V dall’integrale indefinito lungo una generica curva 𝛤  =[−Q4πε0∫drr2]+costV(⃗r)=14πε0Qr+cost𝛤in genere si fissa il potenziale nullo all’infinito:=−Q4πε0(−1r)+cost=14πε0Qr+cost⃗r`1`0V(⃗r→∞)=0⇒14πε0Q(r→∞)+cost=0⇒cost=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#38": "39Il potenziale elettrostatico: carica puntiforme\nQAB⃗Ecalcoliamo la differenza di potenziale tra i punti A e BΔVAB=VA−VB=∫BA⃗E⋅d⃗l=...=Q4πε0[−1r]BA=Q4πε0(1rA−1rB)=14πε0QrA−14πε0QrBrBrAVA=V(⃗rA)=14πε0QrAVB=V(⃗rB)=14πε0QrBAssumendo il potenziale nullo all’infinito (cost=0)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#39": "40Principio di sovrapposizione del potenzialeDemo: basta usare la proprietà distributiva del prodotto vettorialeUn campo elettrostatico generato da N cariche discrete o da una distribuzione continua di carica è conservativo⃗∇∧⃗E=⃗∇∧(∑⃗Ei)=∑⃗∇∧⃗Ei=⃗0principio di sovrapposizionecampo da carica puntiforme irrotazionaleV(⃗r)=−∫⃗E⋅d⃗l=−∫(∑⃗Ei⋅d⃗l)=∑∫−⃗Ei⋅d⃗l=∑Vi(⃗r)Il potenziale elettrostatico generato da un sistema di cariche gode del principio di sovrapposizione",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#4": "5La struttura microscopica della materiaTutti i  materiali formati da atomi (e molecole) Gli atomi sono composti da •nucleo formato da protoni (carichi positivamente) e neutroni (neutri) •attorno al nucleo orbitano gli elettroni (carichi negativamente)\nLa Forza Elettromagnetica nella Fisica Moderna (II) •!La forza elettromagnetica è la forza dominante nel mondo fisico che conosciamo: –!Tiene uniti gli elettroni al nucleo negli atomi. –!Tiene uniti gli atomi nelle molecole; –!È all’origine delle forze elastiche; –!È all’origine delle forze di tensione delle funi; –!È all’origine delle forze di attrito; –!È all’origine delle forze di resistenza; –!È all’origine delle forze di tensione superficiale dei liquidi; –!È all’origine delle forze di urto; –!È all’origine delle reazioni vincolari. 9!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nLa Forza Elettromagnetica nella Fisica Moderna (III) \n10!Domenico Galli – Fisica Generale B – 1. Elettrostatica!•!Non hanno origine elettromagnetica poche forze comunemente note, tra cui: –!La forza peso (forza gravitazionale); –!La forza che mantiene i pianeti sulle loro orbite (forza gravitazionale); –!La forza che tiene uniti i quark nei nuclei degli atomi (forza nucleare forte). \nLa Composizione della Materia molecolaatomonucleoelettrone\nprotoneneutronequark10 cm!810 cm!12\n10 cm!1310 cm!13(<10 cm)!1811!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nI Costituenti della Materia: le Particelle Elementari (Fermioni) \n12!Domenico Galli – Fisica Generale B – 1. Elettrostatica!1 MeV/c2 = 1.783 ! 10–30 kg  Q=23eQ=!13eu d e !e c s µ\"!µ\"t b #\"!#\"elettrone up down neutrino elettronico neutrino muonico neutrino tauonico muone tauone charm strange top bottom m = 0 m = 0 m = 0 m = 0.5 MeV/c2 m = 8 MeV/c2 m = 15 MeV/c2 \nm = 170000 MeV/c2 m = 4500 MeV/c2 m = 106 MeV/c2 m = 1800 MeV/c2 m = 1600 MeV/c2 m = 300 MeV/c2 leptoni quark Esistite subito dopo il Big Bang. Ora presenti nei raggi cosmici e negli acceleratori Q=!eMateria ordinaria \n1 e = 1.602 ! 10–19 C  \nLa materia ordinaria risulta complessivamente neutra  (stesso numero di protoni ed elettroni) Perché?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#40": "41Potenziale: sistema di N cariche puntiformisistema di N cariche puntiformi\nqi⃗riP(x,y,z)V(x,y,z)=N∑i=1Vi(x,y,z)=N∑i=114πε0qiriIl potenziale in un punto P(x,y,z) è dato dalla somma algebrica dei singoli potenziali generati dalle cariche qi singolarmente ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#41": "42Potenziale: sistemi continui di cariche\nz\ny\nx\nP(x,y,z)⃗r−⃗r′\u0000⃗r⃗r′\u0000dqCarica volumetricaCarica superficialeCarica lineare Warning! come per il campo elettrico, queste sono formule generali da usare con attenzioneV(x,y,z)=14πε0∫dq|⃗r−⃗r′\u0000|V(x,y,z)=14πε0∭τρ(⃗r′\u0000)dτ|⃗r−⃗r′\u0000|V(x,y,z)=14πε0∬Sσ(⃗r′\u0000)dS|⃗r−⃗r′\u0000|V(x,y,z)=14πε0∫lλ(⃗r′\u0000)dl|⃗r−⃗r′\u0000|",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#42": "x\nExd2−d2\n43EsempiSiano date due cariche uguali q+ posizionate sull’asse y di un SdR cartesiano a distanza d dall’origine. Determinare l’espressione del campo e del potenziale elettrostatico sull’asse x.\nx\nd\n⃗E(x,0,0)=q2πε0x(x2+d2)3/2̂ı\ny",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#43": "44EsempioDeterminare il campo ed il potenziale elettrostatico generato da un filo rettilineo indefinito su cui è depositata uniformemente una carica con densità lineare 𝜆 ⃗E=λ2πε0r̂ur ortogonale al filo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#44": "z\nEz45EsempioDeterminare il campo elettrostatico sull’asse di un anello di raggio R su cui è depositata uniformemente una carica Q⃗E=Q4πε0z(z2+R2)32̂k",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#45": "46EsempioDeterminare il campo elettrostatico sull’asse di un disco di raggio R su cui è depositata uniformemente una carica Q⃗E=Qz2πε0R2[1|z|−1(z2+R2)12]̂k",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#46": "47EsempioDeterminare la differenza di potenziale elettrostatico tra due piani indefiniti paralleli, posti a distanza d, su cui è depositata uniformemente una densità superficiale di carica uguale ed opposta\n  +𝜎-𝜎d",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#47": "48Esempio1.7Una sottile barra di plastica ha una densit` a lineare di carica positiva\u0000uniforme. La barra ` e curvata a formadi semicerchio di raggioR. Determinare:a) il potenziale elettrostatico nel centroOdel semicerchio (V=\u00004✏0);b) il campo elettrostatico nel puntoO(~E=\u0000\u00002⇡✏0Rˆ|)1.8Calcolare il campo elettrostatico nei punti dell’assexdi un anello di raggioRuniformemente carico concaricaQ. Descrivere il moto di una caricaq(opposta aQ) e massamche si trova inizialmente ferma inun punto dell’asse vicino al centro dell’anello. (R:~E=Q4⇡✏0x(x2+R2)32ˆı, oscillatore armonico con pulsazione!=qqQ4⇡✏0mR3)1.9Calcolare il campo elettrico lungo l’assexdi un disco di raggioRcaricato uniformemente con densit` a dicarica\u0000.( R :~E=\u0000x2✏0[1|x|\u0000(x2+R2)\u000012])1.10Sia dato un guscio sferico di raggioRe di spessore trascurabile su cui ` e distribuita uniformemente una caricatotaleQ. Calcolare il campo elettrostatico ed il potenziale in tutto lo spazio, in funzione della distanzardalcentro del sistema.1.11Sia data una sfera di raggioRin cui ` e distribuita uniformemente una carica totaleQ. Calcolare il campoelettrostatico ed il potenziale in tutto lo spazio, in funzione della distanzardal centro della sfera.1.12Sia data una sfera di raggioRnel cui volume presente una carica distribuita con densit` a di volume⇢(r)=kr,conkcostante. Calcolare:a) la carica totaleQdella sfera (R:Q=⇡kR4);b) il campo elettrostatico in tutto lo spazio, in funzione della distanzardal centro della sfera (R:~E(r<R)=kr24\"0ˆr,~E(r>R)=kR44\"0r2ˆr);c) il potenziale elettrostatico in un generico punto interno della sfera a distanzardal centro (R:V(r)=k12\"0(4R3\u0000r3)).1.13Sia data un cilindro di raggioRe altezza indeﬁnita in cui ` e distribuita uniformemente una carica con densit` avolumetrica di carica⇢. Calcolare:a) il campo elettrostatico in tutto lo spazio, in funzione della distanzardall’asse del cilindro;b) il potenziale elettrostatico in tutto lo spazio, imponendo che il potenziale sia nullo sulla superﬁcie delcilindro.1.14Si consideri un volume sferico di raggioRin cui ` e presente una caricaQdistribuita con densit` a⇢(r)dipendente dalla distanza radialerdal centro della sfera. Sapendo che il campo elettrico all’interno dellasfera ha modulo costante ed ` e diretto radialmente, determinare l’espressione della densit` a di carica⇢(r). (R:⇢=Q2⇡R2r)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#48": "49Il lavoro della forza elettrostaticaSe il campo elettrostatico è conservativo ⇒ la forza elettrostatica è conservativaℒel=∫BA⃗Fel⋅d⃗l=∫BAq⃗E⋅d⃗l=q∫BA⃗E⋅d⃗l=q(VA−VB)=qΔVAB⃗Fel=q⃗EIl lavoro fatto dalla forza elettrostatica per spostare una carica q dalla posizione A alla posizione BLa forza elettrostatica (di Coulomb) è proporzionale al campo elettrico\nDato che la forza elettrostatica è conservativa, il lavoro non dipende dal percorso",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#49": "50L’energia elettrostaticaPossiamo definire l’energia potenziale elettrostatica di una carica situata in un punto dello spazio in cui è presente un potenziale VUE=qVLa variazione di energia potenziale corrisponde alla variazione di energia cinetica Tin+Uin=Tfin+Ufin⇒ΔT=−ΔUEU si misura in Joule (J) In Fisica delle Particelle si usa l’elettronvolt: (energia cinetica di una carica elementare accelerata da un V olt) 1eV = qe𝛥V =(1.6×10-19 C)×(1V)=1.6×10-19 J Forza elettrostatica conservativa ⇒ energia meccanica totale (cinetica+potenziale) si conserva        ℰ=T+UE",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#5": "6Lo strofinio produce uno spostamento di elettroni da un materiale all’altro.  Sfregando tra di loro i due materiali: •il più alto nella lista si carica ⊕ (cede elettroni) •il più basso nella lista si carica ⊝ (acquista elettroni)Serie triboelettricacuoio vetro capelli lana seta alluminio carta legno ambra gomma argento oro plastica PVC silicone teflonperché sono gli elettroni a spostarsi e non i protoni?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#50": "51Moto di una carica in un campo elettrostatico Un oggetto di massa m e carica q posto in un campo elettrostatico è soggetto alla forza elettrostatica⃗F=q⃗E=m⃗a⃗a=d2⃗rdt2=qm⃗ENote le condizioni iniziali possiamo determinare le equazioni del moto Dato che il campo elettrostatico è conservativo, si può utilizzare anche la conservazione dell’energiaΔT=−ΔUe",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#51": "52Esempio: moto carica in campo costante\n+𝜎Campo piano indefinitoDeterminare la velocità di una particella di massa m e carica q(+), inizialmente ferma (vA=0) su un piano uniformemente carico positivamente, dopo che ha percorso una distanza D\nABa=qmE=qmσ2ε0costante, moto uniformemente acceleratovB=vA+atxB=xA+vAt+12at2v2B−v2A=2a(xB−xA)vB=qmσε0DTB−TA=UA−UBTB=12mv2BTA=0UA−UB=q(VA−VB)=q∫BA⃗E⋅d⃗l==q∫BAσ2ε0dx=qσ2ε0(xB−xA)=qσ2ε0D12mv2B=qσ2ε0Dv2B=2qmσ2ε0DDin alternativa (conservazione energia)⃗E=σ2ε0̂ıx",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#52": "53Esempio: deflessione in doppio strato-𝜎+𝜎\nxy\nm, q+⃗v0=v0x̂ı⃗E=σε0̂𝚥\n𝛼Calcolare l’angolo di deflessione di una particella di massa m e carica q che attraversa con velocità iniziale v0x un doppio strato di lunghezza LL",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#53": "54EsempioTre particelle identiche, aventi massa m e carica q,  sono poste ai vertici di un triangolo equilatero di lato L. Inizialmente le cariche sono ferme. Ad un certo istante una delle tre cariche viene lasciata libera. Determinare la velocità che la carica acquista dopo aver percorso una distanza L (risolvere per m=2 kg, q=5𝜇C, L=3m)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#54": "55Integrale di superficie una funzione vettorialeSiano date in ℝ3 una funzione vettoriale e una superficie S ⃗F(x,y,z)\nnel limite N→∞ e 𝛥Si→0 , definiamo l’integrale di superficie di una funzione vettoriale\nS𝛥SîniPi⃗F(Pi)αîniSuddividiamo S in N superfici infinitesime 𝛥Si e consideriamo su di esse i punti Pi(xi,yi,zi), i corrispondenti versori      normali a 𝛥Si  N∑i=1⃗F(Pi)⋅̂niΔSi=N∑i=1F(Pi)cosαiΔSi∬S⃗F⋅̂ndSconsideriamo  la somma ⃗F(x,y,z)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#55": "56Flusso di un campo vettorialeIl concetto di flusso viene introdotto nello studio della dinamica dei fluidi Consideriamo un tubo (di sezione infinitesima) attraversato da un fluido incompressibile (es H20) con velocità     d⃗S=̂ndS⃗vIl flusso del fluido attraverso una sezione dS del tubo è definito:dΦ=⃗v⋅̂ndSNotazione alternativa:dΦ=⃗v⋅d⃗Srisolvendo il prodotto scalare:\n̂n⃗vαdSd𝛴sezione trasversa d𝛴=dS cos𝛼dΦ=vdScosα=vdΣ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#56": "57Flusso di un campo vettorialeConsiderando più tubi (di flusso) su una generica superficie S \nFlusso di un Campo Vettoriale (III) •!La quantità di fluido che ha attraversato nel tempo !t la sezione && del tubo è pari al volume di un cilindro avente la stessa base del tubo e un’altezza pari a v &t, cioè: •!Il flusso del fluido sarà pertanto: \n37!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!V=\"v!t!\"!v()=#V#t=\"v#t#t=\"v!!!v!vt=t0t=t0+!tv!tv!t!v!v!v!v\nFlusso di un Campo Vettoriale (IV) •!Possiamo anche esprimere il flusso ''  (v) utilizzando una sezione obliqua S invece che una sezione trasversale &. •!Si ha: \n38!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!=Scos\"!viˆn=!vˆn1\"cos\"=vcos\"#S!v()=!v=Svcos\"=!viˆnS!ˆn!Sr!v\nFlusso di un Campo Vettoriale (V) •!Consideriamo ora il caso in cui la velocità del fluido non è uniforme sulla sezione del tubo. –!È il caso, per esempio, di un flusso laminare di un fluido viscoso, per il quale la velocità al centro del tubo è maggiore della velocità in prossimità delle pareti. •!In tal caso scomponiamo il tubo in tanti tubicini di sezione trasversale infinitesima              . Il flusso attraverso una qualunque sezione di un tubicino infinitesimo vale: •!Il flusso totale si ottiene sommando il flusso attraverso un insieme di tubicini che coprono completamente la sezione del tubo: \n39!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v()=!viˆndSS\"\"d!=dScos\"d!!SdSd!!vd#dS!v()=vd!=vdScos\"=!viˆndS(volume di fluido che attraversa nell’unità di tempo la superficie S) \nFlusso di un Campo Vettoriale (VI) •!La superficie S potrebbe anche non essere piana, ma l’espressione: è ugualmente valida, in quanto le superfici infinitesime dS possono essere considerate piane e il prodotto scalare        tiene conto della loro inclinazione rispetto alla velocità. \n40!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v(volume di fluido che attraversa nell’unità di tempo la superficie S) \ndSdS!viˆnd!!S!v()=!viˆndSS\"\"\nFlusso di un Campo Vettoriale (III) •!La quantità di fluido che ha attraversato nel tempo !t la sezione && del tubo è pari al volume di un cilindro avente la stessa base del tubo e un’altezza pari a v &t, cioè: •!Il flusso del fluido sarà pertanto: \n37!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!V=\"v!t!\"!v()=#V#t=\"v#t#t=\"v!!!v!vt=t0t=t0+!tv!tv!t!v!v!v!v\nFlusso di un Campo Vettoriale (IV) •!Possiamo anche esprimere il flusso ''  (v) utilizzando una sezione obliqua S invece che una sezione trasversale &. •!Si ha: \n38!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!=Scos\"!viˆn=!vˆn1\"cos\"=vcos\"#S!v()=!v=Svcos\"=!viˆnS!ˆn!Sr!v\nFlusso di un Campo Vettoriale (V) •!Consideriamo ora il caso in cui la velocità del fluido non è uniforme sulla sezione del tubo. –!È il caso, per esempio, di un flusso laminare di un fluido viscoso, per il quale la velocità al centro del tubo è maggiore della velocità in prossimità delle pareti. •!In tal caso scomponiamo il tubo in tanti tubicini di sezione trasversale infinitesima              . Il flusso attraverso una qualunque sezione di un tubicino infinitesimo vale: •!Il flusso totale si ottiene sommando il flusso attraverso un insieme di tubicini che coprono completamente la sezione del tubo: \n39!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v()=!viˆndSS\"\"d!=dScos\"d!!SdSd!!vd#dS!v()=vd!=vdScos\"=!viˆndS(volume di fluido che attraversa nell’unità di tempo la superficie S) \nFlusso di un Campo Vettoriale (VI) •!La superficie S potrebbe anche non essere piana, ma l’espressione: è ugualmente valida, in quanto le superfici infinitesime dS possono essere considerate piane e il prodotto scalare        tiene conto della loro inclinazione rispetto alla velocità. \n40!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v(volume di fluido che attraversa nell’unità di tempo la superficie S) \ndSdS!viˆnd!!S!v()=!viˆndSS\"\"\nFlusso del campo vettoriale     attraverso la superficie S⃗vΦs(⃗v)=∬S⃗v⋅̂ndS",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#57": "58Linee di flusso di un campo vettoriale\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆnConsideriamo la traiettoria 𝛾 di una particella del fluido  ➡  in ogni punto la traiettoria è tangente alla velocità vettoriale della particella La linea di flusso 𝛾 è una linea sempre tangente al vettore velocità delle particelle che si trovano nei punti della linea",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#58": "59Linee di flusso del campo elettricoAnche per il campo elettrico possiamo rappresentare graficamente le linee di flusso (o linee di campo) • tangenti in ogni punto al vettore campo elettrico • orientate con il verso del campo elettrico • in numero (per unità di superficie trasversale), proporzionali al modulo del campo elettrico \nTeorema della Divergenza (II) •!Per comprendere il significato del Teorema della Divergenza: immaginiamo di suddividere il volume V in tanti cubetti infinitesimi, di volume: •!Per ogni cubetto si ha, per quanto abbiamo visto: per cui si ha, per ogni cubetto (che ha 6 facce): !viˆndSS\"!!=!\"i!vdVV!!!\n57!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!V1,!V2,!V3,…!!i!v()P()=limV\"P{}!vSV()\"##iˆndSdVV###=$tot!v()%V!!i!v()Pi()\"Vi=#toti()!v()=#ki()!v()k=16$!V1!V2!V3!V4!V5!V6!V8!V7!V10!V9V\nTeorema della Divergenza (III) •!Distinguiamo ora, tra le facce dei cubetti, le facce interne e le facce esterne: –!Le facce interne separano un cubetto da un cubetto adiacente; –!Le facce esterne fanno parte della frontiera del volume totale V. •!Sommando le divergenze dei cubetti, i contributi dei flussi delle facce interne si cancellano tra loro: –!Il flusso uscente dal cubetto i verso il cubetto j adiacente è opposto al flusso dal cubetto j al cubetto i. •!Si ha pertanto: \n58!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!i!v()Pi()\"Vii=1N#=$ki()!v()k=16#i=1N#=$ki()!v()facceesterne#=!viˆn\"Sfacceesterne#!!i!vdVV%%%=!viˆndSS\"%%!V1!V2!V3!V4!V5!V6!V8!V7!V10!V9V\nLinee di Flusso del Campo Elettrico •!Come tutti i campi vettoriali, anche il campo elettrico si può rappresentare graficamente con le linee di flusso (o linee di campo), ovvero con linee: –!Tangenti in ogni punto al vettore campo elettrico         ; –!Orientate col verso del campo elettrico         ; –!In numero, per unità di superficie trasversale, proporzionale al modulo del campo elettrico            .   !E!r()  !E!r()\n59!  !E!r()\nDomenico Galli – Fisica Generale B – 1. Elettrostatica!\nAngolo Solido •!Come è noto, l’angolo piano (in radianti) è definito come il rapporto tra un arco di circonferenza l centrata nel vertice e il raggio r: •!L’angolo solido \" si definisce in maniera analoga come il rapporto tra la parte di superficie sferica S (centrata nel vertice), intercettata dal cono centrato nel vertice e il quadrato del raggio della sfera: •!L’angolo solido si misura in steradianti (sr). lr  !=lr\"0,2#$%$%!=Sr2\"0,4#$%&'S!r\n60!Domenico Galli – Fisica Generale B – 1. Elettrostatica!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#59": "Classificazione delle superfici • aperta: compatta e con bordo • chiusa: compatta e priva di bordo • orientabile: ha due facce\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆn\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆn\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆn\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆn\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆnAperta e orientabile\nSfera: chiusa e orientabileToroide: chiusa e orientabileNastro di Möbius: aperta e non orientabileBottiglia  di Klein: chiusa e  non-orientabile60Superfici in ℝ3",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#6": "7Isolanti e conduttoriisolanti  •la carica elettrica resta localizzata  •vetro, plastica, gomma conduttori  •cariche libere di muoversi •metalli warning: classificazione un po’ riduttiva (liquidi, semiconduttori,…)                  nota: inizieremo con esempi di materiali isolanti, i conduttori saranno trattati in seguito",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#60": "61Superfici chiuse e orientabili in ℝ3Nelle superfici chiuse e orientabili, in ogni punto della superficie  possiamo distinguere la normale esterna dalla normale interna Convenzione:  per calcolare i flussi si utilizza la normale esterna   :  positivo il flusso uscente dal volume delimitato dalla superficie chiusa negativo il flusso entrante nel volume delimitato dalla superficie chiusân\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆn̂n̂n̂n\nLinee di Flusso di un Campo Vettoriale •!Consideriamo ora la traiettoria ! di una particella di fluido: –!Essa è in ogni suo punto tangente alla velocità vettoriale della particella. •!Definiamo quindi linea di flusso ! una linea che è sempre tangente al vettore velocità delle particelle di fluido che si trovano nei punti della linea. \n41!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u0001\u0001\nLinee di Flusso di un Campo Vettoriale (II) •!Possiamo tracciare le linee di flusso tanto più fitte quanto maggiore è la velocità del fluido. •!Più precisamente possiamo tracciare le linee in modo che il numero di linee di flusso che attraversa l’unità di superficie di una sezione trasversale sia proporzionale alla velocità del fluido. \n42!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!v1!v1!v2!v2>!v1\nSuperfici Chiuse e Orientabili di R3 •!Una superficie è chiusa se è compatta e priva di bordo. •!Una superficie è orientabile se ha due facce; è non-orientabile se ha una faccia sola. \n43!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nAperta e Orientabile Chiusa e Orientabile (sfera) Chiusa e Orientabile (toro)    Aperta e Non-orientabile (nastro di Möbius) Chiusa e Non-orientabile (bottiglia di Klein) \nSuperfici Chiuse e Orientabili di R3 (II) •!Nelle superfici chiuse e orientabili si può distinguere la normale esterna dalla normale interna in ogni punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza la normale esterna   : –!Questo equivale a considerare positivo il flusso uscente dal volume delimitato dalla superficie chiusa e negativo il flusso entrante nel volume delimitato dalla superficie chiusa. \n44!Domenico Galli – Fisica Generale B – 1. Elettrostatica!\nˆnˆnˆnˆnˆnˆnˆn̂n̂n̂n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#61": "Superfici Aperte di R3 •!Nelle superfici aperte non si può distinguere la normale esterna dalla normale interna in un punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza l’orientamento     indicato dalla regola della mano destra sulla base dell’orientamento della linea di bordo: \n45!Domenico Galli – Fisica Generale B – 1. Elettrostatica!ˆn\nFlusso di un Campo Vettoriale attraverso una Superficie Chiusa e Orientabile •!Consideriamo ora il flusso della velocità di un fluido attraverso una superficie chiusa. •!Per semplicità consideriamo la superficie totale di un cubo. •!Consideriamo positivo il flusso uscente dal cubo e negativo il flusso entrante nel cubo. •!Se il fluido è incompressibile e non si sono al suo interno sorgenti (in cui si produce fluido) o pozzi (scarichi, in cui il fluido scompare), allora tanto fluido entra nel cubo quanto ne esce: –!Il flusso attraverso la superficie totale è nullo. –!Il cerchietto attorno al simbolo di integrale       indica che la superficie di integrazione è chiusa. 46!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!tot!v()=!viˆndSStot\"\"\"==!1!v()+!2!v()+!3!v()+!4!v()+!5!v()+!6!v()=0532146!!!\nFlusso di un Campo Vettoriale attraverso una Superficie Chiusa e Orientabile (II) •!Se il flusso attraverso la superficie totale è positivo: allora dentro il cubo è presente una sorgente che produce fluido. •!Se il flusso attraverso la superficie totale è negativo: allora dentro il cubo è presente un pozzo (cioè uno scarico) in cui il fluido scompare. 47!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!tot!v()=!viˆndSStot\"\"\"==!1!v()+!2!v()+!3!v()+!4!v()+!5!v()+!6!v()>0532146\n!tot!v()=!viˆndSStot\"\"\"==!1!v()+!2!v()+!3!v()+!4!v()+!5!v()+!6!v()<0532146\nL’Operatore Divergenza \n48!Domenico Galli – Fisica Generale B – 1. Elettrostatica!•!Consideriamo una funzione vettoriale della posizione P: •!Si definisce l’operatore “divergenza” come: •!L’operatore divergenza si applica a una funzione vettoriale; il risultato è uno scalare: !!=ˆı\"\"x+ˆ!\"\"y+ˆk\"\"z!v=!vP()=!vx,y,z()=vxx,y,z()ˆı+vyx,y,z()ˆ!+vzx,y,z()ˆk\nEsempio:\"vx,y,z()=x2+y2()ˆı+x2+z2()ˆ!+zˆk!V\"\"i\"v()x,y,z()=2x+1!!!!i!v=div!v=\"vx\"x+\"vy\"y+\"vz\"z!!i!v=ˆı\"\"x+ˆ!\"\"y+ˆk\"\"z#$%&'(ivxˆı+vyˆ!+vzˆk()P!!3()\"v\"#\"\"vP()!VP!!3()\"$i\"v\"#\"\"\"$i\"v()P()!!%&'('62Superfici aperte e orientabili in ℝ3Nelle superfici aperte non possiamo distinguere la normale esterna dalla normale interna Convenzione:  per calcolare i flussi attraverso una superficie aperta si utilizza l’orientamento    indicato dalla regola della mano destra sulla base dell’orientamento della linea del bordo:̂n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#62": "63L’angolo solidorl𝛼Angolo piano rapporto tra arco di circonferenza l e raggio r\nr𝛺𝛴Angolo solido rapporto tra la parte di superficie sferica 𝛴 intercettata dal cono centrato nel vertice e il quadrato del raggio della sfera α=lr∈[0,2π[L’angolo solido si misura in steradianti (sr)Ω=Σr2∈[0,4π]dα=dlrinfinitesimoinfinitesimodΩ=dΣr2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#63": "64Il flusso del campo elettricod𝛴̂nαdS⃗Eprendendo la superficie d𝛴 ortogonale al campo elettrico:Flusso infinitesimo del campo elettrico attraverso una superficie dSdΦS(⃗E)=⃗E⋅̂ndS=EdScosα=EdΣFlusso del campo elettrico attraverso una superficie estesa SΦS(⃗E)=∬S⃗E⋅̂ndS=∬SEcosαdS=∬SEdΣdΣ=dScosα",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#64": "65Il flusso del campo elettricoConsideriamo ora una superficie chiusa contenente al suo interno una carica elettrica puntiforme q̂n\nd𝛴dSqdΦS(⃗E)=⃗E⋅̂ndS=EdScosα=EdΣFlusso attraverso l’intera superficie S  (             ):∬SdΩ=4πil flusso dipende solo dall’angolo solido perché E è radialeΦS(⃗E)=∬S⃗E⋅̂ndS=qε0Notazione per integrale su superficie chiusaFlusso infinitesimo attraverso un elemento dS:α\nΦS(⃗E)=∬SdΦS=∬S⃗E⋅̂ndS=q4πε0∬SdΩ=qε0=(q4πε0r2)(r2dΩ)=q4πε0dΩ⃗E",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#65": "66Il flusso del campo elettrico\nd𝛴2d𝛴1\nr1r2dΩ=dΣ1r21=dΣ2r22Se q è esterna alla superficie chiusa, il numero di linee di campo che entrano nella superficie è uguale al numero di linee di campo che escono dalla superficie\nd𝛴2dS2q\nd𝛴1dS1̂n1̂n2α1α2⃗E1⋅̂n1=cosα1<0⃗E2⋅̂n2=cosα2>0\ndΦS1(⃗E1)+dΦS2(⃗E2)==⃗E1⋅̂n1dS1+⃗E2⋅̂n2dS2=E1dS1cosα1+E2dS2cosα2=E1(−dΣ1)+E2dΣ2=q4πε0r21(−r21dΩ)+q4πε0r22(−r22dΩ)=0dΦS1(⃗E1)+dΦS2(⃗E2)=⃗E1⋅̂n1dS1+⃗E2⋅̂n2dS2\ndΦS1(⃗E1)+dΦS2(⃗E2)==⃗E1⋅̂n1dS1+⃗E2⋅̂n2dS2=E1dS1cosα1+E2dS2cosα2=E1(−dΣ1)+E2dΣ2=q4πε0r21(−r21dΩ)+q4πε0r22(r22dΩ)=0dΦS1(⃗E1)+dΦS2(⃗E2)==⃗E1⋅̂n1dS1+⃗E2⋅̂n2dS2=E1dS1cosα1+E2dS2cosα2=E1(−dΣ1)+E2dΣ2=q4πε0r21(−r21dΩ)+q4πε0r22(r22dΩ)=0dΦS1(⃗E1)+dΦS2(⃗E2)==⃗E1⋅̂n1dS1+⃗E2⋅̂n2dS2=E1dS1cosα1+E2dS2cosα2=E1(−dΣ1)+E2dΣ2=q4πε0r21(−r21dΩ)+q4πε0r22(r22dΩ)=0Flusso attraverso l’intera superficie:Flusso infinitesimo:=E1dS1cosα1+E2dS2cosα2=E1(−dΣ1)+E2dΣ2=q4πε0r21(−r21dΩ)+q4πε0r22(r22dΩ)=0ΦS(⃗E)=∬S⃗E⋅̂ndS=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#66": "67Il flusso del campo elettricoSe all’interno della superficie chiusa ci sono N cariche qi puntiformi, per il principio di sovrapposizione del campo elettrico, il flusso vale: dove QS è la carica contenuta all’interno della superficie S q3q2q5q7q6q1q8q4qN×××S𝜏(S)QS=∑iqintQS=∭τ(S)ρdτΦS(⃗E)=∬S⃗E⋅̂ndS=QSε0Cariche discrete Cariche distribuite su continuo V olume 𝜏(S) contenuto in superficie S ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#67": "68La Legge di Gauss del campo elettricoIl flusso del campo elettrico attraverso una superficie chiusa S è uguale al rapporto tra la carica elettrica QS contenuta all’interno della superficie e la costante dielettrica ΦS(⃗E)=∬S⃗E⋅̂ndS=QSε0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#68": "69EsempiDeterminare il campo elettrico generato da un filo rettilineo indefinito su cui è depositata uniformemente una carica con densità lineare 𝜆 (usando la legge di Gauss) ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#69": "70EsempiDeterminare il campo elettrico generato da un piano indefinito su cui è depositata uniformemente una carica con densità superficiale 𝜎 (usando la legge di Gauss)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#7": "8Elettrizzazione per induzione (elettrostatica)\nElettroscopio a foglieAvvicinando un corpo carico all’elettroscopio, le foglie metalliche (conduttori) si allontanano Le componenti metalliche “sentono\" la vicinanza di carica elettrica L’effetto svanisce quando si allontana la carica",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#70": "71EsempiDeterminare il campo elettrico ed il potenziale generato da un guscio sferico di raggio R su cui è depositata uniformemente una carica Q",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#71": "72EsempioSia data una sfera di raggio R contenente una carica Q distribuita uniformemente. a) Determinare il campo elettrostatico in tutto lo spazio b) Calcolare il potenziale in un generico punto  esterno alla sfera (assumendo nullo il potenziale all’infinito) c) Calcolare il potenziale in un generico punto  interno alla sfera (assumendo nullo il potenziale all’infinito) d) Calcolare la differenza di potenziale tra il centro e la superficie della sfera",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#72": "73La divergenza di un campo vettoriale\nConsideriamo il flusso di un campo  vettoriale     attraverso una superficie chiusa S che delimita un volume 𝜏⃗FΦS(⃗F)=∬S⃗F⋅̂ndSDividiamo idealmente il volume 𝜏  in due volumi 𝜏1 e 𝜏2, usando una superficie di separazione D (diaframma). Siano S1 e S2 le superfici chiuse che delimitano 𝜏1 e 𝜏2 (D⊂S1,S2)𝜏SD𝜏2𝜏1S1S2Possiamo riscrivere il flussoΦS(⃗F)=∬S1⃗F⋅̂ndS1+∬S2⃗F⋅̂ndS2̂n2̂n1i contributi al flusso attraverso D si annullano⃗F⋅̂n1D=−⃗F⋅̂n2D",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#73": "74La divergenza di un campo vettorialeSuddividendo il volume 𝜏 in N volumi 𝜏i , limitatati da altrettante superfici SiDefinizione di divergenza di un campo vettorialediv⃗F=limτi→0ΦSi(⃗F)τiΦSi(⃗F)=∬Si⃗F⋅̂nidSiΦS(⃗F)=N∑i=1ΦSi(⃗F)La divergenza è il flusso uscente per unità di volume  • è una grandezza scalare, funzione delle coordinate • può variare da punto a punto𝜏iSi⃗FS𝜏⃗∇=(∂∂x,∂∂y,∂∂z)=∂∂x̂ı+∂∂ŷ𝚥+∂∂ẑkdiv⃗F=⃗∇⋅⃗FUtilizzando l’operatore “nabla”:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#74": "75Il teorema della divergenzaIl flusso di un campo vettoriale attraverso una superficie S chiusa è pari all’integrale sul volume 𝜏 (delimitato da S !!!) della divergenza di tale campo vettoriale ∬S⃗F⋅̂ndS=∭τdiv⃗Fdτ=N∑i=1τi∬Si⃗F⋅̂nidSiτiNel limite N →∞ e 𝜏i →d𝜏, sostituiamo 𝛴→∫∫∫ ⟶∭τdiv⃗FdτΦS(⃗F)=∬S⃗F⋅̂ndS=N∑i=1∬Si⃗F⋅̂nidSiIl teorema della divergenza è una relazione tra un integrale di superficie e un integrale di volume",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#75": "76La legge di Gauss in forma localeCombiniamo il teorema della divergenza e la legge di Gauss, in presenza di una distribuzione continua di carica∭τ(S)div⃗Edτ=∭τ(S)ρε0dτdiv⃗E=ρε0∬S⃗E⋅̂ndS=∭τ(S)div⃗Edτ∬S⃗E⋅̂ndS=∭τ(S)ρε0dτLegge di GaussTeorema della divergenzaGli integrali sono sullo stesso volume",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#76": "77Significato fisico della divergenzaLa divergenza di un campo ci da un’informazione sul comportamento locale delle linee di campo le linee di campo si incontrano nei punti in cui la divergenza del campo è diversa da zero: • convergono nel punto se il valore della divergenza è negativo • divergono dal punto se il valore della divergenza è positivo In un punto in cui la divergenza è nulla, le linee di campo non si incontrano Se un campo ha divergenza sempre nulla, allora esso si definisce solenoidale",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#77": "78Significato fisico della divergenzadiv⃗E=ρε0Il campo elettrico ha divergenza non nulla solo nei punti in cui esiste una densità di carica Nel vuoto, la divergenza del campo elettrico è nulla \nz\ny\nx⃗r⃗r′\u0000In tali punti, le linee di campo si incontranodiv⃗E(⃗r′\u0000)=ρ(⃗r′\u0000)ε0div⃗E(⃗r)=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#78": "79Potenziale e legge di GaussLegge di Gauss in forma locale⃗∇⋅⃗E=ρε0⃗E=−⃗∇V⃗∇⋅(−⃗∇V)=ρε0∇2V=−ρε0Campo elettrostaticoEquazione di Poisson\nIl laplaciano del potenziale è proporzionale alla densità di carica Equazione alle derivate seconde, note le condizioni al contorno ammette un’unica soluzione∂2V∂x2+∂2V∂y2+∂2V∂z2=−ρε0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#79": "80EsempioCalcolare la divergenza del campo e il flusso attraverso una superficie sferica di raggio R centrata nell’origine ⃗F=k⃗r",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#8": "9Elettrizzazione per induzione (elettrostatica)\nElettrizzazione per induzione anche su materiali isolanti\nMicroscopicamente, le molecole della carta “risentono” la vicinanza di cariche elettriche ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#80": "81Lorenzo Rinaldi Dipartimento di Fisica e Astronomia lorenzo.rinaldi@unibo.it https://www.unibo.it/sitoweb/lorenzo.rinaldi/\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\09-elettrostatica.pdf#9": "10Elettrizzazione per contatto\nIn caso di contatto, parte della carica si trasferisce (e resta) sul conduttoreCaso particolare: due conduttori di stessa forma e dimensione; inizialmente A ha una carica Q+Dopo aver messo in contatto A e B, la carica si ridistribuisce in parti ugualiAB++++++++++++++++Q+\nAB++++++++12Q+12Q+++++++++\nCarica totale si conserva!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#0": " Elettrostatica dei conduttori CdS Ingegneria Informatica A.A. 2019/20",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#1": "2Materiali isolanti e conduttoriisolanti  •le carica elettrica restano localizzate, sono vincolate a muoversi all’interno delle molecole •Un campo elettrico esterno non produce movimento di cariche, se non su piccolissima scala: deformazione e orientamento delle molecole (azioni sui dipoli) conduttori  •cariche (elettroni di conduzione) libere di muoversi sul conduttore (moto su reticolo cristallino) •comportamento degli elettroni simile ad un gas •in presenza di un campo esterno o di un eccesso di carica, le cariche si redistribuiscono sul conduttore",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#10": "11Campo elettrico in prossimità della superficie dei conduttori\nIn presenza di un conduttore, le linee di campo esterne vengono deviate dalla presenza di addensamenti locali di carica sulla superficie del conduttoreVicino al conduttore le linee di campo esterne saranno sempre perpendicolari alla superficie",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#11": "12Campo elettrico in prossimità della superficie dei conduttori⃗E=ÊnCalcoliamo il flusso attraverso un cilindretto di dimensioni infinitesime • asse ortogonale a superficie conduttore • contributo al flusso solo da base esterna⃗E=0dΦ(E)=⃗E⋅̂ndS=EdSFlusso attraverso base infinitesimaCarica contenuta nel cilindro (intersezione con la superficie del conduttore)dQS=σdSapplicando la legge di GaussEdS=σε0dSE=σε0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#12": "13Teorema di Coulomb⃗E=σε0̂nIl campo elettrostatico in prossimità dei conduttori è sempre ortogonale alla superficie del conduttore ed il modulo è proporzionale alla densità superficiale di carica La densità superficiale di carica 𝜎=𝜎(x,y,z) può variare sulla superficie, di conseguenza varierà anche l’intensità del campo elettrico Il campo elettrico subisce una discontinuità nel passaggio dall’esterno all’interno del conduttore",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#13": "14Conduttori caviSulla superficie interna di un conduttore cavo  • la carica totale è nulla  • non si osservano cariche localizzate \nS⃗E=0ΦS(⃗E)=QSε0=0La prima affermazione si dimostra applicando la legge di Gauss, utilizzando la condizione che il campo elettrico interno al conduttore è nullo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#14": "15Conduttori cavi\n-+++++----⃗E≠0⃗E=0𝛤Ipotesi (per assurdo): distribuzioni locali di carica sulla superficie interna⇒ campo all’interno della cavità non nullo ⇒ circuitazione lungo linea chiusa 𝛤 non-nulla⇒ violazione della conservatività del campo elettrostatico Sulla superficie interna di un conduttore cavo  • la carica totale è nulla  • non si osservano cariche localizzate \n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#15": "16Schermo elettrostatico\n  Se un conduttore dotato di cavità viene esposto a un campo elettrico esterno, il campo elettrico all’interno della cavità è comunque nullo e non vi sono cariche elettriche indotte sulla superficie della cavità stessa.    In altre parole il conduttore scherma l’interno della cavità dai campi elettrici all’esterno (gabbia di Faraday)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#16": "17Induzione completa\n+QQ’’=+Q+++\n++++\n+Q’=-Q--------⃗E≠0Poniamo una carica puntiforme all’interno della cavità di un conduttore neutroLa carica genera un campo con linee radiali che poi curvano per diventare perpendicolari alla superficie interna induzione completa: tutte le linee di forza si chiudono sul conduttore Sulla superficie interna si induce una carica Q’ complessivamente uguale e opposta a +QSGauss è salvo: Q+Q’=0Conduttore neutro ⇒ carica Q’’=+Q indotta sulla superficie esterna⃗E=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#17": "18Induzione completa\n+QQ’’=+QQ’=-Q+++++++++++\n++++\n+--------⃗E≠0Poniamo all’interno della cavità una generica carica (anche su un conduttore)  Un conduttore cavo trasferisce sulla propria superficie esterna una carica uguale al valore complessivo delle cariche contenute all’interno della cavità.  ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#18": "BB\n19Potenziale elettrostatico nei conduttoriADifferenza di potenziale tra due punti del conduttoreVA−VB=∫BA⃗E⋅d⃗l=0⃗E=0A⃗E⊥d⃗l⇒VA=VB∀A,B\nTutti i punti del conduttore sono equipotenziali  (la differenza di potenziale tra due qualsiasi punti del conduttore è sempre nulla)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#19": "20Potenziale di un conduttore sfericoUn conduttore carico (con carica Q), di forma sferica di raggio R è equivalente ad un guscio sferico uniformemente caricoCampo elettrico (calcolato con legge di Gauss):⃗E(r<R)=0⃗E(r>R)=14πε0Qr2̂urPer simmetria, la densità superficiale di carica deve essere uniforme (altrimenti avrei campi elettrici tangenti)\nRQ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#2": "3Premesse~F=m~a•Scegliamo un sistema di riferimento inerziale: -un corpo non soggetto a forze si muove con velocità costante -  •Supponiamo che ci sia il vuoto nello spazio interposto tra i conduttori e le eventuali cariche esterne •Lavoriamo con conduttori solidi (es. metalli) •Poniamoci in condizioni di ELETTROSTATICA",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#20": "21Potenziale di un conduttore sfericoCalcolo del potenziale in un generico punto a distanza r dal centro della sfera (assumendo V∞=0)V(r)=V(r)−V(∞)=∫∞r⃗E⋅d⃗rEsternamente (come carica puntiforme)V(r>R)=∫∞r⃗E⋅d⃗r=∫∞r14πε0Qr2dr=Q4πε0[−1r]∞r=14πε0QrV(r<R)=∫∞r⃗E⋅d⃗r=∫Rr⃗E(r<R)⋅d⃗l+∫∞R⃗E(r≥R)⋅d⃗l=InternamenteE(r)rRV(r)rRDiscontinuità del campo⃗E(r>R)=14πε0Qr2̂urCostante!=0+∫∞RQ4πε01r2dr=Q4πε0[−1r]∞R=Q4πε01R",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#21": "22Esempio1.15Calcolare l’energia elettrostatica di una sfera di raggioRin cui ` e distribuita uniformemente una carica condensit` a⇢costante. (R:4⇡⇢2R515\"0)1.16In una certa regione di spazio sono presenti i due campi vettoriali~E1=K1xˆı+K2y2ˆ|+K1zˆke~E2=K2xyˆı+K2x2ˆ|. Determinarea) il gradiente della grandezza~E1·~E2(R: (2K1K2+2K21)xyˆı+(K1K2x2+1K22X2y)ˆ|);b) quale dei due campi pu essere considerato elettrostatico (R:~E1);1.17Si consideri il campo~F(x, y, z)=\u00002xˆı\u0000z2ˆ|\u0000ayzˆk. Determinare:a) per quali valori diail campo risulta conservativo (R:a=2);b) il potenziale'generato dal campo~F(R:'=x2+yz2);c) la densit` a di carica⇢che genera il campo~F(R:⇢=\u00002✏0(y+ 1))1.18Si consideri il campo~F(x, y, z)=2xˆı\u0000zˆ|\u0000ayˆk. Determinare:a) per quali valori diail campo risulta conservativo (R:a=1);b) il potenziale'generato dal campo~F(R:'=yz\u0000x2);c) la densit` a di carica⇢che genera il campo~F(R:⇢=\u00002✏0)1.19Sia dato il campo~E(x, y, z)=↵(4xˆı+zˆ|+yˆk).a) Veriﬁcare che~E` e conservativo; (R: veriﬁcare che~r⇥~E= 0)b) calcolare il ﬂusso di~Eattraverso un cubo di spigoloLcon un vertice nell’origine del sistema diriferimento e tre spigoli posizionati sui tre semiassi positivi; (R:\u0000=4↵L3)c) calcolare la carica totale contenuta nel cubo, utilizzando il teorema di Gauss sia in forma integrale chedi↵erenziale (R:Q=4↵\"0L3)2 Elettrostatica dei conduttori2.1Una sfera conduttrice di raggior1=5 cm porta una caricaQ1=+10\u00006C. Un guscio sferico di materialeconduttore, concentrico alla prima sfera, di raggio internor2=10cm e raggio esternor3=12cm ` e caricato conuna caricaQ2=10Q1. Nell’ipotesi che il sistema sia nel vuoto, calcolare:a) la densit` a di carica superﬁciale\u00002sulla superﬁcie interna del guscio sferico (R:\u00002\u0000Q14⇡r22=\u00008·10\u00006C/m);b) la di↵erenza di potenziale tra i due conduttori. (R:\u0000V=Q14⇡\"0r2\u0000r1r1r2= 15kV)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#22": "23Esempio2.2Si consideri un sistema formato da un volume sferico di raggioain cui ` e contenuta una carica +Qdistribuitauniformemente nel volume, e da un sottile guscio di materiale conduttore di raggiob(b>a), concentrico alvolume sferico, sul quale ` e depositata una carica\u0000Q. Determinare:a) l’espressione del campo elettrostatico in tutto lo spazio in funzione della distanzardal centro delsistema, disegnando un graﬁco qualitativo dell’andamento del campo; (R:~E(r<a),~E(a<r<b)v e d ies 1.12,~E(r>b) = 0)b) l’espressione del potenziale sulla superﬁcie del volume sferico (inr=a, considerando nullo il potenzialeall’inﬁnito). (R:V(a)=Q4⇡\"0(1a\u00001b))2.3Due sfere conduttrici di raggiR1=2 m eR2=3 m , caricate inizialmente ciascuna con caricaQ=1 mC, vengonocollegate da un ﬁlo conduttore sottile. Nel caso in cui le sfere siano poste a distanza tale da poter trascuraree↵etti di induzione elettrostatica, come si ridistribuisce la carica?(R:Q1=2QR1R1+R2=0.8 mC,Q2=2QR2R1+R2=1.2mC)2.4Calcolare la capacit` a di un condensatore sferico.2.5Calcolare la capacit` a di un condensatore cilindrico.2.6Dato il circuito rappresentato in ﬁgura 1 (\u0000VAB= 300V,C1=3µF,C2=2µF,C3=4µF), deter-minare la carica e la di↵erenza di potenziale di ciascun condensatore. (R:Q1=0.6mC, Q2=0.2mC, Q3=0.4mC,\u0000V1= 200V,\u0000V2=\u0000V3= 100V)ABC1C3C2ABC1C2C3\nC1C2SFigure 1:2.7Dato il circuito rappresentato in ﬁgura 2 (\u0000VAB= 12V,C1=C2=2µF,C3=5µF), determinare:a) la carica e la di↵erenza di potenziale ai capi di ogni condensatore del circuito (R:Q1=Q2=12µC, Q3= 60µC,\u0000V1=\u0000V2=6V,\u0000V3= 12V);b) l’energia accumulata su ciascun condensatore (R:U1=U2= 36µJ, U3= 360µJ).",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#23": "24Ridistribuzione delle caricheQ0\nR1Siano date due sfere conduttrici di raggi R1 e R2 con R1 > R2 Inizialmente sulla prima sfera c’è una carica Q0, la seconda sfera è scarica Le sfere sono poste a distanza tale da poter trascurare effetti di induzione elettrostatica\nSuccessivamente le sfere vengono connesse con un sottile cavo conduttore. Come si ridistribuisce la carica? \nR2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#24": "25Ridistribuzione delle cariche\nR1\nR2Dobbiamo calcolare le cariche finali sulle due sfere: Q1 e Q2Per la conservazione della carica (il sistema è isolato): Q1 + Q2 =Q0Le due sfere unite formano un unico conduttore  ⇒ equipotenziale V1 = V2 Q1Q2V1=14πε0Q1R1V2=14πε0Q2R214πε0Q1R1=14πε0Q2R2Q1R2=Q2R1Q1R1=Q2R2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#25": "26Ridistribuzione delle cariche\nR1\nR2Q1Q2Risolvendo il sistema: {Q1+Q2=Q0Q1R2=Q2R1{Q2=Q0−Q1Q1R2=(Q0−Q1)R1{Q2=Q0−Q1Q1(R1+R2)=Q0R1Q1=R1R1+R2Q0Q2=R2R1+R2Q0La carica si redistribuisce proporzionalmente al raggioCaso particolare R1 = R2 Q1=Q2=Q02",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#26": "27Potere delle punte\nR1\nR2Q1Q2Cosa succede alle densità di carica (e ai campi elettrici delle sfere?)σ1=Q14πR21σ2=Q24πR22Q1=σ14πR21Q2=σ24πR22V1 = V2  ⇒Q1R1=Q2R2σ14πε0R21R1=σ14πε0R22R2σ1R1=σ2R2La densità superficiale di carica è maggiore sulla sfera più piccolaσ1=(R2R1)σ2⟶R1>R2σ2>σ1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#27": "28Potere delle punteConsideriamo un conduttore di una forma generica, con raggio di curvatura che varia da punto a punto della superficie.⃗E=σε0̂nIn vicinanza delle punte il campo elettrico                 può essere molto intenso La densità di cariche è inversamente proporzionale al raggio di curvatura \nmaggiore addensamento di carica sulle punte\n+ + + + + + + + + + + + + + ++++++++++++++++",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#28": "29Potere delle punte\nIn vicinanza delle punte dei conduttori le densità di carica elettrostatica ed i campi elettrostatici possono essere molto intensi Campi molto intensi possono causare l’espulsione di cariche dal conduttoreLe cariche espulse subiscono forti accelerazioni, guadagnando energia cineticaInteragendo con l’aria, provocano un riscaldamento del mezzo per cui si osservano “scintille”",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#29": "30Collegamento a terra\nR1\nR2Q0Poniamoci nella condizione limite: R1 ≫ R2 Inizialmente carica Q0 sul conduttore piccolo.Q1=R1R1+R2Q0⟶R1≫R2Q0Q2=R2R1+R2Q0⟶R1≫R20La carica fluisce interamente sul conduttore più grandeLa Terra può essere considerata come un enorme conduttore, da cui si capisce il significato di collegamento a terra (o messa a terra, ground)VT=Q4πε0RT⟶RT≈6400km0Collegando i due conduttori:In elettrotecnica si utilizza il potenziale di terra come valore di riferimento del potenzialeSimbolo  messa a terra",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#3": "4Conduttori in presenza di carica esterna\n+++++++\n+++++++\nConduttorebacchetta caricaoscilloscopio a foglieL’oscilloscopio misura la presenza di caricaL’oscilloscopio misura una maggiore presenza di caricaInduzione elettrostatica (spostamento di cariche sul conduttore)+++++++-------",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#30": "31Massa e messa a terra\nTerraIn elettrotecnica la massa (chassis) è la scatola metallica di un’apparecchiatura elettrica Si comporta come una gabbia di Faraday La massa è utilizzata per assegnare il potenziale di riferimento comune delle componenti elettriche\nmassaguasto delle componenti elettriche ⇒ eccesso di cariche sulla massa (pericolo!)Collegando la massa a terra, si scaricano pericolosi eccessi di caricacomponenti  elettrici/elettronici",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#31": "32Capacità di un conduttoreIl potenziale di un conduttore isolato è proporzionale alla carica presente sul conduttoreC=QVDefiniamo la capacità di un conduttoreNel S.I. la capacità si misura in Farad (F):  1F=1C/1VLa capacità quantifica l’attitudine di un conduttore ad accumulare carica ad un dato potenziale  La capacità dipende solo dalla forma e dalle dimensioni del conduttore e dal mezzo che lo circonda (nel nostro caso il vuoto, per ora)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#32": "33Capacità di un conduttore sferico\nRQConsideriamo una sfera conduttrice di raggio R con carica QC=QV=QQ4πε0R=4πε0RLa capacità dipende solamente da fattori geometriciEsempiCapacità di una sfera di raggio R=1m nel vuoto: \nCapacità di un Conduttore •!Dato un conduttore isolato nello spazio, il suo potenziale elettrostatico risulta proporzionale alla carica presente sul conduttore stesso.  •!L’inverso della costante di proporzionalità viene chiamata capacità del conduttore nel vuoto ed è una costante caratteristica della sua forma geometrica e delle sue dimensioni. •!Nel Sistema Internazionale la capacità si misura in Farad (F), ovvero in C/V (Coulomb/Volt) e le sue dimensioni sono: Q=CVC!\"#$=Q!\"#$V!\"#$=IT!\"#$ML2T%3I%1!\"#$=M%1L%2T4I2!\"#$\n21!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!\nCapacità di un Conduttore (II) •!Si osservi per inciso che, definito il Farad, la costante dielettrica del vuoto si scrive: •!Calcoliamo ora la capacità di un conduttore sferico. Come abbiamo visto, se Q è la carica del conduttore, il suo potenziale è: •!Segue che: !0=8.85\"10#12N#1m#2C2=8.85\"10#12Fm  V=14!\"0QR  C=QV=Q14!\"0QR=4!\"0R  C=4!\"0R(conduttore sferico) 22!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!E!++++++++QR!rSO,r()\nCapacità di un Conduttore (III) •!Se consideriamo una sfera conduttrice di raggio R = 1 m, la sua capacità sarà: •!Il Globo Terrestre (raggio R = 6.4%106 m) ha una capacità pari a: e dunque minore di un mF (milliFarad).   C=4!\"0R=4!#8.85#10$12#1F=1.11#10$10F=111pF   C=4!\"0R=4!#8.85#10$12#6.4#106F=712µF\n23!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!\nCapacità di un Conduttore all’Interno di una Cavità •!Se a un conduttore si avvicina un altro conduttore neutro e isolato, aumenta la capacità del primo conduttore. •!Calcoliamo la capacità di una sfera racchiusa nella cavità sferica di un conduttore (con le due superfici sferiche concentriche): V=!14\"#0Qr2$(%,R)&'(ˆnidP!\"!==!14\"#0Qr2$(%,R1)&'(ˆnidP!\"!!14\"#0Qr2$(R1,R)&'(ˆnidP!\"!==!0!14\"#0Qr2$(R1,R)&'(ˆnidP!\"!=!14\"#0Qr2$(R1,R)&'(ˆnidP!\"!\n24!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!E!+Q!R1!Q!\n!!!!!!!R+++++++Capacità della Terra R=6400 km: \nCapacità di un Conduttore •!Dato un conduttore isolato nello spazio, il suo potenziale elettrostatico risulta proporzionale alla carica presente sul conduttore stesso.  •!L’inverso della costante di proporzionalità viene chiamata capacità del conduttore nel vuoto ed è una costante caratteristica della sua forma geometrica e delle sue dimensioni. •!Nel Sistema Internazionale la capacità si misura in Farad (F), ovvero in C/V (Coulomb/Volt) e le sue dimensioni sono: Q=CVC!\"#$=Q!\"#$V!\"#$=IT!\"#$ML2T%3I%1!\"#$=M%1L%2T4I2!\"#$\n21!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!\nCapacità di un Conduttore (II) •!Si osservi per inciso che, definito il Farad, la costante dielettrica del vuoto si scrive: •!Calcoliamo ora la capacità di un conduttore sferico. Come abbiamo visto, se Q è la carica del conduttore, il suo potenziale è: •!Segue che: !0=8.85\"10#12N#1m#2C2=8.85\"10#12Fm  V=14!\"0QR  C=QV=Q14!\"0QR=4!\"0R  C=4!\"0R(conduttore sferico) 22!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!E!++++++++QR!rSO,r()\nCapacità di un Conduttore (III) •!Se consideriamo una sfera conduttrice di raggio R = 1 m, la sua capacità sarà: •!Il Globo Terrestre (raggio R = 6.4%106 m) ha una capacità pari a: e dunque minore di un mF (milliFarad).   C=4!\"0R=4!#8.85#10$12#1F=1.11#10$10F=111pF   C=4!\"0R=4!#8.85#10$12#6.4#106F=712µF\n23!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!\nCapacità di un Conduttore all’Interno di una Cavità •!Se a un conduttore si avvicina un altro conduttore neutro e isolato, aumenta la capacità del primo conduttore. •!Calcoliamo la capacità di una sfera racchiusa nella cavità sferica di un conduttore (con le due superfici sferiche concentriche): V=!14\"#0Qr2$(%,R)&'(ˆnidP!\"!==!14\"#0Qr2$(%,R1)&'(ˆnidP!\"!!14\"#0Qr2$(R1,R)&'(ˆnidP!\"!==!0!14\"#0Qr2$(R1,R)&'(ˆnidP!\"!=!14\"#0Qr2$(R1,R)&'(ˆnidP!\"!\n24!Domenico Galli – Fisica Generale B – 2. Elettrostatica dei conduttori metallici!E!+Q!R1!Q!\n!!!!!!!R+++++++",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#33": "34CondensatoriIl condensatore è un sistema formato da due conduttori carichi per i quali si verifica induzione completa (tutte le linee di forza uscenti da un conduttore incontrano l’altro conduttore) I due conduttori sono le armature del condensatore Lo spazio interposto tra le armature è l’intercapedineLa capacità del condensatore è definita come rapporto tra la carica (presente con segno opposto sui due conduttori) e la differenza di potenziale tra i due conduttoriC=QΔV\n+Q-Q++++++++++++____________La capacità di un condensatore dipende solo dalla geometria, dalla forma e dal materiale interposto tra i conduttoriSimbolo  condensatore",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#34": "35Capacità di un condensatore pianoIl condensatore piano (condensatore a facce piane e parallele) è costituito da due armature piane di superficie S poste parallelamente a piccola distanza d (d≪S, trascuriamo effetti di bordo)\nSdSd+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |+Q-QIl sistema è equivalente al doppio strato⃗E={σε0̂n=QS1ε0interno0esternoCapacità:C=QΔV=QQdε0S=ε0Sd• proporzionale alla superficie delle armature • inversamente proporzionale alla distanza tra le armatureΔV=∫d0Edz=Ed=QdSε0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#35": "36Capacità di un condensatore sferico\nR1+Q-QR2Un condensatore sferico è costituito da una sfera conduttrice di raggio R1 racchiusa all’interno di una cavità sferica di raggio R2 di un conduttore sfericoΔV=V1−V2=∫R2R1⃗E⋅d⃗r=Q4πε0∫R2R1drr2=Q4πε0[−1r]R2R1=Q4πε0(1R1−1R2)Differenza di potenziale tra le armatureCapacità del condensatore sfericoC=QΔV=QQ4πε0(1R1−1R2)=4πε0(R1R2R2−R1)Nel limite R1→ R2, definendo d=R2-R1 C=4πε0(R1R2R2−R1)⟶R1→R24πε0R2d=ε0Sdcapacità del condensatore piano",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#36": "37Capacità di un condensatore cilindricoUn condensatore cilindrico è costituito da un cilindro conduttore di raggio R1 racchiuso all’interno di una cavità cilindrica di raggio R2 di un conduttore cilindrico (nell’approssimazione R1,R2≪h , trascurando eff. bordo)⃗E=Q2πε0hr̂rCampo elettrico internoR2R1hΔV=∫R2R1Edr=∫R2R1Qdr2πε0hr=Q2πε0hlnR2R1Differenza di potenziale tra le armatureC=QQ2πε0hlnR2R1=2πε0hlnR2R1CapacitàNel limite R1→ R2, definendo d=R2-R1 lnR2R1=lnR1+R2−R1R1=ln(1+R2−R1R1)=ln(1+dR1)≅dRC=2πε0hdR=ε02πRhd=ε0Sdcapacità del condensatore piano",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#37": "38Sistemi di condensatori in paralleloI condensatori sono dispositivi dipolari  (hanno due capi di connessione)Connessione in parallelo (gli elementi circuitali sono alla stessa differenza di potenziale 𝛥V1=𝛥V2=VA-VB=𝛥VAB): Q1=C1𝛥V1=C1𝛥VAB Q2=C2𝛥V2=C2𝛥VABQTOT=Q1+Q2=(C1+C2)𝛥VAB=CTOT𝛥VABCTOT=C1+C2   \nla capacità del sistema formato da due (o più) condensatori collegati in parallelo è uguale alla somma delle singole capacitàCTOT=∑CiC1C2+Q2-Q1VAVB-Q2+Q1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#38": "39Sistemi di condensatori in serieConnessione in serie (gli elementi circuitali sono collegati con un solo polo in comune)Conduttore isolato e neutro -Q1+Q2=0 ⇒  Q1=Q2 I condensatori in serie hanno la stessa caricaC1C2+Q2+Q1VAVB-Q1-Q2VM=QC1+QC2=Q(1C1+1C2)=QCTOTVA−VB=(VA−VM)+(VM−VB)=1CTOT=1C1+1C2CTOT=C1C2C1+C21CTOT=∑1Ci\nL’inverso della capacità del sistema formato da due o più condensatori collegati in serie è uguale alla somma degli inversi delle singole capacità ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#39": "40Esempio\nserie o parallelo?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#4": "5Conduttori in presenza di carica esterna\n+++++++In presenza di un campo elettrostatico esterno le cariche del conduttore si spostano fino a raggiungere una nuova condizione di equilibrio (𝛥t∼10-9 s)Equilibrio ⇒ cariche ferme ⇒ forza nulla ⇒ campo elettrico complessivamente nulloLe cariche del  conduttore si dispongono in maniera tale da generare un campo interno      (indotto) che annulla il campo esterno⃗E⃗E⃗E′\u0000Il campo elettrico interno ai conduttori è sempre nullo\n+++++++-------⃗E′\u0000⃗E⃗Econd=⃗E+⃗E′\u0000=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#40": "41Esercizio2.2Si consideri un sistema formato da un volume sferico di raggioain cui ` e contenuta una carica +Qdistribuitauniformemente nel volume, e da un sottile guscio di materiale conduttore di raggiob(b>a), concentrico alvolume sferico, sul quale ` e depositata una carica\u0000Q. Determinare:a) l’espressione del campo elettrostatico in tutto lo spazio in funzione della distanzardal centro delsistema, disegnando un graﬁco qualitativo dell’andamento del campo; (R:~E(r<a),~E(a<r<b)v e d ies 1.12,~E(r>b) = 0)b) l’espressione del potenziale sulla superﬁcie del volume sferico (inr=a, considerando nullo il potenzialeall’inﬁnito). (R:V(a)=Q4⇡\"0a3(1a\u00001b))2.3Due sfere conduttrici di raggiR1=2 m eR2=3 m , caricate inizialmente ciascuna con caricaQ=1 mC, vengonocollegate da un ﬁlo conduttore sottile. Nel caso in cui le sfere siano poste a distanza tale da poter trascuraree↵etti di induzione elettrostatica, come si ridistribuisce la carica?(R:Q1=2QR1R1+R2=0.8 mC,Q2=2QR2R1+R2=1.2mC)2.4Calcolare la capacit` a di un condensatore sferico.2.5Calcolare la capacit` a di un condensatore cilindrico.2.6Dato il circuito rappresentato in ﬁgura 1 (\u0000VAB= 300V,C1=3µF,C2=2µF,C3=4µF), deter-minare la carica e la di↵erenza di potenziale di ciascun condensatore. (R:Q1=0.6mC, Q2=0.2mC, Q3=0.4mC,\u0000V1= 200V,\u0000V2=\u0000V3= 100V)ABC1C3C2ABC1C2C3\nC1C2SFigure 1:2.7Dato il circuito rappresentato in ﬁgura 2 (\u0000VAB= 12V,C1=C2=2µF,C3=5µF), determinare:a) la carica e la di↵erenza di potenziale ai capi di ogni condensatore del circuito (R:Q1= 12µC, Q2=Q3= 60µC,\u0000V1=\u0000V2=6V,\u0000V3= 12V);b) l’energia accumulata su ciascun condensatore (R:U1=U2= 36µJ, U3= 360µJ).2.2Si consideri un sistema formato da un volume sferico di raggioain cui ` e contenuta una carica +Qdistribuitauniformemente nel volume, e da un sottile guscio di materiale conduttore di raggiob(b>a), concentrico alvolume sferico, sul quale ` e depositata una carica\u0000Q. Determinare:a) l’espressione del campo elettrostatico in tutto lo spazio in funzione della distanzardal centro delsistema, disegnando un graﬁco qualitativo dell’andamento del campo; (R:~E(r<a),~E(a<r<b)v e d ies 1.12,~E(r>b) = 0)b) l’espressione del potenziale sulla superﬁcie del volume sferico (inr=a, considerando nullo il potenzialeall’inﬁnito). (R:V(a)=Q4⇡\"0a3(1a\u00001b))2.3Due sfere conduttrici di raggiR1=2 m eR2=3 m , caricate inizialmente ciascuna con caricaQ=1 mC, vengonocollegate da un ﬁlo conduttore sottile. Nel caso in cui le sfere siano poste a distanza tale da poter trascuraree↵etti di induzione elettrostatica, come si ridistribuisce la carica?(R:Q1=2QR1R1+R2=0.8 mC,Q2=2QR2R1+R2=1.2mC)2.4Calcolare la capacit` a di un condensatore sferico.2.5Calcolare la capacit` a di un condensatore cilindrico.2.6Dato il circuito rappresentato in ﬁgura 1 (\u0000VAB= 300V,C1=3µF,C2=2µF,C3=4µF), deter-minare la carica e la di↵erenza di potenziale di ciascun condensatore. (R:Q1=0.6mC, Q2=0.2mC, Q3=0.4mC,\u0000V1= 200V,\u0000V2=\u0000V3= 100V)ABC1C3C2ABC1C2C3\nC1C2SFigure 1:2.7Dato il circuito rappresentato in ﬁgura 2 (\u0000VAB= 12V,C1=C2=2µF,C3=5µF), determinare:a) la carica e la di↵erenza di potenziale ai capi di ogni condensatore del circuito (R:Q1= 12µC, Q2=Q3= 60µC,\u0000V1=\u0000V2=6V,\u0000V3= 12V);b) l’energia accumulata su ciascun condensatore (R:U1=U2= 36µJ, U3= 360µJ).",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#41": "42Energia elettrostatica di un sistema di caricheEnergia potenziale elettrostatica di una carica situata in un punto dello spazio in cui è presente un potenziale V:  U=qV  Rappresenta il lavoro che bisogna fare sulla carica q per portarla dall’infinito al punto in cui il potenziale vale Vq1Per portare la prima carica nella posizione finale, non occorre fare lavoroPer portare la seconda carica, occorre fare un lavoro ℒ=∫⃗F⋅d⃗l=∫q2⃗E1⋅d⃗l⃗r12q2=q2∫⃗E1⋅d⃗l=q2V1(r12)=q2q14πε0r12U12=q1q24πε0r12=U21Energia del sistema di due cariche:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#42": "43Energia elettrostatica di un sistema di caricheq1Per portare una terza carica, occorre fare un lavoro ℒ=∫⃗F⋅d⃗l=∫q3⃗Etot⋅d⃗l=q3∫(⃗E1+⃗E2)⋅d⃗l⃗r12q2=q3[∫⃗E1⋅d⃗l+∫⃗E2⋅d⃗l]=q3[V1(r13)+V2(r23)]=⃗r13⃗r23q3=q3V1(r13)+q3V2(r23)=q3q14πε0r13+q3q24πε0r23U13=U31=q1q34πε0r13U23=U32=q2q34πε0r23UE=U12+U13+U23Energia elettrostatica del sistema di 3 cariche:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#43": "44Energia elettrostatica di un sistema di caricheUij=qiqj4πε0rijUE=12(U12+U21+U13+U31+U23+U32)Energia elettrostatica del sistema di 3 cariche:Utilizzando una notazione compatta:Vi=3∑j=1j≠iqj4πε0rijUE=123∑i=1qiVi=123∑i=1qi3∑j=1j≠iqj4πε0rij",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#44": "45Energia elettrostatica di un sistema di caricheL’energia elettrostatica totale di un sistema di N cariche puntiformi èqiqj⃗rijL’energia elettrostatica di un sistema è equivalente al lavoro necessario per portare le N cariche nella configurazione finale UE=12N∑i=1qiVi=12N∑i=1N∑j=1j≠iqiqj4πε0rij=N∑i=1N∑j>iqiqj4πε0rij",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#45": "46EsercizioEsercitazioni di Fisica Generale T2 - provvisorioLorenzo Rinaldi10/10/20171 Elettrostatica nel vuoto1.1Due piccole palline di sughero identiche di massamhanno ugual caricaq. Esse sono appese a due ﬁli dilunghezzal, a loro volta vincolati in un medesimo punto. In condizioni di equilibrio, determinare l’angolo✓che i due ﬁli formano con la verticale (risolvere nell’approssimazione✓⇡0 (R:✓=3qq216⇡✏0mgl2).1.2Tre cariche positive puntiformi identicheq1=q2=q3=4 mC sono disposte su un piano cartesiano ortogonalerispettivamente nei punti di coordinate (0;3m), (0;-1m) e (-1m;1m). Una quarta carica positivaq4=2 mC ` eposta nel punto di coordinate (1m;1m). Determinare:a) la forza a cui ` e sottoposta la caricaq4;( R :~F=q1q44⇡✏08p5+25100ˆı=3.09ˆıJ)b) l’energia necessaria a spostare la caricaq4dalla posizione iniziale (1m;1m) all’origine del sistema diriferimento. (R:L=q1q44⇡✏055+15p2+12p330=4.65J )1.3Tre cariche puntiformi sono poste ai vertici di un triangolo equilatero di latoa=10cm. Sapendo cheq1=\u00004·10\u00007C,q2=+ 2·10\u00007Ceq3=+ 1·10\u00007C, determinare l’energia elettrostatica del sistema. (R:U=\u000010q24⇡✏0a=\u00009·10\u00003J)1.4Quattro particelle con la stessa caricaQ=-1nC si trovano ai vertici di un quadrato di latol=12cm. Si calcoli:a) il modulo dell’intensit` a del campo elettrico nel centroOdel quadrato e nel punto medioMdi ciascunlato (R:EO= 0,EM=4p525q⇡✏0l2=8.93⇥102V/m);b) la di↵erenza di potenziale tra i puntiOeM;(R:\u0000V=q⇡✏0l(1 +p5/5\u0000p2)=1.12 V)c) il lavoro che si deve compiere per avvicinare le cariche e disporle ai vertici di un quadrato di latol/2.(R:L=q24⇡✏0⌃i,j>i1rij=4.06⇥10\u00007J)1.5Si consideri una carica elettrica distribuita uniformemente con densit` a di carica lineare\u0000=10\u00005C/m su diun ﬁlo di lunghezzaL=10 cm. Si calcoli il campo elettrico in un puntoAposto a distanzah=3 cm daun’estremit` a del ﬁlo, perpendicolarmente ad esso. (Ex=\u00004⇡✏0hLph2+L2,Ey=\u00004⇡✏0h(hph2+L2\u00001))1.6Sia data una sbarretta di lunghezza L e dimensioni trasversali trascurabili, disposta lungo il semiasse dellexpositive in un sistema di riferimento avente l’origine coincidente con uno degli estremi. Sulla barretta ` edepositata una carica Q con densit` a lineare\u0000=kx. Determinare in funzione diQed iL, l’espressione delpotenziale generato dalla barretta nel puntoP=( 2L,0,0). (V=Q2⇡✏0L(ln 4\u00001))1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#46": "47Energia elettrostaticaNel caso in cui le cariche siano distribuite con una densità 𝜌 su un volume 𝜏UE=12∫τρVdτUtilizzando la legge di Gauss in forma locale: ρ=ε0⃗∇⋅⃗E⃗∇⋅(V⃗E)=V⃗∇⋅⃗E+⃗E⋅⃗∇VV⃗∇⋅⃗E=⃗∇⋅(V⃗E)−⃗E⋅⃗∇VUE=12∫τρVdτ=12∫τε0⃗∇⋅⃗EVdτUso le proprietà del prodotto scalareUE=ε02∫τ(⃗∇⋅(V⃗E)−⃗E⋅⃗∇V)dτUE=ε02∫τ⃗∇⋅(V⃗E)−∫τ⃗E⋅⃗∇Vdτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#47": "48Densità di energia del campo elettrico⃗∇V=−⃗EUE=ε02∫τ⃗∇⋅(V⃗E)−∫τ⃗E⋅⃗∇Vdτ∫div⃗Fdτ=∮⃗F⋅̂ndSUE=ε02∮SV⃗E⋅̂ndS+∫τ⃗E⋅⃗EdτGli integrali vanno calcolati su tutto lo spazio in  cui è presente il campo elettrico Il campo elettrico si estende e si annulla all’infinito se la carica 𝜌 è localizzataIl flusso all’infinito è nullo (E si annulla all’infinito) ⃗E⋅⃗E=E2uE=12ε0E2densità di energia del campo elettrostaticoUE=∫spazio12ε0E2dτUE=∫spaziouEdτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#48": "49Densità di energia del campo elettrico\nuE=12ε0E2densità di energia del campo elettrostatico (quantità di energia per unità di volume) L’energia elettrostatica è localizzata nel campo elettrico (e non nella carica)UE=∫spazio12ε0E2dτUE=∫spaziouEdτUE=12∫τρVdτuE=dUEdτEspressioni dell’energia elettrostaticavolume in cui è contenuta la caricavolume in cui è presente il campo elettrico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#49": "50Energia di un condensatoreSd+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |+q-qCondensatore (piano) carico con carica q+dqdℒ=dqΔVq=dqqCIl lavoro complessivo per caricare completamente il condensatore dalla carica 0 alla carica Q:ℒ=∫Q0dℒ=∫Q0dqqC=1C∫Q0qdq=12Q2CL’energia elettrostatica accumulata in un condensatore è Ue=12Q2C=12CΔV2=12QΔVIl lavoro (di una forza esterna) per portare una carica +dq dall’armatura di destra a quella di sinistra è:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#5": "6Elettrizzazione per contatto\n++++++++++++++++In caso di contatto parte della carica sulla bacchetta si trasferisce al conduttoreLa carica resta sul conduttore dopo aver rimosso il contatto (misurabile con oscilloscopio)\n+++++++++++++ConduttoreConduttore⃗Econd=0Nuova situazione di equilibrio ⇒",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#50": "51Energia di un condensatoreIn un condensatore piano la capacità vale: Sd+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |+Q-QC=ε0SdRiscriviamo l’energia: La differenza di potenziale tra le armature: ΔV=EdL’energia è pari alla densità di energia integrata su tutto lo spazio dove si estende il campo (il campo è nullo esternamente al condensatore)densità di energia elettrostaticavolume interno del condensatoreUE=12CΔV2=12ε0Sd(Ed)2=12ε0dSE2=(12ε0E2)(dS)=uEτ=∫τuEdτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#51": "52Energia del condensatore\nSi può pensare di utilizzare un condensatore al posto di una batteria (chimica) ricaricabile? Svantaggi: •Ingombro. La densità di energia (energia per unità di volume) di un condensatore è enormemente minore di quella di una batteria. •Potenziale non costante. Mano mano che si scarica, la differenza di potenziale ai capi di un condensatore diminuisce (proporzionalmente alla carica).Vantaggi: •Velocità. Un condensatore si può caricare molto velocemente e può produrre intensità di corrente molto elevate scaricandosi (flash macchine fotografiche) •Durata. Una batteria si esaurisce dopo alcune migliaia di cicli di carica-scarica, mentre un condensatore ha una durata teoricamente illimitata.  •Basse temperature. Funzionano anche a -40° C, temperatura alla quale le normali batterie non sono in grado di operare. ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#52": "53Forza tra le armature di un condensatoreLe armature di un condensatore hanno cariche opposte: si attraggonoSx+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |+Q-QUE=12Q2C=12Q2xε0Steniamo fissa un’armatura e applichiamo una forza esterna opposta a quella attrattiva, in modo tale che il lavoro della forza esterna bilanci la variazione di energia del condensatore⃗F⃗FestdUE=δℒest=FestdxdUE=12Q2dxε0S=δℒest=Festdxpossiamo definire la pressione elettrostatica:Per calcolare la forza tra le armature di un condensatore piano partiamo dall’energiaForza tra le armature ⃗F=−⃗Fest=−Q22ε0Ŝnp=FS=Q22ε0S2=σ22ε0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#53": "54EsempioABC1C3C2ABC1C2C3\nC1C2SFigure 2:2.8Siano dati due condensatori rispettivamente di capacit` aC1=3m FeC2=4 mF disposti come in ﬁgura 3.Inizialmente le armature del condensatore 1 sono poste ad una d.d.p.\u0000V1= 100 V, il condensatore 2 ` escarico e l’interruttoreSaperto. Una volta collegati tramite la chiusura diS, i due condensatori arrivanodopo un transitorio ad una fase di equilibrio. Calcolare:a) l’energia immagazzinata nel sistema prima della chiusura dell’interruttoreS(R:U=U1= 15J);b) l’energia immagazzinata nel sistema dopo la chiusura dell’interruttoreS(R:U=c21\u0000V212(c1+c2)=6.43J);c) dimostrare che la situazione di equilibrio corrisponde ad un minimo di energia elettrostatica del sistema(S: scrivereUin funzione del potenziale sul condensatoreV1, e poidU(V1)dV1= 0).ABC1C3C2ABC1C2C3\nC1C2SFigure 3:2.9Un condensatore a facce piane parallele poste ad una distanzaD` e inizialmente caricato in modo da possedereuna energia elettrostatica pari aUin= 10\u00004J. Supponendo di mantenere isolato il condensatore si allontaninola due armature di una quantit` a\u0000x=D/2. Calcolare il lavoro fatto dalla forza esterna.(R:L=\u000012Uin)2.10Una lastra a forma di parallelepipedo di spessorebe areaSviene inserita parallelamente all’interno di uncondensatore piano ideale avente le armature di areaSdistanti tra di loroa>b. Determinare la variazionedi energia elettrostatica nei due casi in cui il processo avviene rispettivamente a carica e a di↵erenza dipotenziale costante. (R: le energie ﬁnali dipenderanno dalla capacit` a ﬁnalecF=\"0Sa\u0000b, a seconda se siacostante la carica ovvero la d.d.p.)2.11Un condensatore piano ideale formato da due armature quadrate di latoLdisposte parallelamente a distanzad. Il condensatore ` e isolato e su di esso ` e depositata una caricaQ. Inizialmente tra le armature c’` e ilvuoto. Successivamente si introduce nel condensatore, parallelamente alle facce del condensatore, una lastra",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#54": "55EsempioABC1C3C2ABC1C2C3\nC1C2SFigure 2:2.8Siano dati due condensatori rispettivamente di capacit` aC1=3m FeC2=4 mF disposti come in ﬁgura 3.Inizialmente le armature del condensatore 1 sono poste ad una d.d.p.\u0000V1= 100 V, il condensatore 2 ` escarico e l’interruttoreSaperto. Una volta collegati tramite la chiusura diS, i due condensatori arrivanodopo un transitorio ad una fase di equilibrio. Calcolare:a) l’energia immagazzinata nel sistema prima della chiusura dell’interruttoreS(R:U=U1= 15J);b) l’energia immagazzinata nel sistema dopo la chiusura dell’interruttoreS(R:U=c21\u0000V212(c1+c2)=6.43J);c) dimostrare che la situazione di equilibrio corrisponde ad un minimo di energia elettrostatica del sistema(S: scrivereUin funzione del potenziale sul condensatoreV1, e poidU(V1)dV1= 0).ABC1C3C2ABC1C2C3\nC1C2SFigure 3:2.9Un condensatore a facce piane parallele poste ad una distanzaD` e inizialmente caricato in modo da possedereuna energia elettrostatica pari aUin= 10\u00004J. Supponendo di mantenere isolato il condensatore si allontaninola due armature di una quantit` a\u0000x=D/2. Calcolare il lavoro fatto dalle forze del campo elettrico.(R:L=\u000012Uin)2.10Una lastra a forma di parallelepipedo di spessorebe areaSviene inserita parallelamente all’interno di uncondensatore piano ideale avente le armature di areaSdistanti tra di loroa>b. Determinare la variazionedi energia elettrostatica nei due casi in cui il processo avviene rispettivamente a carica e a di↵erenza dipotenziale costante. (R: le energie ﬁnali dipenderanno dalla capacit` a ﬁnalecF=\"0Sa\u0000b, a seconda se siacostante la carica ovvero la d.d.p.)2.11Un condensatore piano ideale formato da due armature quadrate di latoLdisposte parallelamente a distanzad. Il condensatore ` e isolato e su di esso ` e depositata una caricaQ. Inizialmente tra le armature c’` e il",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#55": "56Condensatori con dielettriciCosa succede se riempiamo con un materiale isolante l’intercapedine di un condensatore?+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  +−  ++𝜎-𝜎-𝜎P𝜎PNel dielettrico le cariche non si muovono.Però a livello microscopico le molecole (dipolari) possono orientarsiSulle superfici del dielettrico a contatto con le armature del condensatore si osserva un eccesso di carica 𝜎P (carica di polarizzazione)Le cariche di polarizzazione creano un campo elettrico opposto al campo del condensatoreIl campo elettrico totale (e di conseguenza la differenza di potenziale) diminuisce La capacità del condensatore aumenta⃗E0⃗E′\u0000",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#56": "57Il dipolo elettrico\nTeorema della Divergenza (II) •!Per comprendere il significato del Teorema della Divergenza: immaginiamo di suddividere il volume V in tanti cubetti infinitesimi, di volume: •!Per ogni cubetto si ha, per quanto abbiamo visto: per cui si ha, per ogni cubetto (che ha 6 facce): !viˆndSS\"!!=!\"i!vdVV!!!\n57!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!V1,!V2,!V3,…!!i!v()P()=limV\"P{}!vSV()\"##iˆndSdVV###=$tot!v()%V!!i!v()Pi()\"Vi=#toti()!v()=#ki()!v()k=16$!V1!V2!V3!V4!V5!V6!V8!V7!V10!V9V\nTeorema della Divergenza (III) •!Distinguiamo ora, tra le facce dei cubetti, le facce interne e le facce esterne: –!Le facce interne separano un cubetto da un cubetto adiacente; –!Le facce esterne fanno parte della frontiera del volume totale V. •!Sommando le divergenze dei cubetti, i contributi dei flussi delle facce interne si cancellano tra loro: –!Il flusso uscente dal cubetto i verso il cubetto j adiacente è opposto al flusso dal cubetto j al cubetto i. •!Si ha pertanto: \n58!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!!i!v()Pi()\"Vii=1N#=$ki()!v()k=16#i=1N#=$ki()!v()facceesterne#=!viˆn\"Sfacceesterne#!!i!vdVV%%%=!viˆndSS\"%%!V1!V2!V3!V4!V5!V6!V8!V7!V10!V9V\nLinee di Flusso del Campo Elettrico •!Come tutti i campi vettoriali, anche il campo elettrico si può rappresentare graficamente con le linee di flusso (o linee di campo), ovvero con linee: –!Tangenti in ogni punto al vettore campo elettrico         ; –!Orientate col verso del campo elettrico         ; –!In numero, per unità di superficie trasversale, proporzionale al modulo del campo elettrico            .   !E!r()  !E!r()\n59!  !E!r()\nDomenico Galli – Fisica Generale B – 1. Elettrostatica!\nAngolo Solido •!Come è noto, l’angolo piano (in radianti) è definito come il rapporto tra un arco di circonferenza l centrata nel vertice e il raggio r: •!L’angolo solido \" si definisce in maniera analoga come il rapporto tra la parte di superficie sferica S (centrata nel vertice), intercettata dal cono centrato nel vertice e il quadrato del raggio della sfera: •!L’angolo solido si misura in steradianti (sr). lr  !=lr\"0,2#$%$%!=Sr2\"0,4#$%&'S!r\n60!Domenico Galli – Fisica Generale B – 1. Elettrostatica!-+   Il dipolo elettrico è un sistema formato da 2 cariche elettriche in quiete, di uguale valore assoluto ma segno opposto (Q e –Q), poste a una distanza fissata d.  \nx\nz\ny+Q-QdMolti materiali isolanti sono formati da molecole che hanno una struttura “dipolare”. \nDefiniamo il momento di dipolo ⃗p=(Qd)̂k\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#57": "58Azioni meccaniche su un dipolo elettricoCalcoliamo il momento della forza esercitato da un campo esterno su un dipolo\n⃗M=⃗p∧⃗E\nDipolo Elettrico (IV) •!Si ha: \n•!Dunque il potenziale di un dipolo elettrico decresce con la distanza come 1/r2.      !pi!r=QdversP+!P!()i!r=Qdrcos!   V!r()\"d#rQ4!\"0dr2cos#=14!\"01r2!pi!rr   V!r()\"d#r14!\"0!pi!rr3\n9!Domenico Galli – Fisica Generale B – 3. Problema Generale dell'Elettrostatica!qr++ – + QQ!r!rxyzd!\nDipolo Elettrico (V) •!Per calcolare il campo elettrico del dipolo scriviamo il potenziale in coordinate cartesiane e calcoliamo il gradiente:    Vx,y,z()!d\"r14!\"0xpx+ypy+zpzx2+y2+z2()32\n   Exx,y,z()=!\"V\"x\"d#r!14#$0\"\"xxpx+ypy+zpzx2+y2+z2()32==!14#$0pxx2+y2+z2()32!xpx+ypy+zpz()32x2+y2+z2()122xx2+y2+z2()3==14#$03xxpx+ypy+zpz()x2+y2+z2()52!pxx2+y2+z2()32%&'''()***=14#$03!pi!r()xr5!pxr3%&''()**10!Domenico Galli – Fisica Generale B – 3. Problema Generale dell'Elettrostatica!\nDipolo Elettrico (VI) •!Ripetendo il calcolo per le componenti y e z si ottiene: •!Il campo elettrico di un dipolo elettrico decresce con la distanza come 1/r3:    !Ex,y,z()\"d#r14!\"03!pi!r()!rr5#!pr3$%&&'())\n11!Domenico Galli – Fisica Generale B – 3. Problema Generale dell'Elettrostatica!qr++ – + QQ!r!rxyzd!!Ex,y,z()\"d#r14!\"03!pi!r()!rr5#!pr3$%&&'())=14!\"01r33!piˆr()ˆr#!p$%'(\nDipolo Elettrico (VII) •!Calcoliamo ora il momento della forza esercitato da un campo elettrico esterno su di un dipolo elettrico. •!Trattandosi di due forze di uguale modulo QE, medesima direzione e verso opposto, le cui rette di azione distano d sin !, si ha: \nQ!Qd– !pF!!F!!!Esind!+    !M=!p!!E   M=Fb=QE()dsin!()=Qd()Esin!()=pEsin!\n12!Domenico Galli – Fisica Generale B – 3. Problema Generale dell'Elettrostatica!M=FEdsinθ=(QE)dsinθ=(Qd)Esinθ=pEsinθLe due forze hanno stesso modulo QE, stessa direzione e verso opposto il momento delle forze (prendendo come polo una delle due cariche) è⃗M=⃗rd∧⃗FE",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#58": "59Elettrostatica dei dielettrici+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |⃗E0−  +⃗pOgni singolo atomo/molecola del dielettrico ha un momento di dipolo elettrico ⃗p(⃗p=q⃗d)Ogni dipolo sentirà un momento delle forze e tenderà ad allinearsi con il campo:⃗M=⃗p∧⃗E0Definiamo il momento di dipolo medio          (media di tutti i dipoli) ⟨⃗p⟩sia                 il numero di atomi/molecole per unità di volumen=NΔτSi definisce il vettore polarizzazione ⃗P=n⟨⃗p⟩[C/m2] come densità di carica • indica il grado di allineamento degli atomi/molecole in un dielettrico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#59": "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |+𝜎L-𝜎L+𝜎P-𝜎P\n+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\n60Elettrostatica dei dielettrici|⃗P|=σpIl modulo del vettore polarizzazione è la densità di carica di polarizzazione⃗P=σp̂n⃗P=σp̂nIn un dielettrico isotropo e omogeneo le cariche di polarizzazione sono distribuite solo superficialmente (±𝜎P)Chiamiamo la carica libera (±𝜎L) quella sulle armature del condensatoreSi definisce il vettore spostamento elettrico⃗D=σL̂n⃗D=σL̂n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#6": "7Carica interna al conduttore\nSConsideriamo una generica superficie chiusa S interna al conduttore  ΦS(⃗E)=∬S⃗E⋅̂ndS=QSε0Per la legge di GaussInternamente al conduttore il campo elettrico è nullo, quindi la carica interna ad S sarà sempre nullaQS=∭τ(S)ρdτ=0All’interno del conduttore non ci sono cariche in eccesso (cariche positive e negative hanno uguale densità)⃗E=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#60": "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |+𝜎L-𝜎L𝜎P-𝜎P\n+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\n61Elettrostatica dei dielettriciIl campo all’interno del dielettrico⃗E=⃗E0+⃗EP⃗EP=−σPε0̂n⃗E0=σLε0̂n⃗D=ε0⃗E+⃗P=⃗Dε0−⃗Pε0=σL̂nε0−σP̂nε0In un dielettrico isotropo e omogeneo i vettori campo elettrico, polarizzazione e spostamento sono paralleliIn un dielettrico isotropo e omogeneo si definisco le due quantità adimensionali suscettività dielettrica 𝜒 (𝜒≥0) e la costante dielettrica relativa 𝜀R (𝜀R≥1), legati dalla relazione 𝜒=𝜀R-1Il campo elettrico ed il vettore polarizzazione sono legati dalla relazione⃗P=ε0χ⃗E=ε0(εR−1)⃗E",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#61": "62Elettrostatica dei dielettrici⃗P=ε0χ⃗E=ε0(εR−1)⃗E⃗D=ε0⃗E+⃗P⃗D=ε0⃗E+ε0(εR−1)⃗E=ε0εR⃗EUtilizzando il vettore spostamento elettrico, formuliamo la legge di Gauss (in forma locale e integrale) in funzione delle sole cariche libere 𝜌L e QL :⃗∇⋅⃗D=ρL∬⃗D⋅̂ndS=QLIn un dielettrico isotropo e omogeneo il vettore spostamento elettrico è proporzionale al campo elettrico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#62": "Capacità di un condensatore piano con dielettrico63Condensatori con dielettriciΔV=ΔV0εrC=QΔV=εrQΔV0=εrC0C=ε0εrSdSe riempiamo un condensatore con un dielettrico isotropo e omogeneo, il campo elettrico totale vale:\nCapacità di un condensatore con dielettricoDi conseguenza, la differenza di potenziale tra le armature⃗E=⃗Dε0εR=σL̂nε01εR=⃗E0εRcampo elettrico con condensatore vuoto",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#63": "64Costanti dielettriche relativematerialecostante dielettrica relativa 𝜀R Aria1.00059Acqua distillataca. 80Etanolo25Petrolio2.1Vetro comune5 ÷ 10Plexiglas3.40Mica8Ebanite2Paraffina2.1Glicerolo42.6Ossido di titanio90 ÷ 170Titanati di Ba-Sr1000 ÷ 10000",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#64": "Lorenzo Rinaldi Dipartimento di Fisica e Astronomia lorenzo.rinaldi@unibo.it https://www.unibo.it/sitoweb/lorenzo.rinaldi/\n65",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#7": "8Carica superficiale di un conduttore La carica in eccesso (dovuto ad elettrizzazione per induzione o per contatto) si  dispone sulla superficie del conduttore Microscopicamente la carica occupa uno spessore di 10-10 m (dimensioni atomiche)Nei conduttori le cariche in eccesso si dispongono in superficie, in una configurazione tale che il campo elettrico interno al conduttore sia nullo\n+++++++++++++++++++++++Conduttore elettrizzato\n--++++++++++-----------Conduttore polarizzato per induzione elettrostatica100 pm",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#8": "9Campo elettrico in prossimità della superficie dei conduttoriIn equilibrio elettrostatico le cariche si dispongono in superficieIl campo generato da tali cariche non può avere componenti tangenti alla superficie, altrimenti si osserverebbero movimenti di cariche⃗EEnEt⃗F⃗E=Ên\nIl campo elettrico è sempre normale alla superficie dei conduttori",
    "data_test\\rootfolder\\università\\FisicaGenerale\\10-conduttori.pdf#9": "10Campo elettrico in prossimità della superficie dei conduttoriIl campo elettrico è sempre normale alla superficie dei conduttoriSi dimostra in maniera formale calcolando la circuitazione del campo lungo una linea chiusa 𝛤 che interseca la superficieIl campo è nullo all’interno del conduttoreConservatività del campo elettrostatico⇒∫BA⃗E⋅d⃗l=∫BA⃗E⋅̂utdl=0⟺⃗E=ÊnSABCD0=∮L⃗E⋅d⃗l=∫BA⃗E⋅d⃗lAB+∫CB⃗E⋅d⃗lBC+∫DC⃗E⋅d⃗lCD+∫AD⃗E⋅d⃗lDA∫DC⃗E⋅d⃗lCD=0∫CB⃗E⋅d⃗lBC,∫AD⃗E⋅d⃗lDA⟶BC,AD→00d⃗lAB=̂utdlABSia 𝛤 un rettangolo di vertici ABCD • lati AB e CD sufficientemente piccoli e paralleli a S • lati BC e AD infinitesimi di ordine superiore rispetto a AB e CD\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#0": "1 Correnti elettriche CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#1": "2Corrente elettricaIn condizione statiche, il campo elettrico all’interno dei conduttori è sempre nullo altrimenti gli elettroni sarebbero accelerati e addio condizione staticaCosa succede se tramite un artificio esterno (generatore) si pone una differenza di potenziale (d.d.p.) tra due punti del conduttore?Gli elettroni di conduzione si mettono in moto ed il conduttore risulta percorso da una corrente elettrica ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#10": "11EsempioDeterminare numero di elettroni di conduzione e velocità di deriva in un filo di rame di raggio r=0.8mm percorso uniformemente da una corrente i=15A Il rame ha densità di massa 𝛿=8.96 g/cm3 e peso atomico A=6335 g/mol mediamente si avrà nC=1 elettrone di conduzione per atomon=nCδNAA=1×8.96 g cm−3×6.022×1023mol−16355 g mol−1=8.45×1022cm−3j=iS=iπr2=15Aπ×0.82×10−6 m=7.46×106 Am−2elettroni di conduzione per unità di volumedensità di correntevelocità di derivavd=jnqe=7.46×106Am−28.45×1022 cm−3×1.6×10−19As=0.55mmsquantità di carica in moto per unità di volumenqe=8.45×1022cm−3×1.6×10−19C=13.6×103 C/m3≈14 C/mm3",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#11": "12Conservazione della carica elettrica\nConsideriamo una superficie S chiusa e orientata, interna ad un conduttoreil flusso di una corrente di densità j attraverso S è dato da⃗𝚥⃗𝚥̂ncarica che passa attraverso S nell’unità di tempo (corrente uscente) flusso positivo ⇒ carica diminuisce\nPrincipio di conservazione della carica elettrica La carica che attraversa la superficie chiusa S è pari alla variazione di carica complessiva contenuta in SΔq=qout−qinΦS(⃗𝚥)=∬S⃗𝚥⋅̂ndS=i=qin−qoutΔt=−ΔqΔt→Δt→0−dqdt=iuscente",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#12": "13Equazione di continuitàLa carica interna alla superficie S può essere scritta in funzione della densità di carica: q=∭τSρdτ𝜏S è il volume delimitato dalla superficie S=−∂∂t∭τSρdτ=∭τS(−∂ρ∂t)dτ∬S⃗𝚥⋅̂ndS=∭τS⃗∇⋅⃗𝚥dτΦS(⃗𝚥)=∬S⃗𝚥⋅̂ndS=−dqdtteorema della divergenzastesso dominio di integrazione 𝜏S⃗∇⋅⃗𝚥=−∂ρ∂tequazione di continuità ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#13": "14Equazione di continuità⃗∇⋅⃗𝚥+∂ρ∂t=0La divergenza del vettore densità di corrente bilancia la variazione di caricaL’equazione descrive una situazione locale (o differenziale) in ogni punto del volume in cui scorre corrente.  Una variazione di cariche corrisponde ad un moto di cariche non solenoidale  (le cariche non si muovono su linee chiuse)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#14": "15Condizioni stazionarie\nSi hanno condizioni stazionarie se la carica entrante è pari alla carica uscente⃗𝚥⃗𝚥̂nΔq=qout−qin=0La carica q internamente a S si mantiene costanteΦS(⃗𝚥)=∬S⃗𝚥⋅̂ndS=0⃗∇⋅⃗𝚥=0Il flusso della densità di corrente è nulloIl campo densità di corrente è solenoidale (linee di campo sempre chiuse)−dqdt=iuscente=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#15": "16Prima legge di OhmConsideriamo un conduttore filiforme ai cui estremi c’è una differenza di potenziale Internamente al filo scorre una corrente proporzionale alla differenza di potenzialeLa costante di proporzionalità tra l’intensità di corrente e la differenza di potenziale è la resistenza elettricaΔV=RiTale relazione (prima legge di Ohm) è una legge empirica, valida a temperature ordinarie costanti La resistenza si misura in Ohm (Ω)    1Ω=1V/1Asimbolo circuitale della resistenza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#16": "17Seconda legge di OhmLa resistenza di un conduttore omogeneo, filiforme di lunghezza l e sezione S vale R=ρRlSR=ρRlSresistività elettrica dipende dalla natura del materiale si misura in ΩmMaterialeResistività (Ωm)Argento1,62 × 10−8Rame1,68 x 10−8Oro2,35 × 10−8Alluminio2,75 × 10−8Tungsteno5,25 × 10−8Ferro9,68 × 10−8Platino10,6 × 10−8Acqua di mare2.00 × 10−1Acqua potabiletra 2.00×101 e 2.00×103Silicio puro (non drogato)2,5 × 103Vetrotra 1010   e 1014Ariatra 1.30×1016 e 3.30×1016Quarzo fusocirca 1016",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#17": "18Leggi di Ohm in forma locale\ndSdl⃗𝚥Nell’interno di un conduttore, consideriamo un sottile cilindro di base dS e lunghezza dl percorso da una corrente di densità ⃗𝚥VAVBsia dV=VA-VB la differenza di potenziale ai capi del cilindro (VA>VB)dV=Rdi=ρRdldSdiper le due leggi di Ohmdi=jdSdV=Edl⃗𝚥=σC⃗E⃗E=ρR⃗𝚥E=ρRjσC=1ρRConduttività  (o conducibilità) elettricail vettore densità di corrente ha stessa direzione e verso del campo elettrico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#18": "19Resistenze in serieVA\nVBVMDue (o più) resistenze in serie sono attraversate dalla stessa corrente i (per equazione di continuità)R1R2i{VA−VM=R1iVM−VB=R2i(VA−VM)+(VM−VB)=VA−VB=(R1+R2)isomma membro a membroRTOT=∑iRiRTOT=R1+R2\nla resistenza del sistema formato da due (o più) resistenze collegate in serie è uguale alla somma delle singole resistenzeApplicando la legge di Ohm (caduta ohmica) ai capi di ciascuna resistenza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#19": "20Resistenze in parallelo\nVBVAR1R2i1i2Due (o più) resistenze in parallelo hanno la stessa differenza di potenziale VA-VBi1=VA−VBR1i2=VA−VBR2i=(VA−VB)(1R1+1R2)=VA−VBRTOTi=i1+i2=VA−VBR1+VA−VBR2=\nL’inverso della resistenza del sistema formato da due o più resistenze collegate in parallelo è uguale alla somma degli inversi delle singole resistenze Applicando la legge di Ohm ai capi di ciascuna resistenza1RTOT=1R1+1R21RTOT=∑i1Ri",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#2": "3Modello di Drude-LorentzSe la d.d.p. è costante nel tempo, lo sarà anche il campo elettrico e la forza sugli elettroniCi si aspetta che il moto sia uniformemente accelerato (forza e accelerazioni costanti)Sperimentalmente, però, si trova che la velocità media degli elettroni è proporzionale al campo⟨⃗ve⟩∝⃗E⟨⃗ae⟩∝⃗EModello di Drude-Lorentz  gli elettroni si comportano come cariche libere di un gas nel reticolo cristallino, soggette al campo elettrico ed interagenti con le cariche del reticolo \n_\n_\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+⃗E=−⃗∇V",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#20": "Esempio\n21\nEsempio\n22metallica quadrata di spessored/2 e latoL/2. Determinare il lavoro fatto per introdurre interamente lalastra all’interno del condensatore. (R:L=\u0000DQ210\"0L2)2.12Tre condensatori di capacit` aC1=0.5µF,C2=0.8µF,C3=0.1µF, sono collegati in serie (vedi ﬁgura 4). IpuntiAeBsono collegati inizialmente ad un generatore di tensioneV0=100 V.a) calcolare la carica elettrica su ciascun condensatore. Successivamente i condensatori vengono staccatidal generatore e il puntoBviene collegato ad un punto tra i condensatoriC1eC2.b) determinare la variazione di energia nelle↵ettuare il nuovo collegamento.   FISICA GENERALE II  -  TEST di VERIFICA     (27-10-2015)  Costanti: c = 3x108  m/s  ε0 = 8.85x10-12    F/m  G = 6.67x10-11  Nm2/kg2 e= 1.60x10-19  C me = 9.11x10-31 kg  mp = 1.67x10-27 kg   ESERCIZIO 1  Si consideri il campo F(x,y,z)=2xi-zj-ayk. Determinare: a) per quali valori di a il campo risulta conservativo; b) il potenziale generato dal campo F.  ESERCIZIO 2  Tre cariche positive puntiformi identiche q1= q2=q3=4mC sono disposte su un piano cartesiano ortogonale rispettivamente nei punti di coordinate (0,3m), (0,-1m) e (-1m,1m). Una quarta carica positiva q4=2mC  è posta nel punto di coordinate (1m,1m). Determinare: a) la forza a cui è sottoposta la carica q4; b) l’energia necessaria a spostare la carica q4 dalla posizione iniziale (1m,1m) all’origine del sistema di riferimento.  ESERCIZIO 3 Si consideri un sistema formato da un volume sferico di raggio a in cui è contenuta una carica +Q distribuita uniformemente nel volume, e da un sottile guscio di materiale conduttore di raggio b (b>a), concentrico al volume sferico, sul quale è depositata una carica –Q. Determinare: a) l’espressione del campo elettrostatico in tutto lo spazio in funzione della distanza r dal centro del sistema, disegnando un grafico qualitativo dell’andamento del campo; b) l’espressione del potenziale sulla superficie del volume sferico (in r=a, considerando nullo il potenziale all’infinito).   ESERCIZIO 4  Tre condensatori di capacità C1=0.5 µF, C2=0.8 µF,  C3=0.1 µF, sono collegati in serie (vedi figura). I punti A e B sono collegati inizialmente ad un generatore di tensione V0=100 V. a) calcolare la carica elettrica su ciascun condensatore. Successivamente i condensatori vengono staccati dal generatore e il punto B viene collegato ad un punto tra i condensatori C1 e C2. b) determinare la variazione di energia nell’effettuare il nuovo collegamento.              \nFigure 4:3 Correnti elettriche3.1Un conduttore cilindrico cavo di lunghezzad=2cm ha raggia=2mm eb=5mm; esso ` e costituito da unasostanza con resistivit` a⇢=2⌦m. Una f.e.m.E=20 V pu` o essere applicata al conduttore in modo chela corrente ﬂuisca parallelamente all’asse del cilindro oppure radialmente dalla superﬁcie interna a quellaesterna. Calcolare nei due casi l’intensit` a di correnteiche percorre il conduttore, la potenza dissipata e ladensit` a di corrente sulle superﬁci terminali.3.2Un resistore di forma cilindrica di sezioneA` e composto da una parte di lunghezzal1fatta di materiale diresistivit` a⇢1=⇢e da un’altra parte di lunghezzal2fatta di materiale di resistivit` a⇢2=3⇢. Il resistore ` eattraversato da una correnteIuniformemente sulla sezioneA. Determinare:a) l’intensit` a dei campi elettriciE1eE2nelle due parti del resistore;b) la di↵erenza di potenziale ai capi del resistore;c) il valore della carica elettrica presente sulla superﬁcie di separazione tra i due materiali che formano ilresistore.3.3Nel circuito in ﬁgura 5, calcolare l’intensit` a di correntei, il potenziale nei quattro vertici e il bilancioenergetico (R= 50⌦,E1=50 V,r1= 20⌦,E2=100 V,r2= 30⌦)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#21": "22Effetto JouleQuando una corrente i scorre attraverso un conduttore filiforme, la carica che attraversa una sezione S in un tempo dt è: dq=i dt Il lavoro compiuto dal campo elettrico nello spostamento della carica nell’intervallo dt èδℒ=dU=ΔVdq=ΔVidt=(Ri)idt=Ri2dtLa potenza (energia per unità di tempo) spesa dal campo elettrico per sostare la carica èP=dUdt=Ri2La potenza viene persa negli urti degli elettroni di conduzione con gli atomi del conduttore, i  quali aumentano la propria energia vibrazionale (la potenza viene dissipata in calore)\nEffetto Joule aumento della temperatura del conduttore attraversato da correnteP=iΔV=ΔV2R",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#22": "23Effetto Joule: interpretazione microscopicaLavoro compiuto dal campo sugli N elettroni contenuti in un volume d𝜏 in un tempo dt=ndτqe⃗E⋅d⃗l=⃗E⋅⃗𝚥dτdtdPdτ=δℒdτdt=⃗E⋅⃗𝚥\nEffetto Joule in forma locale Relazione locale che esprime la potenza per unità di volume come prodotto scalare del campo elettrico per la densità di correnten=Ndτδℒ=NqeΔV=ndτqe⃗E⋅⃗vddt=⃗E⋅(nqe⃗vd)dτdt",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#23": "24Superconduttori\nLa resistività è funzione lineare della temperaturaρR=ρ0(1+αT)Alcuni metalli (Hg, Al, Pb, Ti, Zn, …) o altre leghe al di sotto di una temperatura critica Tc prossima allo zero assoluto (0°K=-273.15 °C) mostrano una resistività nullaIn tali condizioni di superconduttività, le correnti circolano senza dissipazione di energia e i superconduttori non si riscaldano, anche con correnti molto intense𝜌0,𝛼 costanti T temperatura in °K ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#24": "25Generatori di forza elettromotriceSi è  detto che per avere una corrente in un conduttore è necessario stabilire una differenza di potenziale in due punti del conduttore  Per loro natura i conduttori sono equipotenziali. Per forzare una d.d.p occorre connettere il conduttore ad un generatore di forza elettromotrice (o generatore elettrico)Consideriamo un semplice circuito formato da un generatore (pila o batteria) e da una resistenzaIn tale circuito la circuitazione del campo elettrico è diversa da zero (altrimenti non avremmo corrente)\nGeneratori Elettrici (II) • Segue che nel generatore ci debbono essere forze di natura non elettrica le quali non sono conservative e determinano il moto delle cariche: – Nelle pile e batterie avvengono reazioni chimiche di ossidoriduzione nelle quali è energeticamente favorito il movimento delle cariche contro il campo elettrico. – Nelle dinamo e negli alternatori è presente un campo magnetico che muove le cariche elettriche in direzione opposta al campo elettrico. – Nel generatore di Van der Graaf un’azione meccanica esterna trasporta le cariche elettriche in direzione opposta al campo elettrico. \n29Domenico Galli – Fisica Generale B – 3. Corrente Elettricaiiii!E!E1V1V2V2VG∮⃗E⋅d⃗l=∮ρR⃗𝚥⋅d⃗l≠0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#25": "26Generatori di forza elettromotriceNel circuito il campo avrà due componenti\nGeneratori Elettrici (II) • Segue che nel generatore ci debbono essere forze di natura non elettrica le quali non sono conservative e determinano il moto delle cariche: – Nelle pile e batterie avvengono reazioni chimiche di ossidoriduzione nelle quali è energeticamente favorito il movimento delle cariche contro il campo elettrico. – Nelle dinamo e negli alternatori è presente un campo magnetico che muove le cariche elettriche in direzione opposta al campo elettrico. – Nel generatore di Van der Graaf un’azione meccanica esterna trasporta le cariche elettriche in direzione opposta al campo elettrico. \n29Domenico Galli – Fisica Generale B – 3. Corrente Elettricaiiii!E!E1V1V2V2VG⃗E=⃗Es+⃗Em⃗Es⃗E=⃗Es+⃗EmAll’interno del generatore si deve aggiungere il campo elettromotore        (NON conservativo)⃗EmLe forze interna ai generatori sono non conservative (di natura chimica o altro). Il loro effetto è quello di trasportare e mantenere le cariche interne ad una differenza di potenziale 𝛥V=V2-V1Nei conduttori si avrà solo campo elettrostatico⃗Es",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#26": "27Generatori di forza elettromotrice∮⃗E⋅d⃗l=∮(⃗Es+⃗Em)⋅d⃗l=∮⃗Es⋅d⃗l+∮⃗Em⋅d⃗lV2V1⃗Es⃗Em⃗EsIl campo elettromotore è definito solo internamente al generatore, non è conservativo e la sua circuitazione è definita forza elettromotrice =ℰ\nℰ=∫21⃗Em⋅d⃗l=−∫21⃗ES⋅d⃗l=V2−V1=ΔVIn condizioni stazionarie (generatore non connesso al circuito) le cariche sono ferme, pertanto ∮(⃗Es+⃗Em)⋅d⃗l=0La forza elettromotrice è uguale alla differenza di potenziale (Tensione del generatore)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#27": "28Generatori di forza elettromotriceGeneratori ideali  • la tensione ai capi del generatore si mantiene costanteGeneratori reali  • la tensione ai capi del generatore presenta una caduta ohmica • occorre considerare la resistenza interna del generatore (in serie al circuito)+_\n+_simbolo circuitale generatore idealesimbolo circuitale generatore reale",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#28": "Prima legge di Kirchhoff (dei nodi)\n29+i1\nS1\n+i2-i3-i4-i5S\nS2S5S4S3In condizioni stazionarie (fissato un intervallo 𝛥t, la carica entrante deve bilanciare la carica uscente) ΦS(⃗𝚥)=∬S⃗𝚥⋅̂ndS=0=∑entrantiik−∑uscentiik∬Sk⃗𝚥k⋅̂nkdSk={+ikentrantenelnodo−ikuscentedalnodo∬S⃗𝚥⋅̂ndS=∑k∬Sk⃗𝚥k⋅̂nkdSkConsideriamo N fili che si congiungono in un nodo Siano Si le superfici di intersezione tra S e le sezioni dei fili \nPrima legge di Kirchhoff (dei nodi) In qualunque nodo di un circuito la corrente totale entrante è uguale alla corrente uguale uscente∑nodoik==∑entrantiik−∑uscentiik=0i1+i2−i3−i4−i5=0i1+i2=i3+i4+i5",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#29": "Legge di Ohm generalizzata\n30Prendiamo in considerazione un circuito aperto (ramo)iA+_+_BCDRR1R2𝓔1𝓔2Fissiamo arbitrariamente un verso di percorrenza della corrente ( es. da A verso D)In condizioni stazionare la corrente entrante in A è pari a quella uscente da DCiascun elemento del circuito è percorso dalla stessa corrente i (tutti gli elementi sono in serie)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#3": "4\n_\n_\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+Gli elettroni subiscono urti, cedendo energia cineticaModello di Drude-Lorentz\n⃗ae=qeme⃗EII principio dinamicaNell’intervallo di tempo tra due urti consecutivi l’elettrone si muove di moto uniformemente accelerato:Nell’urto sulle cariche positive, l’elettrone cede energia • l’elettrone rallenta • gli atomi del reticolo aumentano la loro energia vibrazionale (gli atomi del reticolo vibrano sempre a T>0° K)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#30": "Legge di Ohm generalizzata\n31A+_+_BCDRR1R2𝓔1𝓔2Applichiamo la prima legge di Ohm ai capi dei vari elementiVA-VB=Ri              Caduta di potenziale ai capi di RVB-VC=R1i-𝓔1       Caduta di potenziale ai capi di R1, il generatore 𝓔1 fa salire il potenzialeVC-VD=R2i+𝓔2     Caduta di potenziale ai capi di R2, il generatore 𝓔2 fa scendere il potenzialeVA-VD=Ri+R1i+R2i+𝓔2-𝓔1 =(R+R1+R2)i+𝓔2-𝓔1  Differenza di potenziale ai capi dell’intero ramo   ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#31": "Legge di Ohm generalizzata\n32A+_+_BCDRR1R2𝓔1𝓔2+_VA-VD+(𝓔1-𝓔2)=RTOT i “Fissato”  il verso della corrente, stabiliamo anche il verso in cui diminuisce il potenziale (le cariche positive della corrente fluisco dal potenziale maggiore a quello minore)Convenzione dei generatori in un circuito (segno della tensione) + se la corrente “entra” nel polo negativo ed “esce” dal polo positivo -  se la corrente “entra” nel polo positivo ed “esce” dal polo negativo +_+_",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#32": "Legge di Ohm generalizzata\n33\u0000V+XkEk=RTOTiLegge di Ohm su un ramo di un circuito apertoLa differenza di potenziale ai capi di un ramo aperto di un circuito sommata alle tensioni erogate dai generatori è uguale alla caduta di tensione sulla resistenza totale del ramoN.B. Se dai calcoli numerici:  • la corrente ha segno positivo, allora essa circola nello stesso verso che si era supposto inizialmente • la corrente ha segno negativo, allora essa circola nel verso opposto a quello che si era supposto inizialmente",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#33": "Seconda legge di Kirchhoff (delle maglie)\n34A+_+_BCDRR1R2𝓔1𝓔2RConsideriamo un ramo chiuso (maglia)Connettendo i due capi, la differenza di potenziale si annulla: 𝛥V=0La legge di Ohm viene riformulata:XkEk=RTOTi\nSeconda legge di Kirchhoff (o delle maglie) Su qualunque maglia di un circuito la caduta di potenziale è uguale  alla somma delle tensioni erogate dai generatori",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#34": "Circuiti ideali e reali\n35+_Circuiti elettrici costituiti da fili conduttori, resistenze, generatori e altri elementi collegati tra loroIn un circuito ideale, gli elementi hanno resistenza interna nulla (escluso resistenza)filo conduttoreresistenzacondensatoregeneratore+_In un circuito reale, gli elementi sono schematizzati introducendo elementi resistivi",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#35": "Circuiti RC in regime transitorio\n36+_T𝓔CRConsideriamo il circuito (ideale) formato da una resistenza, un condensatore a da un generatore di forza elettromotriceInizialmente l’interruttore T è aperto, il condensatore è scaricoAd un dato istante iniziale t=0 l’interruttore viene chiuso. Cosa succede nel circuito? Circola corrente? Potenziale ai capi di resistenza e condensatore? Carica sul condensatore?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#36": "Circuiti RC in regime transitorio\n37𝛥VC𝛥VR+_T𝓔CRLe differenze di potenziale ai capi di R e C varieranno al passare del tempo:ΔVR(t)=Ri(t)ΔVC(t)=Q(t)Ci(t) corrente che circola nel circuitoQ(t) carica sul condensatore all’istante t=0 il condensatore è scarico Q(0)=0Applicando la legge delle maglie: ℰ=ΔVR(t)+ΔVC(t)ℰ=Ri(t)+Q(t)C",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#37": "Circuiti RC in regime transitorio\n38𝛥VC𝛥VR+_T𝓔CRℰ=Ri(t)+Q(t)CLa carica Q(t) che dal generatore fluisce verso il condensatore è legata alla corrente che circola nel circuito dalla relazionei(t)=dQ(t)dtQ(t)=∫t0i(t′\u0000)dt′\u0000L’equazione delle maglie è a tutti gli effetti un’equazione integro-differenziale0=Rdidt+1CdQdt=Rdidt+iCderivando tutto rispetto a t",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#38": "390=Rdidt+1CdQdt=Rdidt+iCCircuiti RC in regime transitorio𝛥VC𝛥VR+_T𝓔CRdidt=−iRCRisolviamo per separazione delle variabiliEquazione omogenea in i(t)dii=−dtRC∫i(t)i(0)di′\u0000i′\u0000=−1RC∫t0dt′\u0000lni(t)i(0)=−tRCi(t)i(0)=e−tRCi(t)=i(0)e−tRCAll’instante iniziale si haℰ=Ri(0)+Q(0)Ci(0)=ℰRi(t)=ℰRe−tRCQ(0)=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#39": "Carica di un condensatore\n40i(t)=ℰRe−tRC𝛥VC𝛥VR+_T𝓔CRCorrente che circola nel circuito in funzione del tempo=ℰR[−RCexp(−t′\u0000RC)]t0=−ℰC(e−tRC−1)=ℰC(1−e−tRC)Q(t)=∫t0i(t′\u0000)dt′\u0000=∫t0ℰRexp(−t′\u0000RC)dt′\u0000Q(t)=ℰC(1−e−tRC)ΔVR(t)=ℰe−tRCDifferenza di potenziale ai capi della resistenzaCarica sul condensatore in funzione del tempoΔVC(t)=ℰ(1−e−tRC)Differenza di potenziale ai capi del condensatoreτ=RCCostante di tempo del circuito RC",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#4": "5Modello di Drude-LorentzSupponiamo che l’elettrone abbia un urto all’istante t. Sia  p(t)dt la probabilità di avere un urto in un intervallo [t, t+dt]. ⟨t⟩=∫∞0tp(t)dtIl tempo medio che intercorre tra due urti:nel tempo tra due urti consecutivi, la velocità aumenterà linearmente con l’accelerazione⃗ve=qeme⃗Etla velocità media di un elettrone sara:⟨⃗ve⟩=∫∞0⃗ve(t)p(t)dt=∫∞0qeme⃗Etp(t)dt=qeme⃗E⟨t⟩",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#40": "41Carica di un condensatore\nTransitori in un Circuito RC. Chiusura del Circuito (VI) 0fiR=itf!VRttf!VCtCfQ\n13!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito •!Supponiamo ora che, inizialmente, il deviatore si trovi nella posizione 1, con il condensatore C completamente carico (Q = Cf ). Supponiamo poi che a un certo istante, t = 0, il deviatore venga commutato nella posizione 0. •!Avremo l’equazione integrale: •!Derivando si ottiene l’equazione differenziale:   0=!VRt()+!VCt()==Rit()+1Ci\"t()d\"t0t#+Q0()C  0=Rdidtt()+1Cit()14!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (II) •!Risolvendo: •!All’istante iniziale si ha:   dii=!1RCdt\"d#i#ii0i$%&=!1RCd#t0t'\"ln#i()*+i0i=!1RC#t()*+0tlnii0=!tRC\"i=i0e!tRC  0=!VR0()+!VC0()=Ri0+f\"i0=#fR it()=!fRe!tRC15!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (III) •!Si ottiene inoltre: !VRt()=Rit()=\"fe\"t/RC!VCt()=1Ci#t()d#t0t$+!VC0()=\"1CfRe\"#t/RCd#t0t$+f==\"fRC\"RCe\"#t/RC%&'(0t+f=\"f1\"e\"t/RC()+f=fe\"t/RCQt()=C!VCt()=Cfe\"t/RC!VRt()=\"fe\"t/RC!VCt()=fe\"t/RCQt()=Cfe\"t/RC!VRt()t\"#$\"$$0!VCt()t\"#$\"$$0Qt()t\"#$\"$$016!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0ℰRi(t)=ℰRe−tRC\nTransitori in un Circuito RC. Chiusura del Circuito (VI) 0fiR=itf!VRttf!VCtCfQ\n13!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito •!Supponiamo ora che, inizialmente, il deviatore si trovi nella posizione 1, con il condensatore C completamente carico (Q = Cf ). Supponiamo poi che a un certo istante, t = 0, il deviatore venga commutato nella posizione 0. •!Avremo l’equazione integrale: •!Derivando si ottiene l’equazione differenziale:   0=!VRt()+!VCt()==Rit()+1Ci\"t()d\"t0t#+Q0()C  0=Rdidtt()+1Cit()14!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (II) •!Risolvendo: •!All’istante iniziale si ha:   dii=!1RCdt\"d#i#ii0i$%&=!1RCd#t0t'\"ln#i()*+i0i=!1RC#t()*+0tlnii0=!tRC\"i=i0e!tRC  0=!VR0()+!VC0()=Ri0+f\"i0=#fR it()=!fRe!tRC15!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (III) •!Si ottiene inoltre: !VRt()=Rit()=\"fe\"t/RC!VCt()=1Ci#t()d#t0t$+!VC0()=\"1CfRe\"#t/RCd#t0t$+f==\"fRC\"RCe\"#t/RC%&'(0t+f=\"f1\"e\"t/RC()+f=fe\"t/RCQt()=C!VCt()=Cfe\"t/RC!VRt()=\"fe\"t/RC!VCt()=fe\"t/RCQt()=Cfe\"t/RC!VRt()t\"#$\"$$0!VCt()t\"#$\"$$0Qt()t\"#$\"$$016!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0ΔVR(t)=ℰe−tRCΔVR(t)=ℰe−tRC\nTransitori in un Circuito RC. Chiusura del Circuito (VI) 0fiR=itf!VRttf!VCtCfQ\n13!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito •!Supponiamo ora che, inizialmente, il deviatore si trovi nella posizione 1, con il condensatore C completamente carico (Q = Cf ). Supponiamo poi che a un certo istante, t = 0, il deviatore venga commutato nella posizione 0. •!Avremo l’equazione integrale: •!Derivando si ottiene l’equazione differenziale:   0=!VRt()+!VCt()==Rit()+1Ci\"t()d\"t0t#+Q0()C  0=Rdidtt()+1Cit()14!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (II) •!Risolvendo: •!All’istante iniziale si ha:   dii=!1RCdt\"d#i#ii0i$%&=!1RCd#t0t'\"ln#i()*+i0i=!1RC#t()*+0tlnii0=!tRC\"i=i0e!tRC  0=!VR0()+!VC0()=Ri0+f\"i0=#fR it()=!fRe!tRC15!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (III) •!Si ottiene inoltre: !VRt()=Rit()=\"fe\"t/RC!VCt()=1Ci#t()d#t0t$+!VC0()=\"1CfRe\"#t/RCd#t0t$+f==\"fRC\"RCe\"#t/RC%&'(0t+f=\"f1\"e\"t/RC()+f=fe\"t/RCQt()=C!VCt()=Cfe\"t/RC!VRt()=\"fe\"t/RC!VCt()=fe\"t/RCQt()=Cfe\"t/RC!VRt()t\"#$\"$$0!VCt()t\"#$\"$$0Qt()t\"#$\"$$016!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0ℰΔVC(t)=ℰ(1−e−tRC)\nTransitori in un Circuito RC. Chiusura del Circuito (VI) 0fiR=itf!VRttf!VCtCfQ\n13!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito •!Supponiamo ora che, inizialmente, il deviatore si trovi nella posizione 1, con il condensatore C completamente carico (Q = Cf ). Supponiamo poi che a un certo istante, t = 0, il deviatore venga commutato nella posizione 0. •!Avremo l’equazione integrale: •!Derivando si ottiene l’equazione differenziale:   0=!VRt()+!VCt()==Rit()+1Ci\"t()d\"t0t#+Q0()C  0=Rdidtt()+1Cit()14!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (II) •!Risolvendo: •!All’istante iniziale si ha:   dii=!1RCdt\"d#i#ii0i$%&=!1RCd#t0t'\"ln#i()*+i0i=!1RC#t()*+0tlnii0=!tRC\"i=i0e!tRC  0=!VR0()+!VC0()=Ri0+f\"i0=#fR it()=!fRe!tRC15!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (III) •!Si ottiene inoltre: !VRt()=Rit()=\"fe\"t/RC!VCt()=1Ci#t()d#t0t$+!VC0()=\"1CfRe\"#t/RCd#t0t$+f==\"fRC\"RCe\"#t/RC%&'(0t+f=\"f1\"e\"t/RC()+f=fe\"t/RCQt()=C!VCt()=Cfe\"t/RC!VRt()=\"fe\"t/RC!VCt()=fe\"t/RCQt()=Cfe\"t/RC!VRt()t\"#$\"$$0!VCt()t\"#$\"$$0Qt()t\"#$\"$$016!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0Q(t)=ℰC(1−e−tRC)ℰC",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#41": "42Scarica di un condensatoreTCRSia dato un circuito formato da un condensatore e una resistenza Q(0)=Q0Inizialmente l’interruttore T è aperto, il condensatore è carico con Q(0)=Q0Calcoliamo quanto vale l’energia dissipata sulla resistenza i(t)=dQ(t)dtL’equazione della maglia alla chiusura dell’interruttore èΔVR(t)+ΔVC(t)=0Ri(t)+Q(t)C=0RdQdt+QC=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#42": "43Scarica di un condensatoreTCRQ(0)=Q0RdQdt+QC=0dQdt=−QRCdQQ=−dtRCPer separazione delle variabiliEq differenziale omogeneaQ(t)=Q0e−tRCCarica sul condensatorei(t)=−Q0RCe−tRC=−VcRe−tRCCorrente che circola nel circuitolnQ(t)Q(0)=−tRC",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#43": "44Scarica di un condensatore\nTransitori in un Circuito RC. Apertura del Circuito (IV) •!Si noti che la carica del condensatore diminuisce nel tempo tendendo al valore limite 0.  •!Commutando il deviatore su 0 si ottiene perciò la scarica del condensatore. \n17!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (V) 0fiR=!itf!RV!ttfCV!tCfQ\n18!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nhttp://campus.cib.unibo.it/2475/ Domenico Galli Dipartimento di Fisica domenico.galli@unibo.it http://www.unibo.it/docenti/domenico.galli https://lhcbweb.bo.infn.it/GalliDidattica \ni(t)=−VcRe−tRC\nTransitori in un Circuito RC. Apertura del Circuito (IV) •!Si noti che la carica del condensatore diminuisce nel tempo tendendo al valore limite 0.  •!Commutando il deviatore su 0 si ottiene perciò la scarica del condensatore. \n17!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nTransitori in un Circuito RC. Apertura del Circuito (V) 0fiR=!itf!RV!ttfCV!tCfQ\n18!Domenico Galli – Fisica Generale B – 5. Circuiti in Corrente Continua!RCf!+01RV!CV!iAMBt=0\nhttp://campus.cib.unibo.it/2475/ Domenico Galli Dipartimento di Fisica domenico.galli@unibo.it http://www.unibo.it/docenti/domenico.galli https://lhcbweb.bo.infn.it/GalliDidattica \nQ0Q(t)=Q0e−tRC\n−Q0RC=−VcR",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#44": "45Scarica di un condensatoreTCRQ(0)=Q0UR=∫∞0PRdt=∫∞0Ri2dt=∫∞0R[−VCRe−tRC]2dtL’energia dissipata è pari all’integrale della potenza nel tempo=12CV2C[−e−∞RC+e−0RC]=V2CR∫∞0e−2tRCdt=V2CR[−RC2e−2tRC]∞0UR=12CV2C=UCL’energia che era inizialmente accumulata nel condensatore viene interamente dissipata sulla resistenza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#45": "DRAM \n46\nIl condensatore può essere utilizzato come cella di memoria in alternativa al flip-flop Cella di memoria formata da condensatore + transistor Lo stato di carica del condensatore determina lo stato logico Il transistor è usato per pilotare la lettura/scrittura (funziona come un interruttore) Ad ogni lettura/scrittura, tutti i C di un array vengono ri-caricati/scaricatiLinea indirizziLinea dati",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#46": "DRAM \n47\nPRO economicità, alta densità, velocità di accesso ~10 ns (un po’ più lente delle SRAM)\nCONS carica sui C diminuisce nel tempo (effetti dissipativi) necessario un circuito di “refresh” che faccia delle letture/scritture “fittizie” (con frequenza del kHz)  Problemi in ambienti ad elevata radiazione (centrali nucleari, detector, spazio): particelle cariche da raggi cosmici o da decadimenti radioattivi possono alterare gli stati  logici",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#47": "Lorenzo Rinaldi Dipartimento di Fisica e Astronomia lorenzo.rinaldi@unibo.it https://www.unibo.it/sitoweb/lorenzo.rinaldi/\n48",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#5": "6Modello di Drude-LorentzVelocità media o velocità di deriva proporzionale al campo: stessa direzione e verso opposto gli elettroni possiedono anche una velocità dovuta all’agitazione termica, ma si può dimostrare che essa ha valore medio nullo (perché casuale in ogni direzione) Valori tipici: velocità di termica a temperatura ambiente ~100 km/s velocità di deriva: qualche mm/s⃗vd=⟨⃗ve⟩=qeme⃗E⟨t⟩",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#6": "7Intensità di correnteConsideriamo un conduttore all’interno del quale è mantenuta una differenza di potenziale VA-VB\nNel S.I. l’unità di misura della corrente è l’Ampere (A) (grandezza fisica fondamentale) \nSdqVAVBi=limΔt→0ΔqΔt=dqdtData una sezione S, interna al conduttore, definiamo la corrente elettrica come la quantità di carica che attraversa il conduttore per unità di tempo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#7": "8Intensità di correntei=dqdtLa carica che attraversa la superficie S è al netto delle cariche positive e negative Dal punto di vista sperimentale, in elettromagnetismo, il moto di una carica positiva è equivalente al moto di una carica negativa che procede in verso opposto\nSdq=dq++dq-VAVBConvenzione: verso positivo delle correnti quello in cui si muovono i portatori di carica positivi  la corrente ha verso opposto alla velocità di deriva degli elettroni",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#8": "9Intensità di corrente\n̂nα\ndSdS´VAVB⟨⃗v+⟩Consideriamo un tubo (di flusso) cilindrico di sezione infinitesimafissiamo il verso della corrente concorde alla velocità di derivain un intervallo dt, avremo una quantità di carica dq (positiva) che attraversa il volume d𝜏 delimitato dalle superfici orientate S e S´  n=Ndτnumero di portatori di carica N per unità di volumecarica elementare (positiva)d𝜏dq=Ne+=nq+edτdτ=[(⃗vd⋅̂n)dt]dS(⃗vd⋅̂n)dtaltezza del cilindretto obliquo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\11-correnti.pdf#9": "10Densità di correnteDefiniamo il vettore densità di corrente elettricai=dqdt=∬S⃗𝚥⋅̂ndSRiscriviamo la carica che attraversa il volume d𝜏\n̂nα\ndSdS´VAVB⃗𝚥d𝜏\nCorrente (infinitesima) che attraversa una superficie dSL’intensità di corrente è pari al flusso della densità di corrente attraverso la sezione del conduttore\nFlusso di un Campo Vettoriale (III) •!La quantità di fluido che ha attraversato nel tempo !t la sezione && del tubo è pari al volume di un cilindro avente la stessa base del tubo e un’altezza pari a v &t, cioè: •!Il flusso del fluido sarà pertanto: \n37!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!V=\"v!t!\"!v()=#V#t=\"v#t#t=\"v!!!v!vt=t0t=t0+!tv!tv!t!v!v!v!v\nFlusso di un Campo Vettoriale (IV) •!Possiamo anche esprimere il flusso ''  (v) utilizzando una sezione obliqua S invece che una sezione trasversale &. •!Si ha: \n38!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!=Scos\"!viˆn=!vˆn1\"cos\"=vcos\"#S!v()=!v=Svcos\"=!viˆnS!ˆn!Sr!v\nFlusso di un Campo Vettoriale (V) •!Consideriamo ora il caso in cui la velocità del fluido non è uniforme sulla sezione del tubo. –!È il caso, per esempio, di un flusso laminare di un fluido viscoso, per il quale la velocità al centro del tubo è maggiore della velocità in prossimità delle pareti. •!In tal caso scomponiamo il tubo in tanti tubicini di sezione trasversale infinitesima              . Il flusso attraverso una qualunque sezione di un tubicino infinitesimo vale: •!Il flusso totale si ottiene sommando il flusso attraverso un insieme di tubicini che coprono completamente la sezione del tubo: \n39!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v()=!viˆndSS\"\"d!=dScos\"d!!SdSd!!vd#dS!v()=vd!=vdScos\"=!viˆndS(volume di fluido che attraversa nell’unità di tempo la superficie S) \nFlusso di un Campo Vettoriale (VI) •!La superficie S potrebbe anche non essere piana, ma l’espressione: è ugualmente valida, in quanto le superfici infinitesime dS possono essere considerate piane e il prodotto scalare        tiene conto della loro inclinazione rispetto alla velocità. \n40!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v(volume di fluido che attraversa nell’unità di tempo la superficie S) \ndSdS!viˆnd!!S!v()=!viˆndSS\"\"\nFlusso di un Campo Vettoriale (III) •!La quantità di fluido che ha attraversato nel tempo !t la sezione && del tubo è pari al volume di un cilindro avente la stessa base del tubo e un’altezza pari a v &t, cioè: •!Il flusso del fluido sarà pertanto: \n37!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!V=\"v!t!\"!v()=#V#t=\"v#t#t=\"v!!!v!vt=t0t=t0+!tv!tv!t!v!v!v!v\nFlusso di un Campo Vettoriale (IV) •!Possiamo anche esprimere il flusso ''  (v) utilizzando una sezione obliqua S invece che una sezione trasversale &. •!Si ha: \n38!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!=Scos\"!viˆn=!vˆn1\"cos\"=vcos\"#S!v()=!v=Svcos\"=!viˆnS!ˆn!Sr!v\nFlusso di un Campo Vettoriale (V) •!Consideriamo ora il caso in cui la velocità del fluido non è uniforme sulla sezione del tubo. –!È il caso, per esempio, di un flusso laminare di un fluido viscoso, per il quale la velocità al centro del tubo è maggiore della velocità in prossimità delle pareti. •!In tal caso scomponiamo il tubo in tanti tubicini di sezione trasversale infinitesima              . Il flusso attraverso una qualunque sezione di un tubicino infinitesimo vale: •!Il flusso totale si ottiene sommando il flusso attraverso un insieme di tubicini che coprono completamente la sezione del tubo: \n39!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v()=!viˆndSS\"\"d!=dScos\"d!!SdSd!!vd#dS!v()=vd!=vdScos\"=!viˆndS(volume di fluido che attraversa nell’unità di tempo la superficie S) \nFlusso di un Campo Vettoriale (VI) •!La superficie S potrebbe anche non essere piana, ma l’espressione: è ugualmente valida, in quanto le superfici infinitesime dS possono essere considerate piane e il prodotto scalare        tiene conto della loro inclinazione rispetto alla velocità. \n40!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!S!v(volume di fluido che attraversa nell’unità di tempo la superficie S) \ndSdS!viˆnd!!S!v()=!viˆndSS\"\"\n⃗𝚥di=(dqdt)dS=⃗𝚥⋅̂ndSdq=nqe[(⃗vd⋅̂n)dt]dS=[(nqe⃗vd)⋅̂n]dSdt⃗𝚥=nqe⃗vd",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#0": "1 Cmpi magnetici stazionari CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#1": "2Fenomeni magneticiI fenomeni magnetici sono noti dall’antichità  (Talete, Archimede, Cinesi…) Il minerale magnetite (FeO+Fe2O3+FeO4) ha la capacità di attrarre oggetti contenenti ferro o materiali ferrosi\nEsistenza forze magnetiche\nLimatura di ferro vicino ad una calamita è attratta maggiormente dagli estremi (poli magnetici) in cui sembra si concentri la forza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#10": "Rotore e circuitazione del campo magnetico\n11\nLe linee del campo magnetico sono sempre chiuse  𝛤La circuitazione lungo una generica linea chiusa 𝛤 sarà in generale non nullaAnche il rotore del campo magnetico (thm Stokes) sarà in generale non nullo⃗∇∧⃗B≠0\nIl campo magnetico NON è conservativo∮Γ⃗B⋅d⃗l≠0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#11": "Magneti, cariche elettriche, correnti\n12Le forze a cui sono sottoposti gli aghi magnetici corrispondono a quelle dei dipoli elettrici, piuttosto che a quelle delle cariche singoleNon ci sono interazioni tra magneti e cariche elettriche fermeCosa succede se avviciniamo un magnete a delle cariche in movimento ? (filo percorso da corrente)\nConsideriamo un esperimento in cui colleghiamo un filo conduttore ad un generatore  di f.e.m. Poniamo un ago magnetico vicino al tratto di filo rettilineo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#12": "Esperimento di Oersted (1820)\n13\nA circuito aperto, l’ago non sente nessuna forza e resta fermoChiudendo il circuito, nel filo passa corrente e l’ago si orienta perpendicolarmente al filoInvertendo la polarità del generatore, l’ago ruota in senso opposto",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#13": "Esperimento di Faraday (1821)\n14\nPoniamo un filo conduttore in un campo magneticoSe nel conduttore passa corrente, esso sente una forzapossiamo bilanciare (→ misurare) la forza con dei pesi",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#14": "Esperimento di Ampère (1820)\n15\nEsperimento con due fili rettilinei e paralleli percorsi da corrente\nI fili si attraggono se le correnti hanno lo stesso versoI fili si respingono se le correnti hanno verso opposto",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#15": "Cariche in movimento e campi magnetici\n16Si osservano interazioni di tipo magnetico tra: •magneti •magneti e fili percorsi da corrente  •fili percorsi da correntePossiamo concludere che le correnti generano dei campi magneticiMa le correnti sono cariche in movimento\nI campi magnetici sono generati da cariche in movimento sia macroscopicamente (correnti) che microscopicamente (magneti)Domanda: cariche in movimento sono l’unico modo per generare campi magnetici?I movimenti possono essere anche microscopici (elettroni che orbitano attorno ai nuclei, all’interno delle calamite)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#16": "Magnetostatica nel vuoto\n17Consideriamo un circuito in corrente stazionaria i e consideriamo un piccolo tratto      che sia  •libero di muoversi su connessioni flessibili e mediante un dinamometro. •elettricamente neutro •orientato con il verso della corrente •immerso in un campo magnetico d⃗ld⃗FIl tratto di filo     subisce una forza     con le seguenti caratteristiche:d⃗l|d⃗F|∝i|d⃗l|d⃗F⊥d⃗ld⃗F=0Quando la corrente (    )  e il campo magnetico sono parallelid⃗l\niRdinamometro a molle𝓔(entrante nel piano)⃗Bd⃗l",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#17": "Seconda legge di Laplace\n18d⃗F=id⃗l∧⃗BUn tratto di filo percorso da corrente ed immerso in un campo di induzione magnetica subisce una forza descritta da:\nDefinizione operativa del campo induzione magnetica⃗Bla direzione ed il verso di       sono determinati dalla corrente che circola nel filod⃗l\nRegola della mano destra",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#18": "19Seconda legge di Laplaced⃗F=id⃗l∧⃗BCosa succede a livello microscopico nel filo?Una sezione dS del filo sarà attraversata da una densità di corrente di modulo j=i/dSid⃗l=idSdSd⃗l=⃗𝚥(dSdl)=⃗𝚥dτdi=⃗𝚥⋅̂ndS⃗𝚥 orientato come d⃗lForza magnetica sull’intero volume del filodτ volume infinitesimo⃗F=∫filo⃗𝚥∧⃗BdτForza magnetica su un volume infinitesimod⃗Fτ=⃗𝚥∧⃗Bdτ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#19": "Forza magnetica su cariche puntiformi\n20La densità di corrente era stata definita come:⃗𝚥=nq⃗vdn=Ndτ⃗vdNumero di portatori di carica per unità di volumevelocità di derivaLa forza magnetica per unità di volume diventad⃗Fτ=⃗𝚥∧⃗Bdτ=nq⃗vd∧⃗Bdτ=Nq⃗vd∧⃗B⃗F=q⃗v∧⃗BIn base a questa relazione, possiamo generalizzare al caso di una singola carica puntiforme q che in moto con velocità    , in presenza di un campo di induzione magnetica    , subisce una forza  (forza di Lorentz)  ⃗B⃗v",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#2": "Fenomeni magnetici\n3Le calamite esercitano forze tra di loro, che possono essere attrattive o repulsive In analogia con l’elettrostatica, possiamo introdurre la definizione di poli magnetici NORD e SUD La definizione deriva dal fatto che la Terra si comporta come una calamita in una calamita il polo sud si orienta verso il sud geografico e il polo nord con il nord terrestre\nS\nN",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#20": "Forza di Lorentz\n21⃗F=q⃗v∧⃗BMetodo alternativo per definire il campo induzione magnetica (usando una singola carica)Tale relazione è puntuale (vale in ciascun punto dello spazio) ed è più precisa della seconda legge di Laplace (definita su un tratto     )d⃗lDall’espressione della Forza di Lorentz ricaviamo che il campo magnetico ha le dimensioni di una forza su carica e velocità. Nel S.I. il campo magnetico si misura in Tesla (T)  1T=1V 1s/1m2 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#21": "Forza di Lorentz\n22⃗F=q⃗v∧⃗BIn presenza di un campo di induzione magnetica: •Cariche ferme non soggette a forza di Lorentz  •Cariche in movimento soggette a forza di Lorentz \nLa forza di Lorentz è sempre perpendicolare al campo induzione magnetica \nLa forza di Lorentz è sempre perpendicolare alla velocità  (centripeta)  \nLa forza di Lorentz non compie lavoro sulla carica in moto (è conservativa???)\ndirezione della forza data dalla regola della mano destraF=qvBsinαModulo della forza di Lorentz𝛼",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#22": "Forza di Lorentz generalizzata\n23⃗F=q⃗E+q⃗v∧⃗BSe sono presenti sia un campo elettrico che un campo magnetico la forza di Lorentz si scrive⃗F=q(⃗E+⃗v∧⃗B)Le cariche elettriche interagiscono con il campo elettrico ed il campo magnetico •Cariche ferme sentono solo gli effetti del campo elettrico •Cariche in moto sentono sia l’effetto del campo elettrico che del campo magneticoCosa succede che se cambiamo Sistema di Riferimento? (esempio, se scegliamo un SdR solidale con la carica in moto?)Le leggi della Fisica devono essere invarianti: non devono dipendere dal SdR.  Importante indizio del fatto che campo elettrico e campo magnetico sono strettamente legati: sono due aspetti della stessa entità fisica: il campo elettromagnetico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#23": "Moto di cariche in campi magnetici\n24Studiamo il moto di una carica q che si muove con velocità costante, perpendicolare ad un campo magnetico uniformeLa forza di Lorentz è ortogonale a velocità e campo magnetico. Forza e velocità sono complanari. ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⃗B⃗F⃗FPer calcolare il raggio di curvatura R dell’orbita ricordiamo che nella cinematica di un moto curvilineo l’accelerazione è⃗a=dvdt̂ut+v2R̂n⃗vcostanteForza centripeta: moto circolare uniforme⃗v⃗vR",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#24": "Moto di cariche in campi magnetici\n25⃗F=m⃗a=mv2R̂n⃗F=q⃗v∧⃗B=qvB̂n̂n=⃗v∧⃗Bvbsinα=1mv2R=qvBForza centripetaForza di Lorentzdirezione e verso della forza di LorentzLa forza di Lorentz è centripetaR=mvqBRaggio di curvatura⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⃗BR⃗F⃗F⃗v⃗v",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#25": "Moto di cariche in campi magnetici\n26Calcoliamo il periodo di rotazione⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ La velocità angolare (o frequenza angolare)ω=vR=vqBmv=qBm(T=2πω)Il periodo e la frequenza non dipendono né dal raggio né dalla velocità T=2πRv=2πvmvqB=2πmqB⃗F⃗F⃗v⃗v⃗BR",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#26": "Circuito in campo magnetico\n27Utilizziamo la seconda legge di Laplace per determinare le azioni meccaniche cui è soggetto un circuito percorso da corrente  immerso in un campo magnetico  d⃗F=id⃗l∧⃗BSupponiamo che il circuito sia: •rigido (non cambia forma) •la corrente i sia mantenuta costante da un generatore di f.e.m. (anche se vedremo che B tende a modificare la corrente nel circuito…)La forza totale sul circuitoIl momento (delle forze) totale sul circuito⃗rDistanza tre dl e il polo⃗F=i∮d⃗l∧⃗B⃗M=∮⃗r∧d⃗F=i∮⃗r∧(d⃗l∧⃗B)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#27": "Spira in un campo magnetico\n28Consideriamo il caso semplice di una spira rettangolare di lati a e b immersa in un campo induzione magnetica uniforme (diretto lungo l’asse z).⃗B̂n1ba234𝜃Calcoliamo la forza agente su ogni lato⃗F1=∫1id⃗l∧⃗B=∫1iBdl̂𝚥=ilB̂𝚥i⃗F1l=axy⃗F2=∫2id⃗l∧⃗B=∫2iBdl(−̂ı)=−ilB̂ıl=b⃗F3=∫3id⃗l∧⃗B=∫3iBdl(−̂𝚥)=−ilB̂𝚥=−⃗F1⃗F4l=al=b⃗Ftot=∑⃗Fi=0La forza totale sulla spira è nulla⃗F2⃗F3⃗F4=∫4id⃗l∧⃗B=∫4iBdl̂ı=ilB̂ı=−⃗F2z",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#28": "Spira in un campo magnetico\n29zSe il sistema è visto dall’alto:⃗B̂n1ba234𝜃ixyzTuttavia il momento delle forze sarà non nulloCalcoliamo il momento rispetto al centro della spira⃗r1⃗M1=⃗r1∧⃗F1=0⃗F1⃗r1//⃗F1⃗F4⃗r2⃗B\n̂n𝜃ixb/2b/2𝜃𝜃⃗M3=⃗r3∧⃗F3=0⃗r3//⃗F3⃗F2⃗M2=⃗r2∧⃗F2=(b2)(iaB)sinθ(̂𝚥)⃗M4=⃗r4∧⃗F4=(b2)(iaB)sinθ(̂𝚥)⃗r4⃗Mtot=⃗M1+⃗M2=i(ab)Bsinθ(̂𝚥)=iŜn∧⃗BIl momento delle forze è diretto verso l’alto (lungo asse di rotazione)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#29": "Momento magnetico di una spira\n30⃗M=i(ab)Bsinθ(̂𝚥)=iŜn∧⃗BIl momento delle forze è proporzionale alla corrente e al prodotto vettoriale tra superficie della spira orientata e campo induzione magneticaDefiniamo il momento magnetico della spira di area S percorsa da corrente i ⃗m=iŜnS=ab superficie della spiraè il versore normale alla spira orientato in verso tale che esso vede circolare la corrente in verso antiorario (regola della mano destra)̂n\n̂n⃗M=⃗m∧⃗BIl momento delle forze è uguale al prodotto vettoriale del momento magnetico della spira per il campo induzione magnetica(si dimostra che la relazione vale per spire di qualsiasi forma                     )d⃗m=îndS",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#3": "Interazioni tra calamite\n4\nPoli opposti (N-S) si attraggonoPoli stesso segno (N-N o S-S) si respingono",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#30": "Teorema di equivalenza di Ampère\n31\nUna spira percorsa da corrente immersa in un campo magnetico si comporta come un dipolo magnetico elementare (ago magnetico) di momento                , perpendicolare al piano della spira e orientato con la regola della mano destra⃗m=iŜnSulla spira agisce un momento di forze solo se il campo magnetico ed il momento della spira formano un angolo 𝜃≠0 La coppia di forze è nulla quando campo magnetico e momento magnetico sono allineati (𝜃=0)La relazione tra momento delle forze su una spira e campo induzione è analoga al dipolo elettrico immerso in un campo elettrico⃗M=⃗p∧⃗E",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#31": "Esempio\n32v\tv\tv\tv\tv\tv\tv\tv\tR1R2iFigure 13:5.6Un disco isolante di raggioRe spessore trascurabile, uniformemente carico con caricaQ, ruota con velocit` aangolare costante!attorno all’asse passante per il centro. Determinare:a) il campo magnetico~Bnel centro del disco;b) il momento magnetico~mdel disco.5.7Una spira quadrata di latoL= 10 cm, percorsa da una correntei=1.3 A in senso antiorario, ` e posta in unaregione di spazio dove ` e presente un campo magnetico uniforme avente intensit` aB=2.1 T. Sapendo che duelati della spira sono paralleli al campo magnetico, calcolare:a) la forza agente sulla spira;b) il momento delle forze~Magente sulla spira.5.8Si consideri il circuito rappresentato in ﬁgura 14. Il tratto semicircolare del circuito ` e immerso in un uncampo magnetico uniforme~B=B0ˆ|. Determinare la forza che agisce sul circuito.\nrεv\tv\tv\tv\tRxy\nFigure 14:6 Legge di Amp` ere6.1Determinare il campo magnetico generato da un solenoide cilindrico di raggioRcomposto danspire perunit` a di lunghezza, percorse da una corrente stazionariai.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#32": "Galvanometro\n33Il galvanometro è uno strumento utilizzato per misurare piccole intensità di corrente\nIl momento delle forze magnetiche sulla spira è bilanciato dal momento delle forze elasticheMolla a spiraleMmolla=−kαk costante elastica 𝛼 angolo di aperturaAll’equilibrio⃗Mmolla=⃗MMi=kαSBSistema fatto in modo che 𝜃≈90° 𝜃≈90° MM=−iSBsinθ≃−iSBMisura della corrente",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#33": "Motore elettrico\n34Il motore elettrico trasforma energia elettrica in energia meccanica\nLa spira percorsa da corrente è messa in rotazione dall’interazione con il campo magneticoPer mantenere la rotazione sempre nello stesso senso si usano delle spazzole in contatto sul commutatore per invertire il verso della corrente nella spira",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#34": "Altoparlante\n35\nimpulso elettrico modulato dalla frequenza sonoraIl filo è collegato rigidamente al cono di cartoneQuando nel filo passa corrente (variabile nel tempo, modulata sulla frequenza sonora), esso sente la forza magnetica e mette in vibrazione l’altoparlante La vibrazione del cono produce onde sonore (conversione di energia elettrica in energia meccanica delle onde sonore)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#35": "Prima legge di Laplace\n36\nEvidenze sperimentali mostrano che i fili percorsi da corrente generano campi magneticid⃗B=μ0i4πd⃗l∧̂urr2d⃗B=μ0i4πd⃗l∧⃗rr3Consideriamo un tratto di filo infinitesimo      percorso da corrente i. Sperimentalmente si osserva che il campo magnetico generato a distanza    vale: ⃗rd⃗ld⃗lentrante se     e nel piano del foglio⃗rd⃗lxyz⃗r⃗B⨂iLa costante 𝜇0 è la permeabilità magnetica del vuotoμ0=4π×10−7VsmA=4π×10−7NA2=4π×10−7HmHenry (H)  1H=1𝛺 1s",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#36": "Prima legge di Laplace\n37d⃗lxyz⃗Bi⨂⃗r−⃗r′\u0000⃗r⃗r′\u0000d⃗B=μ0i4πd⃗l∧(⃗r−⃗r′\u0000)(⃗r−⃗r′\u0000)3Notazione in forma più generale⃗B=μ04π∫lid⃗l∧(⃗r−⃗r′\u0000)(⃗r−⃗r′\u0000)3Il campo generato da un intero circuito l si ottiene integrandoVerifichiamo che il campo B è solenoidale⃗∇⋅⃗B=⃗∇⋅(μ0i4πd⃗l∧⃗rr3)=μ0i4π[(⃗∇∧d⃗l)⋅⃗rr3−d⃗l⋅(⃗∇∧⃗rr3)]=0⃗∇⋅(⃗A∧⃗B)=(⃗∇∧⃗A)⋅⃗B−⃗A⋅(⃗∇∧⃗B)=0  un vettore costante è irrotazionale=0  un vettore radiale è irrotazionale",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#37": "Prima legge di Laplace\n38|d⃗B|=μ0i4πdlsinθr2Modulo del campo magnetico:d⃗lxyz⃗r⃗B⨂iθL’angolo 𝜃 è tra la direzione della corrente e la posizione del punto in cui calcoliamo il campo magnetico se sin𝜃=0;180°  ⇒ dB=0: lungo la direzione della corrente non viene generato campo magnetico dB∝1r2come la legge di Coulomb per le cariche puntiformidB∝idipende dall’intensità della corrente, e quindi dal numero di portatori di caricadB⊥d⃗ldB⊥d⃗rperpendicolare al piano definito da corrente e posizione",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#38": "Legge di Biot-Savart\n39Determinare il campo induzione magnetica generato da un filo rettilineo di lunghezza indefinita, percorso da corrente i \n⃗B=μ0i2πr̂utLegge di Biot-Savart• il campo magnetico ha intensità inversamente proporzionale alla distanza dal filo • le linee di campo sono circonferenze nel piano trasverso al filo, centrate sul filo stesso • L’orientazione delle linee di campo segue la regola della mano destra ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#39": "Forza tra due fili percorsi da corrente\n40\nL’esperimento di Ampère evidenzia la forza tra due fili percorsi da correnteConsideriamo due fili rettilinei di lunghezza indefinita, paralleli, posti a distanza d, percorsi da correnti i1 e i2 (iniziamo con il caso di correnti equiverse)di1i2Il filo 1 genera a distanza d un campo magnetico ⃗B1⨂B1=μ0i12πdun tratto di filo dl2 sente una forza magneticad⃗F12=i2d⃗l2∧⃗B1d⃗l2(II Legge Laplace)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#4": "Campo magnetico\n5In analogia con l’elettrostatica, viene naturale introdurre un campo vettoriale      detto  campo magnetico⃗Bconvenzione: le linee di forza del campo magnetico entrano nel polo sud e escono dal polo nord Le linee di campo sono tangenti alla direzione lungo la quale si allineano gli aghi magneticiL’intensità è proporzionale al momento delle forze sull’ago\nS\nN\nS\nN\nS\nN\nS\nN\nS\nN\nS\nNIl campo      può essere anche definito come campo induzione magnetica⃗B",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#40": "41d⨂d⃗F12=i2d⃗l2∧⃗B1Forza tra due fili percorsi da correntedF12=i2dl2B1sinθ=i2dl2μ0i12πd𝜃=90°Forza sul filo 2:Modulo della forzadF12=μ02πi1i2ddl2i1i2Direzione di dF12: perpendicolare ai fili, diretta da 2 verso 1 Analogamente, sul filo 1dF21=μ02πi1i2ddl1d⃗F21=iid⃗l1∧⃗B2=−d⃗F21La forza dF21 esercitata dal filo 2 sul filo 1 è uguale e opposta (attrattiva)d⃗F12d⃗l2⃗B1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#41": "Forza tra due fili percorsi da corrente\n42d𝜃=90°i1i2⨂Se le correnti correnti i1 e i2 sono dirette in verso opposto le forze tra i fili saranno anch’esse opposte e repulsivePer il terzo principio della dinamica, le forze tra i due fili interagenti devono sempre essere uguali e opposteSu un tratto di filo L finito, basta integrare su dl|⃗F|=μ02πi1i2dLForza per unità di lunghezzadFdl=μ02πi1i2dd⃗F12d⃗l2⃗B1",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#42": "Definizione operativa della corrente\n43Un ampere è l'intensità di corrente elettrica che, se mantenuta in due conduttori lineari paralleli, di lunghezza infinita e sezione trasversale trascurabile, posti a un metro di distanza l'uno dall'altro nel vuoto, produce tra questi una forza pari a 2 × 10-7 N per ogni metro di lunghezza.Tale definizione fissa anche il valore di 𝜇0 . La definizione operativa della corrente (e quindi della carica elettrica) sono fatte attraverso la misura di una forza  ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#43": "Campi magnetici da cariche puntiformi in moto\n44d⃗B=μ0i4πd⃗l∧̂urr2Se esprimiamo la corrente in funzione della densità di corrente:id⃗l=idSdSd⃗l=⃗𝚥(dSdl)=⃗𝚥dτ⃗𝚥=nq⃗vdn=Ndτ⃗vdNumero di portatori di carica per unità di volumevelocità di deriva=μ04π⃗𝚥∧⃗rr3dτ=Nμ04πq⃗vd∧⃗rr3La prima legge di Laplace può essere riformulataCampo magnetico generato da N portatori di caricaUna singola carica in movimento genera un campo magnetico che a distanza r dalla carica vale⃗B=μ04πq⃗v∧⃗rr3",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#44": "Campi magnetici da cariche puntiformi in moto\n45⃗B=μ04πq⃗v∧⃗rr3Tale formula vale in un Sistema di Riferimento in cui q si muove con velocità vIn un SdR in cui la carica è ferma si ha che B=0 !!!Ricordiamo che a distanza r, la carica genera un campo elettrico⃗E=q4πε0⃗rr3Ammettendo che questa relazione sia valida anche per cariche in motoq⃗rr3=4πε0⃗E⃗B=μ04π⃗v∧q⃗rr3=⃗B=μ0ε0⃗v∧⃗E=1c2⃗v∧⃗Ec=1μ0ε0Chiara relazione tra campi elettrico e magnetico generati da una carica in motovelocità della luce nel vuoto…=3×108m/sμ04π⃗v∧⃗E4πε0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#45": "Campo da spira circolare\n46Determinare il campo induzione magnetica sull’asse di una spira circolare di raggio R percorsa da corrente i \nBz==μ0i2R2(R2+z2)3/2=μ02πm(R2+z2)3/2⃗m=iŜk=iπR2̂k",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#46": "Esempio\n474.3In uno spettrometro di massa (vedi ﬁgura 12), un fascio di ioni viene prima fatto passare attraverso unselettore di velocit` a, costituito da un condensatore piano che genera un campo elettrico~Euniforme nelladirezione ˆx, immerso in un campo magnetico uniforme~Bortogonale a ˆx.a) Se una caricaqviene lanciata in direzione ˆyin corrispondenza di un foro su una lamina che blocca lecariche, quale deve essere la sua velocit` ava\u0000nch´ e riesca a passare?b) Determinare a quale distanzaldal foro impattano gli ioni che passano attraverso i foro, attraversandola regione in cui ` e presente un campo magnetico~B0ortogonale alla direzione di moto .\nEvi✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ B B’ xyFigure 12:4.4Una striscia conduttrice di rame di sezione rettangolareab, cona= 1mm eb=3mm ` e disposta prependi-colarmente ad un campo di induzione magnetica di moduloB=2T. La striscia` e percorsa da una correntei= 50A. Sapendo che nel rame la densit` a degli elettroni liberi ` en=8.5⇥1023elettroni/m3, si calcoli ladi↵erenza di potenziale sui lati opposti della striscia.5 Magnetostatica nel vuoto5.1Calcolare il campo magnetico al centro di una spira quadrata di latoLpercorsa da una correntei.5.2Due ﬁli rettillinei percorsi entrambi da correnti di stessa intensit` ai1=i2=i, sono disposti paralleli all’asseydi un sistema di riferimento cartesiano e intersecano l’assexa distanze±adall’origine. Studiare il campomagnetico lungo gli assi cartesiani nei due casi in cui le correnti siano concordi e discordi.5.3Un nastro di lunghezza inﬁnita, spessore trascurabile e larghezzaa` e percorso da una corrente superﬁcialeuniformei. Determinare il valore del campo magnetico~Bin un punto a distanzaldal bordo, giacente sullostesso piano del nastro.5.4Calcolare il campo magnetico sull’asse di una spira circolare di raggioRpercorsa da correntei.5.5Si consideri il sistema rappresentato in ﬁgura 13. Determinare il valore del campo magnetico nel centro dellaspira circolare in funzione delle resistenzeR1,R2e della correnteiche circola sui rami esterni.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#47": "Esempio\n484.3In uno spettrometro di massa (vedi ﬁgura 12), un fascio di ioni viene prima fatto passare attraverso unselettore di velocit` a, costituito da un condensatore piano che genera un campo elettrico~Euniforme nelladirezione ˆx, immerso in un campo magnetico uniforme~Bortogonale a ˆx.a) Se una caricaqviene lanciata in direzione ˆyin corrispondenza di un foro su una lamina che blocca lecariche, quale deve essere la sua velocit` ava\u0000nch´ e riesca a passare?b) Determinare a quale distanzaldal foro impattano gli ioni che passano attraverso i foro, attraversandola regione in cui ` e presente un campo magnetico~B0ortogonale alla direzione di moto .\nEvi✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ ✕ B B’ xyFigure 12:4.4Una striscia conduttrice di rame di sezione rettangolareab, cona= 1mm eb=3mm ` e disposta prependi-colarmente ad un campo di induzione magnetica di moduloB=2T. La striscia` e percorsa da una correntei= 50A. Sapendo che nel rame la densit` a degli elettroni liberi ` en=8.5⇥1023elettroni/m3, si calcoli ladi↵erenza di potenziale sui lati opposti della striscia.5 Magnetostatica nel vuoto5.1Calcolare il campo magnetico al centro di una spira quadrata di latoLpercorsa da una correntei.5.2Due ﬁli rettillinei percorsi entrambi da correnti di stessa intensit` ai1=i2=i, sono disposti paralleli all’asseydi un sistema di riferimento cartesiano e intersecano l’assexa distanze±adall’origine. Studiare il campomagnetico lungo gli assi cartesiani nei due casi in cui le correnti siano concordi e discordi.5.3Un nastro di lunghezza inﬁnita, spessore trascurabile e larghezzaa` e percorso da una corrente superﬁcialeuniformei. Determinare il valore del campo magnetico~Bin un punto a distanzaldal bordo, giacente sullostesso piano del nastro.5.4Calcolare il campo magnetico sull’asse di una spira circolare di raggioRpercorsa da correntei.5.5Si consideri il sistema rappresentato in ﬁgura 13. Determinare il valore del campo magnetico nel centro dellaspira circolare in funzione delle resistenzeR1,R2e della correnteiche circola sui rami esterni.v\tv\tv\tv\tv\tv\tv\tv\tR1R2iFigure 13:5.6Un disco isolante di raggioRe spessore trascurabile, uniformemente carico con caricaQ, ruota con velocit` aangolare costante!attorno all’asse passante per il centro. Determinare:a) il campo magnetico~Bnel centro del disco;b) il momento magnetico~mdel disco.5.7Una spira quadrata di latoL= 10 cm, percorsa da una correntei=1.3 A in senso antiorario, ` e posta in unaregione di spazio dove ` e presente un campo magnetico uniforme avente intensit` aB=2.1 T. Sapendo che duelati della spira sono paralleli al campo magnetico, calcolare:a) la forza agente sulla spira;b) il momento delle forze~Magente sulla spira.5.8Si consideri il circuito rappresentato in ﬁgura 14. Il tratto semicircolare del circuito ` e immerso in un uncampo magnetico uniforme~B=B0ˆ|. Determinare la forza che agisce sul circuito.\nrεv\tv\tv\tv\tRxy\nFigure 14:6 Legge di Amp` ere6.1Determinare il campo magnetico generato da un solenoide cilindrico di raggioRcomposto danspire perunit` a di lunghezza, percorse da una corrente stazionariai.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#48": "Esempio\n49v\tv\tv\tv\tv\tv\tv\tv\tR1R2iFigure 13:5.6Un disco isolante di raggioRe spessore trascurabile, uniformemente carico con caricaQ, ruota con velocit` aangolare costante!attorno all’asse passante per il centro. Determinare:a) il campo magnetico~Bnel centro del disco;b) il momento magnetico~mdel disco.5.7Una spira quadrata di latoL= 10 cm, percorsa da una correntei=1.3 A in senso antiorario, ` e posta in unaregione di spazio dove ` e presente un campo magnetico uniforme avente intensit` aB=2.1 T. Sapendo che duelati della spira sono paralleli al campo magnetico, calcolare:a) la forza agente sulla spira;b) il momento delle forze~Magente sulla spira.5.8Si consideri il circuito rappresentato in ﬁgura 14. Il tratto semicircolare del circuito ` e immerso in un uncampo magnetico uniforme~B=B0ˆ|. Determinare la forza che agisce sul circuito.\nrεv\tv\tv\tv\tRxy\nFigure 14:6 Legge di Amp` ere6.1Determinare il campo magnetico generato da un solenoide cilindrico di raggioRcomposto danspire perunit` a di lunghezza, percorse da una corrente stazionariai.",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#49": "Flusso del campo magnetico\n50Abbiamo già dimostrato che il campo magnetico è solenoidale⃗∇⋅⃗B=0Per il teorema della divergenza, il flusso del campo magnetico attraverso una qualsiasi superficie chiusa sarà nullo: NON esistono cariche magnetiche isolateIl flusso attraverso una superficie aperta avrà un suo valore, non necessariamente nullo, ci torneremo in seguito…∬Saperta⃗B⋅̂ndS∬Schiusa⃗B⋅̂ndS=∭τ(S)⃗∇⋅⃗B=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#5": "6Campo magnetico\nLa limatura di ferro si orienta con il campo magnetico delle calamite\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#50": "Circuitazione del campo magnetico\n51Dal momento che le linee di forza del campo magnetico sono sempre chiuse, ci aspettiamo che la circuitazione può essere non-nullaSemplificando, calcoliamo la circuitazione del campo magnetico generato da un filo rettilineo indefinito percorso da corrente iCalcoliamo la circuitazione lungo una linea chiusa e orientata 𝛤 che concatena il filo. Su un tratto infinitesimo:îtrd𝜙=μ0i2πrrdϕ̂ut⋅d⃗l=rdϕProiezione su circonferenza (arco di circonferenza) ∮Γ⃗B⋅d⃗l=∫2π0μ0i2πdϕ=±μ0iSegno dipende da corrente (regola mano destra)𝛤d⃗l⃗B⋅d⃗l=μ0i2πr̂ut⋅d⃗l⃗B",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#51": "Circuitazione del campo magnetico\n52Se invece la linea chiusa 𝛤  non “concatena” il filo:i𝛤⃗B⋅d⃗l1=μ0i2πr1̂ut⋅d⃗l1=μ0i2πr1r1dϕd⃗l1⃗B⋅d⃗l2=μ0i2πr2̂ut⋅d⃗l2=μ0i2πr2r2(−dϕ)per ogni angolo d𝜙 ci saranno sempre due tratti precorsi in verso oppostole due proiezioni sottendono lo stesso angolo, quindi il contributo alla circuitazione è nullo∮Γ⃗B⋅d⃗l=0d𝜙d⃗l2⃗B⃗B",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#52": "Legge di Ampère\n53Il segno delle correnti si valuta usando la regola della mano destra, rispetto al verso di percorrenza della curva orientata 𝛤\nLa circuitazione del campo magnetico è proporzionale alla somma delle correnti concatenate ∮Γ⃗B⋅d⃗l=μ0conc∑kikLegge di Ampère fornisce un metodo per il calcolo del campo magnetico in particolari condizioni di simmetria",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#53": "54Circuitazione del campo magneticoPossiamo prendere un numero qualsiasi di correnti concatenate, su circuiti di forma arbitraria\n∮Γ1⃗B⋅d⃗l=0∮Γ2⃗B⋅d⃗l=μ0(i1−i2)∮Γ3⃗B⋅d⃗l=μ0(−i1+i2−i3)∮Γ1⃗B⋅d⃗l=μ0i1∮Γ2⃗B⋅d⃗l=μ0(−i2−i3)∮Γ3⃗B⋅d⃗l=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#54": "Densità di corrente concatenata\n55Consideriamo N fili, ciascuno di sezione Sk, percorsi da correnti ikCiascuna corrente può essere scritta in funzione della densità di corrente:ik=∬⃗𝚥k⋅̂nkdSk\ni1-i2i3𝛤SS1S2S3La somma delle correnti concatenate alla curva 𝛤 conc∑kik=conc∑k∬Sk⃗𝚥k⋅̂nkdSk=∬Sconc∑k⃗𝚥k⋅̂nkdSk=∬S⃗𝚥c⋅̂ndSDove S è una generica superficie orientata che ha per bordo 𝛤 e  jc è la densità di corrente concatenata ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#55": "56Il teorema di StokesConsideriamo una superficie S aperta orientata avente come bordo una linea chiusa orientata 𝛤 \n𝛤S(𝛤)̂n̂n\nSuperfici Aperte di R3 •!Nelle superfici aperte non si può distinguere la normale esterna dalla normale interna in un punto della superficie. •!Per convenzione, per calcolare i flussi attraverso una superficie chiusa, si utilizza l’orientamento     indicato dalla regola della mano destra sulla base dell’orientamento della linea di bordo: \n45!Domenico Galli – Fisica Generale B – 1. Elettrostatica!ˆn\nFlusso di un Campo Vettoriale attraverso una Superficie Chiusa e Orientabile •!Consideriamo ora il flusso della velocità di un fluido attraverso una superficie chiusa. •!Per semplicità consideriamo la superficie totale di un cubo. •!Consideriamo positivo il flusso uscente dal cubo e negativo il flusso entrante nel cubo. •!Se il fluido è incompressibile e non si sono al suo interno sorgenti (in cui si produce fluido) o pozzi (scarichi, in cui il fluido scompare), allora tanto fluido entra nel cubo quanto ne esce: –!Il flusso attraverso la superficie totale è nullo. –!Il cerchietto attorno al simbolo di integrale       indica che la superficie di integrazione è chiusa. 46!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!tot!v()=!viˆndSStot\"\"\"==!1!v()+!2!v()+!3!v()+!4!v()+!5!v()+!6!v()=0532146!!!\nFlusso di un Campo Vettoriale attraverso una Superficie Chiusa e Orientabile (II) •!Se il flusso attraverso la superficie totale è positivo: allora dentro il cubo è presente una sorgente che produce fluido. •!Se il flusso attraverso la superficie totale è negativo: allora dentro il cubo è presente un pozzo (cioè uno scarico) in cui il fluido scompare. 47!Domenico Galli – Fisica Generale B – 1. Elettrostatica!!tot!v()=!viˆndSStot\"\"\"==!1!v()+!2!v()+!3!v()+!4!v()+!5!v()+!6!v()>0532146\n!tot!v()=!viˆndSStot\"\"\"==!1!v()+!2!v()+!3!v()+!4!v()+!5!v()+!6!v()<0532146\nL’Operatore Divergenza \n48!Domenico Galli – Fisica Generale B – 1. Elettrostatica!•!Consideriamo una funzione vettoriale della posizione P: •!Si definisce l’operatore “divergenza” come: •!L’operatore divergenza si applica a una funzione vettoriale; il risultato è uno scalare: !!=ˆı\"\"x+ˆ!\"\"y+ˆk\"\"z!v=!vP()=!vx,y,z()=vxx,y,z()ˆı+vyx,y,z()ˆ!+vzx,y,z()ˆk\nEsempio:\"vx,y,z()=x2+y2()ˆı+x2+z2()ˆ!+zˆk!V\"\"i\"v()x,y,z()=2x+1!!!!i!v=div!v=\"vx\"x+\"vy\"y+\"vz\"z!!i!v=ˆı\"\"x+ˆ!\"\"y+ˆk\"\"z#$%&'(ivxˆı+vyˆ!+vzˆk()P!!3()\"v\"#\"\"vP()!VP!!3()\"$i\"v\"#\"\"\"$i\"v()P()!!%&'('𝛤∮Γ⃗F⋅d⃗lDefiniamo la circuitazione del campo lungo la linea chiusa orientata 𝛤 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#56": "57Il teorema di StokesIl flusso del rotore di un campo vettoriale attraverso una superficie S aperta e orientata è uguale alla circuitazione del campo vettoriale lungo il bordo 𝛤 di tale superficie∬S(Γ)(⃗∇∧⃗F)⋅̂ndS=∮Γ⃗F⋅d⃗lIl teorema di Stokes mette in relazione un integrale di superficie con un integrale di linea ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#57": "Il rotore del campo magnetico\n58Riscriviamo la legge di Ampère in funzione della densità di corrente concatenataconc∑kik=conc∑k∬Sk⃗𝚥k⋅̂nkdSk=∬Sconc∑k⃗𝚥k⋅̂nkdSk=∬S⃗𝚥c⋅̂ndSS è una generica superficie con bordo 𝛤 Applichiamo il teorema di Stokes:∮Γ⃗B⋅d⃗l=μ0conc∑kik=μ0∬S⃗𝚥C⋅̂ndS∮Γ⃗B⋅d⃗l=∬S⃗∇∧⃗B⋅̂ndS=μ0∬S⃗𝚥C⋅̂ndSL’uguaglianza è vera per qualsiasi superficie S con bordo 𝛤 \nLegge di Ampère in forma locale ⃗∇∧⃗B=μ0⃗𝚥In ogni punto dello spazio, il rotore del campo magnetico è proporzionale alla densità di corrente in quel punto",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#58": "Esempio\n59Calcolare il campo di un filo di lunghezza indefinita percorso da corrente i utilizzando la legge di Ampère",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#59": "Esempio\n60Determinare in tutto lo spazio il campo magnetico generato da un cilindro conduttore di raggio R e lunghezza indefinita percorso uniformemente da una corrente di intensità i ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#6": "Campo magnetico terrestre\n7\nUn ago magnetico libero di ruotare si orienta  con la linea di campo che esce dal polo nord (del magnete) e entra dal polo sud (del magnete)Ciò significa che la Terra si comporta come un magnete le cui linee di campo escono dal polo sud geografico ed entrano nel polo nord geografico.Convenzionalmente il polo nord magnetico coincide con il polo sud geografico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#60": "Esempio\n61v\tv\tv\tv\tv\tv\tv\tv\tR1R2iFigure 13:5.6Un disco isolante di raggioRe spessore trascurabile, uniformemente carico con caricaQ, ruota con velocit` aangolare costante!attorno all’asse passante per il centro. Determinare:a) il campo magnetico~Bnel centro del disco;b) il momento magnetico~mdel disco.5.7Una spira quadrata di latoL= 10 cm, percorsa da una correntei=1.3 A in senso antiorario, ` e posta in unaregione di spazio dove ` e presente un campo magnetico uniforme avente intensit` aB=2.1 T. Sapendo che duelati della spira sono paralleli al campo magnetico, calcolare:a) la forza agente sulla spira;b) il momento delle forze~Magente sulla spira.5.8Si consideri il circuito rappresentato in ﬁgura 14. Il tratto semicircolare del circuito ` e immerso in un uncampo magnetico uniforme~B=B0ˆ|. Determinare la forza che agisce sul circuito.\nrεv\tv\tv\tv\tRxy\nFigure 14:6 Legge di Amp` ere6.1Determinare il campo magnetico generato da un solenoide cilindrico di raggioRcomposto danspire perunit` a di lunghezza, percorse da una corrente stazionariai.\n⃗B=μonîk=μ0NLîk̂kCampo interno al solenoide (ideale)Esternamente il campo è nullo",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#61": "Esempio\n626.2Determinare il campo magnetico di un solenoide toroidale di raggio maggioreRe raggio internoa, dotatodiNspire percorse da una correntei.(B=µ0Ni2⇡r)6.3Un cavo coassiale di lunghezza indeﬁnita formato da due conduttori, il primo cilindrico di raggioae il secondocilindrico cavo di raggio internobe raggio esternoc. I due conduttori sono percorsi da correnti uniformi, diugual modulo e verso opposto. Tra i due conduttori vi ` e il vuoto. Determinare il campo induzione magneticain tutto lo spazio. (B(r<a)=µ0i2⇡a2r,B(a<r<b)=µ0i2⇡r,B(b<r<c)=µ0i2⇡r(c2\u0000r2)(c2\u0000b2),B(r>c) = 0)6.4Determinare il campo magnetico in tutto lo spazio generato da un cilindro indeﬁnito di raggioRpercorsoda una densit` a di corrente dipendente dalla distanza radiale dall’asse~|(r)=krˆnconkcostante, direttaparallelamente all’asse del cilindro. (B(r<R)=µ0k3r3,B(r>R)=µ0kR33r)6.5Sia dato un circuito composto da un generatore di f.e.m.Vcollegato in serie ad una resistenzaRed a uncondensatore di capacit` aC. Inizialmente il circuito ` e aperto ed il condensatore e scarico. Alla chiusuradel circuito, determinare la corrente di spostamento all’interno del condensatore, in funzione del tempo.(IS=VRe\u0000tRC)6.6Su un condensatore piano, con armature circolari di raggioR=40cm e distanti tra loroh=1cm, viene applicatauna d.d.p. variabile secondo la leggeV(t)=V0sin(2⇡⌫t), conV0=50v e⌫=6MHz,tespresso in secondi.Trascurando gli e↵etti di bordo, calcolare:a) il valore massimo del campo elettrico nel condensatore; (Emax=V0/h)b) il valore massimo della corrente di spostamento; (Is,max=\"0⇡2R2⌫V0/h)c) il valore massimo del campo magnetico indotto all’interno del condensatore alla distanzar=10 cmdall’asse centrale del condensatore. (Bmax=µ0\"0r⇡⌫V0/h)7 Induzione magnetica e legge di Faraday-Neumann-Lenz7.1Un circuito rigido ` e costituito da un ﬁlo conduttore, di resistenzaR=5⌦, rivestito di materiale isolantepiegato a forma di “8” su un piano (vedi ﬁg. 15). L’areaSdella superﬁcie piana delimitata dal ﬁlo ` e ugualealla somma dell’area della prima ansaS1=20cm2e di quella della secondaS2=12 cm2.I lc i r c u i t o ` ei m m e r s oin un campo magnetico uniforme, diretto perpendicolarmente al piano della spira, con verso entrante nelpiano e variabile nel tempo secondo la leggeB=kt, conk=0.04 T/s. Calcolare la corrente indotta nelcircuito, indicando il verso di percorrenza. (i=kR(S2\u0000S1), verso orario inS1)7.2Un solenoide cilindrico di raggior0= 3cm e lunghezzad=100cm ` e costituito daN= 50000 spire percorseda una corrente variabile nel tempo secondo la leggei(t)=i0e\u0000t/⌧, coni0= 50A e⌧= 5s. Si consideri unaspira circolare di raggiorer e s i s t e n z aR=0.5⌦, con piano perpendicolare all’asse del solenoide e centro sutale asse. Nell’approssimazione di solenoide indeﬁnito e trascurando gli e↵eti di autoinduzione della spira,determinare la correntei0indotta nella spira al tempot= 1s per due valori del raggio della spirar=1cm er=5cm. (i0(r<r0)=µ0N⇡ri0e\u0000t/⌧dR⌧),i0(r>r0)=µ0N⇡r0i0e\u0000t/⌧dR⌧))\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#62": "Magnetismo nella materia\n63Cosa succede se riempiamo la parte interna di un solenoide rettilineo ideale?Si osserverà una variazione del campo magnetico: alluminio, platino, sodio: leggero aumento   → materiali PARAMAGNETICIferro, nichel, cobalto: considerevole aumento → materiali FERROMAGNETICI\nmateriali organici, rame, argento: leggera diminuzione  → materiali DIAMAGNETICII materiali ferromagnetici restano magnetizzati ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#63": "64Momento magnetico orbitalePer spiegare il magnetismo nella materia occorre partire dalla struttura microscopicaIn un modello molto semplificato possiamo pensare gli elettroni più esterni in rotazione intorno al nucleoPartiamo dal caso più semplice: l’atomo di idrogenoEssendo la forza coulombiana centripetaricaviamo la velocità di rotazione:14πε0qeqpr2H=mev2rHRH=5.3×10−11m\nRH=5.3×10−11mme=9.1×10−31kg|qe|=|qp|=1.6×10−19Cv=14πε0qeqpmerH=2.2×106m/s",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#64": "65Momento magnetico orbitalepossiamo pensare che un elettrone che ruota intorno al nucleo genera una correnteil momento magnetico dell’elettronei=−qeTdove il periodoT=2πRHvdefinendo il momento angolare orbitale:⃗po=⃗RH∧me⃗vmo=iS=iπR2H=−qev2πRHπR2H=−qev2RHmememo=iS=iπR2H=−qev2πRHπR2H==−qev2RHmeme⃗mo=−qe2me⃗po",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#65": "Momento magnetico di spin\n66Oltre al momento magnetico orbitale, si può osservare che gli elettroni hanno un ulteriore momento magnetico di spin (come se gli elettroni ruotassero intorno al proprio asse)⃗ms=−qeme⃗psIl momento magnetico totale (o intrinseco) è dato da una combinazione di momento orbitale e momento di spin. L’accoppiamento è descritto dalle leggi della meccanica quantisticaIn un generico atomo, il momento magnetico dipende dagli elettroni più esterniIn assenza di campi magnetici esterni, il momento magnetico totale macroscopico è nullo, perché i momenti magnetici degli atomi sono orientati casualmente e la loro somma vettoriale è nulla\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#66": "Materiali diamagnetici\n67In presenza di campo magnetico esterno occorre fare delle distinzioniLa maggior parte dei materiali ha atomi con momento magnetico nullo. In questi materiali, l’effetto di un campo esterno è quello di deviare la traiettoria degli elettroni in moto (forza di Lorenz), inducendo una variazione di velocità (l’elettrone si allontana dal nucleo) e quindi una diminuzione della frequenza di rotazione (precessione di Larmor)L’effetto complessivo è una diminuzione del momento magnetico, che va ad opporsi leggermente al campo magnetico esternoTali materiali sono chiamati diamagnetici (in genere hanno un numero pari di elettroni e struttura simmetrica)\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#67": "Materiali paramagnetici\n68I materiali paramagnetici hanno atomi con momento angolare intrinseco diverso da zero\nI materiali paramagnetici sono caratterizzati da un numero dispari di elettroni  o da strutture atomiche asimmetricheGli atomi si comportano come dipoli magnetici che per effetto di un campo magnetico esterno tendono ad allinearsi  con il campo magnetico esterno, contribuendo ad aumentarne leggermente il valore\nB",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#68": "Vettore magnetizzazione\n69Definiamo il vettore magnetizzazione come il prodotto del momento angolare intrinseco medio del materiale per il numero di atomi per unità di volume⟨⃗m⟩⃗M=n⟨⃗m⟩=Ndτ⟨⃗m⟩dipende dai momenti magnetici orbitali e di spinIl campo magnetico totale nella materia dipenderà dal vettore magnetizzazione:⃗B=⃗B0+μ0⃗MPossiamo definire la densità di corrente di magnetizzazione⃗jM=⃗∇∧⃗MDa cui ricaviamo le relazione⃗∇∧⃗B=⃗∇∧(⃗B0+μ0⃗M)==⃗∇∧⃗B0+μ0⃗∇∧⃗M=Legge di Ampère in forma locale=μ0⃗J+μ0⃗JM",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#69": "Il vettore H\n70⃗∇∧⃗B=μ0(⃗J+⃗JM)Il campo magnetico totale B è generato dalle correnti di conduzione e dalle correnti di magnetizzazione\nPartendo dalla relazione⃗∇∧⃗B=μ0⃗J+μ0⃗∇∧⃗M⃗∇∧(⃗B−μ0⃗Mμ0)=⃗JDefiniamo il vettore H che descrive il campo magnetico nella materia,  in funzione solo delle correnti di conduzione lungo i fili⃗H=⃗B−μ0⃗Mμ0=⃗Bμ0−⃗M⃗∇∧⃗H=⃗J⃗B=μ0⃗H+μ0⃗M",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#7": "Poprietà del campo magnetico\n8La forza magnetica tra due calamite potrebbe essere descritta con una formula simile alla legge di Coulomb (fine 1700)⃗FM=kMm1m2r2̂urm1 e m2 sono le “cariche magnetiche” kM è una costante magneticaLa forza magnetica è proporzionale al prodotto delle cariche magnetiche ed inversamente proporzionale al quadrato della distanza Attrattiva per cariche magnetiche opposte, repulsiva per cariche magnetiche ugualiUnica analogia con forza elettrostatica di Coulomb!!",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#70": "Magnetismo nella materia\n71Nei materiali diamagnetici e paramagnetici, omogenei e isotropi i campi B H e M sono paralleli, e vengono espressi dalle relazioni⃗B=μrμ0⃗HDove:μrPermeabilità magnetica relativa⃗M=(μr−1)⃗H=χm⃗Hχm=(μr−1)Suscettività magnetica   {negativa per diamagneticipositiva per paramagnetici(molto piccola 10-4 —10-6)⃗M=(1μ0−1μ0μr)⃗B",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#71": "Materiali ferromagnetici\n72I materiali ferromagnetici microscopicamente hanno una configurazione elettronica per cui si creano forti interazioni tra momenti orbitali e momenti di spinTali interazioni comportano che momenti magnetici di atomi adiacenti si “accoppiano”, aumentando considerevolmente il loro effetto magnetico rispetto al singolo atomoAll’interno del materiale si creano regioni formate da numerosi dipoli allineati (domini di Weiss) I domini di Weiss hanno tipicamente volumi di 10-12 —10-12 m3 e contengono 1017 —1011 atomi \nSe il materiale non ha subito magnetizzazione, le direzioni dei momenti sono casuali",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#72": "73Materiali ferromagneticiQuando un materiale ferromagnetico viene posto in un campo magnetico esterno i momenti si allineano con il campo magnetico, generando un allargamento (una fusione) dei domini di Weiss\nPonendo campi magnetici sempre più intensi, si arriva ad una condizione di saturazioneIl materiale mantiene una magnetizzazione residua anche fuori dal campo magneticoI domini di Weiss vengono distrutti se il materiale viene riscaldato fino ad una temperatura critica (di Curie),che per il Fe vale ~1000°K",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#73": "Ciclo di isteresi\n74Per i materiali ferromagnetici la permeabilità magnetica non è costante, può essere molto elevata e dipende dalle correnti che generano il campo esterno e dalla storia di magnetizzazione. Inseriamo un cilindro di materiale ferromagnetico in un solenoide:\nLa curva a è detta di prima magnetizzazionediminuendo il campo H fino ad azzerarlo (curva b) nel materiale si ha una magnetizzazione residuaInvertendo il campo H, si raggiunge un valore critico HC per cui la magnetizzazione è nulla H generato da corrente nel solenoideCampo B del ferromagneteCampo M del ferromagnete",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#74": "Memorie di massa magnetiche\n75I supporti magnetici sono largamente utilizzati per l’archiviazione dei datiPer esempio gli hard disk sono formati da dischi di alluminio o vetro rivestiti da una pellicola di materiale ferromagneticoLa memorizzazione dell’informazione avviene associando un bit di magnetizzazione (verso di magnetizzazione) su un certo numero di domini di WeissLa densità di informazione è data dal numero di domini di Weiss che costituiscono un singolo bit, moltiplicato per la loro estensione superficiale media, rapportato alla superficie di archiviazione disponibileL’accesso ai dati avviene utilizzando testine magnetoresistive che variano la resistenza al variare del campo magnetico (in lettura) e viceversa (in scrittura)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#75": "Lorenzo Rinaldi Dipartimento di Fisica e Astronomia lorenzo.rinaldi@unibo.it https://www.unibo.it/sitoweb/lorenzo.rinaldi/\n76",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#8": "Poli magnetici\n9\nSperimentalmente, se si spezza una calamita si otterranno due nuove calamiteI poli magnetici esistono sempre a coppie di eguale valore e segno opposto: dipoli magneticiFino ad ora non è stato mai osservato un polo magnetico isolato (monopolo magnetico)Conseguenza: il campo magnetico ha proprietà molto diverse dal campo elettrostatico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\12-magnetostatica.pdf#9": "Legge di Gauss per il campo magnetico\n10\nLe linee del campo magnetico sono sempre chiuse  (non possiamo isolare singoli poli magnetici)Il campo magnetico è solenoidale⃗∇⋅⃗B=0Di conseguenza (thm divergenza) scegliendo una qualunque superficie chiusa∬S⃗B⋅̂ndS=0La densità volumetrica di cariche magnetiche è sempre nulla (solo dipoli)S",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#0": "1 Campi elettrici e magnetici variabili nel tempo CdS Ingegneria Informatica A.A. 2019/20 ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#1": "Campi elettrici in condizioni stazionarie\n2⃗∇⋅⃗E=ρε0Legge di Gauss per il campo elettrico Forma integrale: il flusso del campo elettrico attraverso una superficie chiusa è proporzionale alla carica elettrica contenuta nella superficie Forma differenziale: Le cariche elettriche generano il campo elettricoΦS(⃗E)=∬S⃗E⋅̂ndS=QSε0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#10": "Legge di Ampère e equazione di continuità\n11⃗∇⋅⃗𝚥+∂ρ∂t=0La divergenza della densità di corrente compare nell’equazione di continuità (si veda cap. su correnti)In condizioni stazionarie (indipendenti dal tempo), la densità di carica è costante e la sua derivata è nulla.  In tale situazione , quindi ritroviamo che la legge di Ampère continua ad essere valida in condizioni stazionarie. Cosa succede nel caso più generale, in condizioni non necessariamente stazionarie?⃗∇⋅⃗𝚥=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#11": "Legge di Ampère in condizioni non stazionarie\n12⃗∇⋅⃗𝚥+∂ρ∂t=0Sostituiamo nell’equazione di continuità e invertiamo gli ordini di derivazioneDalla legge di Gauss in forma locale                             ricaviamo  ⃗∇⋅⃗E=ρε0ρ=ε0⃗∇⋅⃗E⃗∇⋅⃗𝚥+∂∂t(ε0⃗∇⋅⃗E)=0⃗∇⋅⃗𝚥+ε0⃗∇⋅∂⃗E∂t=0⃗∇⋅(⃗𝚥+ε0∂⃗E∂t)=0il vettore  ha sempre divergenza nulla!(⃗𝚥+ε0∂⃗E∂t)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#12": "tale vettore è la somma di due termini: • densità di corrente di conduzione    (dovuta a cariche in moto) • densità di corrente di spostamento              (dovuta a variazione di campo elettrico)Legge di Ampère-Maxwell\n13(⃗𝚥+ε0∂⃗E∂t)⃗𝚥ε0∂⃗E∂t⃗∇∧⃗B=μ0⃗𝚥+μ0ε0∂⃗E∂tAggiungendo il termine di densità di corrente di spostamento nell’equazione di Ampere, otteniamo la legge di Ampère-Maxwell",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#13": "Legge di Ampère-Maxwell\n14⃗∇∧⃗B=μ0⃗𝚥+μ0ε0∂⃗E∂tIl rotore del campo magnetico è proporzionale alla somma della densità di corrente di conduzione  e alla variazione del campo elettrico⃗∇⋅(μ0⃗𝚥+μ0ε0∂⃗E∂t)=0In altre parole, il campo magnetico può essere generato da cariche in moto e da campi elettrici variabili nel tempoLa legge di Ampère-Maxwell è valida sempre, sia in regime stazionario che non stazionario. Infatti la divergenza della somma dei termini di densità di corrente di spostamento e conduzione è sempre nulla",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#14": "Corrente di spostamento\n15A partire dalla densità di corrente di spostamento ⃗𝚥S=ε0∂⃗E∂tDefiniamo la corrente di spostamento come il flusso della densità di corrente attraverso una superficie aperta S: La corrente di spostamento è proporzionale alla variazione del flusso del campo elettrico e non dipende da cariche in movimento.Si osserva una corrente di spostamento nelle regioni di spazio in cui c’è un campo elettrico variabile. Esempio: all’interno di un condensatore in regime transotorio (carica/scarica)is=∬S⃗𝚥s⋅̂ndS=∬Sε0∂⃗E∂t⋅̂ndS=ε0ddt∬S⃗E⋅̂ndS=ε0dΦS(⃗E)dt",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#15": "Legge di Ampère-Maxwell in forma integrale\n16In forma integrale, la circuitazione del campo magnetico lungo una linea chiusa rimane proporzionale alla somma delle correnti concatenate, considerando sia le correnti di conduzione che le correnti di spostamento∮Γ⃗B⋅d⃗l=μ0conc∑k(ic+is)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#16": "Corrente di spostamento in condensatore\n17Consideriamo un circuito RC in fase di scarica. Inizialmente sul condensatore si ha una carica Q0.  Alla chiusura dell’interruttore la carica sul condensatore varia con la legge:TCRQ(0)=Q0Q(t)=Q0e−tRCNel circuito si avrà una corrente di conduzione (dovuta alle cariche che fuoriescono dal condensatore):ic(t)=dQ(t)dt=−Q0RCe−tRC",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#17": "Corrente di spostamento in condensatore\n18Q(t)Supponiamo che il condensatore sia a facce piane e parallele. Nel condensatore carico con carica Q(t) c’è un campo elettrico (normale alla superficie S delle armature):Il campo elettrico dipende dal tempo, quindi nel condensatore si ha una densità di corrente di spostamento:  ⃗E=σε0̂n=Q(t)ε0Ŝn=Q0e−tRCε0Ŝn⃗𝚥s=ε0∂⃗E∂t=ε0∂∂t(Q0e−tRCε0S)̂n=−Q0SRCe−tRĈned una corrente di spostamento:is(t)=∬S⃗𝚥s⋅̂ndS=∬S−Q0SRCe−tRĈn⋅̂ndS=−Q0SRCe−tRC∬SdS=−Q0RCe−tRC⃗E",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#18": "Corrente di spostamento in condensatore\n19ic(t)=is(t)=−Q0RCe−tRCNell’esempio del condensatore si trova che la corrente di conduzione e la corrente di spostamento hanno lo stesso valoreTale risultato è conseguenza dell’equazione di continuità, che lega le correnti alle variazioni di carica.𝛴⃗Eic(t)is(t)\nS1S2𝛤Se infine poniamo 𝛴=S1+S2, ritroviamo la validità generale della legge di Ampère (Maxwell) Considerando una qualsiasi superficie chiusa 𝛴 che “avvolge” metà condensatore∮Γ⃗B⋅d⃗l=μ0∬S1⃗𝚥c⋅̂ndS=μ0∬S2⃗𝚥s⋅̂ndS∬Σ(⃗𝚥c+⃗𝚥s)⋅̂ndS=∬Σ(⃗𝚥c⋅̂n+⃗𝚥s⋅̂n)dS=∬Σ(jc−js)dS=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#19": "Esempio\n206.2Determinare il campo magnetico di un solenoide toroidale di raggio maggioreRe raggio internoa, dotatodiNspire percorse da una correntei.(B=µ0Ni2⇡r)6.3Un cavo coassiale di lunghezza indeﬁnita formato da due conduttori, il primo cilindrico di raggioae il secondocilindrico cavo di raggio internobe raggio esternoc. I due conduttori sono percorsi da correnti uniformi, diugual modulo e verso opposto. Tra i due conduttori vi ` e il vuoto. Determinare il campo induzione magneticain tutto lo spazio. (B(r<a)=µ0i2⇡a2r,B(a<r<b)=µ0i2⇡r,B(b<r<c)=µ0i2⇡r(c2\u0000r2)(c2\u0000b2),B(r>c) = 0)6.4Determinare il campo magnetico in tutto lo spazio generato da un cilindro indeﬁnito di raggioRpercorsoda una densit` a di corrente dipendente dalla distanza radiale dall’asse~|(r)=krˆnconkcostante, direttaparallelamente all’asse del cilindro. (B(r<R)=µ0k3r3,B(r>R)=µ0kR33r)6.5Sia dato un circuito composto da un generatore di f.e.m.Vcollegato in serie ad una resistenzaRed a uncondensatore di capacit` aC. Inizialmente il circuito ` e aperto ed il condensatore e scarico. Alla chiusuradel circuito, determinare la corrente di spostamento all’interno del condensatore, in funzione del tempo.(IS=VRe\u0000tRC)6.6Su un condensatore piano, con armature circolari di raggioR=40cm e distanti tra loroh=1cm, viene applicatauna d.d.p. variabile secondo la leggeV(t)=V0sin(2⇡⌫t), conV0=50v e⌫=6MHz,tespresso in secondi.Trascurando gli e↵etti di bordo, calcolare:a) il valore massimo del campo elettrico nel condensatore; (Emax=V0/h)b) il valore massimo della corrente di spostamento; (Is,max=\"0⇡2R2⌫V0/h)c) il valore massimo del campo magnetico indotto all’interno del condensatore alla distanzar=10 cmdall’asse centrale del condensatore. (Bmax=µ0\"0r⇡⌫V0/h)7 Induzione magnetica e legge di Faraday-Neumann-Lenz7.1Un circuito rigido ` e costituito da un ﬁlo conduttore, di resistenzaR=5⌦, rivestito di materiale isolantepiegato a forma di “8” su un piano (vedi ﬁg. 15). L’areaSdella superﬁcie piana delimitata dal ﬁlo ` e ugualealla somma dell’area della prima ansaS1=20cm2e di quella della secondaS2=12 cm2.I lc i r c u i t o ` ei m m e r s oin un campo magnetico uniforme, diretto perpendicolarmente al piano della spira, con verso entrante nelpiano e variabile nel tempo secondo la leggeB=kt, conk=0.04 T/s. Calcolare la corrente indotta nelcircuito, indicando il verso di percorrenza. (i=kR(S2\u0000S1), verso orario inS1)7.2Un solenoide cilindrico di raggior0= 3cm e lunghezzad=100cm ` e costituito daN= 50000 spire percorseda una corrente variabile nel tempo secondo la leggei(t)=i0e\u0000t/⌧, coni0= 50A e⌧= 5s. Si consideri unaspira circolare di raggiorer e s i s t e n z aR=0.5⌦, con piano perpendicolare all’asse del solenoide e centro sutale asse. Nell’approssimazione di solenoide indeﬁnito e trascurando gli e↵eti di autoinduzione della spira,determinare la correntei0indotta nella spira al tempot= 1s per due valori del raggio della spirar=1cm er=5cm. (i0(r<r0)=µ0N⇡ri0e\u0000t/⌧dR⌧),i0(r>r0)=µ0N⇡r0i0e\u0000t/⌧dR⌧))",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#2": "Campi elettrici in condizioni stazionarie\n3Conservatività del campo elettrostatico⃗∇∧⃗E=0La circuitazione del campo elettrostatico lungo qualsiasi linea chiusa è nullaIrrotazionalità del campo elettrostatico: implica che il campo è conservativo e che possiamo definire il potenziale elettrostatico V tale che ⃗E=−⃗∇V∮Γ⃗E⋅d⃗l=0Il campo elettromotore in una pila non è conservativo. Tale relazione è verificata solamente nel caso stazionario. ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#20": "Interazioni tra magneti e circuiti\n21Abbiamo visto che le correnti generano campi magnetici (prima legge di Laplace, Biot-Savart) E’ vero anche il contrario?Se teniamo un magnete fermo vicino ad un circuito, in esso non si osserva corrente\nSe muoviamo il magnete verso il circuito, allora si osserva una corrente (nell’intervallo in cui il magnete è in movimento)\nIl movimento in verso opposto, “induce” nel circuito una corrente di segno opposto",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#21": "Interazioni tra circuiti percorsi da corrente\n22\nUn effetto analogo si osserva tra due circuiti posti in vicinanzaNel circuito di sinistra si osserva una corrente per un breve intervallo di tempo dopo la chiusura/apertura dell’interruttore (effetto transitorio con corrente variabile nel tempo)\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#22": "Correnti indotte\n23\nEstraendo una spira fatta di materiale conduttore da una regione in cui è presente un campo magnetico, si misura una corrente sulla spira  • si ha corrente anche se il campo magnetico è uniforme• la corrente è massima se il piano della spira è ortogonale al campo magnetico• la corrente è nulla se il piano della spira è parallelo al campo magneticoDal momento che sul conduttore ci sono cariche libere, proviamo a spiegare il fenomeno in termini di forza di Lorentz",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#23": "Correnti indotte\n24⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⃗BConsideriamo un sistema formato da due binari conduttori, paralleli e connessi elettricamente⃗vPoniamo una barretta conduttrice ortogonale ai binari e mettiamola in movimento con velocità costanteSe il sistema è posto in un campo magnetico (costante, uniforme, ortogonale al piano del circuito) nel circuito circola corrente, come se ci fosse un generatore di forza elettromotrice",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#24": "Correnti indotte\n25⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⃗B⃗FLe cariche libere sulla barretta sentono una forza di Lorentz:⃗v⃗F=qe⃗v∧⃗BLe cariche in un tratto dl è come se fossero sottoposte agli effetti di un campo elettromotoreN.B. gli elettroni si muovono verso l’alto, quindi la corrente convenzionalmente circola in verso orariod⃗ldℰ=⃗E⋅d⃗l=⃗Fqe⋅d⃗l=⃗v∧⃗B⋅d⃗l",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#25": "Correnti indotte\n26⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⃗BLa velocità può essere scritta come d⃗ld⃗x⃗v=d⃗xdtIn un intervallo dt, la barretta si sarà spostata di un tratto dxdℰ=(⃗v∧⃗B)⋅d⃗l=(d⃗xdt∧⃗B)⋅d⃗lUtilizzando le proprietà del prodotto misto:dℰ=(d⃗xdt∧⃗B)⋅d⃗l=(d⃗l∧d⃗xdt)⋅⃗B=solo dx dipende dal tempo, B e  dl sono costanti=ddt[(d⃗l∧d⃗x)⋅⃗B]⃗v",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#26": "Induzione elettromagnetica\n27⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⃗Bd⃗ld⃗x\nd⃗l∧d⃗x=−̂n(dldx)=−̂ndSNegativo, perché “entrante” (verso opposto a B)dℰ=ddt[⃗B⋅(d⃗l∧d⃗x)]dℰ=−ddt[⃗B⋅̂ndS]⃗B⋅̂ndS=dΦS(⃗B)⃗vdSFlusso infinitesimo del campo magnetico attraverso dS",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#27": "Induzione elettromagnetica\n28⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ \nℰ=−ddt∬S⃗B⋅̂ndS=−dΦS(⃗B)dtNel circuito si genera una forza elettromotrice indotta opposta (segno meno) alla variazione del flusso del campo magnetico concatenato con la spiraIntegrando su tutta l’area S spazzata dalla barretta \nSi può dimostrare che tale relazione è valida ogni volta in cui si verifica una variazione temporale del flusso concatenato del campo magnetico⃗Bd⃗ld⃗x⃗vS",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#28": "Legge di Faraday-Neumann-Lenz\n29La variazione temporale del flusso di un campo magnetico “induce” una forza elettromotriceℰind=−ddt∬S⃗B⋅̂ndS=−dΦS(⃗B)dtLa forza elettromotrice indotta si oppone alla variazione del flusso che l’ha generata Tale legge rappresenta un ulteriore metodo per generare una corrente in un conduttore  (in aggiunta a forze elettrochimiche di pile e batterie)Il segno meno nell’equazione (storicamente attribuito di Lenz) è conseguenza del 3° principio della dinamica (azione-reazione) e quindi della conservazione dell’energia",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#29": "Induzione elettromagnetica\n30ℰind=−ddt∬S⃗B⋅̂ndS=−dΦS(⃗B)dt1)area della spira variabile nel tempo 2)campo magnetico variabile nel tempo 3)moto relativo di una spira rispetto ad  campo magnetico  Con aggiunta di tutte le possibili combinazioni delle situazioni elencateConsiderando un generico circuito chiuso (una spira), il flusso del campo magnetico concatenato con la spira può variare al verificarsi di tre tre principali situazioni:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#3": "Campi magnetici in condizioni stazionarie\n4⃗∇⋅⃗B=0Legge di Gauss per il campo magnetico Il flusso del campo magnetico attraverso una superficie chiusa è sempre nullo non possiamo isolare cariche magnetiche (monopoli)Il campo magnetico ha sempre divergenza nulla (è solenoidale). Le linee di campo sono sempre chiuse su loro stesse∬⃗B⋅̂ndS=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#30": "Esempio\n31Si consideri un circuito rettangolare con un lato di lunghezza L in movimento con velocità v0 costante. La resistenza totale del circuito vale R. Il circuito è completamente immerso in un campo magnetico costante ed uniforme, ortogonale al piano del circuito. Calcolare: a)l’espressione della corrente indotta nel circuito b)la forza necessaria per mantenere la velocità costanteCampo magnetico costante (e uniforme) e area della spira variabile⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ ⊙      ⊙      ⊙      ⊙       ⊙      ⊙ V0 L L6. Un circuito (vedi figura) è costituito di due binari conduttori paralleli di resistività trascurabile, posti ad una distanza L l’uno dall’altro, collegati da un conduttore fisso di resistenza 2R, e da un’asta metallica, anch’essa di resistività trascurabile, che può scorrere senza attrito sui due binari. Il circuito è immerso in un campo di induzione magnetica variabile 0||BKt=G diretto perpendicolarmente al piano in figura in verso uscente (K0 è una costante positiva nota). Inizialmente l’asta si trova ad una distanza L dal conduttore fisso e si muove con velocità V0 costante verso destra. Determinare: a. il verso di rotazione della corrente nel circuito; b. l’espressione dell’intensità della corrente che circola nel circuito; c. l’espressione del modulo F della forza che viene applicata all’asta per mantenerne costante la velocità.   7. Un circuito (vedi figura) è costituito di due binari conduttori paralleli di resistività trascurabile, posti ad una distanza L l’uno dall’altro, collegati da un conduttore fisso di resistenza R, e da un’asta metallica, anch’essa di resistività trascurabile, che può scorrere senza attrito sui due binari. Il circuito è immerso in un campo di induzione magnetica variabile 0||BKt=G diretto perpendicolarmente al piano in figura in verso uscente (K0 è una costante positiva nota). Inizialmente l’asta si trova ad una distanza 2L dal conduttore fisso e  s i  m u o v e  c o n  v e l o c i t à  2 V0 c o s t a n t e  v e r s o  d e s t r a .  Determinare: a. il verso di rotazione della corrente nel circuito; b. l’espressione dell’intensità della corrente che circola nel circuito; c. l’espressione del modulo F della forza che viene applicata all’asta per mantenerne costante la velocità.   8. Un circuito elettrico è costituito da due binari conduttori paralleli di resistenza trascurabile posti ad una distanza 2D, da una conduttore fisso di resistenza 2R e da un’asta metallica AB di resistenza trascurabile che può scorrere senza attrito sui due binari (vedi figura). La posizione dell’asta AB varia nel tempo secondo la r e l a z i o n e  x ( t )  =  2 x0(1 - cosωt), con x0 ed ω costanti positive note. Il circuito è immerso in un campo induzione magnetica B, diretto perpendicolarmente al piano del circuito, la cui intensità varia nel tempo secondo la relazione B(t)=2B0(1 + cosωt), con B0 costante positiva nota. Determinare: a. la forza elettromotrice indotta nel circuito; b. il valore massimo iM dell’intensità di corrente che circola nel circuito; c. la forza che agisce sull’asta AB.       2V0 2L L\nV(t) 2R xB2D A⃗B",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#31": "Campo magnetico variabile nel tempo e spira ferma\n32S2S1Figure 15:7.3Una spira quadrata conduttrice di latol=20 cm e resistenzaR=0.1⌦si trova ad una distanza ﬁssaa=80cm da un ﬁlo rettilineo indeﬁnito percorso da una correntei. Due dei lati della spira sono paralleli al ﬁlo.Calcolare:a) il ﬂusso del campo magnetico generato dal ﬁlo , supponendo che la corrente sia costantei0=3A (\u0000=µ0li02⇡lna+la=2.68·10\u00008Wb);b) la f.e.m. massima indotta sulla spira supponendo che la corrente sul ﬁlo vari con secondo la leggei(t)=i0cos(!t), con!= 2 rad/s; (Emax=µ0li0!2⇡lna+la=5.36·10\u00008V)c) la massima potenza dissipata dalla spira, nel caso di corrente variabile nel tempo. (Pmax=E2maxR=2.9·10\u000014W)7.4Una bacchetta conduttrice di lunghezzaL=9.83cmer e s i s t e n z aR= 415 m⌦viene fatta muovere con velocit` acostantev=4.86 m/s su dei binari conduttori (di resistenza trascurabile) paralleli. La bacchetta si muovein un campo magnetico generato da una correntei= 110Ache scorre in un ﬁlo parallelo ai binari, a distanzaa= 10.2mm. Calcolare:a) la corrente indotta che scorre nella spira; (iind=\u0000µ0iv2⇡Rln\u0000a+La\u0000)b) la forza che bisogna applicare esternamente alla bacchetta per tenerla in moto uniforme; (Fest=µ0iiind2⇡ln\u0000a+La\u0000)c) confrontare la potenza dissipata sulla bacchetta con la potenza fornita dalla forza esterna. (P=Ri2ind=Fv)7.5Una spira quadrata di latoL=30 cm, resistenzaR=2⌦e massam= 10g, si muove senza attrito suun piano orizzontale con velocit` av0=1 m/s, perpendicolare ad un lato. Ad un certo istantet0la spiraentra in una regione in cui presente un campo magnetico uniforme e costante di moduloB=0.5 T, direttoperpendicolaremente al piano della spira (il bordo della regione con il campo magnetico ` e parallelo al latodella spira che sta entrando, come mostrato in ﬁgura 16). Calcolare:a) la velocit` a della spira nell’istantet0in cui essa ` e entrata completamente nella regione con campomagnetico; (v(t0)=v0\u0000B2L3mR)b) la corrente che percorre la spira nell’istantet0;(i(t0)=\u0000BLv(t0)R)c) la potenza dissipata all’istantet0.(P=B2L2v(t0)2R)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#32": "33Campo magnetico costante (non uniforme) e spira in movimento F1) Un circuito rigido quadrato, di lato L=100cm, è costituito di un filo di alluminio (resistività ρ=2.56 10-8 Ωm) di sezione S=10 mm2. Esso si trova nel piano xy con i lati paralleli ai due assi, ed è immerso (nel vuoto) in un campo di induzione magnetica uniforme di modulo Bz= 0,5T diretto lungo l’asse z nel verso positivo, limitato all’area grigia di figura. Il circuito, inizialmente tutto immerso nel campo magnetico, trasla con parallelamente all’asse x con velocità che viene mantenuta costante di modulo V0= 20 cm/s. Calcolare, giustificando:  1) il verso della corrente indotta (orario o antiorario), con riferimento alla figura; 2) l’intensità  di tale corrente nel circuito durante il moto; 3) l’energia totale dissipata nel circuito per effetto Joule; 4) il lavoro effettuato per portare il circuito completamente fuori del campo.              F2) Una spira rigida a forma di triangolo equilatero di lato L=2m, massa M=100g, e resistenza R=10 Ω ,  s i  m u o v e  c o n  v e l o c i t à  c o s t a n t e  V0 =  1 0  m / s  l u n g o  l ’ a s s e  x .  N e l semipiano delle x positive è presente un campo induzione magnetica uniforme di modulo B=0.5 T diretto lungo z nel verso positivo, mentre nel semipiano delle x negative B è identicamente nullo. Calcolare: 1)  il verso della corrente indotta (orario o antiorario), con riferimento alla figura; 2) il flusso di B c o n c a t e n a t o  c o n  i l  c i r c u i t o ,  n e l l ’ i s t a n t e  i n  c u i  m e t à  d e l l ’ a r e a  d e l  3) la corrente massima che circola nel circuito durante il moto; 4) l’espressione vettoriale della forza che agisce sul lato BC del circuito, all’istante       ijBv0L\nijBLABCUna spira quadrata conduttrice di lato L e resistenza R si muove con velocità costante v0 in una regione dove è presente un campo magnetico uniforme, limitato ad una regione rettangolare. a)determinare la corrente indotta sulla spira  b)il lavoro per estrarre la spira fuori dalla regione in cui è presente il campo magnetico c)l’energia dissipata per effetto Joule",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#33": "Generatore elettrico\n34\nConsideriamo un sistema di N spire rotanti con velocità angolare costante in un campo magnetico uniformesia S=area delle spire, 𝜑=𝜔t angolo tra vettore normale al piano della spira e campo magneticoin ogni istante il flusso valeΦ(⃗B)=N⃗B⋅̂nS=NBScosφ=NBScosωtnelle spire ci sarà una fem indotta:ℰ=−dΦ(⃗B)dt=−NBS(−ωsinωt)=NBSωsinωt\nfem alternata\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#34": "Correnti di Foucault\n35Le correnti di Foucault (o correnti parassite) si osservano nei conduttori in presenza di campi magnetici il cui flusso varia nel tempo. Esse sono una conseguenza del fenomeno dell’induzione magnetica. Tali correnti sono dovute al moto degli elettroni causato dalle fem indotte nel conduttore\nL’effetto di tali correnti è quello di creare campi magnetici che si oppongono alla variazione che le hanno generate: effetto “frenante”Per minimizzare gli effetti delle correnti parassite, occorre “tagliare” il conduttore\n",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#35": "Correnti di Foucault\n36Le correnti di Foucault possono anche generare calore per effetto JouleTale meccanismo è alla base dei fornelli ad induzione\nPerché non tutte le pentole funzionano sulle cucine ad induzione?",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#36": "Forma locale della legge di FNL\n37ℰ=∮Γ⃗E⋅d⃗l=∬SΓ⃗∇∧⃗E⋅̂ndSℰ=∮Γ⃗E⋅d⃗l=−ddt∬SΓ⃗B⋅̂ndS=∬SΓ−∂⃗B∂t⋅̂ndS⃗∇∧⃗E=−∂⃗B∂tApplichiamo il teorema di StokesCombinando con la legge di Faraday-Neumann-LenzIl campo E è detto campo elettrico indotto  Un campo magnetico variabile nel tempo è una sorgente di campo elettrico ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#37": "Forma locale della legge di FNL\n38⃗∇∧⃗E=−∂⃗B∂t• Il campo elettrico indotto dalla variazione di un campo magnetico ha rotore non-nullo: non è conservativoIn ogni punto dello spazio in cui è presente un campo magnetico variabile nel tempo, in quel punto si genera un campo elettrico• Il campo elettrico generato da cariche elettriche ha sempre rotore nullo ed è conservativo (campo elettrostatico)Il campo elettrico può essere generato da campi magnetici variabili nel tempo oppure da cariche elettriche",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#38": "Induzione mutua\n39Per la prima legge di Laplace, il campo magnetico dipende linearmente dalla corrente che l’ha generato:d⃗B=μ0i4πd⃗l∧̂rr2Il flusso del campo magnetico sarà dunque proporzionale alla corrente:Φ(⃗B)=MiDove M è un coefficiente che dipende solamente dalla geometria (forma) del circuito percorso da correntei⃗B\nSe le correnti sono variabili nel tempo:ℰind=−dΦ(⃗B)dt=−Mdidt",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#39": "Induzione mutua\n40Consideriamo due circuiti percorsi da correnti i1 e i2, che generano rispettivamente i campi magnetici B1 e B2 Il flusso di B1 attraverso il circuito 2 èΦ1(⃗B2)=M21i2Φ2(⃗B1)=M12i1Il flusso di B2 attraverso il circuito 1 èi1i2Si può dimostrare che M12=M21=MM è detto coefficiente di mutua induzioneNel sistema internazionale si misura in Henry (H)  1H=Tm2/A",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#4": "Campi magnetici in condizioni stazionarie\n5∮Γ⃗B⋅d⃗l=μ0conc∑kik⃗∇∧⃗B=μ0⃗𝚥Legge di Ampère la circuitazione del campo magnetico è proporzionale alla somma delle correnti concatenate con la linea di circuitazioneIn forma locale, il rotore del campo magnetico è proporzionale alla densità di corrente. Il campo magnetico NON è conservativo Il campo magnetico è generato da correnti (cariche in movimento)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#40": "Esempio\n41habFigure 18:7.9Un solenoide torioidale costituito daN= 1000 spire ciascuna di raggior=1cm. Il raggio maggiore delsolenoide ` eR=10cm. Un ﬁlo di lunghezza indeﬁnita ` e posto lungo l’asse del toroide ed ` e percorso verso l’altoda una corrrente variabile nel tempoiF=\u0000tcon\u0000= 100A/s. Calcolare:a) il ﬂusso\u0000T(BF) del campo magnetico generato dal ﬁlo attraverso le spire del toroide; (\u0000T(BF)=µ0N\u0000tr2R)b) la f.e.m indotta nel toroide; (E=\u0000µ0N\u0000r2R)c) l’induttanzaLTdel toroide. (LT=µ0Nr22R)7.10Siano dati due solenoidi cilindrici aventi gli assi coincidenti. Il primo solenoide di lunghezzal` e formato daNspire di areaA. Esso ` e posto all’interno del secondo solenoide pi` u grande, di lunghezzalS>> lea v e n t eNSspire di areaS> >A. Calcolare il coe\u0000ciente di mutua induzione. (M=µ0ANNslS)7.11Si considerino due spire circolari concentriche sullo stesso piano di raggireR> >r. La spira piccola ` epercorsa da una correnteir(t)=i0sin(!t). Determinare la f.e.m. indotta sulla spira grande. (f.e.m.=\u0000µ0⇡r22Ri0!cos!t)7.12Una spira rettangolare di latiaebpercorsa da correnteis=i0sin(!t) ` e posta con il latoaa distanzabda un ﬁlo rettilineo di lunghezzaL> >a. Determinare la di↵erenza di potenzialeEindai capi del ﬁlo.(Eind=\u0000µ0a!i02⇡ln 2 cos!t)7.13Si consideri il circuito mostrato in ﬁgura 19 composto da due induttanzeL1=L2=L,d at r er e s i s t e n z eR1=R2=R3=R, da un generatore di resistenza internar=R/2 che fornisce una forza elettromotriceEe da un interruttoreTinizialmente aperto. Determinare:a) la corrente elettrica che circola nelle tre resistenze in funzione del tempo; (i(t)=ER(1\u0000e\u0000RLt))Determinare inoltre in regime stazionario:b) il valore del potenziale nel puntoA;(VA= 0)c) l’energia totale immagazzinata nel sistema; (U=12LE2R2)d) la potenza dissipata nel sistema. (P=E2R)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#41": "Autoinduzione\n42i⃗BUn circuito percorso da corrente genera un campo magnetico Tale campo avrà un flusso concatenato con il circuito stessoSe la corrente varia nel temp, nel circuito si genererà una fem autoindottaΦ(⃗B)=LiL è detto coefficiente di autoinduzione (o induttanza) e si misura in Henryℰind=−dΦ(⃗B)dt=−LdidtLa fem autoindotta si oppone alla variazione di corrente che l’ha generata ",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#42": "Esempio\n43Calcolare l’induttanza di un solenoide cilindrico ideale di lunghezza l formato da N spire circolari di raggio r \n9. Circuiti in Corrente Alternata Fisica Generale B \nhttp://campus.cib.unibo.it/2482/ March 29, 2011 \nAutoinduzione •!Consideriamo un solenoide percorso da una corrente variabile nel tempo.  •!Esso genera un campo magnetico, entro il volume cilindrico delimitato dal solenoide, anch’esso variabile nel tempo. •!Tale campo magnetico, a sua volta, essendo variabile nel tempo, genera una forza elettromotrice indotta nel solenoide che si sovrappone alla forza elettromotrice esterna. •!Questo fenomeno prende il nome di autoinduzione. i2!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (II) •!Se nel solenoide scorre una corrente di intensità i(t) e n è il numero di spire per unità di lunghezza, il campo magnetico all’interno del solenoide, come abbiamo visto, è diretto lungo l’asse del solenoide e ha intensità: •!Se S è l’area della sezione del solenoide, il flusso di tale campo magnetico concatenato con una spira vale: ovvero, se N è il numero totale di spire e l è la lunghezza del solenoide: Bt()=µ0it()n!spira!Bt()()=Bt()S=µ0it()nS!spira!Bt()()=µ0it()nS=µ0it()NSli!B!B3!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (III) •!Il flusso del campo magnetico concatenato con le N spire del solenoide vale: •!La forza elettromotrice autoindotta vale perciò: •!La costante di proporzionalità: è chiamata coefficiente di autoinduzione o induttanza. !solenoide!Bt()()=N!spira!Bt()()=µ0it()N2Slft()=!d\"solenoide!Bt()()dt=!µ0N2Sldidt\n4!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!if!B!B  L=µ0N2SlSupponiamo che il solenoide sia percorso da una corrente i(t) variabile nel tempo. Internamente al solenoide vi è un campo magnetico (dipendente dal tempo): Il flusso del campo attraverso una singola spira vale ⃗BS=πr2area di una spiraB(t)=μ0Nli(t)Φspira(⃗B(t))=B(t)S=μ0Nli(t)πr2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#43": "Esempio\n44Il flusso attraverso l’intero solenoide sarà pari ad N volte il flusso attraverso una singola spira  \n9. Circuiti in Corrente Alternata Fisica Generale B \nhttp://campus.cib.unibo.it/2482/ March 29, 2011 \nAutoinduzione •!Consideriamo un solenoide percorso da una corrente variabile nel tempo.  •!Esso genera un campo magnetico, entro il volume cilindrico delimitato dal solenoide, anch’esso variabile nel tempo. •!Tale campo magnetico, a sua volta, essendo variabile nel tempo, genera una forza elettromotrice indotta nel solenoide che si sovrappone alla forza elettromotrice esterna. •!Questo fenomeno prende il nome di autoinduzione. i2!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (II) •!Se nel solenoide scorre una corrente di intensità i(t) e n è il numero di spire per unità di lunghezza, il campo magnetico all’interno del solenoide, come abbiamo visto, è diretto lungo l’asse del solenoide e ha intensità: •!Se S è l’area della sezione del solenoide, il flusso di tale campo magnetico concatenato con una spira vale: ovvero, se N è il numero totale di spire e l è la lunghezza del solenoide: Bt()=µ0it()n!spira!Bt()()=Bt()S=µ0it()nS!spira!Bt()()=µ0it()nS=µ0it()NSli!B!B3!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (III) •!Il flusso del campo magnetico concatenato con le N spire del solenoide vale: •!La forza elettromotrice autoindotta vale perciò: •!La costante di proporzionalità: è chiamata coefficiente di autoinduzione o induttanza. !solenoide!Bt()()=N!spira!Bt()()=µ0it()N2Slft()=!d\"solenoide!Bt()()dt=!µ0N2Sldidt\n4!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!if!B!B  L=µ0N2Sl⃗BΦsolenoide(⃗B(t))=NΦspira(⃗B(t))=Nμ0Nli(t)πr2=μ0N2li(t)πr2L’induttanza L si calcola come il rapporto tra il flusso “autoindotto”  (autoflusso) e la corrente:L=Φsolenoide(⃗B)i(t)=μ0N2lπr2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#44": "Esempio\n45habFigure 18:7.9Un solenoide torioidale costituito daN= 1000 spire ciascuna di raggior=1cm. Il raggio maggiore delsolenoide ` eR=10cm. Un ﬁlo di lunghezza indeﬁnita ` e posto lungo l’asse del toroide ed ` e percorso verso l’altoda una corrrente variabile nel tempoiF=\u0000tcon\u0000= 100A/s. Calcolare:a) il ﬂusso\u0000T(BF) del campo magnetico generato dal ﬁlo attraverso le spire del toroide; (\u0000T(BF)=µ0N\u0000tr2R)b) la f.e.m indotta nel toroide; (E=\u0000µ0N\u0000r2R)c) l’induttanzaLTdel toroide. (LT=µ0Nr22R)7.10Siano dati due solenoidi cilindrici aventi gli assi coincidenti. Il primo solenoide di lunghezzal` e formato daNspire di areaA. Esso ` e posto all’interno del secondo solenoide pi` u grande, di lunghezzalS>> lea v e n t eNSspire di areaS> >A. Calcolare il coe\u0000ciente di mutua induzione. (M=µ0ANNslS)7.11Si considerino due spire circolari concentriche sullo stesso piano di raggireR> >r. La spira piccola ` epercorsa da una correnteir(t)=i0sin(!t). Determinare la f.e.m. indotta sulla spira grande. (f.e.m.=\u0000µ0⇡r22Ri0!cos!t)7.12Una spira rettangolare di latiaebpercorsa da correnteis=i0sin(!t) ` e posta con il latoaa distanzabda un ﬁlo rettilineo di lunghezzaL> >a. Determinare la di↵erenza di potenzialeEindai capi del ﬁlo.(Eind=\u0000µ0a!i02⇡ln 2 cos!t)7.13Si consideri il circuito mostrato in ﬁgura 19 composto da due induttanzeL1=L2=L,d at r er e s i s t e n z eR1=R2=R3=R, da un generatore di resistenza internar=R/2 che fornisce una forza elettromotriceEe da un interruttoreTinizialmente aperto. Determinare:a) la corrente elettrica che circola nelle tre resistenze in funzione del tempo; (i(t)=ER(1\u0000e\u0000RLt))Determinare inoltre in regime stazionario:b) il valore del potenziale nel puntoA;(VA= 0)c) l’energia totale immagazzinata nel sistema; (U=12LE2R2)d) la potenza dissipata nel sistema. (P=E2R)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#45": "Circuiti con induttanze\n46\nUn solenoide inserito all’interno di un circuito percorso da corrente variabile nel tempo si comporta come un generatore di forza elettromotrice (fem autoindotta) con polarità opposta alla variazione di corrente\n9. Circuiti in Corrente Alternata Fisica Generale B \nhttp://campus.cib.unibo.it/2482/ March 29, 2011 \nAutoinduzione •!Consideriamo un solenoide percorso da una corrente variabile nel tempo.  •!Esso genera un campo magnetico, entro il volume cilindrico delimitato dal solenoide, anch’esso variabile nel tempo. •!Tale campo magnetico, a sua volta, essendo variabile nel tempo, genera una forza elettromotrice indotta nel solenoide che si sovrappone alla forza elettromotrice esterna. •!Questo fenomeno prende il nome di autoinduzione. i2!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (II) •!Se nel solenoide scorre una corrente di intensità i(t) e n è il numero di spire per unità di lunghezza, il campo magnetico all’interno del solenoide, come abbiamo visto, è diretto lungo l’asse del solenoide e ha intensità: •!Se S è l’area della sezione del solenoide, il flusso di tale campo magnetico concatenato con una spira vale: ovvero, se N è il numero totale di spire e l è la lunghezza del solenoide: Bt()=µ0it()n!spira!Bt()()=Bt()S=µ0it()nS!spira!Bt()()=µ0it()nS=µ0it()NSli!B!B3!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (III) •!Il flusso del campo magnetico concatenato con le N spire del solenoide vale: •!La forza elettromotrice autoindotta vale perciò: •!La costante di proporzionalità: è chiamata coefficiente di autoinduzione o induttanza. !solenoide!Bt()()=N!spira!Bt()()=µ0it()N2Slft()=!d\"solenoide!Bt()()dt=!µ0N2Sldidt\n4!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!if!B!B  L=µ0N2Sl⃗BSimbolo circuitale dell’induttanza +_𝓔(t)ℰautoindotta=−dΦsolenoide(⃗B(t))dt=−Ldi(t)dtREquazione del circuitoℰ(t)−Ldi(t)dt=Ri(t)ℰ(t)+ℰautoindotta=Ri(t)La fem autoindotta si “oppone” alla variazione di corrente che l’ha generata",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#46": "Induttanze in serie\n47+_𝓔(t)RL1L2In serie, le due induttanze sono percorse dalla stessa correnteΔV=ΔV1+ΔV2=−L1di(t)dt−L2di(t)dt=−(L1+L2)di(t)dt=−Ltotdi(t)dtDifferenza di potenziale ai capi delle due induttanze\nL’induttanza del sistema formato da due (o più) induttanze collegate in serie è uguale alla somma delle singole induttanzeLtot=∑iLitrascuriamo gli effetti di mutua induzione",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#47": "Induttanze in parallelo\n48\n+_𝓔(t)RL1L2In parallelo, le due induttanze sono alla stessa differenza di potenzialetrascuriamo gli effetti di mutua induzionedi1dt=−ΔVL1di2dt=−ΔVL2=−ΔVL1−ΔVL2=i=i1+i2Per la legge dei nodididt=di1dt+di2dt\nL’inverso dell’induttanza del sistema formato da due o più induttanze collegate in parallelo è uguale alla somma degli inversi delle singole induttanza 1Ltot=1L1+1L2=−ΔV(1L1+1L2)=−ΔVLtot1Ltot=∑i1Li",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#48": "Energia magnetica\n49\n9. Circuiti in Corrente Alternata Fisica Generale B \nhttp://campus.cib.unibo.it/2482/ March 29, 2011 \nAutoinduzione •!Consideriamo un solenoide percorso da una corrente variabile nel tempo.  •!Esso genera un campo magnetico, entro il volume cilindrico delimitato dal solenoide, anch’esso variabile nel tempo. •!Tale campo magnetico, a sua volta, essendo variabile nel tempo, genera una forza elettromotrice indotta nel solenoide che si sovrappone alla forza elettromotrice esterna. •!Questo fenomeno prende il nome di autoinduzione. i2!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (II) •!Se nel solenoide scorre una corrente di intensità i(t) e n è il numero di spire per unità di lunghezza, il campo magnetico all’interno del solenoide, come abbiamo visto, è diretto lungo l’asse del solenoide e ha intensità: •!Se S è l’area della sezione del solenoide, il flusso di tale campo magnetico concatenato con una spira vale: ovvero, se N è il numero totale di spire e l è la lunghezza del solenoide: Bt()=µ0it()n!spira!Bt()()=Bt()S=µ0it()nS!spira!Bt()()=µ0it()nS=µ0it()NSli!B!B3!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (III) •!Il flusso del campo magnetico concatenato con le N spire del solenoide vale: •!La forza elettromotrice autoindotta vale perciò: •!La costante di proporzionalità: è chiamata coefficiente di autoinduzione o induttanza. !solenoide!Bt()()=N!spira!Bt()()=µ0it()N2Slft()=!d\"solenoide!Bt()()dt=!µ0N2Sldidt\n4!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!if!B!B  L=µ0N2Sl⃗B+_𝓔(t)RL’induttanza percorsa da corrente variabile nel tempo si comporta come un generatore con polarità opposta alla variazione correnteInnalzare la corrente di un valore di equivale a far passare nell’induttanza una carica q in un tempo dt (dq=idt)Per spostare la carica occorre contrastare la fem autoindottaℰautoindotta=−Ldidtδℒ=−ℰautoindottadq=−(−Ldidt)(idt)Occorre fare un lavoro “contro” la forza elettromotrice autoindotta",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#49": "Energia magnetica\n50Se inizialmente nell’induttanza non circola corrente i(0)=0, per portare il circuito a corrente i occorre compiere un lavoroℒ=∫i0Lidi=12Li2δℒ=−ℰautoindottadq=−(−Ldidt)(idt)Lavoro per aumentare la corrente di un valore di\nIl lavoro accumula energia nell’induttanza.Energia magnetica accumulata in un’induttanzaUB=12Li2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#5": "Equazioni di Maxwell (caso stazionario)\n6⃗∇⋅⃗E=ρε0⃗∇∧⃗E=0⃗∇⋅⃗B=0⃗∇∧⃗B=μ0⃗𝚥Forma locale(differenziale)",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#50": "Densità di energia magnetica\n51\n9. Circuiti in Corrente Alternata Fisica Generale B \nhttp://campus.cib.unibo.it/2482/ March 29, 2011 \nAutoinduzione •!Consideriamo un solenoide percorso da una corrente variabile nel tempo.  •!Esso genera un campo magnetico, entro il volume cilindrico delimitato dal solenoide, anch’esso variabile nel tempo. •!Tale campo magnetico, a sua volta, essendo variabile nel tempo, genera una forza elettromotrice indotta nel solenoide che si sovrappone alla forza elettromotrice esterna. •!Questo fenomeno prende il nome di autoinduzione. i2!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (II) •!Se nel solenoide scorre una corrente di intensità i(t) e n è il numero di spire per unità di lunghezza, il campo magnetico all’interno del solenoide, come abbiamo visto, è diretto lungo l’asse del solenoide e ha intensità: •!Se S è l’area della sezione del solenoide, il flusso di tale campo magnetico concatenato con una spira vale: ovvero, se N è il numero totale di spire e l è la lunghezza del solenoide: Bt()=µ0it()n!spira!Bt()()=Bt()S=µ0it()nS!spira!Bt()()=µ0it()nS=µ0it()NSli!B!B3!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!\nAutoinduzione (III) •!Il flusso del campo magnetico concatenato con le N spire del solenoide vale: •!La forza elettromotrice autoindotta vale perciò: •!La costante di proporzionalità: è chiamata coefficiente di autoinduzione o induttanza. !solenoide!Bt()()=N!spira!Bt()()=µ0it()N2Slft()=!d\"solenoide!Bt()()dt=!µ0N2Sldidt\n4!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!if!B!B  L=µ0N2Sl⃗BS=πr2Ricordando l’espressione dell’induttanza di un solenoideL=μ0N2lSIl campo magnetico vale:B=μ0Nlii=Blμ0NL’energia magnetica sarà pari aUB=12Li2=12(μ0N2lS)(Blμ0N)2=12(μ0N2lS)(B2l2μ20N2)=B22μ0(lS)volume del solenoideL’energia magnetica è il prodotto di una densità di energia per il volume del solenoide",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#51": "Densità di energia magnetica\n52uB=B22μ0Definiamo la densità di energia del campo magnetico:L’energia magnetica è localizzata in ogni punto dello spazio in cui è presente il campo magneticoUB=∭spaziouBdτL’energia del campo magnetico si calcola come l’integrale sul volume in tutto lo spazio in cui è presente il campo magnetico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#52": "Circuiti RL in regime transitorio\n53Come abbiamo fatto per i condensatori, analizziamo un circuito composto da un generatore di forze elettromotrice costante, una resistenza e un’induttanza \n+_T𝓔LR\nInizialmente l’interruttore è aperto (non circola corrente i(0)=0, l’induttanza è scarica)Ad un dato istante iniziale t=0 l’interruttore viene chiuso. Scriviamo l’equazione della maglia:ℰ+ℰind=Riℰ−Ldidt=Ri",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#53": "Circuiti RL in regime transitorio\n54ℰ−Ldidt=Rididt=−RL(i−ℰR)Separiamo le variabilidi(i−ℰR)=−RLdtIntegriamo tra i(0) e i(t) e tra t=0 e t∫i(t)i(0)di(i−ℰR)=∫t0−RLdtln(i−ℰR)i(t)i(0)=−RLtlni(t)−ℰRi(0)−ℰR=−RLti(t)−ℰRi(0)−ℰR=e−RLtImponendo la condizione iniziale i(0)=0i(t)−ℰR=−ℰRe−RLt",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#54": "Circuiti RL in regime transitorio\n55i(t)=ℰR(1−e−RLt)\nTransitori in un Circuito RL. Chiusura del Circuito (IV) •!Per trovare la costante i0, imponiamo la condizione iniziale: \n•!La quantità # = L/R, che ha le dimensioni di un tempo, viene detta costante di tempo del circuito. i0()=0\"fR+i0e!RL0=fR+i0=0\"i0=!fRit()=fR!fRe!RLt  it()=fR1!e!RLt\"#$%&'\n45!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0\n Transitori in un Circuito RL. Chiusura del Circuito (V) •!Si ottiene inoltre: VR=Rit()=f1!e!RLt\"#$%&'VL=Ldidtt()=LfR!e!RLt\"#$%&'!RL\"#$%&'=fe!RLtit()=fR1!e!RLt\"#$%&'VRt()=f1!e!RLt\"#$%&'VLt()=fe!RLt()****+****it()t!\"#!##fRVRt()t!\"#!##fVLt()t!\"#!##046!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0\nTransitori in un Circuito RL. Chiusura del Circuito (VI) tfRi\ntfRVfLVt47!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0\nTransitori in un Circuito RL. Apertura del Circuito •!Consideriamo il circuito in figura e supponiamo ora che, inizialmente, il deviatore si trovi nella posizione 1, con l’induttanza L percorsa da una corrente di intensità costante (i = f /R). •!Supponiamo poi che a un certo istante, t = 0, il deviatore venga commutato nella posizione 0. •!Avremo l’equazione differenziale: •!L’integrale generale è: Ldidtt()+Rit()=0i0()=fR!\"##$##  it()=i0e!RLt48!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0ℰRAndamento nella corrente in un circuito RL alla chiusura dell’interruttore L/R ha le dimensioni du un tempo (costante di tempo)inizialmente la corrente è nulla e si porta ad un valore asintoticoℰind=−Ldidt=−LℰRRLe−RLt⟶t→∞0in regime stazionario (t→∞ ) la fem autoindotta si annullal’induttanza si comporta come un filo a resistenza nullanell’induttanza vi è immagazzinata un’energia magneticaUB=12Li2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#55": "56Circuiti RL in regime transitorioSia dato un circuito formato da un condensatore e un’induttanza Inizialmente l’interruttore T è aperto, l’induttanza è carica con UB=UoCalcoliamo quanto vale l’energia dissipata sulla resistenza L’equazione della maglia alla chiusura dell’interruttore èTLR\nℰind=Ri−Ldidt=Rididt=−RLiRisolvendo l’eq. differenziale per separazione delle variabili:dii=−RLdti(t)=i(0)e−RLtlni(t)i(0)=−RLt",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#56": "57Circuiti RL in regime transitorioi(t)=i(0)e−RLtUB=12Li(0)2La corrente iniziale si ricava dalla condizione iniziale di energia immagazzinata nell’induttanza\nTransitori in un Circuito RL. Apertura del Circuito (II) •!Per trovare la costante i0, imponiamo la condizione iniziale: \n•!Si ottiene inoltre: i0()=fR\"i0e!RL0=i0=fR\"i0=fR it()=fRe!RLtVR=Rit()=fe!RLtVL=Ldidtt()=LfRe!RLt!RL\"#$%&'=!fe!RLt\n49!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0\nTransitori in un Circuito RL. Apertura del Circuito (III) •!Riassumendo: \n•!La corrente che scorre nel circuito dopo che è stato escluso il generatore di tensione prende il nome di extracorrente di apertura. it()=fRe!RLtVRt()=fe!RLtVLt()=!fe!RLt\"#$$$%$$$it()t!\"#!##0VRt()t!\"#!##0VLt()t!\"#!##0\n50!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0\nTransitori in un Circuito RL. Apertura del Circuito (IV) fRti\ntfRVf!LVt51!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0\nTransitori in un Circuito RL. Apertura del Circuito (V) •!Se dopo avere escluso il generatore il circuito rimane aperto, si osserva una scarica elettrica tra i contatti dell’interruttore. •!Il motivo è nel fatto che il flusso del campo magnetico nell’induttanza passa in un tempo estremamente breve dal valore iniziale f/R al valore finale 0. •!Segue che la derivata          è estremamente elevata, e con essa è estremamente elevata la f.e.m. autoindotta: ft()=!L\"i\"t=!L0!fR\"t\"t#0$#$$%  didt\n52!Domenico Galli – Fisica Generale B – 9. Circuiti in Corrente Alternata!RLf!+01VRVLiAMBt=0i(0)=2UBLi(0)L’energia dissipata sulla resistenza per effetto Joule durante l’intero processo di scarica:UR=∫∞0Ri2dt=R∫∞0(i(0)e−RLt)2dt=R∫∞0i2(0)e−2RLtdt=R2UBL∫∞0e−2RLtdt=R2UBL[−L2Re−2RLt]∞0=UBTutta l’energia accumulata nell’induttanza viene dissipata per effetto Joule sulla resistenza",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#57": "Esempio\n58habFigure 18:7.9Un solenoide torioidale costituito daN= 1000 spire ciascuna di raggior=1cm. Il raggio maggiore delsolenoide ` eR=10cm. Un ﬁlo di lunghezza indeﬁnita ` e posto lungo l’asse del toroide ed ` e percorso verso l’altoda una corrrente variabile nel tempoiF=\u0000tcon\u0000= 100A/s. Calcolare:a) il ﬂusso\u0000T(BF) del campo magnetico generato dal ﬁlo attraverso le spire del toroide; (\u0000T(BF)=µ0N\u0000tr2R)b) la f.e.m indotta nel toroide; (E=\u0000µ0N\u0000r2R)c) l’induttanzaLTdel toroide. (LT=µ0Nr22R)7.10Siano dati due solenoidi cilindrici aventi gli assi coincidenti. Il primo solenoide di lunghezzal` e formato daNspire di areaA. Esso ` e posto all’interno del secondo solenoide pi` u grande, di lunghezzalS>> lea v e n t eNSspire di areaS> >A. Calcolare il coe\u0000ciente di mutua induzione. (M=µ0ANNslS)7.11Si considerino due spire circolari concentriche sullo stesso piano di raggireR> >r. La spira piccola ` epercorsa da una correnteir(t)=i0sin(!t). Determinare la f.e.m. indotta sulla spira grande. (f.e.m.=\u0000µ0⇡r22Ri0!cos!t)7.12Una spira rettangolare di latiaebpercorsa da correnteis=i0sin(!t) ` e posta con il latoaa distanzabda un ﬁlo rettilineo di lunghezzaL> >a. Determinare la di↵erenza di potenzialeEindai capi del ﬁlo.(Eind=\u0000µ0a!i02⇡ln 2 cos!t)7.13Si consideri il circuito mostrato in ﬁgura 19 composto da due induttanzeL1=L2=L,d at r er e s i s t e n z eR1=R2=R3=R, da un generatore di resistenza internar=R/2 che fornisce una forza elettromotriceEe da un interruttoreTinizialmente aperto. Determinare:a) la corrente elettrica che circola nelle tre resistenze in funzione del tempo; (i(t)=ER(1\u0000e\u0000RLt))Determinare inoltre in regime stazionario:b) il valore del potenziale nel puntoA;(VA= 0)c) l’energia totale immagazzinata nel sistema; (U=12LE2R2)d) la potenza dissipata nel sistema. (P=E2R)  37. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=2L, da tre resistenze R1= R2= R3= 2R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e  d a  u n  i n t e r r u t t o r e  T  i n i z i a l m e n t e  aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;       Determinare inoltre in regime stazionario (t Æ ∞): b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   38. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=L, da tre resistenze R1= R2= R3= R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e da un interruttore T inizialmente aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;            Determinare inoltre in regime stazionario (t Æ ∞):  b. il valore del potenziale nel punto B; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   39. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=2L, da tre resistenze R1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e da due condensatori di capacità C1= C2=C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  40. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=L, da tre resistenze R1=R2=R3=2R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e  da due condensatori di capacità C1= C2=2C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  L1 r ε R2 R1A B \nR3 L2 T \nA L1 ε R2 R1R3L2 L3 C1 C2 \nA L1 εR2 R1 R3L2 L3 C1 C2 L1 r ε R2 A B \nR1R3L2T Figure 19:7.14Si consideri il circuito mostrato in ﬁgura 20 composto da tre induttanzeL1=L2=L3=2L,d at r er e s i s t e n z eR1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotriceEe da due condensatori di capacita` aC1=C2=C. Determinare in regime stazionario:a) la corrente elettrica che circola nelle tre resistenze; (i=ER1+R3)b) lenergia totale immagazzinata nel sistema; (U=12LE2R2+12CE2)c) la potenza dissipata nel sistema. (P=E2R1+R3)  37. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=2L, da tre resistenze R1= R2= R3= 2R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e  d a  u n  i n t e r r u t t o r e  T  i n i z i a l m e n t e  aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;       Determinare inoltre in regime stazionario (t Æ ∞): b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   38. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=L, da tre resistenze R1= R2= R3= R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e da un interruttore T inizialmente aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;            Determinare inoltre in regime stazionario (t Æ ∞):  b. il valore del potenziale nel punto B; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   39. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=2L, da tre resistenze R1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e da due condensatori di capacità C1= C2=C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  40. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=L, da tre resistenze R1=R2=R3=2R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e  da due condensatori di capacità C1= C2=2C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  L1 r ε R2 R1A B \nR3 L2 T \nA L1 ε R2 R1R3L2 L3 C1 C2 \nA L1 εR2 R1 R3L2 L3 C1 C2 L1 r ε R2 A B \nR1R3L2T \nFigure 20:",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#58": "Esempio\n59  37. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=2L, da tre resistenze R1= R2= R3= 2R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e  d a  u n  i n t e r r u t t o r e  T  i n i z i a l m e n t e  aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;       Determinare inoltre in regime stazionario (t Æ ∞): b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   38. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=L, da tre resistenze R1= R2= R3= R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e da un interruttore T inizialmente aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;            Determinare inoltre in regime stazionario (t Æ ∞):  b. il valore del potenziale nel punto B; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   39. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=2L, da tre resistenze R1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e da due condensatori di capacità C1= C2=C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  40. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=L, da tre resistenze R1=R2=R3=2R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e  da due condensatori di capacità C1= C2=2C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  L1 r ε R2 R1A B \nR3 L2 T \nA L1 ε R2 R1R3L2 L3 C1 C2 \nA L1 εR2 R1 R3L2 L3 C1 C2 L1 r ε R2 A B \nR1R3L2T Figure 19:7.14Si consideri il circuito mostrato in ﬁgura 20 composto da tre induttanzeL1=L2=L3=2L,d at r er e s i s t e n z eR1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotriceEe da due condensatori di capacita` aC1=C2=C. Determinare in regime stazionario:a) la corrente elettrica che circola nelle tre resistenze; (i=ER1+R3)b) lenergia totale immagazzinata nel sistema; (U=12LE2R2+12CE2)c) la potenza dissipata nel sistema. (P=E2R1+R3)  37. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=2L, da tre resistenze R1= R2= R3= 2R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e  d a  u n  i n t e r r u t t o r e  T  i n i z i a l m e n t e  aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;       Determinare inoltre in regime stazionario (t Æ ∞): b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   38. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=L, da tre resistenze R1= R2= R3= R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e da un interruttore T inizialmente aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;            Determinare inoltre in regime stazionario (t Æ ∞):  b. il valore del potenziale nel punto B; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   39. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=2L, da tre resistenze R1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e da due condensatori di capacità C1= C2=C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  40. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=L, da tre resistenze R1=R2=R3=2R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e  da due condensatori di capacità C1= C2=2C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  L1 r ε R2 R1A B \nR3 L2 T \nA L1 ε R2 R1R3L2 L3 C1 C2 \nA L1 εR2 R1 R3L2 L3 C1 C2 L1 r ε R2 A B \nR1R3L2T \nFigure 20:  37. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=2L, da tre resistenze R1= R2= R3= 2R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e  d a  u n  i n t e r r u t t o r e  T  i n i z i a l m e n t e  aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;       Determinare inoltre in regime stazionario (t Æ ∞): b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   38. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=L, da tre resistenze R1= R2= R3= R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e da un interruttore T inizialmente aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;            Determinare inoltre in regime stazionario (t Æ ∞):  b. il valore del potenziale nel punto B; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   39. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=2L, da tre resistenze R1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e da due condensatori di capacità C1= C2=C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  40. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=L, da tre resistenze R1=R2=R3=2R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e  da due condensatori di capacità C1= C2=2C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  L1 r ε R2 R1A B \nR3 L2 T \nA L1 ε R2 R1R3L2 L3 C1 C2 \nA L1 εR2 R1 R3L2 L3 C1 C2 L1 r ε R2 A B \nR1R3L2T Figure 19:7.14Si consideri il circuito mostrato in ﬁgura 20 composto da tre induttanzeL1=L2=L3=2L,d at r er e s i s t e n z eR1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotriceEe da due condensatori di capacita` aC1=C2=C. Determinare in regime stazionario:a) la corrente elettrica che circola nelle tre resistenze; (i=ER1+R3)b) lenergia totale immagazzinata nel sistema; (U=12LE2R2+12CE2)c) la potenza dissipata nel sistema. (P=E2R1+R3)  37. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=2L, da tre resistenze R1= R2= R3= 2R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e  d a  u n  i n t e r r u t t o r e  T  i n i z i a l m e n t e  aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;       Determinare inoltre in regime stazionario (t Æ ∞): b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   38. Si consideri il circuito mostrato in figura composto da due induttanze L1= L2=L, da tre resistenze R1= R2= R3= R, da un generatore di resistenza interna r=R/2 che fornisce una forza elettromotrice ε e da un interruttore T inizialmente aperto. Determinare:  a. la corrente elettrica che circola nelle tre resistenze in funzione del tempo;            Determinare inoltre in regime stazionario (t Æ ∞):  b. il valore del potenziale nel punto B; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.   39. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=2L, da tre resistenze R1=R2=R3=R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e da due condensatori di capacità C1= C2=C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  40. Si consideri il circuito mostrato in figura composto da tre induttanze L1=L2=L3=L, da tre resistenze R1=R2=R3=2R, da un generatore di resistenza interna trascurabile che fornisce una forza elettromotrice ε e  da due condensatori di capacità C1= C2=2C. Determinare in regime stazionario: a. la corrente elettrica che circola nelle tre resistenze; b. il valore del potenziale nel punto A; c. l’energia totale immagazzinata nel sistema. d. la potenza dissipata nel sistema.  L1 r ε R2 R1A B \nR3 L2 T \nA L1 ε R2 R1R3L2 L3 C1 C2 \nA L1 εR2 R1 R3L2 L3 C1 C2 L1 r ε R2 A B \nR1R3L2T \nFigure 20:4",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#59": "Le equazioni di Maxwell\n60⃗∇⋅⃗E=ρε0∬S⃗E⋅̂ndS=QSε0⃗∇⋅⃗B=0⃗∇∧⃗E=−∂⃗B∂t∮Γ⃗E⋅d⃗l=−ddt∬S⃗B⋅̂ndS∮Γ⃗B⋅d⃗l=μ0conc∑k(ic+is)⃗∇∧⃗B=μ0⃗𝚥+μ0ε0∂⃗E∂tForma differenzialeForma integrale∬S⃗B⋅̂ndS=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#6": "Equazioni di Maxwell (caso stazionario)\n7∮Γ⃗E⋅d⃗l=0∮Γ⃗B⋅d⃗l=μ0conc∑kikForma integrale∬S⃗E⋅̂ndS=QSε0∬S⃗B⋅̂ndS=0",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#60": "Le equazioni di Maxwell\n61Le quattro equazioni di Maxwell descrivono completamente l’elettromagnetismo A partire da esse è possibile ricavare tutte le leggi dell’elettromagnetismo, dall’elettrostatica, alle correnti, alle forze elettriche e magnetiche Dalle equazioni di Maxwell si evince che i campi elettrico e magnetico sono strettamente legati tra di loro e che essi sono due modi di manifestarsi della stessa entità chiamata campo elettromagnetico Partendo dalle equazioni di Maxwell, si dimostra che il campo elettromagnetico si propaga attraverso onde elettromagnetiche, le quali hanno sempre una componente di campo elettrico e una di campo magnetico",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#61": "Argomenti Facoltativi\n62• Equazione delle onde elettromagnetiche • Onde elettromagnetiche piane • Teorema di Poynting ed energia trasportata dalle onde elettromagnetiche",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#62": "Onde elettromagnetiche\n63Prendiamo in considerazione le quattro equazioni di Maxwell in  assenza di cariche e di correnti di conduzione⃗∇⋅⃗E=0⃗∇⋅⃗B=0⃗∇∧⃗E=−∂⃗B∂t⃗∇∧⃗B=μ0ε0∂⃗E∂t∇2⃗E=ε0μ0∂2⃗E∂t2Combinando le quattro relazioni e utilizzando le proprietà delle operazioni tra operatori si ricavano le due equazioni di D’Alambert per campo elettrico e magnetico∇2⃗B=ε0μ0∂2⃗B∂t2ε0μ0=1c2c=3×108 m/s velocità della luce nel vuoto",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#63": "Lorenzo Rinaldi Dipartimento di Fisica e Astronomia lorenzo.rinaldi@unibo.it https://www.unibo.it/sitoweb/lorenzo.rinaldi/\n64",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#7": "Legge di Ampère su condensatore\n8Scriviamo la legge di Ampère in funzione della densità di corrente ∮Γ⃗B⋅d⃗l=μ0∬S⃗𝚥C⋅̂ndSTale legge deve essere vera per qualsiasi superficie aperta S avente per bordo la linea chiusa 𝛤Applichiamo tale legge in un circuito con condensatore:\nS1S2𝛤S1 interseca il filo  S2 passa nell’intercapedine del condensatore (senza intersecare il filo) Entrambe hanno come bordo la linea chiusa 𝛤",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#8": "Legge di Ampère su condensatore\n9\nS1S2𝛤In condizioni stazionarie, nei rami di circuiti con condensatori non circola corrente quindi la circuitazione è nulla: legge di Ampère è soddisfatta per entrambe le superfici Cosa succede in regime transitorio, quando si ha una corrente:                   \nLa legge di Ampère                                         è valida solo in condizioni stazionarie ∮Γ⃗B⋅d⃗l=μ0∬S⃗𝚥C⋅̂ndSi(t)=ℰRe−tRCIn tal caso il flusso attraverso S1 è diverso da zero, mentre risulta nullo attraverso S2",
    "data_test\\rootfolder\\università\\FisicaGenerale\\13-elettromagnetismo.pdf#9": "Legge di Ampère\n10Oltre al caso del condensatore, la legge di Ampère presenta un altro problema formale⃗∇∧⃗B=μ0⃗𝚥⃗∇⋅(⃗∇∧⃗B)=μ0⃗∇⋅⃗𝚥⃗∇⋅(⃗∇∧⃗B)=0Si dimostra facilmente che la divergenza del rotore di un campo vettoriale è sempre nulla (qualunque sia il campo)⃗∇⋅⃗𝚥=?Non è vero invece che la divergenza della densità di corrente sia sempre nulla In quali condizioni è nulla? (il vettore densità di corrente è solenoidale?) Applicando l’operatore divergenza ad entrambi i membri dell’equazione di Ampère in forma differenziale:",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#0": "Brevi appunti di Fondamenti di\nAutomatica\nprof. Stefano Panzieri\nDipartimento di Ingegneria\nUniversit\u0012 a degli Studi \\ROMA TRE\"\nUNIVERSI TÀ DEGLI  STUDIROMA\nTRE\n15 marzo 2022\n1",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#1": "Stefano Panzieri\nIndice\nIndice 2\n1 La Trasfomata di Laplace e la Funzione\ndi Trasferimento 3\n1.1 Introduzione . . . . . . . . . . . . . . . . . 3\n1.2 Una trasformata elementare . . . . . . . . 5\n1.3 Alcune propriet\u0012 a . . . . . . . . . . . . . . 8\n1.4 Trasformata dell'impulso . . . . . . . . . . 15\n1.5 Trasformate dei polinomi . . . . . . . . . . 17\n1.6 Inversione della Trasformata di Laplace . . 19\n1.7 Andamento delle antitrasformate nel tempo 23\n1.8 Applicazione delle Trasformate di Laplace\nalle equazioni di\u000berenziali . . . . . . . . . 27\n1.9 Un esempio: il carrello . . . . . . . . . . . 36\n1.10 Esempi di funzioni di trasferimento . . . . 40\nRev. 0.3 Appunti di Automatica 2 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#10": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nTraslazione nel tempo\nUn risultato duale del precedente \u0012 e dato dalla propriet\u0012 a di\ntraslazione nel tempo. Notiamo che \u0012 e meglio premoltipli-\ncare la funzione f(t) per un gradino unitario per evitare\nche, traslando a destra, rientri nell'integrale della trasfor-\nmata una quantit\u0012 a diversa da zero. Immaginando che a\nsia maggiore di zero:\nL\u0000f\u000e\u00001(t\u0000a)f(t\u0000a)g=e\u0000asF(s) (1.11)\nDimostrazione: dalla de\fnizione si ha\n1Z\n0\u0000\u000e\u00001(t\u0000a)f(t\u0000a)e\u0000stdt\nposto\u001c=t\u0000a, ovverot=\u001c+asi ottiene:\n1Z\n\u0000a\u0000\u000e\u00001(\u001c)f(\u001c)e\u0000s\u001ce\u0000sad\u001c=F(s)e\u0000as\nL'estremo inferiore \u0012 e ovviamente minore di zero per cui\npu\u0012 o essere sostituito, vista la presenza del gradino in zero,\ncon 0\u0000.z\nConvoluzione\nL'operatore di convoluzione \u0012 e molto utilizzato nell'ambi-\nto dello studio dei sistemi lineari de\fniti da un'equazione\nRev. 0.3 Appunti di Automatica 11 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#11": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\ndi\u000berenziale lineare a coe\u000ecienti costanti. Abbiamo gi\u0012 a\nvisto, infatti, che l'uscita di un sistema descritto dalla\nsua risposta impulsiva h(t), e a cui venga applicato un\ningressou(t), \u0012 e data dalla convoluzione dei due segnali\ny(t) =h(t)\u0003u(t): (1.12)\nNell'ambito della Trasformata di Laplace la convo-\nluzione viene a sempli\fcarsi e si riduce a un semplice\nprodotto, vediamo come.\nSupponiamo che la funzione g(t) sia de\fnita come la\nconvoluzione di due funzioni f1(t) ef2(t):\ng(t) =f1(t)\u0003f2(t) =tZ\n0\u0000f1(t\u0000\u001c)f2(\u001c)d\u001c (1.13)\nil risultato sar\u0012 a:\nL\u0000fg(t)g=L\u0000ff1(t)g\u0001 L\u0000ff2(t)g (1.14)\nDimostrazione: supponiamo che f1(t) = 0 pert<0,\ncosa che abbiamo gi\u0012 a detto essere vera nell'ambito del-\nlo studio che stiamo facendo. Di conseguenza, potremo\nriscrivere l'eq. 1.13 estendendo l'estremo superiore:\ng(t) =f1(t)\u0003f2(t) =1Z\n0\u0000f1(t\u0000\u001c)f2(\u001c)d\u001c\nRev. 0.3 Appunti di Automatica 12 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#12": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nA questo punto facciamo la Trasformata di Laplace di\ng(t):\nG(s) =Z1\nt=0\u0000\u0014Z1\n\u001c=0\u0000f1(t\u0000\u001c)f2(\u001c)d\u001c\u0015\ne\u0000stdt\n=Z1\n\u001c=0\u0000f2(\u001c)\u0014Z1\nt=0\u0000f1(t\u0000\u001c)e\u0000stdt\u0015\nd\u001c\n=Z1\n\u001c=0\u0000f2(\u001c)\u0001F1(s)e\u0000s\u001cd\u001c\n=F1(s)Z1\n\u001c=0\u0000f2(\u001c)e\u0000s\u001cd\u001c=F1(s)\u0001F2(s) (1.15)\nche dimostra chiaramente l'eq. 1.14 z\nNotiamo che, grazie a questa propriet\u0012 a, la relazione\n1.12 diventer\u0012 a semplicemente:\nY(s) =H(s)G(s): (1.16)\nDerivazione\nVediamo adesso cosa succede quando proviamo a trasfor-\nmare una funzione derivata rispetto al tempo. Il risultato\n\u0012 e il seguente:\nL\u0000\u001ad\ndtf(t)\u001b\n=sF(s)\u0000f(0\u0000) (1.17)\nDimostrazione: Notiamo che\nd\ndt\u0000\nf(t)e\u0000st\u0001\n=df(t)\ndte\u0000st\u0000f(t)se\u0000st\nRev. 0.3 Appunti di Automatica 13 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#13": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\ne quindi, integrando per parti,\nL\u0000\u001ad\ndtf(t)\u001b\n=Z1\n0\u0000df(t)\ndte\u0000stdt=\n=Z1\n0\u0000d\ndt\u0000\nf(t)e\u0000st\u0001\ndt+Z1\n0\u0000f(t)se\u0000stdt\n=\u0002\nf(t)e\u0000st\u00031\n0\u0000+sF(s)\n=\u0000f(0\u0000) +sF(s) (1.18)\nche dimostra l'eq. 1.17 z\nSi possono ottenere, con un procedimento analogo,\nanche le seguenti propriet\u0012 a:\nL\u0000\u001ad2\ndt2f(t)\u001b\n=s2F(s)\u0000sf(0\u0000)\u0000df\ndt\f\f\f\f\nt=0\u0000(1.19)\nL\u0000\u001ad3\ndt3f(t)\u001b\n=s3F(s)\u0000s2f(0\u0000)\u0000sdf\ndt\f\f\f\f\nt=0\u0000\u0000d2f\ndt2\f\f\f\f\nt=0\u0000\n(1.20)\nIntegrazione\nQuesta \u0012 e, invece, la trasformata di Laplace di una fun-\nzione integrata tra 0\u0000et.\nL\u0000\u001aZt\n0\u0000f(\u001c)d\u001c\u001b\n=1\nsF(s) (1.21)\nDimostrazione: Dalla trasformata della convoluzione\nL\u0000\u001aZt\n0\u0000f(\u001c)g(t\u0000\u001c)d\u001c\u001b\n=F(s)\u0001G(s)\nRev. 0.3 Appunti di Automatica 14 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#14": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nse poniamo g(t) =\u000e\u00001(t)\nL\u0000\u001aZt\n0\u0000f(\u001c)\u000e\u00001(t\u0000\u001c)d\u001c\u001b\n=F(s)\u00011\ns\nL\u0000\u001aZt\n0\u0000f(\u001c)d\u001c\u001b\n=F(s)\u00011\ns\nz\n1.4 Trasformata dell'impulso\nL'impulso matematico di Dirac \u000e0(t) \u0012 e il \\limite\" di una\ndistribuzione ad area costante e unitaria e per questo non\npu\u0012 o essere considerato una vera e propria funzione. L'im-\npulso rimane de\fnito per lo pi\u0012 u tramite le sue propriet\u0012 a,\ncome ad esempio questa che fu data per primo proprio\nda Dirac: Z1\n\u00001\u000e0(x)\u001e(x)dx=\u001e(0) (1.22)\nSe volessimo de\fnire una distribuzione (successione) di\nfunzioni che abbiano come \\limite\" l'impulso di Dirac do-\nvremmo ricorrere, ad esempio, a delle funzioni Gaussiane\ncome le seguenti:\n\u000e0(x) = lim\nk!1r\nk\n\u0019e\u0000kx2\nRev. 0.3 Appunti di Automatica 15 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#15": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nAbbiamo per\u0012 o usato la parola limite in maniera impro-\npria, in quanto questa de\fnizione della \u000e0(x) non \u0012 e pro-\npriamente formale. E' invece vero il seguente limite che\ngeneralizza l'eq. 1.22:\nZb\na\u000e0(x)f(x)dx= lim\nk!1Zb\nar\nk\n\u0019e\u0000kx2f(x)dx (1.23)\nAdesso, abbiamo gi\u0012 a detto che l'uscita di un sistema\nlineare si pu\u0012 o scrivere come convoluzione tra la risposta\nimpulsiva e l'ingresso, ovvero:\ny(t) =h(t)\u0003u(t) =Zt\n0h(t\u0000\u001c)u(\u001c)d\u001c\nSe in ingresso poniamo un impulso d Dirac \u000e0(t) avremo\ncome uscita:\ny(t) =Zt\n0h(t\u0000\u001c)\u000e0(t)d\u001c=h(t)\nda cui comprendiamo anche perch\u0013 e h(t) viene chiamata\nrisposta impulsiva.\nSe ci chiediamo quale sia la trasformata di Laplace\ndell'impulso unitario basta sostituirlo nella de\fnizione\ndella Trasformata:\nL\u0000f\u000e0(t)g=Zt\n0\u0000\u000e0(\u001c)e\u0000st=\u0002\ne\u0000st\u0003\nt=0= 1 (1.24)\nRev. 0.3 Appunti di Automatica 16 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#16": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\n1.5 Trasformate dei polinomi\nVediamo adesso di de\fnire la Trasformata di Laplace di\nuna funzione polinomiale. A tale scopo de\fniamo un ter-\nmine generico di grado k\u00001 utilizzando ancora il simbolo\n\u000e:\n\u000e\u0000k(t) =(\n0t<0\ntk\u00001\n(k\u00001)!t\u00150(1.25)\nperk= 1;2;::: \u0012 e una funzione di grado k\u00001 dit.\nPoich\u0013 e per k= 1;2;:::\nd\ndttk\nk!=tk\u00001\n(k\u00001)!\nsfruttando la propriet\u0012 a della trasformata dell'integrale\nL\u0000\u001atk\n(k)!\u001b\n=L\u0000\u001aZtk\u00001\n(k\u00001)!dt\u001b\n=1\nsL\u0000\u001atk\u00001\n(k\u00001)!\u001b\n(1.26)\nPer k=1 a destra nell'eq. 1.26 c'\u0012 e la trasformata di Lapla-\nce della costante \u000e\u00001(t), secondo l'eq. 1.25, che abbiamo\ngi\u0012 a visto essere pari a 1 =s. Per cui, sempre per k= 1 la\ntrasformata della rampa lineare \u000e\u00002, pari a zero prima di\nRev. 0.3 Appunti di Automatica 17 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#17": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nt= 0 e poi pari a tpert>0 varr\u0012 a 1=s2. Riassumendo:\nL\u0000f\u000e0(t)g= 1 (impulso) (1.27)\nL\u0000f\u000e\u00001(t)g=1\ns(gradino) (1.28)\nL\u0000f\u000e\u00002(t)g=1\ns2(rampa) (1.29)\nL\u0000f\u000e\u00003(t)g=1\ns2(rampa parabolica) (1.30)\nIn generale\nL\u0000f\u000e\u0000k(t)g=L\u0000\u001atk\u00001\n(k\u00001)!\u001b\n=1\nsk(1.31)\noppure\nL\u0000\b\ntk\t\n=k!\nsk+1\nPolinomi per esponenziali\nVediamo cosa succede se moltiplichiamo un polinomio\nper un esponenziale:\nL\u0000\u001at(k\u00001)ept\n(k\u00001)!\u001b\n=1\n(s\u0000p)k(1.32)\nRev. 0.3 Appunti di Automatica 18 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#18": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nDimostrazione:\nL\u0000\u001at(k\u00001)ept\n(k\u00001)!\u001b\n=1Z\n0\u0000t(k\u00001)ept\n(k\u00001)!e\u0000stdt\n=1Z\n0\u0000t(k\u00001)\n(k\u00001)!e\u0000(s\u0000p)tdt\npostos\u0000p=\u0018\n1Z\n0\u0000t(k\u00001)\n(k\u00001)!e\u0000\u0018tdt=1\n\u0018k=1\n(s\u0000p)k\nz\n1.6 Inversione della Trasformata\ndi Laplace\nL'inversione della Trasformata di Laplace \u0012 e un proble-\nma che si pone, ad esempio, nel calcolo dell'uscita di\nun sistema lineare. Infatti, una volta che si sia trovata\nl'espressione dell'uscita nel dominio di Laplace, normal-\nmente utilizzando la relazione 1.16, possiamo voler ritor-\nnare nel dominio del tempo per calcolarne e gra\fcarne\nl'andamento.\nRev. 0.3 Appunti di Automatica 19 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#19": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nOvviamente esiste una formulazione integrale dell'an-\ntitrasformata di Laplace, chiamata anche integrale di Brom-\nwich o formula inversa di Mellin, che \u0012 e data da questo\nintegrale di linea:\nL\u00001fF(s)g=1\n2\u0019ilim\nT!1Z\r+iT\n\r\u0000iTestF(s)ds (1.33)\ndove l'integrazione avviene lungo la linea verticale <(s) =\r\nnel piano complesso, con \rmaggiore della parte reale di\ntutte le singolarit\u0012 a di F(S). Questo assicura che la li-\nnea di contorno sia nella regione di convergenza della\nTrasformata di Laplace.\nTuttavia, l'applicazione di questo integrale non risulta\nnecessaria nel momento in cui le funzioni da antitrasfor-\nmare siano formate da un rapporto di polinomi, cosa che\nsuccede sempre in ambito sistemi dinamici descritti da\nuna equazione di\u000berenziale lineare e a coe\u000ecienti costan-\nti a cui applichiamo come ingresso un qualsiasi polinomio\no una funzione sinusoidale.\nIn e\u000betti, in questo caso \u0012 e pi\u0012 u semplice ricondurre la\nY(s) a una somma di frazioni elementari che siano simili\nalle trasformate gi\u0012 a note. In particolare, sappiamo gi\u0012 a\nRev. 0.3 Appunti di Automatica 20 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#2": "Capitolo 1\nLa Trasfomata di\nLaplace e la Funzione\ndi Trasferimento\n1.1 Introduzione\nLo strumento della Trasformata di Laplace, che prende\nil nome da Pier-Simon Laplace il quale la utilizz\u0012 o nel-\nl'ambito della teoria delle probabilit\u0012 a in un suo trattato\nTh\u0013 eorie analytique des Probabilit\u0013 es (1812), fu probabil-\nmente introdotta inizialmente da Eulero e divenne molto\npopolare nell'ambito delle equazioni di\u000berenziali grazie\na Oliver Heaviside. Si, quello della funzione a gradino,\n3",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#20": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\ncome antitrasformare le seguenti funzioni razionali fratte:\nTermine elementare \u0000!antitrasformata\n1\u0000!\u000e0(t)\n1\nsk\u0000!t(k\u00001)\n(k\u00001)!\n1\n(s\u0000p)\u0000!ept\n1\n(s\u0000p)k\u0000!t(k\u00001)ept\n(k\u00001)!\n!\ns2+!2\u0000!sinwt\ns\ns2+!2\u0000!coswt\n!\n(s+a)2+!2\u0000!e\u0000atsinwt\ns\n(s+a)2+!2\u0000!e\u0000atcoswt\ndove le ultime due sono state ricavate applicando il teore-\nma della traslazione in salle due immediatamente prece-\ndenti. Come si vede manca ancora qualcosa che ci dia\nla possibilit\u0012 a di antitrasformare una qualsiasi frazione\nche a denominatore abbia un termine trinomio con ra-\ndici complesse e coniugate e a numeratore un qualunque\npolinomio di primo grado. Notiamo per\u0012 o che combinan-\ndo linearmente le ultime due con due coe\u000ecienti bec\nRev. 0.3 Appunti di Automatica 21 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#21": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\npossiamo scrivere:\nbs+c!\n(s+a)2+!2=bs+c!\ns2+ 2as+a2+!2\n\u0000!e\u0000at(bsinwt+ccoswt)\ndove con una opportuna scelta di a;b;c e!siamo in grado\ndi rappresentare una qualsiasi frazione del tipo\nb1s+b0\ns2+a1s+a0\nponendo:b=b1,c!=b0, 2a=a1ea2+!2=a0.\nDecomposizione in poli e residui\nA questo punto vediamo come sia possibile analizzare una\nTrasformata di Laplace espressa come rapporto di polino-\nmi il cui grado sia tale da avere il grado del denominatore\npi\u0012 u alto di quello del denominatore.\nIn algebra, la decomposizione poli e residui, o decom-\nposizione in fratti semplici, \u0012 e la conversione di una fun-\nzione razionale fratta in una somma di termini elementari\npi\u0012 u semplici del tipo\nR\n(s\u0000p);R\n(s\u0000p)n\nconpreale o anche numero complesso p=\u001b+j!.\nRev. 0.3 Appunti di Automatica 22 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#22": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nLa decomposizione in poli e residui prevede che i fratti\nsemplici si compongano utilizzando le radici del denomi-\nnatore (poli) e siano presenti nella forma\nRi\ns\u0000pi\nnel caso di poli semplici pi, o nella forma\nRn\n1\n(s\u0000pi)+Rn\u00001\n1\n(s\u0000pi)2+\u0001\u0001\u0001+R1\n1\n(s\u0000pi)n\nnel caso di poli pidi molteplicit\u0012 a pari a n.\nIl calcolo dei coe\u000ecienti Rk\ni(residui) procede secondo\nquesta espressione:\nRk\ni=1\n(k\u00001)!dk\u00001\ndsk\u00001[(s\u0000pi)nF(s)]\f\f\f\ns=pi(1.34)\nche nel caso di poli semplici si riduce a\nRi= [(s\u0000pi)F(s)]\f\f\f\ns=pi(1.35)\n1.7 Andamento delle\nantitrasformate nel tempo\nAl \fne di caratterizzare al meglio gli andamenti delle pos-\nsibili risposte di un sistema, \u0012 e conveniente de\fnire alcuni\nRev. 0.3 Appunti di Automatica 23 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#23": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nparametri che ci potranno aiutare a comprendere questi\nandamenti e a confrontare in maniera e\u000ecace andamenti\ndi\u000berenti. In particolare, considereremo poli reali e poli\ncomplessi coniugati di molteplicit\u0012 a pari a uno.\nParametri dei poli reali\nNel caso di trasformate contenenti poli reali semplici del\ntipo\nYt(s) =Pbisi\n(s\u0000p)(:::)=R\n(s\u0000p)+:::\ndove abbiamo supposto che Rsia il residuo del polo p, la\nrisposta nel tempo conterr\u0012 a un termine del tipo\ny(t) =Rept\nche vale nell'origine y(0) =Re la cui derivata, calcolata\nsempre nell'origine, vale _ y(0) =Rp. Questo signi\fca che,\nipotizzando p < 0, ovvero una dinamica convergente a\nzero, la tangente in 0, di equazione y=Rpx+Rincon-\ntrer\u0012 a l'asse delle ascisse esattamente in x=\u00001\np, come si\nvede dalal Fig. 1.1. La grandezza1\npviene chiamata nor-\nmalmente costante di tempo e indicata con \u001c. Il modo\nreale convergente con costante di tempo pari a \u001cdopo 3\u001c\nsi \u0012 e ridotto del 95%, quindi la dinamica si \u0012 e quasi esau-\nrita. Dopo 6 \u001cil valore residuo \u0012 e circa pari a 0 :0025R,\npraticamente quasi del tutto estinto.\nRev. 0.3 Appunti di Automatica 24 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#24": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nFigura 1.1: Poli reali\nParametri dei poli complessi e coniugati\nNel caso di trasformate contenente poli complessi e co-\nniugati, e quindi radici del tipo p=\u001b+j!ep\u0003=\u001b\u0000j!,\navremo\nYt(s) =Pbisi\n(s2+a1s+a0)(:::)=R\n(s\u0000p)+R\u0003\n(s\u0000p\u0003)+:::\nDoveReR\u0003sono i residui, coniugati, dei due poli coniu-\ngati.\nE' possibile, a questo punto, esprimere i due residui\nnella notazione polare modulo e fase\nR=jRjej';\nR\u0003=jRje\u0000j';\ndove il residuo coniugato \u0012 e stato scritto come un nu-\nmero che ha lo stesso modulo e fase cambiata di segno.\nIn questo maniera, scrivendo l'antitrasformata nel modo\nRev. 0.3 Appunti di Automatica 25 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#25": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nconsueto:\ny(t) =jRjej'e(\u001b+j!)t+jRje\u0000j'e(\u001b\u0000j!)t\n=jRje\u001bt[ej(!t+')+e\u0000j(!t+')]\n= 2jRje\u001bt[ej(!t+')+e\u0000j(!t+')]\n2\n= 2jRje\u001btcos(!t+')\nche, come gi\u0012 a visto, porta a una espressione oscillante con\nun modulo che pu\u0012 o convergere a zero oppure divergere a\nseconda del segno della parte reale dei poli.\nTerminologia\n(s\u0000p)(s\u0000p\u0003) =s2\u00002!s+ (!2+\u001b2) =s2+ 2\u0010!ns+!2\nn\nDove!n\u0012 e la pulsazione naturale; e \u0010\u0012 e il coe\u000eciente\ndi smorzamento\ndove se\u0010\u00151 i poli del sistema sono reali\npoi se \u0012 e proprio uguale a 1 lo smorzamento \u0012 e massimo\ne non ci sono oscillazioni\nse invece\u0010 <0 il sistema diverge\nSe poi lo smorzamento \u0012 e proprio pari a zero ho poli\nsull'asse immaginario\np1;2=\u0000\u0010!n\u0006p\n\u00102!2\nn\u0000!2\nn\nPosso poi determinare, attraverso la regola del segno\nse le radici sono solo a parte reale negativa, infatti:\nRev. 0.3 Appunti di Automatica 26 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#26": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nFigura 1.2: Poli complessi e coniugati\nse c'\u0012 e una permanenza del segno ho radici negative,\naltrimenti nel caso in cui c'\u0012 e variazione del segno ho radici\npositive\n1.8 Applicazione delle\nTrasformate di Laplace alle\nequazioni di\u000berenziali\nTrasformazione di una equazione\ndi\u000berenziale lineare di ordine n\nandny\ndtn+::::+a1dy\ndt+a0y(t) =bmdmu\ndtm+::::+b0u(t)\nL\u0000h\ndny(t)\ndtni\n=snY(s)\u0000sn\u00001y(0)\u0000sn\u00002y(0)\u0000::::\u0000y(n\u00001)(0) =\nsnY(s)\u0000Pn\u00001\nk=0sn\u0000k\u00001y(k)(0) (dove il termine con la som-\nmatoria sono le condizioni iniziali)\nquindi:ansnY(s)+::::+a1sY(s)+a0Y(s)+CI(n\u00001)\ny(s) =\nbmsmU(s) +::::+b0U(s) +CI(m\u00001)\nu (s)\nRisolvendo per Y(s) si avr\u0012 a :\nRev. 0.3 Appunti di Automatica 27 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#27": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nY(s) =bmsm+:::+b0\nansn+::::+a0U(s)\u0000CIy(n\u00001)(s)\nansn+::::+a0+CI(m\u00001)\nu (s)\nansn+::::+a0\ndove il primo termine lo consideriamo come A il se-\ncondo come B e il terzo come C\nInoltre notiamo come al denominatore compaiano in\ntutti gli addendi, questo \u0012 e il polinomio dell'equazione\nomogenea.\nDe\fnizione dell'evoluzione libera e della\nrisposta forzata\nU(s)Pbisi\nPaisi\u0000!A\nCIyPaisi\u0000!B\nCIuPaisi\u0000!C\nI terminiAeCsono nulli se u(t) = 0 quindi B rap-\npresenta l'evoluzione libera del sistema; in particolare C\n\u0012 e nullo seu(t) = 0 pert\u00140.\nVisto che il denominatore di Ce diB\u0012 e lo stesso se\nconverge l'evoluzione libera converge anche C\nIl denominatore di Acontiene i poli di Bpi\u0012 u quelli\ndella trasformata dell'ingresso quindi:\n\u000fI modi presenti nell'uscita sono quelli propri del\nsistema pi\u0012 u quelli dell'ingresso\nRev. 0.3 Appunti di Automatica 28 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#28": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\n\u000fSe i modi del sistema convergono a zero nel lungo\nperiodo rimangono solo quelli dell'ingresso e quindi\nil sistema \u0012 e stabile\nQuindi intuitivamente possiamo dire che un sistema \u0012 e\nstabile se basta azzerare l'ingresso per riportare il sistema\na riposo.\nDe\fnizione della funzione di\ntrasferimento e di sistema nel senso di\nLaplace\nLa funzione di trasferimento del sistema descritto dall'e-\nquazione di\u000berenziale \u0012 e:\nG(s) =Pbisi\nPaisi\nUn sistema \u0012 e descritto quasi completamente (salvo\ncancellazioni) dalla sua funzione di trasferimento\nL'analisi della G(s) ci permette di determinare facil-\nmente:\n\u000fLa stabilit\u0012 a asintotica: Re[pi]<0\n\u000fVelocit\u0012 a di convergenza: maggiore se Re[pi] minore\n\u000fComportamento oscillatorio pi=p\u0003\njcomplessi\nRev. 0.3 Appunti di Automatica 29 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#29": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\n\u000fValore pert\u0000!1 dell'uscita (regime): lim\ns!0sG(s)U(s)\nspessou(t) =\u000e\u00001\u0000!U(s) =1\ns\nTrasfomata di Laplace della\nconvoluzione e legame tra la trasformata\ndi Fourier e quella di Laplace\nData:\ng(t) =f1(t)\nf2(t) =tZ\n0\u0000f1(t\u0000\u001c)f2(\u001c)d\u001c\nallora:\nL\u0000fg(t)g=L\u0000ff1(t)g\u0001 L\u0000ff2(t)g\nDimostrazione:\nG(s) =Z1\n0\u0000\u0014Z1\n0\u0000f1(t\u0000\u001c)f2(\u001c)d\u001c\u0015\ne\u0000stdt=Z1\n0\u0000f2(\u001c)\u0014Z1\n0\u0000f1(t\u0000\u001c)e\u0000stdt\u0015\nd\u001c=\n=Z1\n0\u0000f2(\u001c)\u0001F1(s)e\u0000s\u001cd\u001c=F1(s)Z1\n0\u0000f2(\u001c)e\u0000s\u001cd\u001c=F1(s)\u0001F2(s)\nIl valore di g(t) int0dipende dal passato\nSerie di Fourier:\nf(t) =F(t\u0006KT) periodica di periodo T\nf(t) =A0\n2+P1\nR=1\u0002\nAkcos2\u0019k\nTt+Bksin2\u0019k\nTt\u0003\nRev. 0.3 Appunti di Automatica 30 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#3": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nche invent\u0012 o per calcolare cosa succedesse a un circuito\nelettrico quando veniva improvvisamente alimentato.\nComunque sia andata, oggi la Trasformata di Laplace\nla studiamo perch\u0013 e ancora introduce notevoli sempli\fca-\nzioni nell'analisi e la ricerca di soluzione delle equazioni\ndi\u000berenziali lineari e ci consente, inoltre, di fare un po'\ndi analisi armonica, ovvero analizzare il comportamento\ndi un sistema quando applichiamo un segnale sinusoi-\ndale. Inoltre, nello studio della stabilit\u0012 a di un sistema\na controreazione porta a dei risultati molto semplici da\nutilizzare.\nIl presente Capitolo \u0012 e dedicato alla de\fnizione del-\nla Trasformata di Laplace, la presentazione di alcune sue\npropriet\u0012 a, e la sua applicazione alle equazioni di\u000berenziali\nlineari. Immaginando che un sistema possa essere descrit-\nto con una equazione di\u000berenziale lineare vedremo come,\nutilizzando Laplace, lo stesso possa essere espresso come\nfunzione di trasferimento , ovvero un oggetto matematico\nottenuto da una particolare trasformata di Laplace.\nDe\fnizione\nLa trasformata di Laplace unilatera, associata a una fun-\nzione di variabile reale, come pu\u0012 o essere nel nostro caso\nRev. 0.3 Appunti di Automatica 4 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#30": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\n\u000fA0\n2=1\nTT\n2R\n\u0000T\n2f(t)dt[valore medio]\n\u000fAk=2\nTT\n2R\n\u0000T\n2f(t)cos2\u0019k\nTtdt;\n\u000fBk=2\nTT\n2R\n\u0000T\n2f(t)sin2\u0019k\nTtdt\nUtilizzando i numeri complessi:\nf(t) =\n2\u00191X\nk=\u00001F(k\n)ejk\nt\ndove:\nF(k\n) =1\nTT\n2Z\n\u0000T\n2f(t)e\u0000jk\ntdt\ncon \n =2\u0019\nT\nLa serie \u0012 e per i segnali periodici, un segnale qualsiasi\npu\u0012 o essere visto come periodico con periodo in\fnito\nSeT\u0000!1 , \n\u0000!0 ,k\u0019\u0000!!\nAllora:\nF(!) =1Z\n\u00001f(t)e\u0000j!tdt\nRev. 0.3 Appunti di Automatica 31 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#31": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nF(t) =1Z\n\u00001F(!)ej!td!\nMa proprio i casi pi\u0012 u interessanti creano problemi\nInvece di trasformare f(t) trasformiamo:\n\u000e\u00001(t)f(t)e\u0000\u001bt\nPongos=\u001b+j!\ne avr\u0012 a :\nF(\u001b+j!) =1Z\n0f(t)e\u0000\u001bte\u0000j!tdt\nQuindi:\nF(s) =1Z\n0f(t)e\u0000stdt\nLAPLACE\nVediamo quindi che esiste una diretta corrispondenza\ntra la serie di Fourier e la tra trasformata di Laplace.\nRev. 0.3 Appunti di Automatica 32 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#32": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nDe\fnizione della risposta impulsiva\ncome antitrasformata della funzione di\ntrasferimento\nData:\nY(s) =G(s)U(s)\nAssumiamo U(s) = 1\u0000!u(t) =\u000e(t) Impulso\ny(t) =g(t) RISPOSTA IMPULSIVA\nPossiamo dire che l'impulso ha area unitaria quindi:\n1Z\n\u00001\u000e0(t)dt= 1\nMa anche con la convoluzione : y(t) =tR\n0u(\u001c)g(t\u0000\n\u001c)d\u001c=tR\n0\u000e(t)g(t\u0000\u001c)d\u001c=g(t)\nInoltre seu(t) =\u000e\u00001(t)U(s) =1\nsGradino\nY(s) =G(s)\ns\ny(t) =tZ\n0g(\u001c)d\u001c=g\u00001(t)\nQuesta \u0012 e la risposta indiciale cio\u0012 e l'integrale della\nrisposta impulsiva\nRev. 0.3 Appunti di Automatica 33 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#33": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nModi propri di un sistema\nL'antitrasformata di:\nG(s) =Pbisi\nPaisi\n\u0012 e composta, o meglio, \u0012 e combinazione lineare di:\n\u000fEsponenziali: eat,eatsin (!t+')\n\u000fPolinomi del tempo: t0(che \u0012 e una costante), t,t2\n2,\nt3\n3!\n\u000fPolinomi(t) per esponenziali: teat\n\u000fEventualmente impulsi nell'origine al limite della\ncausalit\u0012 a : \u000e0(t)\nQuesti sono i modi naturali del sistema, dove il lo-\nro numero \u0012 e pari all'ordine dell'equazione di\u000berenziale(le\nsinusoidi contano 2)\nLe caratteristiche del sistema si ri\rettono sulla posi-\nzione dei poli sul piano s ( Re[s];Im[s]), infatti la con-\nvergenza dipende dalla loro parte reale che dovr\u0012 a essere\nminore di 0.\nRev. 0.3 Appunti di Automatica 34 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#34": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nDe\fnizione di stabilit\u0012 a asintotica\nSi pu\u0012 o parlare di stabilit\u0012 a asintotica se Re[pi]<0\nNel caso del polo semplice \u0000! lim\nt!1epit\u0000!0;\nnel caso del polo doppio \u0000! lim\nt!1tepit\u0000!0 comun-\nque\nMa cosa succede se Re[pi] = 0 ?\nSe semplice, l'evoluzione libera contiene una costante\nquindi il sistema \u0012 e stabile ma non asintoticamente. Se in-\nvece \u0012 e multiplo, contiene una rampa, una parabola quin-\ndi instabile. Per poli immaginari puri, si hanno sinusoidi\nnel caso di poli semplici o divergenti polinomialmente nel\ncaso di poli multipli.\nDe\fnizione di regime transitorio e di\nregime permanente\nDato un sistema stabile la risposta forzata pu\u0012 o essere\ncomposta dal periodo di tempo prima dell'estinzione dei\nmodi naturali del sistema dove l'uscita si assesta det-\nto REGIME TRANSITORIO , mentre il periodo di tem-\npo che lo segue dove rimane solo la parte con i poli\ndell'ingresso \u0012 e detto REGIME PERMANENTE .\nRev. 0.3 Appunti di Automatica 35 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#35": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nRisposta al permanente di un sistema\nasintoticamente stabile\n1.9 Un esempio: il carrello\nFacciamo un esempio meccanico e proviamo a scrivere\nun modello matematico (il sistema) immaginando un in-\ngresso da applicare e calcolando l'andamento nel tempo\ndell'uscita.\nCome modello prendiamo un carrellino di massa M\nche si muova su un piano orizzontale con un attrito vi-\nscoso pari a Dspinto da una forza fe(t). Immaginiamo\npure che la grandezza di uscita, quella a cui questa volta\nsiamo interessati, sia la sua velocit\u0012 a v(t).\nCome forza scegliamo una forza costante applicata a\npartire dat= 0:\nfe(t) =\u000e\u00001(t)L\u0000!Fe(s) =1\ns:\nL'equazione di equilibrio che governa il sistema \u0012 e la\nseguente:\nM_v(t) =fe(t)\u0000Dv(t) (1.36)\nTrasformando l'equazione di\u000berenziale 1.36, dove il pun-\ntino indica la derivazione rispetto al tempo, abbiamo:\nM[sV(s)\u0000v(0)] =Fe(s)\u0000DV(s)\nRev. 0.3 Appunti di Automatica 36 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#36": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\ndove abbiamo immaginato che il carrello possa avere una\ncondizione iniziale, ovvero una velocit\u0012 a iniziale diversa\nda zero. Se poi la velocit\u0012 a iniziale fosse zero, v(0) = 0, si\navrebbe pi\u0012 u semplicemente:\n[sM+D]V(s) =Fe(s)\nda cui, ricavando V(s):\nV(s) =1\nMs+DFe(s) =1\ns+ 11\ns: (1.37)\ndove abbiamo posto anche M= 1 eD= 1.\nPossiamo subito chiederci a quale valore tenda la velo-\ncit\u0012 a pert!1 e applicando il Teorema del valore \fnale\notterremo\nlim\ns!0s1\ns+ 11\ns= 1:\nA questo punto possiamo applicare la decoposizione\nin poli e residui per calcolare l'antitrasformata e quindi\nl'andamento dell'uscita:\nV(s) =R1\ns+ 1+R2\ns=1\ns\u00001\ns+ 1\ncon\nR1= (s+ 1)1\n(s+ 1)s\f\f\f\ns=\u00001=\u00001\nR2=s1\n(s+ 1)s\f\f\f\ns=0= 1\nRev. 0.3 Appunti di Automatica 37 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#37": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nFigura 1.3: Evoluzione libera e forzata\nquindi antitrasformando otterremo:\nv(t) =L\u00001\n\u0000\u00141\ns\u00001\ns+ 1\u0015\n=\nL\u00001\n\u0000\u00141\ns\u0015\n+L\u00001\n\u0000\u0014\u00001\ns+ 1\u0015\n=\u000e\u00001(t) +e\u0000t\nIn sostanza abbiamo risolto un'equazione di\u000berenziale\ntramite un'equazione algebrica.\nSe poi poniamo una condizione iniziale diversa da zero\nv(0) = 0:5\nV(s) =1\nMs+DFe(s) +v(0)\nMs+D=1\ns+ 11\ns+0:5\ns+ 1\ne 'antitrasformata dell'evoluzione libera, indichiamola con\nvl(t) varr\u0012 a\nvl(t)0:5e\u0000t:\nIn Fig. 1.3 troviamo la rappresentazione dei vari ter-\nmini. Il primo termine(1\ns+11\ns) lo abbiamo indicato con A\ne rappresenta la risposta forzata cio\u0012 e l'uscita causata dal-\nl'applicazione dell'ingresso; il secondo termine(0:5\ns+1), indi-\nchato conB\u0012 e l'evoluzione libera, cio\u0012 e l'uscita del sistema\nRev. 0.3 Appunti di Automatica 38 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#38": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nFigura 1.4: Andamento della velocit\u0012 a del carrellino\na partire da una condizione iniziale diversa da zero. No-\ntiamo che l'unico polo della risposta libera coincide con il\npolo di1\n(s+1)che \u0012 e la parte della risposta forzata dovuta\nsolo al sistema.\nSe supponessimo che dopo 5 secondi la forza appli-\ncata torni a 0 potremmo studiare cosa succede in alme-\nno due modi: il primo consiste nello studiare la risposta\ncompleta all'ingresso: \u000e\u00001(t)\u0000\u000e\u00001(t\u00005); il secondo \u0012 e\ninvece quello di immaginare il sistema in una condizione\niniziale di\u000berente, quella raggiunta con l'applicazione del\nprimo gradino, e da li considerare l'evoluzione libera del\nsistema.\nSi ottiene comunque il risultato in Fig. 1.4\nNotiamo che ponendo u(t) = 0 a partire da una condi-\nzione iniziale diversa da zero il sistema torna nella condi-\nzione di riposo v(t) = 0. Possiamo dedurne che il sistema\nsia asintoticamente stabile.\nRev. 0.3 Appunti di Automatica 39 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#39": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\n1.10 Esempi di funzioni di\ntrasferimento\n\u000fRisposte tipiche: impulsiva e indiciale\nLa risposta impulsiva \u0012 e di scarso interesse pratico\nperch\u0013 e gli impulsi non esistono \fsicamente ma \u0012 e\nimportante perch\u0013 e consente di vedere tutti e soli i\nmodi del sistema.\nInfatti le risposte canoniche si ricavano integrando\nquella impulsiva.\nLa risposta al gradino, detta anche risposta indicia-\nle, si ottiene integrando quella impulsiva; mentre\nla risposta alla rampa si ottiene integrando quella\nindiciale.\nTra l'altro \u0012 e proprio sulla risposta al gradino che si\nde\fniscono le speci\fche di progetto nel dominio del\ntempo.\n\u000fSistemi del primo ordine: il carrellino (FdT+C.I.)\nDate le condizioni per cui il sistema si trova a riposo\nMy::+Dy:=f(t)y(0) = 0,y:(0) = 0\ne considerando D=M= 1fa=Dx:\nY(s) [Ms2+Fs] =F(s) ;f(t) =\u000e\u00001(t)\nRev. 0.3 Appunti di Automatica 40 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#4": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nil tempo, rimane de\fnita dal seguente integrale:\nL\u0000ff(t)g:=Z1\n0\u0000f(t)e\u0000stdt:=F(s) (1.1)\ncons=\u001b+j!2C(dovef (t) = 0pert< 0). Unilatera\nin quanto uno degli estremi, quello inferiore, \u0012 e pari a 0\u0000\ne d'altro canto, nello studio di sistemi si suppone che la\nparte interessante e diversa da zero sia presente soltanto\npert >0 essendo tutta la storia del sistema, da \u00001 a\n0 riassunta nella condizione iniziale. Non solo, lo studio\ndella risposta di un sistema a un ingresso normalmente\npresuppone che quest'ultimo venga applicato a partire da\nt= 0 e sia completamente nullo prima.\nIl codominio della Trasformata di Laplace \u0012 e il piano\ndei numeri complessi e questo signi\fca che per interpre-\ntare correttamente questo operatore bisogna conoscere\nbene i numeri complessi e le operazioni su di essi.\n1.2 Una trasformata elementare\nVisto che le soluzioni delle equazioni di\u000berenziali lineari a\ncoe\u000ecienti costanti omogenee sono combinazioni lineari\ndi esponenziali reali o complessi, a questo punto vale la\npena studiare la trasformata di Laplace della funzione\nesponenziale. Sar\u0012 a una delle pochissime che ci serviranno\nin questo ambito.\nRev. 0.3 Appunti di Automatica 5 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#40": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nY(s) =1\ns(Ms+D)1\ns=1\ns2(Ms+D)\navr\u0012 a un doppio polo nell'origine quindi:\nR(1)\n1\ns+R(2)\n1\ns2+R2\ns+ 1\nOra vado ad e\u000bettuare il calcolo dei residui:\n{R(1)\n1= lim\ns!0d\ndsh\ns2 1\n(s+1)s2i\n= lim\ns!0\u00001\n(s+1)2=\u00001\n{R(2)\n1= lim\ns!0s2 1\ns2(s+1)= 1\n{R2= lim\ns!\u00001(s+ 1)1\ns2(s+1)= 1\nQuindi:\nY(s) =\u00001\ns+1\ns2+1\ns+ 1\ne\ny(t) =\u000e\u00001(t)\u0002\n\u00001 +t+e\u0000t\u0003\nVediamo quindi che il sistema non \u0012 e asintoticamen-\nte stabile e quindi il transitorio non si annulla!!\n\u000fSistemi del secondo ordine: massa-molla-smorzatore\nDate:\nf(t) =\u000e1(t) e date le stesse condizioni per cui il\nsistema si trova a riposo:\nRev. 0.3 Appunti di Automatica 41 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#41": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\ny(0) = 0 ey:(0) = 0\nIn questo caso invece consideriamo: M= 1 ,D= 2\n,K= 101\nMy::(t) +Dy:(t) +Ky(t) =f(t)\nMs2Y(s) +DsY (s) +KY(s) =F(s)\nRisolviamo quindi per Y(s) ottenendo cos\u0012 \u0010 :\nY(s) =1\nMs2+Ds+KF(s) =1\ns2+ 2s+ 1011\ns=N\nD\nconp1= 0 ,p2=\u00001\u000010j,p3=p\u0003\n2\nAnche in questo caso calcoliamo i residui:\nR1= lim\ns!0sN\nD=1\n101\u0000!1\n101\u000e\u00001(t)\nR2= lim\ns!\u00001\u000010j(s+ 1 + 10j)N\nD=\u00001\n202\u0000j\n2020\nR3=R\u0003\n2\nR2\ns+1+10j+R3\ns+1\u000010j=\u0000s+2\n101(s2+2s+101)\nOra dobbiamo antitrasformare: \u0000s+2\n101(s2+2s+101)\nI poli sono complessi quindi usiamo:\ne\u0000\u001btsin (!t)\u0000!!\n(s+\u001b)2+!2=!\ns2+2\u001bs+(\u001b2+!2)\ne\u0000\u001btcos (!t)\u0000!s+\u001b\ns2+2\u001bs+(\u001b2+!2)\nda cui\u001b= 1!= 10 e possiamo risolvere:\nRev. 0.3 Appunti di Automatica 42 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#42": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nA(s+\u001b) +B!=\u0000(s+ 2)A= 1,B= 0:1\nE quindi l'antitrasformata sar\u0012 a:\n(\u0000e\u0000tcos 10t\u0000e\u0000t\n10sin 10t)\nQuindi avr\u0012 o :\ny(t) =1\n101\u0014\n\u000e\u00001(t)\u0000(e\u0000tcos 10t\u0000e\u0000t\n10sin 10t)\u0015\ndove\u000e\u00001(t) \u0012 e simile all'ingresso ed il termine ( e\u0000tcos 10t\u0000\ne\u0000t\n10sin 10t) \u0012 e il transitorio legato all'ingresso.\n\u000fSe il sistema \u0012 e asintoticamente stabile in uscita ci\nritroviamo l'ingresso\n\u000fCoincidenza di poli sull'asse immaginario\nNel paragrafo 3.6.2 abbiamo parlato dei parametri\ndei poli complessi e coniugati e abbiamo introdotto\nil coe\u000eciente di smorzamento( \u0010).\nDopo di che ci siamo so\u000bermati cosa succedeva al\nsistema nel caso che \u0010fosse<0 oppure\u00151; veden-\ndo poi che nel caso in cui fosse stato proprio pari a\n0 siamo in presenza di poli sull'asse immaginario.\nRev. 0.3 Appunti di Automatica 43 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#5": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nL\u0000feptg=Z1\n0\u0000epte\u0000stdt=1\np\u0000s\u0002\ne(p\u0000s)t\u00031\n0\u0000=1\ns\u0000p\n(1.2)\ndove per risolvere l'integrale abbiamo supposto che\n<[s]><[p] =\u001b\u0003\ne\u001b\u0003prende il nome di ascissa di convergenza. Il termi-\nnee\u0000stfacilita ovviamente la convergenza dell'integrale\nin quanto un esponenziale il cui argomento abbia parte\nreale negativa tende a zero pi\u0012 u velocemente di qualsiasi\npolinomio e questo sar\u0012 a importante quando tenteremo di\ntrasformare dei segnali di ingresso di tipo polinomiale di\ngrado qualsiasi.\nNaturalmente il fatto che l'integrale sia sommabile\nsolo in una parte del piano complesso non ci preoccupa\npi\u0012 u di tanto e quindi scriveremo:\nL\u0000\b\nept\t\n=1\ns\u0000p(1.3)\nAd esempio consideriamo il segnale e\u00002t. La sua tra-\nsformata \u0012 e la seguente:\ne\u00002tL\u0000!1\ns+ 2\nNotiamo che a denominatore \u0012 e presente un polinomio\ndi grado pari a uno e che la sua radice \u0012 e proprio \u00002.\nRev. 0.3 Appunti di Automatica 6 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#6": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nPotremmo quasi dedurre che, data una trasformata di\nLaplace, la presenza di radici a parte reale negativa al\ndenominatore (che chiameremo poli), \u0012 e legata al fat-\nto che il segnale di partenza, nel tempo, converge a\nzero. Come vedremo pi\u0012 u avanti, questa osservazione\n\u0012 e generalizzabile a polinomi a denominatore di grado\nqualsiasi.\nTrasformate derivate da quella\nelementare\nDalla trasformata appena vista se ne possono ricavare\naltre come casi particolari. Ad esempio, sostituendo p=\nj!otteniamo:\nL\u0000\b\nej!t\t\n=1\ns\u0000j!\nAltro caso particolare che possiamo subito calcola-\nre \u0012 e la trasformata del gradino unitario o funzione di\nHeaviside:\n\u000e\u00001(t) =\u001a1t\u00150\n0t<0\nL\u0000f\u000e\u00001(t)g=1\ns\nOvviamente l'ascissa di convergenza del gradino risul-\nta uguale a 0.\nRev. 0.3 Appunti di Automatica 7 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#7": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nper quel che riguarda le funzioni sin e cos si possono\nutilizzare le formule di Eulero e quindi ricondurci alla\ntrasformata gi\u0012 a analizzata:\nL\u0000fsin (!t)g=L\u0000\u001aej!t\u0000e\u0000j!t\n2j\u001b\n=!\ns2+!2(1.4)\nL\u0000fcos (!t)g=L\u0000\u001aej!t+e\u0000j!t\n2\u001b\n=s\ns2+!2(1.5)\nNotiamo che la parte reale delle radici del denomina-\ntore, le quali valgono \u0006j!, \u0012 e nulla. Infatti i segnali in\nquestione non convergono ne divergono.\n1.3 Alcune propriet\u0012 a\nAnalizziamo adesso alcune semplici propriet\u0012 a della Tra-\nsformata di Laplace. Saranno utili nel resto del libro.\nLinearit\u0012 a\nLa trasformata di Laplace \u0012 e un operatore lineare. La\ncosa si deduce dal fatto che alla base c'\u0012 e un integrale la\ncui linearit\u0012 a \u0012 e gi\u0012 a stata dimistrata nei corsi di analisi:\nl'integrale della somma di due funzioni \u0012 e pari alla somam\ndegli integrali delle due funzioni. In questo caso possiamo\nRev. 0.3 Appunti di Automatica 8 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#8": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nscrivere:\nL\u0000fc1f1(t) +c2f2(t)g=c1L\u0000ff1(t)g+c2L\u0000ff2(t)g\n(1.6)\nConiugazione\nFacile da dimostrare \u0012 e anche la propriet\u0012 a di coniugazione:\nbaster\u0012 a scrivere nell'integrale della trasformata s\u0003invece\ndis. Per cui:\nF(s\u0003) =F\u0003(s) (1.7)\nTeorema del valore iniziale\nQuesto teorema consente di determinare asintoticamente\nil valore int= 0 di una funzione di classe C1e la cui tra-\nsformata di Laplace abbia ascissa di convergenza minore\ndi zero. Quindi non pu\u0012 o essere usato per funzioni che\nhanno impulsi nell'origine. Nell'ipotesi che esista \fnito il\nlim\nt!0+f(t)\nallora:\nlim\nt!0+f(t) = lim\ns!1sF(s) (1.8)\nRev. 0.3 Appunti di Automatica 9 di 43",
    "data_test\\rootfolder\\università\\FondamentiDiAutomatica\\Trasformata_di_Laplace.pdf#9": "Capitolo 1. La Trasfomata di Laplace e la Funzione di\nTrasferimento Stefano Panzieri\nTeorema del valore \fnale\nAllo stesso modo, solo se l'ascissa di convergenza di F(s)\n\u0012 e minore o uguale a zero ed esiste\nlim\nt!1f(t)\nallora:\nlim\nt!1f(t) = lim\ns!0\u0000sF(s) (1.9)\nQuest'ultimo risultato \u0012 e molto utile e verr\u0012 a molte vol-\nte utilizzato nel prosieguo per analizzare il comportamen-\nto di un sistema di controllo per t!1 .\nMoltiplicazione per un esponenziale\nCosa succede se moltiplichiamo una funzione per un espo-\nnenziale nel tempo eat? Questo risultato, noto anche co-\nme traslazione in frequenza quando a=j!\u0012 e un numero\nimmaginario, torner\u0012 a utile studiando i segnali campiona-\nti.\nL\u0000\b\neatf(t)\t\n=1Z\n0\u0000e\u0000(s\u0000a)tf(t)dt=F(s\u0000a) (1.10)\nLa dimostrazione \u0012 e ovvia, basta inserire l'esponenziale\nnell'integrale che de\fnisce la trasformata e poi e\u000bettua-\nre una sostituzione di variabile s0=s\u0000a. Alla \fne,\ntornando in ssi otterr\u0012 a il risultato dell'eq. 1.10.\nRev. 0.3 Appunti di Automatica 10 di 43",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#0": "Geometria e Combinatoria marcella.sama@uniroma3.itL10: Basi di spazi vettoriali (16-17)Argomenti lezione:•Dipendenza e indipendenza lineare•Basi•Dimensione•Una base per Sol(SO) •Dimensioni di sottospazi vettoriali •Calcolo di dimensioni e basi ",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#1": "Dipendenza e indipendenza lineare\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#10": "23/11/2011Teorema: Un singolo vettore vdi uno spazio vettoriale Vèlinearmente indipendente se e solo se v≠ 0. Teorema: Dati i vettori v1, v2, …, vr(con r> 1).                       Se uno di essi è combinazione lineare dei rimanenti,           allora i vettori v1, v2, …, vrsono linearmente dipendenti.Dimostrazione: Uno dei vettori, detto vi, è combinazione linearedei rimanenti.  Dunque esistono scalari h1, h2, …, hi–1,hi+1 , . . . , hrtali che:Dipendenzae indipendenza lineare\nAbbiamo hi≠ 0. Pertanto v1, v2, …, vrsono linear. dipendenti.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#11": "23/11/2012Teorema: Dati i vettori v1, v2, …, vr(con r> 1). Se abbiamo ∑!\"#$𝑘!𝑣!=0con ki≠ 0allora viè combinazione lineare dei rimanenti.Dipendenzae indipendenza lineare\nDimostrazione: \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#12": "23/11/2013Osservazione: Se v1, v2, …, vrsono linearmente dipendentiallora almeno uno è combinazione lineare dei rimanenti.Esempio: Consideriamo i seguenti polinomi: f1(x) := 1 + x,  f2(x) := x2,  f3(x) := 2 + 2x–x2e  f4(x) := 2x–x3.  Notiamo che 2 f1(x) –f2(x) –f3(x) + 0 f4(x) = 0Dunque, i polinomi sono linearmente dipendenti. Ora  f3(x) è combinazione lineare di f1(x), f2(x) e f4(x): f3(x) = 2 f1(x) –f2(x) –0 f4(x) Ma f4(x) nonè combinazione lineare di f1(x),  f2(x) e f3(x) !(le loro combinazioni lineari possono avere al più grado 2)Dipendenzae indipendenza lineare\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#13": "23/11/2014Osservazione: Due vettori v1e v2sono linearmente dipendenti   se e solo se uno di essi è multiplodell’altro.Esempi:v1:= (2, 1, 3) e v2:= (1, 1, 2) sono linearmente dipendenti ? No, perchè nessuno è multiplo dell’altro. Quindi: 0v1–0v2= 0 v1:= (4, 2, 6) e v2:= (6, 3, 9) sono linearmente dipendenti ? Sì, perchè (4, 2, 6) è multiplo di (6, 3, 9). Infatti: 3v1–2v2= 0 v1:= (0, 0, 0) e v2:= (1, 1, 2) sono linearmente dipendenti ?Sì, perchè (0, 0, 0) è multiplo di (1, 1, 2). Infatti: 1v1+ 0v2= 0 Dipendenzae indipendenza lineare\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#14": "23/11/2015Teorema: Se v1, v2, …, vrsono linearmente dipendenti allora v1,v2, …, vr,vr+1 sono linearm. dipendenti qualunque sia vr+1.Dimostrazione:Dobbiamo trovare una combinazione lineare non banale di v1, v2, …, vr, vr+1 che dia come risultato il vettore nullo. Sappiamo che esiste una combinazione lineare di v1, v2, …, vrcon coefficienti non tutti nulliche è uguale al vettore nullo: Dipendenzae indipendenza lineare\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#15": "23/11/2016Teorema: Se i vettori v1, v2, …, vrsono linearmente indipendenti, e vr+1è un vettore che nonè combinazione lineare di v1, v2, …, vr,allora v1, v2, …, vr, vr+1 sono linearmente indipendenti.Dimostrazione: Dobbiamo mostrare che se abbiamo una combinazione linearedi v1, v2, …, vr, vr+1 uguale al vettore nullo:allora tuttii coefficienti kisono nulli.Per ipotesi: v1, v2, …, vrsono linearmente indipendenti, da cui abbiamo che k1= k2=…= kr = 0. Anchekr+1=0, altrimenti vr+1sarebbe combinazione lineare di     v1, v2, ..., vr.Dipendenzae indipendenza lineare\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#16": "Basi di uno spazio vettoriale\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#17": "23/11/2018BasiDefinizione: I vettori v1, v2, ... , vrdi uno spazio vettoriale Vcostituiscono una basedi Vse sono verificate entrambe le proprietà:  1.V= L(v1, v2, ... , vr); 2.v1, v2, ... , vrsono linearmente indipendenti.Esempio: Verificare che f1(x) := 1,  f2(x) := 1 + x,  f3(x) := 1 + x+ x2, f4 (x) := 1 +x+ x2+ x3costituiscono una base di R4[x].\n(ovvero un generico polinomio di grado <4)Il sistema è Cramerianoe, quindi, ammette un’unica soluzione (prop. 1).Nel caso a=b=c=d=0, la soluzioneè quella banale k1=k2=k3=k4=0 (prop. 2).Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#18": "23/11/2019BasiTeorema: Se v1, v2, ... , vrcostituiscono una basedi uno spazio V, allora ogni vettore vdi Vequivale a un’unicacombinazione linearedei vettoriv1, v2, ... , vr( ovvero 𝑣=∑!\"#$𝑘!𝑣!). → k1, k2,..., krsono le rcomponentidi vrispetto alla basev1, v2,..., vrOsservazione 1:  Quando parliamo di componentidi un vettore dobbiamo sempre specificare rispetto a quale base ci riferiamo, perchèle componenti dello stesso vettore rispetto a basi diverse sono (in generale) diverse.Osservazione 2: Le componenti dell'i-esimo vettore virispetto alla base v1, v2,..., vrsono (0, ... , 0, 1, 0, ... , 0) (dove l’unico 1 compare al posto i-esimo). In particolare, il vettore 0 ha componenti (0, ..., 0). Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#19": "23/11/2020BasiTeorema: Se v1, v2, ... , vrcostituiscono una basedi uno spazio V, allora ogni vettore vdi Vequivale a un’unicacombinazione linearedei vettoriv1, v2, ... , vr( ovvero 𝑣=∑!\"#$𝑘!𝑣!). → k1, k2,..., krsono le rcomponentidi vrispetto alla basev1, v2,..., vrDimostrazione: v1, v2, ... , vrgeneranoV, per ogni v segue: v= k1v1+ k2v2+ ... + krvrSupponiamo di scrivere vtramite due diverse combinazioni lineari:Dobbiamo dimostrare che hi = kiper ogni i. Dato chev1, v2, ... , vrsono linear. indip. si ha h1= k1 ,h2= k2 ,..., hr= kr\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#2": "Esempio:A2è combinazione linearedi A1e A3:Segue che                                         si può scrivere:Da cui si ha: L(A1, A2, A3) ÍL(A1, A3) Inoltre sappiamo che L(A1, A3) ÍL(A1, A2, A3)Segue che: L(A1, A3)= L(A1, A2, A3)\n23/11/203Introduzione\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#20": "23/11/2021BasiEsempi: Base canonicaI seguenti vettori costituiscono una base per lo spazio vettoriale Rn:\nDimostriamo che e1:= (1,0) ed e2:= (0,1) formano una base di R2 :\nI vettori e1ed e2generanoR2 :I vettori e1ed e2sono linear. indip. : \nAllora si ha:Da cui segue a1= 0  e  a2= 0.Quindi i vettori e1ed e2costituiscono una base (canonica) di R2.Caso n= 2:\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#21": "23/11/2022BasiEsempi: Base canonicaConsideriamo lo spazio vettoriale M(3, 2, R) e le seguenti matrici: Data una generica matrice di M(3, 2, R): \nSegue che le sei matrici di cui sopra sono generatoridi M(3, 2, R).Le sei matrici sono linear. indip. e formano una basedi M(3, 2, R).Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#22": "Basi canonicheI seguenti vettori costituiscono una base per lo spazio vettoriale Rn:\nParticolarità: Le componenti del vettore ( x1, x2, ... , xn) relative a questa base sono esattamente x1, x2, ... , xn.Una base per M(p, q, R) è formata dalle pqmatrici Eij, dove Eijè la matrice i cui elementi sono tutti 0 tranne quello di posto (i, j) = 1.Particolarità: Le componenti di un vettore (cioè una matrice) relative ad esso sono gli elementi della matrice stessa.Una base per Rn[x], nnaturale,è formata dai polinomi 1, x, x2, ..., xn–1.Particolarità: Il vettore p(x) = a0+ a1x+ a2x2+ ... + an–1xn–1 ha comecomponenti relative ad essa i suoi coefficienti (a0, a1, a2, ..., an–1).23/11/2023Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#23": "23/11/2024BasiEsempio: Abbiamo già verificato che f1(x) := 1,  f2(x) := 1 + x,  f3(x):= 1 + x+ x2, f4 (x) := 1 +x+ x2+ x3costituiscono una base di R4[x]. Determinare le componentidi f (x) = 2x–3x2rispetto alla base data.\nDa cui le componentidi f (x) rispetto alla base data sono (−2, 5, −3, 0).Si dimostra che 1, x, x2, x3formano un’altra base (canonica) per R4[x].Le componentidi f (x) relative a tale base di R4[x] sono (0, 2, −3, 0).Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#24": "Dimensioni di sottospazi vettoriali\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#25": "23/11/2026DimensioneEsempio: Abbiamo visto che dati tre punti A, Be Cdello spazio tali che O, A, Be Csiano non complanari, allora i vettori di V 3(O) v1:= OA, v2:= OBe v3:= OCdi V 3(O) sono linear. indipendenti. Ogni vettore vè uguale a una combinazione linearedi v1, v2e v3. Pertanto v1, v2e v3costituiscono una baseper V 3(O).  Dato che qualsiasi base di V 3(O) è formata da tre vettori,si dice che la dimensionedi V 3(O) è uguale a 3. In generale chiamiamo dimensionedi uno spazio vettorialeil numero di vettori che compongono una sua base. →→→\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#26": "23/11/2027DimensioneTeorema del completamento: Sia Vuno spazio vettoriale aventeuna base formata dai vettori e1, e2, ... , en. Siano poi assegnati rvettori linearmente indipendenti v1, v2, ... , vrdi V, con r≤n.        Si dimostra che è possibile scegliere opportunamente rvettori tra quelli della base e1, e2, ... , ene sostituirli con i vettori v1, v2, ... , vrin modo tale da ottenere una base di V.Teorema:SiaVunospazioaventeunabaseformatadanvettori.Allora assegnati comunque nvettori di Vlinearmente indipendenti,si dimostra che questi formano una base di V.Teorema: In uno spazio Vavente una base formata da nvettorinon vi possono essere più di nvettori linearmente indipendenti.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#27": "23/11/2028DimensioneTeorema: In uno spazio Vavente una base formata da nvettorinon vi possono essere più di nvettori linearmente indipendenti.Teorema: Sia Vuno spazio avente una base formata da nvettori. Allora ogni altra base di Vè formata da nvettori.Definizione: Se uno spazio Vha una base formata da n vettori, si dice che Vha dimensioneuguale a n : dim V = n.Definizione: Se uno spazio vettoriale Vè formato dal solo vettore nullo (dim V= 0) o ha una base formata da nvettori (dim V = n)diciamo che Vha dimensione finita.Se uno spazio vettoriale nonè dotato di una base formata da un numero finito di vettori, allora lo spazio ha dimensione infinita.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#28": "23/11/2029DimensioneTeorema: Si può dimostrare che:1. dim V 2(O) = 22. dimV 3(O) = 33. dimRn= n4. dimM(p, q, R) = pq5. dimRn[x] = n6. R[x] ha dimensione infinitaGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#29": "23/11/2030DimensioneTeorema: Se Vèuno spaziovettorialedidimensione nallora:1. non esistonopiudinvettorilinearmente indipendenti;2. daticomunquervettoriconr< n(anchese linear. indipendenti), essinon possonoesseregeneratori(tantomenouna base) diV;3. datinvettorilinear. indipendenti, essiformanouna base diV;4. daticomunquengeneratori, essiformanouna base diV.Osservazione: Datinvettoridiuno spaziovettorialedidim. n, per individuarese essiformanouna base, possiamocontrollare:•cheglinvettorisianogeneratori, oppure•cheglinvettorisianolinearmente indipendenti.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#3": "23/11/204IntroduzioneOsservazione: In qualunque spazio vettoriale V, se un vettore vr+1è combinazione linearedei vettori v1, v2, ... , vrallora si ha: L(v1, v2, ... , vr) = L(v1, v2, ... , vr, vr+1) Interpretazione: Se uno spazio vettoriale Vè generatodai vettori   v1, v2, ... , vr, vr+1e uno di essi è combinazione lineare degli altri, allora lo possiamo scartaree ottenere rvettori che generano V.Idea: Potremmo applicare lo stesso ragionamento agli rvettori che generano V individuando r–1 vettori generatori. Iterando ilprocesso, a una certa iterazione non possiamo più scartare vettori. Definizione: I vettori v1, v2, ... , vrsono linearmente dipendenti se esistono k1, k2, ... , krnon tutti nullitali che:∑!\"#$𝑘!𝑣!=0Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#30": "23/11/2031DimensioneTeorema: Se Vèuno spaziovettorialedidimensione nallora:1. non esistonopiudinvettorilinearmente indipendenti;2. daticomunquervettoriconr< n(anchese linear. indipendenti), essinon possonoesseregeneratori(tantomenouna base) diV;3. datinvettorilinear. indipendenti, essiformanouna base diV;4. daticomunquengeneratori, essiformanouna base diV.Esempio: Stabilirese ivettoriv1:= (1, 2, 1), v2:= (1, 1, 1),v3:= (0, 1, 2), v4:= (1, 1, 3) diR3sono linearmente indipendenti.SappiamochedimR3= 3. Segue 4 vettoridiR3comunquesceltisono linearmente dipendenti. In particolarev1, v2, v3, v4sono linearmente dipendenti.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#31": "23/11/2032DimensioneTeorema: Se Vèuno spaziovettorialedidimensione nallora:1. non esistonopiudinvettorilinearmente indipendenti;2. daticomunquervettoriconr< n(anchese linear. indipendenti), essinon possonoesseregeneratori(tantomenouna base) diV;3. datinvettorilinear. indipendenti, essiformanouna base diV;4. daticomunquengeneratori, essiformanouna base diV.Esempio: Stabilirese leseguentimatricigeneranoM(2, 2, R): Sappiamo che dim M(2, 2, R) = 4. Segue per generare M(2, 2, R) sono necessari almeno 4 vettori.Dunque A1, A2, A3non generano M(2, 2, R).  Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#32": "Basi per le soluzioni di un sistema lineare omogeneo\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#33": "23/11/2034Una base per Sol(SO)Teorema: Consideriamo un sistema lineare omogeneo SO: AX= 0 di p equazioni in qincognite x1, x2, ... , xq. Sia rk A = r, allora lo spazio delle soluzioni Sol(SO) ha dimq–r.  Una base per SOpuò essere determinata nel modo seguente:•Si determinano le soluzioni del sistema con il metodo che si preferisce (Rouchè-Capelli o Gauss): sappiamo che q–rincognite scelte opportunamente fungeranno da h1, h2, ... , hq–rparametri;•Il primo vettore della base si ottiene assegnando a h1il valore 1             e agli altri parametri il valore 0;•Il secondo vettore della base si ottiene assegnando a h2il valore 1  e agli altri parametri il valore 0;•... L’ultimo vettore della base si ottiene assegnando ad hq–ril valore 1 e agli altri parametri il valore 0.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#34": "23/11/2035Una base per Sol(SO)Esempio: Sia dato il seguente sistema lineare omogeneo SO :\nPossiamo trasformare il sistema nel seguente sistema equivalente:\nAssegniamo ora a y, w,tdei valori parametrici: \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#35": "23/11/2036Una base per Sol(SO)Esempio: Scriviamo ora le soluzioni come matrici di M(5, 1, R):\nSol(SO) è generato dai vettori S1, S2e S3: \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#36": "23/11/2037Una base per Sol(SO)Esempio: Sol(SO) è generatodai vettori S1, S2e S3 : Per stabilire se S1, S2e S3formano una base per Sol(SO), verifichiamo se sono linear. indipendenti:\nQuesta uguaglianza si verifica soloquando h1= h2= h3= 0.S1, S2e S3sono una base di Sol(SO) che ha dimensione 3.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#37": "23/11/2038Una base per Sol(SO)Esempio: Determinare una base per il seguente sottospazio Edi R4\nPer ottenere dei generatori di Eponiamo: h1:= 1, h2:= 0 e h3:= 0 ottenendo il vettore (0, 1, 0, 0);h1:= 0, h2:= 1 e h3:= 0 ottenendo il vettore (3/2, 0, 1, 0);h1:= 0, h2:= 0 e h3:= 1 ottenendo il vettore (–1, 0, 0, 1).h1= x2, h2= x3, h3= x4\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#38": "23/11/2039Dimensioni di sottospazi vettorialiTeorema: Sia Vuno spazio vettoriale di dimensione finita e sia Eun sottospazio di V. Allora Eha dimensione finita e dim E ≤dim VTeorema: Sia Vuno spazio vettoriale e dim V= n. Allora: •Esiste un sottospazio Econ dim E= 0, formato dal vettore nullo.•Esiste un sottospazio di dim. uguale a n, formato da Vstesso.Esempi: Abbiamo visto che V 2(O)ha dim. uguale a 2. Pertanto i suoi sottospazi possono avere dim. uguale a 0, 1 o 2: •L’unico sottospazio di dimensione 0 consiste nel vettore nullo;•I sottospazi di dimensione 1 sono quelli del tipo {OP| PÎr};•L’unico sottospazio di dimensione 2 è V 2(O) stesso.→Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#39": "23/11/2040Dimensioni di sottospazi vettorialiTeorema: Sia Vuno spazio vettoriale di dimensione finita e sia Eun sottospazio di V. Allora Eha dimensione finita e dim E ≤dim VTeorema: Sia Vuno spazio vettoriale e dim V= n. Allora: •Esiste un sottospazio Econ dim E= 0, formato dal vettore nullo.•Esiste un sottospazio di dim. uguale a n, formato da Vstesso.Esempi: Abbiamo visto che V 3(O)ha dim. uguale a 3. Pertanto i suoi sottospazi possono avere dim. uguale a 0, 1, 2 o 3: •L’unico sottospazio di dimensione 0 consiste nel vettore nullo;•I sottospazi di dimensione 1 sono quelli del tipo {OP| PÎr};•I sottospazi di dimensione 2 sono quelli del tipo {OP| PÎπ};•L’unico sottospazio di dimensione 3 è V 3(O) stesso.→→Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#4": "23/11/205Dipendenza lineareDefinizione: I vettori v1, v2, ... , vrsono linearmente dipendenti se esistono k1, k2, ... , krnon tutti nullitali che:∑!\"#$𝑘!𝑣!=0Esempi:A2è combinazione linearedi A1e A3:\nSegue che le matrici A1, A2e A3sono linearmente dipendenti.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#40": "23/11/2041Dimensioni di sottospazi vettorialiEsercizio(1): Sia S(2, R) il sottospazio vettoriale M(2, 2, R) formato dalle matrici simmetriche. V ogliamo determinarne una base.Consideriamo il sottospazio V1avente come base S1:= V1 ≠S(2, R), perchè c’è almeno un’altra base S2:= Consideriamo il sottospazio V2avente come basi S1e S2 :V2 ≠S(2, R) perchè c’è almeno un’altra base S3:= Consideriamo il sottospazio V3avente come basi S1 , S2e S3 .V3coincide con S(2, R), perché dimS(2, R) < dimM(2, 2, R) = 4. \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#41": "23/11/2042Dimensioni di sottospazi vettoriali\nDobbiamo stabilire se esistono k1, k2e k3 tali che:\nBasta porre k1= b, k2= ae  k3= c. Tutte le matrici simmetriche sono combinazioni lineari di S1, S2 , S3 .Per cui S1, S2 , S3  generano S(2, R).Esercizio(1): Verifichiamo che le matrici S1, S2e S3 generano S(2, R):\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#42": "23/11/2043Calcolo di dimensioni e basiTeorema: Sia Vuno spazio vettoriale e v1, v2, ... , vnuna sua base.Siano u1, u2, ... , usdei vettori che generano il sottospazio Udi V.Decomponiamo u1, u2,..., usrispetto alla base formata da v1, v2,..., vn:Si dimostra che rk A = dim UInoltre il teorema e il calcolo del rango di Aci dicono anche comepossiamo estrarre una base di Uda u1, u2, ... , us.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#43": "23/11/2044Calcolo di dimensioni e basiVediamo come possiamo estrarre una base di Uda u1, u2, ... , us:\n•Se abbiamo calcolato rk Ausando i determinanti dei minori:    Sia Mun minore di Adi ordine ravente determinante non nullo.  Gli rvettori relativi alle colonne di Mformano una basedi U.•Invece, se abbiamo calcolato rk Ariducendo la matrice a scalini:         Sia Bla matrice con rscalini ottenuta dalla matrice A.             Una basedi Usi ottiene prendendo tra i vettori u1, u2, ... , usquei vettori le cui posizioni corrispondono agli scalini di B.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#44": "23/11/2045Calcolo di dimensioni e basiEsercizio(2): Sia Eil sottospazio di R[x] generato da f1(x):= 1+ x –2x2,f2(x) := x+ 3 x4,  f3(x) := 1 –2 x2–3 x4. Calcolare dim Ee una base.La base canonica di R5[x] è formata dai polinomi 1, x, x2, x3, x4 .La matrice relativa ai 3 polinomi rispetto alla base canonica di R5[x]: \nrk A= 2\nConcludiamo che dim E= 2 e una basedi Eè formata da f1(x) e f3(x). Un minore di ordine 2 con determinante ≠ 0 \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#45": "23/11/2046Calcolo di dimensioni e basiEsercizio(3): Verificare che v1:= (1, 2, 1, 0), v2:= (2, 3, 0, 1) e          v3:= (1, 5/2, 2, –1/2) di R4sono linearmente dipendenti.La matrice Arelativa ai 3 vettori rispetto alla base canonica di R4:\nrk A= 2\nGaussDato che rk A= 2, il sottospazio generato da v1, v2, v3ha dim. 2. Segue che i vettori v1, v2, v3sono linearmente dipendenti.Gli scalini sono in prima e seconda posizionenella matrice A. Dunque una baseper L(v1, v2, v3) è formata dai vettori v1e v2 .Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#46": "23/11/2047Calcolo di dimensioni e basiEsercizio(4): Verificare che v1:= x+ x3e v2:= 3 + 2x+ x3 di R4[x]sono linearmente indipendenti tramite la base canonica di R4[x]. Base canonica di R4[x]: e0:= 1, e1:= x, e2:= x2, e3:= x3. Prendiamo la matrice Bavente come colonne le componenti dei vettori v1e v2relativamente alla base canonica (e0 , e1 ,e2 , e3 ).\nrk B= 2v1e v2sono linearmente indipendenti.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#47": "23/11/2048Calcolo di dimensioni e basiTeorema: Sia Auna matrice a nrighe e scolonne. Lo spazio generato dalle colonnedi Aha dimensione uguale a rk A.Lo spazio generato dalle righedi Aha dimensione uguale a rk A.(dato che rk A= rk tA)\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#48": "23/11/2049Calcolo di dimensioni e basiTeorema: Sia Auna matrice a nrighe e scolonne. Lo spazio generato dalle colonnedi Aha dimensione uguale a rk A.Lo spazio generato dalle righedi Aha dimensione uguale a rk A.Osservazione 1: Nonè detto che i sottospazi del teorema coincidano. Se Aha nrighe e scolonne: le sue righe generano un sottospazio di M(1, s, R), mentre le sue colonne generano un sottospazio di M(n, 1, R).Osservazione 2: Anche nel caso in cui la matrice sia Aquadrata:I sottospazi generati da righe e colonne hanno la stessa dimensione,ma nonè detto che questi due sottospazi coincidano. Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#49": "23/11/2050EserciziEsercizio(5): Stabilire se i vettori v1:= (1, 3, 2, 1), v2:= (1, 0, 1, 0), v3:= (1, 0, 2, 0) sono linearmente dipendenti o indipendenti. Calcolare una base e la dimensione per lo spazio L(v1, v2, v3). Consideriamo la matrice Ale cui colonne corrispondono alle componenti dei vettori v1, v2e v3rispetto alla base canonica di R4 :\n•rk A= 3•dimL(v1, v2, v3) = 3•v1 , v2 , v3costituiscono una base per L(v1, v2, v3) Oss.: Se poniamo k1v1+ k2v2+ k3v3= 0, il sistema lineare nelleincognite k1, k2, k3ha Acome matrice dei coefficienti.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#5": "23/11/206Dipendenza lineareDefinizione: I vettori v1, v2, ... , vrsono linearmente dipendenti se esistono k1, k2, ... , krnon tutti nullitali che:∑!\"#$𝑘!𝑣!=0Esempi: I vettori v1:= (2, 4, 4, 2), v2:= (1, 2, 2, 1) e v3:= (1, 2, 3, 3) di R4sono linearmente dipendenti. Infatti si ha:   1v1–2v2+ 0v3= 0 Osservazione: Si noti che nella definizione di vettori linearmente dipendenti nonsi richiede che tuttii coefficienti siano diversi da 0, ma solo che qualcuno di essi (almeno uno) sia diverso da 0.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#50": "23/11/2051EserciziEsercizio(6): Stabilire se i vettori v1:= (1, 0, 1, 0), v2:= (2, 0, 2, 0),v3:= (2, 0, 2, 0) sono linearmente dipendenti o indipendenti. Calcolare una base e la dimensione per lo spazio L(v1, v2, v3). Consideriamo la matrice Ale cui colonne corrispondono alle componenti dei vettori v1, v2e v3rispetto alla base canonica di R4 :\nGauss\n•rk A= 2 •dimL(v1, v2, v3) = 2•Una base è formatada v1e v3(vedi scalini)•Se osserviamo che v2= 2v1 , concludiamo che v1 e v2 sono linearmente dipendenti. Dunque, e.g., L(v1, v2, v3) = L(v1, v3). Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#51": "23/11/2052EserciziEsercizio(7): Stabilire se i vettori (1,−1, 0), (0, 1, −1), (−1, 0, 1) formano una base per R3.Consideriamo la matrice Ale cui colonne corrispondono alle componenti dei tre vettori dati rispetto alla base canonica di R3:\n•det A = 0•rk A < 3•dim. del sottospazio generato < 3•sappiamo che dimR3= 3•segue i tre vettori nongeneranoR3e nonsono una sua baseGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#6": "23/11/207Esempio: I polinomi f1(x) := 1,  f2(x) := 1 + x,  f3(x) := 1 + x+ x2,f4 (x) := 1 +x+ x2+ x3di R[x] sono linearmente indipendenti ? Indipendenza lineareDefinizione: I vettori v1, v2, ... , vrsono linearmente indipendenti se ∑!\"#$𝑘!𝑣!=0è verificata solo quando k1=k2=…= kr= 0\nScriviamo una loro combinazione lineare e poniamola = 0:Dobbiamo risolvere questo sistema:Il sistema ammette solamente la soluzione banale k1=k2=k3= k4 = 0Quindi f1(x), f2(x), f3(x) e f4(x) sono linearmente indipendenti ! Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#7": "23/11/208Esercizio:  v1:= (1, 2, 1, 0), v2:= (2, 3, 0, 1), v3:= (1, 5/2, 2, –1/2)di R4sono linearmente dipendenti o indipendenti ? Dipendenzae indipendenza lineareScriviamo una loro combinazione lineare e poniamola = 0:\nDobbiamo risolvere questo sistema:Pertanto v1, v2e v3sono linearmente dipendenti ! Ad esempio: \nSoluzione:\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#8": "23/11/209Esercizio:  v1:= (1, 2, 1, 0), v2:= (2, 3, 0, 1), v3:= (1, 5/2, 2, –1/2)di R4sono linearmente dipendenti o indipendenti ? Dipendenzae indipendenza lineareScriviamo una loro combinazione lineare e poniamola = 0:\nPotevamo evitaredi risolvere il sistema:\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\10_Basi.pdf#9": "23/11/2010Esercizio:  v1:= (1, 2, 1, 0), v2:= (2, 3, 0, 1), v3:= (1, 5/2, 2, –1/2)di R4sono linearmente dipendenti o indipendenti ? Dipendenzae indipendenza lineareScriviamo una loro combinazione lineare e poniamola = 0:\nIn alternativa studiamo la matrice:Poiché il rango è 2,mentre il numero delle incognite è 3, il sistema ha soluzioni non banali.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#0": "Geometria e Combinatoria marcella.sama@uniroma3.itL11: Intersezione e somma di sottospazi (18)Argomenti lezione:•Introduzione•Intersezione di sottospazi vettoriali •Somma di sottospazi vettoriali•Esercizi su somma e intersezione",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#1": "23/11/202IntroduzioneDati due sottospazi vettoriali Ee Fdi uno spazio vettoriale V,vedremo che: •L’intersezione di E e Fè un sottospazio vettoriale;•L’unione di E e Fnonè sempre un sottospazio vettoriale.Introdurremo allora la somma di due sottospazi vettoriali, ovvero “il più piccolo” sottospazio vettoriale contenente E e F. Studieremo poi la formula di Grassmannper il calcolo della dimensione dell’intersezione di E e Fo della somma di E e F.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#10": "23/11/2011Somma di sottospazi vettorialiTeorema: Siano Ee Fdue sottospazi vettoriali di dimensione finitadi un qualsiasi spazio vettoriale V. Segue E+Fha dimensione finita.In particolare, se e1, e2, ... , epformano una base di Ee  f1, f2, ... , fqformano una base di F, allora i vettori e1, e2, ... , ep, f1, f2, ... , fqgenerano E+ F(ma nonformano necessariamente una basedi E+F).Teorema: Sia Vuno spazio vettoriale di dimensione finita aventeuna base formata dai vettori v1, v2,..., vn. Siano Ee Fdue sottospazi vettoriali di uno spazio vettoriale Vdi dimensione finita. Siano e1, e2, ... , epuna base di E  e   f1, f2, ... , fquna base di F.Allora dim(E+ F) = rk Adove Aè la matrice avente come colonnele componenti di e1, e2,..., ep, f1, f2,..., fqrispetto alla base v1,v2,..., vn. Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#11": "23/11/2012Somma di sottospazi vettorialiEsercizio: Determinare dim(E +F).Consideriamo in R5: Egenerato da e1:=(1,4,0,0,0), e2:=(1,0,0,1,1), e3:=(1,1,0,–1,2);Fgenerato da  f1:= (2,1,1,4,0),  f2 := (–1,1,–1,0,–2). \ne1e2    e3 f1f2rispetto alla base canonicaQuesta matrice ha rango 4. Pertanto dim(E+ F) = 4.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#12": "23/11/2013Somma e intersezione di sottospaziTeorema(Formula di Grassmann): Siano Ee Fdue sottospazi vettoriali di dimensione finita di un qualsiasi spazio vettoriale V.Si dimostra che: dim(E+ F) + dim(EÇF) = dim E + dim FEsempio: Consideriamo in R5: Egenerato da e1:=(1,4,0,0,0), e2:=(1,0,0,1,1), e3:=(1,1,0,–1,2);Fgenerato da  f1:= (2,1,1,4,0),  f2 := (–1,1,–1,0,–2).Abbiamo già calcolato che dim(EÇF) = 1 e che dim(E+ F) = 4Alternativamente possiamo osservare che dim E = 3 e dim F = 2Poi, ad esempio, possiamo prendere dim(EÇF) = 1 e ricavare dim(E+ F) = dim E + dim F –dim(EÇF) = 3 + 2 –1 = 4 Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#13": "23/11/2014Somma e intersezione di sottospaziEsercizio: Sia Vuno spazio vettoriale avente come base e1, e2 , e3, e4Siano dati i vettori: \nSia Eil sottospazio avente come base v1e v2(che sono linear. indip.)Sia Fil sottospazio avente come base v3e v4(che sono linear. indip.)Obiettivo: Vogliamo determinare una base per E+ Fe una base per EÇFGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#14": "23/11/2015Somma e intersezione di sottospaziEsercizio: Sia Vuno spazio vettoriale avente come base e1, e2 , e3, e4Siano dati i vettori: \nAbbiamo che i vettori v1, v2 , v3, v4sono generatori di E+ F\nv1v2 v3v4e1e2e3e4•det A= 0•Il minore di Ain rosso ha det≠ 0•E+ Fha una base formata dai vettori v1, v2 , v3•dim(E+ F) = 3Cerchiamo una base di E+ F\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#15": "23/11/2016Somma e intersezione di sottospaziEsercizio: Sia Vuno spazio vettoriale avente come base e1, e2 , e3, e4Siano dati i vettori: \nUn vettore vÎEse e solo se v= h1 v1+ h2v2per h1e h2ÎRUn vettore vÎFse e solo se v= k1 v3+ k2v4per k1e k2ÎRDa cuivappartiene a EÇFse e solo se  h1 v1+ h2v2= k1 v3+ k2v4h1 v1+ h2v2 –k1v3–k2v4  = 0Ponendo h3= –k1e h4= –k2, si ha:       h1 v1+ h2v2 +h3v3+ h4v4 = 0Cerchiamo una base di EÇFdim(EÇF) = 1\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#16": "23/11/2017Somma e intersezione di sottospaziEsercizio: Sia Vuno spazio vettoriale avente come base e1, e2 , e3, e4Siano dati i vettori: \nUn vettore vÎEse e solo se v= h1 v1+ h2v2per h1e h2ÎRUn vettore vÎFse e solo se v= k1 v3+ k2v4per k1e k2ÎRDa cuivappartiene a EÇFse e solo se  h1 v1+ h2v2= k1 v3+ k2v4Ponendo h3= –k1e h4= –k2, si ha:       h1 v1+ h2v2 +h3 v3+ h4v4 = 0Sostituendo v1, v2 , v3, v4  e riordinando per e1, e2 , e3, e4si ottiene: \nCerchiamo una base di EÇFdim(EÇF) = 1\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#17": "23/11/2018Somma e intersezione di sottospaziEsercizio: Sia Vuno spazio vettoriale avente come base e1, e2 , e3, e4Siano dati i vettori: \nLa combinazione lineare di e1, e2 , e3, e4 (che sono linear. indipend.) è nulla se e solo se i coefficienti sono tutti nulli, ovvero il sistema: rk A= 3\nle soluzioni del sistema dipendono da unparametroCerchiamo una base di EÇFdim(EÇF) = 1\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#18": "23/11/2019Somma e intersezione di sottospaziEsercizio: Sia Vuno spazio vettoriale avente come base e1, e2 , e3, e4Siano dati i vettori: \nLa combinazione lineare di e1, e2 , e3, e4 (che sono linear. indipend.) è nulla se e solo se i coefficienti sono tutti nulli, ovvero il sistema: \nbase di EÇFCerchiamo una base di EÇFdim(EÇF) = 1\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#19": "23/11/2020Somma e intersezione di sottospaziEsercizio: Si considerino i seguenti sottospazi Ee Fdi M(2, 2, R): \nLa matrice Aappartiene a E+ F ? Aappartiene a E+ Fse e solo se esistono una matrice Min Ee una matrice Nin Ftali che A= M+ N.\nsono in contraddizioneGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#2": "Intersezione di sottospazi vettoriali\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#20": "23/11/2021Somma e intersezione di sottospaziEsercizio: Si considerino i seguenti sottospazi Ee Fdi M(2, 2, R): \nLa matrice Aappartiene a E+ F ? Aappartiene a E+ Fse e solo se esistono una matrice Min Ee una matrice Nin Ftali che A= M+ N.\nSegue che la matriceAnonappartiene a E+ F Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#21": "23/11/2022Somma e intersezione di sottospaziEsercizio: Si considerino i seguenti sottospazi Ee Fdi M(2, 2, R): \nLa matrice Bappartiene a E+ F ? Bappartiene a E+ Fse e solo se esistono una matrice Min Ee una matrice Nin Ftali che B= M+ N.\nIl sistema (di 4 equazioni) risultante è risolubileGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#22": "23/11/2023Somma e intersezione di sottospaziEsercizio: Si considerino i seguenti sottospazi Ee Fdi M(2, 2, R): \nLa matrice Bappartiene a E+ F ? Bappartiene a E+ Fse e solo se esistono una matrice Min Ee una matrice Nin Ftali che B= M+ N.\nSegue che la matriceBappartiene a E+ F \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#23": "23/11/2024Somma e intersezione di sottospaziEsercizio: Si considerino i seguenti sottospazi Ee Fdi R4 : \nStabilire se i vettori (1, 3, 1, 0)e (1, 0, 1, –2) appartengono a E+ F.Dato un generico vettore (x1, x2 , x3 , x4) di E, si ha x1+ x2+ x3+ x4= 0Segue possiamo riscrivere questo vettore: (x1, x2, –x1, –x2). Analogamente, si ha il generico vettore di F : (y1, –y1, y3, –y3).\nSvolgendo i calcoli si ottiene che il sistema nonè risolubile. Segue (1,3,1,0) nonappartiene a E+ F.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#24": "23/11/2025Somma e intersezione di sottospaziEsercizio: Si considerino i seguenti sottospazi Ee Fdi R4 : \nStabilire se i vettori (1, 3, 1, 0) e (1, 0, 1, –2)appartengono a E+ F.Dato un generico vettore (x1, x2 , x3 , x4) di E, si ha x1+ x2+ x3+ x4= 0Segue possiamo riscrivere questo vettore: (x1, x2, –x1, –x2). Analogamente, si ha il generico vettore di F : (y1, –y1, y3, –y3).\nSvolgendo i calcoli si ottiene che stavolta il sistema è risolubile. Segue (1,0,1, –2) appartiene a E+ F.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#25": "23/11/2026Somma e intersezione di sottospaziEsercizio: Sia Uil sottospazio vettoriale di R4generato dai vettori   u1:= (2, 1, 3, 1), u2:= (1, 0, 2, 0), u3:= (4, 2, 6, 2) Sia Vil sottospazio generato da v1:= (1, 0, 2, 1), v2:= (1, 1, 1, –1),  v3:= (2, 1, 3, 0), v4:= (1, –1, 0, 0). Determinare una base per Ue la sua dimensione, una base per Ve la sua dimensione. Determinare poi una base per UÇVe U+ V.\nu1u2u3\n•dim U= 2rispetto alla base canonica di R4•una base di Uè data dai vettori u1e  u2Gauss\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#26": "23/11/2027Somma e intersezione di sottospaziEsercizio: Sia Uil sottospazio vettoriale di R4generato dai vettori   u1:= (2, 1, 3, 1), u2:= (1, 0, 2, 0), u3:= (4, 2, 6, 2) Sia Vil sottospazio generato da v1:= (1, 0, 2, 1), v2:= (1, 1, 1, –1),  v3:= (2, 1, 3, 0), v4:= (1, –1, 0, 0). Determinare una base per Ue la sua dimensione, una base per Ve la sua dimensione. Determinare poi una base per UÇVe U+ V.v1v2v3•dim V= 3rispetto alla base canonica di R4•una base di Vè data dai vettori v1, v2e  v4Gauss\nv4\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#27": "23/11/2028Somma e intersezione di sottospaziEsercizio: Sia Uil sottospazio vettoriale di R4generato dai vettori   u1:= (2, 1, 3, 1), u2:= (1, 0, 2, 0), u3:= (4, 2, 6, 2) Sia Vil sottospazio generato da v1:= (1, 0, 2, 1), v2:= (1, 1, 1, –1),  v3:= (2, 1, 3, 0), v4:= (1, –1, 0, 0). Determinare una base per Ue la sua dimensione, una base per Ve la sua dimensione. Determinare poi una base per UÇVe U+ V.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#28": "23/11/2029Somma e intersezione di sottospaziEsercizio: Sia Uil sottospazio vettoriale di R4generato dai vettori   u1:= (2, 1, 3, 1), u2:= (1, 0, 2, 0), u3:= (4, 2, 6, 2) Sia Vil sottospazio generato da v1:= (1, 0, 2, 1), v2:= (1, 1, 1, –1),  v3:= (2, 1, 3, 0), v4:= (1, –1, 0, 0). Determinare una base per Ue la sua dimensione, una base per Ve la sua dimensione. Determinare poi una base per UÇVe U+ V.\n(3, 1, 5, 1) costituisce una base di UÇVdim(UÇV ) = 1 \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#29": "23/11/2030Somma e intersezione di sottospaziEsercizio: Sia Uil sottospazio vettoriale di R4generato dai vettori   u1:= (2, 1, 3, 1), u2:= (1, 0, 2, 0), u3:= (4, 2, 6, 2) Sia Vil sottospazio generato da v1:= (1, 0, 2, 1), v2:= (1, 1, 1, –1),  v3:= (2, 1, 3, 0), v4:= (1, –1, 0, 0). Determinare una base per Ue la sua dimensione, una base per Ve la sua dimensione. Determinare poi una base per UÇVe U+ V.\ndim(UÇV ) = 1 dim(U+ V) = dim U+ dim V–dim (UÇV) = 2 + 3 –1 = 4Segue che U+ V= R4 Allora una base per U+ Vè, ad esempio, la base canonica di R4.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#3": "23/11/204Intersezione di sottospazi vettorialiTeorema: L’intersezione E ÇFdi due sottospazi Ee Fdi uno spazio vettoriale Vè un sottospazio vettoriale di V .Osservazione 1: E ÇFdi due sottospazi vettoriali Ee Fèun sottospazio vettoriale sia di Eche di F. Osservazione 2: Se Ee Fsono due sottospazi di dim. finita, allora anche E ÇFha dimensione finita. Inoltre, si ha: dim (E ÇF) ≤ dimEdim(E ÇF) ≤ dim FGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#4": "23/11/205Intersezione di sottospazi vettorialiEsercizio: Consideriamo i seguenti sottospazi Ee Fdi M(2, 2, R):\nDeterminare E ÇFSia Auna generica matrice dello spazio vettoriale M(2, 2, R):\nAappartiene a Esolo se a22= 0 e a12= a21Aappartiene a Fsolo se a22= 0 e a11= a21\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#5": "23/11/206Intersezione di sottospazi vettorialiEsercizio: Determinare E ÇF.Consideriamo in R5: Egenerato da e1:=(1,4,0,0,0),  e2:=(1,0,0,1,1),  e3:=(1,1,0,–1,2);Fgenerato da f1:= (2,1,1,4,0),  f2 := (–1,1,–1,0,–2). \nEÇFè l’insieme dei multipli del vettore (1, 2, 0, 4, –2).dim(EÇF) = 1Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#6": "Somma di sottospazi vettoriali\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#7": "23/11/208Somma di sottospazi vettorialiEsempio: Consideriamo i sottospazi TR(2) e TR(2) di M(n, n, R).L’insieme TR(2) ÈTR(2) sono le matrici triangolari di ordine 2.Sommando due matrici triangolari ne otteniamo una triangolare?No, ecco un contro-esempio: Abbiamo due matrici triangolari la cui somma nonè triangolare! Quindi nonè detto che l’unione di due sottospazi sia un sottospazio.Definizione: Dati due sottospazi Ee Fdi uno spazio vettoriale Vla somma E+ Fè il sottoinsieme di Vformato da tutte le somme del tipo u+ vcon uÎEe  vÎF.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#8": "23/11/209Somma di sottospazi vettorialiDefinizione: Dati due sottospazi Ee Fdi uno spazio vettoriale Vla somma E+ Fè il sottoinsieme di Vformato da tutte le somme del tipo u+ vcon uÎEe  vÎF.Osservazione: La somma di due sottospazi E+ Fcontiene Ee F.Dimostrazione: Ogni vettore udi Eè anche un vettore di E+ F : la somma di u(che appartiene ad E) con 0 (che appartiene a F).Teorema: La somma E+ Fdi due sottospazi vettoriali Ee Fdi uno spazio vettoriale Vè un sottospazio vettoriale di V. Se Uè un sottospazio vettoriale di Vcontenente sia Esia F, allora U contiene E+ F.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\11_Operazioni_tra_sottospazi.pdf#9": "23/11/2010Somma di sottospazi vettorialiEsercizio: Siano Ee Fsottospazi vettoriali di uno spazio V. Supponiamo che e1, e2 , e3formano una base di E(dimE= 3) e che i vettori f1,  f2formano una base di F(dimF= 2). Osserviamo che un vettore wdi Vappartiene a E+ Fse si può esprimere come sommadi un vettore udi Ee di un vettore vdi F. u= h1 e1+ h2e2+ h3e3per opportuni h1, h2, h3in Rv= k1 f1+ k2f2per opportuni k1, k2in Rw= u + v= h1 e1+ h2e2+ h3e3+ k1 f1+ k2f2Dunque i vettori e1, e2 , e3 ,  f1,  f2generano E+ F. Possiamo allora affermare che dim(E+ F) ≤ 5.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#0": "Geometria e Combinatoria marcella.sama@uniroma3.itL12: Sottospazi affini (19)Argomenti lezione:•Introduzione•Le rette del piano e dello spazio •I piani dello spazio •Sottospazi affini •L’insieme delle soluzioni di un sistema",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#1": "23/11/202IntroduzioneNello spazio vettoriale dei vettori del piano o dello spazio applicati in un punto O:•Le rette passanti per Opossono essere viste come sottospazi vettoriali di dimensione 1. •I piani passanti per Opossono essere visti come sottospazi vettoriali di dimensione 2.Le rette del piano o dello spazio e i piani dello spazio (non necessariamente passanti per O) hanno una struttura algebricapiù generale dei sottospazi vettoriali, chiamata sottospazio affine.L’insieme delle soluzioni di un sistema(anche non omogeneo) di equazioni lineari, quando non è vuoto, è un sottospazio affine.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#10": "23/11/2011Sottospazi affiniTeorema: Sia v+ Eun sottospazio affine di uno spazio vettoriale V. Sia wun vettore di V. Possono sussistere due casi: 1.se wÎv+ Eallora v+ E= w+ E ;2.se wÏv+ Eallora l’intersezione di v+ Ee w+ Eè vuota.Dimostrazione: 2. Supponiamo ora che wÏv+ E . Procediamo per assurdo: Se esistesse un uappartenente all’intersezione di v+ Ee w+ E,avremmo che v+ E= u+ Ee  w+ E= u+ E . Ma allora w+ E= v+ E. Poiché w+ 0 Îw + E(in quanto 0 è un vettore di E) avremmo che wÎv+ E. Segue l’assurdo. Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#11": "23/11/2012Sottospazi affiniTeorema: Sia v+ Eun sottospazio affine di uno spazio vettoriale V. Sia wun vettore di V. Possono sussistere due casi: 1.se wÎv+ Eallora v+ E= w+ E ;2.se wÏv+ Eallora l’intersezione di v+ Ee w+ Eè vuota.Esempio 1: Dato M(2, 2, R), si ha:Verificare che il sottospazio affine C+ S(2, R) coincidecon S(2, R). \nPonendo t1’ := 1 + t1,  t2’ := 2 + t2,  t3’ := 4 + t3, otteniamo:\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#12": "23/11/2013Sottospazi affiniTeorema: Sia v+ Eun sottospazio affine di uno spazio vettoriale V. Sia wun vettore di V. Possono sussistere due casi: 1.se wÎv+ Eallora v+ E= w+ E ;2.se wÏv+ Eallora l’intersezione di v+ Ee w+ Eè vuota.Esempio 2: Dato un sottospazio affine v0+ Edi uno spazio Vdimostrare che: A.se v0ÎEallora  v0+ E=Eè un sottospazio vettoriale;B.se v0ÏE allora  v0+ Enonè un sottospazio vettoriale.A.Se v0ÎEallora v0+ E= 0 + E= E.                                              Quindi il sottospazio affine 0 + Eè un sottospazio vettoriale.B.Se v0ÏEallora v0+ Ee  0 + E= Ehanno intersezione vuota. Pertanto, v0+ Enonè un sottospazio vettoriale.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#13": "Soluzioni di un sistema come spazi\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#14": "23/11/2015L’insieme delle soluzioni di un sistemaEsempio: Prendiamo il seguente sistema risolubile: \nEè un sottospazio vettoriale di M(4, 1, R) di dimensione 2.Segue Sol(S) è un sottospazio affine.In particolare, se h= k= 0 abbiamo la soluzione (–1, 2, 0, 0). Eal variare di he kin R\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#15": "23/11/2016L’insieme delle soluzioni di un sistemaProcedimento generale: Dato un sistema lineare qualsiasi S: AX= Bdi pequazioni nelle qincognite x1, x2, . . . , xq.Se rk A= r, sappiamo che q–r incognite fungeranno da parametri.sottospaziovettoriale Eal variare dei parametri reali h1, h2, ... , hq–r.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#16": "23/11/2017L’insieme delle soluzioni di un sistemaDefinizione: Dato un sistema lineare qualsiasi S: AX= B, chiamiamo sistema omogeneo associatoa Sil sistema SO: AX= 0.In altri termini, il sistema SO si ottiene da Ssemplicemente ponendo = 0 tutti i termini noti delle equazioni che formano S.Esempio: \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#17": "23/11/2018L’insieme delle soluzioni di un sistemaTeorema: Sia S: AX = Bun sistema lineare risolubile. Sia X0una soluzione particolare di S. Sia SO: AX= 0 il sistema omogeneo associato a S.Sol(S) = X0+ Sol(SO) è un sottospazio affineparallelo a Sol(SO). Dimostrazione: Mostriamo che X0+ Sol(SO) ÍSol(S). Un X0+ Sol(SO) si può scrivere X0+ X ’ con X ’ soluzione di SO.X0+ X ’ è soluzione di Sse e solo se A(X0+ X ’) = B.Poiché X0è una soluzione di Sabbiamo che AX0= B. Poiché X ’è una soluzione di SOabbiamo che AX ’ = 0. Segue: A(X0+ X ’) = AX0+ AX ’ = B+ 0 = B .Abbiamo quindi dimostrato che X0+ X ’ appartiene a Sol(S).Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#18": "23/11/2019L’insieme delle soluzioni di un sistemaTeorema: Sia S: AX = Bun sistema lineare risolubile. Sia X0una soluzione particolare di S. Sia SO: AX= 0 il sistema omogeneo associato a S.Sol(S) = X0+ Sol(SO) è un sottospazio affineparallelo a Sol(SO). Dimostrazione: Mostriamo che Sol(S) ÍX0+ Sol(SO). Sia allora X1un elemento di Sol(S), dobbiamo mostrare che X1= X0+ X ’ per qualche X ’ di Sol(SO).Se X1= X0+ X ’ allora si ha X ’ = X1–X0 .Quindi dobbiamo mostrare che AX ’ = A(X1–X0) = 0 .Poiché X1e X0sono soluzioni di Sabbiamo che AX1= Be AX0= B.Segue: AX ’ = A(X1–X0) = AX1–AX0= B –B = 0 .Abbiamo quindi dimostrato che X ’ appartiene a Sol(SO).Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#19": "23/11/2020L’insieme delle soluzioni di un sistemaProposizione: Sia Sun sistema lineare risolubile di pequazioni nelle qincognite x1, x2, . . . , xq. Se la matrice del sistema ha rango rallora il sottospazio affine Sol(S) ha dimensione q–r.Esempio 1: L’insieme sol(S) è un sottospazio affine di R5 ? Dobbiamo verificare che  S≠ Æovvero  Sè risolubile.rk A= 2. Anche la matrice completa del sistema ha rango 2. Segue il sistema Sè risolubile, quindi è un sottospazio affine.Il sistema Sha q= 5 e r= 2  segue  dimSol(S) = 5 –2 = 3.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#2": "23/11/203Le rette del piano e dello spazioTeorema: Sia runa retta e sia P0un suo punto.                               Sia r’ la retta parallela a rpassante per il punto O.                                  Allora PÎrse e solo se esiste QÎr’ tale che: OP= OP0+ OQ .Equivalentemente: Sia Eil seguente sottospazio vettoriale diV 2(O) oppure di V 3(O) di dimensione 1:  E:= {OQ| QÎr’}.PÎrse e solo se il vettore v:= OPappartiene a {v0+ v’ | v’ ÎE}.→→→→→\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#20": "23/11/2021L’insieme delle soluzioni di un sistemaProposizione: Sia Sun sistema lineare risolubile di pequazioni nelle qincognite x1, x2, . . . , xq. Se la matrice del sistema ha rango rallora il sottospazio affine Sol(S) ha dimensione q–r.Esempio 2: L’insieme sol(S) è un sottospazio affine di R5 ? Dobbiamo verificare che  S≠ Æovvero  Sè risolubile.rk A= 2. Stavoltala matrice completa del sistema ha rango 3. Segue il sistema Snonè risolubile, e nonè un sottospazio affine.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#21": "23/11/2022L’insieme delle soluzioni di un sistemaEsercizio: Scrivere le soluzioni del seguente sistema lineare:come sottospazio affine v0+ Edi R4, determinando esplicitamente un vettore v0e una base per E.Risolvendo il sistema si ha:al variare di te uin RLa generica soluzione del sistema può allora essere scritta così:v0E\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#3": "23/11/204Le rette del piano e dello spazioOsservazione 1: Nel teorema precedente abbiamo scelto un punto P0sulla retta r : se scegliamo un differente punto Q0, posto w0:= OQ0abbiamo chePÎrse e solo se il vettore v:= OPappartiene a {w0+ v’ | v’ ÎE}.Segue gli insiemi {v0+ v’ | v’ ÎE} e {w0+ v’ | v’ ÎE} coincidono. \n→\nQ0w0→\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#4": "23/11/205Le rette del piano e dello spazioOsservazione 2: L’uguaglianza tra gli insiemi {v0+ v’ | v’ ÎE} e {w0+ v’ | v’ ÎE} non significa che v0+ v’ = w0+ v’ per v’ ÎE , altrimenti v0= w0. Significa inveceche dato un qualunque vettore v0+ v’ con v’ ÎEesiste un vettore v’’ ÎEtale che v0+ v’ = w0+ v’’. \nQ0w0v’’Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#5": "23/11/206Le rette del piano e dello spazioOsservazione 2: L’uguaglianza tra gli insiemi {v0+ v’ | v’ ÎE} e {w0+ v’ | v’ ÎE} non significa che v0+ v’ = w0+ v’ per v’ ÎE , altrimenti v0= w0. Significa inveceche dato un qualunque vettore v0+ v’ con v’ ÎEesiste un vettore v’’ ÎEtale che v0+ v’ = w0+ v’’. v’ e rsi dicono parallelise QÎr’ parallela a re passante per O.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#6": "Teorema: Sia  πun piano e sia P0un suo punto. Sia π’ il piano parallelo a  πpassante per O. Allora PÎπse e solo se esiste QÎπ’ tale che: OP= OP0+ OQ .Equivalentemente: Sia Eil sottospazio vettoriale di V 3(O)di dimensione 2:   E:= {OQ| QÎπ’}.PÎπse e solo se v:= OPappartiene a {v0+ v’ | v’ ÎE}.23/11/207I piani dello spazio→→→→→Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#7": "Sottospazi Affini\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#8": "23/11/209Sottospazi affiniDefinizione: In uno spazio vettoriale Vsiano dati un vettore v0e un sottospazio vettoriale E. L’insieme v0+ E:= {v0+v’ |v’ ÎE}viene chiamato sottospazio affinepassante per v0e parallelo a E. Per definizione: dim(v0+ E ) = dim(E ).Esempio: Dato lo spazio vettoriale M(2, 2, R), si ha: 1. Mostrare i vettori appartenenti al sottospazio affine A + S (2, R).2. Verificare che A + S (2, R) non è un sottospazio vettoriale.\nA + S (2, R)nonè quindi un sottospazio vettoriale!\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\12_Spazi_affini.pdf#9": "23/11/2010Sottospazi affiniTeorema: Sia v+ Eun sottospazio affine di uno spazio vettoriale V. Sia wun vettore di V. Possono sussistere due casi: 1.se wÎv+ Eallora v+ E= w+ E ;2.se wÏv+ Eallora l’intersezione di v+ Ee w+ Eè vuota.Dimostrazione: 1. Supponiamo che wÎv+ E. Allora w= v+ v’ per qualche v’ ÎE. Mostriamo ora che v+ EÊw+ E.Un vettore di w+ Esi può scrivere come w+ ucon uÎE. Ma allora w+ u= (v+ v’) + u= v+ (v’ + u) Îv+ E.Abbiamo così mostrato che v+ EÊw+ E. Per mostrare v+ EÍw+ Esi procede in maniera analoga.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#0": "15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it1L13: Omomorfismi (26)Argomenti lezione:•Introduzione•Omomorfismi tra spazi vettoriali •Matrice associata a un omomorfismo •Omomorfismo associato a una matrice•Esercizi ",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#1": "15/12/202Omomorfismo\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#10": "15/12/2011Omomorfismi tra spazi vettorialiEsercizio: Data f : M(2, 2, R) →M(2, 2, R) definitada:f (A) := A+ tAVerifichiamo se fè un omomorfismo di spazi vettoriali:1. Prese due matrici Ae Bdi M(2, 2, R), si ha:f(A+ B) = A+ B+ t(A+ B)= A+ B+ tA+ tBf(A) + f(B) = A+ tA+ B+ tBf(A+ B) =f(A) + f(B) 2. Prese una matrice Adi M(2, 2, R) e uno scalare kdi R, si ha:f(kA) = kA+ t(kA) = kA+ ktAkf(A) = k(A+ tA) = kA+ ktAf(kA) = kf(A) =>  fè un omomorfismo di spazi vettoriali su R!15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#11": "15/12/2012Omomorfismi tra spazi vettorialiEsercizio: Data f : R2→R2definita da: f (x,y)= (x2,x + y)Verifichiamo se fè un omomorfismo di spazi vettoriali:1. Presi due vettori u:= (x1 , y1)  e  v:= (x2 , y2) di R2, si ha:f(u + v)       =  f((x1 , y1) + (x2 , y2))=  f(x1+ x2 , y1+ y2)= ((x1+ x2)2 , x1+ x2+ y1+ y2)= (x12+ 2 x1 x2+ x22, x1+ x2+ y1+ y2)f(u) + f(v)  =  f(x1 , y1) + f (x2, y2)= (x12, x1+ y1) + (x22, x2+ y2)= (x12+ x22, x1+ x2+ y1+ y2)Notiamo che  f(u + v) =  f(u) +  f(v)  se e solo se  x1 x2= 0 Infatti tale relazione, e.g., nonè verificata se u:= (1, 0)  e  v:= (1, 0)Dunque  fnonè un omomorfismo di spazi vettoriali ! 15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#12": "15/12/2013Matrice associata a un omomorfismo\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#13": "15/12/2014Matrice associata a un omomorfismoDato un omomorfismo f: V→Wdi spazi vettoriali:Se conosciamo f(u) e f(v) allora conosciamo anche f(u+ v). Possiamo anche determinare l’immagine di vettori del tipo ku. Più in generale: Se f: V→Wè un omomorfismo di spazi vettoriali e se conosciamo l’immagine tramite fdi v1, v2, … , vndi V,di quali altri vettori di Vpossiamo determinare l’immagine? Proposizione: Sia f: V→Wun omomorfismo di spazi vettoriali. Siano v1, v2, … , vnvettori di Ve  k1, k2, . . . , knscalari. Si ha:f (k1v1+ k2v2+ … + knvn) = k1  f (v1) +k2  f (v2) + … + kn  f (vn)Dunque f (v1),  f (v2), … ,  f (vn) ci permettono di determinare l’immagine tramite fdi tutte le combinazioni linearidi v1, v2, … , vn15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#14": "15/12/2015Matrice associata a un omomorfismoProposizione: Siano f: V→W e  g: V→Wdue omomorfismi    di spazi vettoriali. Se v1, v2, . . . , vngenerano Ve  f (vj) = g(vj)     per 1 ≤  j≤ nallora  f= g. Dimostrazione: Bisogna mostrare che f (v) = g(v) per ogni vettore vin V .Sappiamo che vè combinazione lineare di v1, v2, . . . , vn:v= k1v1+ k2v2+…+ knvnAllora per la proposizione vista in precedenza abbiamo chef (v) = f (k1v1+ k2v2+…+ knvn) = k1  f (v1) + k2f (v2) +…+ kn  f (vn)g(v) = g(k1v1+ k2v2+…+ knvn) = k1 g(v1)  + k2 g(v2)  +…+ kn g(vn)Poiché  f (v1) = g(v1), …,  f (vn) = g(vn), abbiamo che f (v) = g(v). 15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#15": "15/12/2016Matrice associata a un omomorfismoSe v1, v2, . . . , vnsono dei generatori di V, un omomorfismo              f: V→Wè completamente descritto tramite f (v1), f (v2), … , f (vn).Domanda: Si trova un’applicazione lineare per ogni v1, v2, ... , vn? La risposta è negativa, vediamo un contro-esempio:Lo spazio vettoriale R2è generato dai vettori (1, 0), (0, 1), (1, 1).Non esiste alcuno omomorfismo f: R2→R3tale che                        f (1, 0) = (0, 0, 0),   f (0, 1) = (0, 0, 0),   f (1, 1) = (1, 0, 0). Infatti se un tale omomorfismo esistesse dovremmo avere:f (1, 1) =f ((1, 0) + (0, 1)) = f (1, 0) + f (0, 1) = (0, 0, 0) + (0, 0, 0) = (0, 0, 0)15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#16": "15/12/2017Matrice associata a un omomorfismoSe però i generatori considerati costituiscono una baseabbiamo:Teorema: Siano dati: due spazi vettoriali Ve W ; una base di Vcostituita dai vettori e1, e2, ... , en; i vettori w1, w2, ... , wndi W. Esiste un unico f: V→Wtale che:  f (ej) = wjper 1 ≤ j≤ n.Osservazione: Notiamo che non abbiamo fatto alcuna richiesta sui vettori w1, w2, ... , wndi W : ne che siano linearmente indipendenti ne che generino W.\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#17": "15/12/2018Matrice associata a un omomorfismoDefinizione: Sia f: V→Wun omomorfismo di spazi vettoriali didimensione finita. Fissiamo una base di Vformata dai e1, e2, … , en e una base di Wformata dai f1,  f2, … , fm . Possiamo esprimere ciascun vettore f (ej) come combinazione lineare dei f1,  f2, … ,  fm :f (e1) = a11  f1+ a21f2+…+ am1  fmf (e2) = a12  f1+ a22  f2+…+ am2  fm...f (en) = a1nf1+ a2n  f2+…+ amn  fmLa matrice Adi M (m, n, R) è detta matrice rappresentativa di frispetto alle basi e1, e2, … , ene   f1,  f2, … , fmLa j-esima colonna di Aè data dalle componenti del vettore f (ej)  rispetto alla base formata dai vettori f1,  f2, … , fm\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#18": "15/12/2019Matrice associata a un omomorfismoEsercizio: Sia f : R3→R2l’omomorfismo f (x, y, z) = (x+ y, y+z)e1:= (1, 0, 0), e2:= (1, 1, 0), e3:= (1, 1, 1) formano una base per R3f1:= (1, 1),  f2:= (2, 1) formano una base per R2 Determiniamo la matrice rappresentativa di  frispetto a tali basi.      f (e1) = f (1, 0, 0) = (1, 0)f (e2) = f (1, 1, 0) = (2, 1)f (e3) = f (1, 1, 1) = (2, 2) Decomponiamo le immagini rispetto alla base composta da f1  e  f2:f (e1) = (1, 0) = − (1, 1) + 1 (2, 1) = −1 f1+ 1 f2f (e2) = (2, 1) = 0 (1, 1) + 1 (2, 1) =  0f1  + 1 f2f (e3) = (2, 2) = 2 (1, 1) + 0 (2, 1) = 2f1+ 0 f2La matrice rappresentativa di  f  rispetto alle basi assegnate è A. \n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#19": "15/12/2020Matrice associata a un omomorfismoEsercizio: Sia f : R3→R2l’omomorfismo f (x, y, z) = (x+ y, y+z)Prendiamo due basi differenti: e'1:= (0, 1, 1), e'2:= (1, 0, 1), e'3:= (1, 1, 0) formano una base per R3f '1:= (0, 2),  f '2:= (1, 1) formano una base per R2Determiniamo la matrice A’rappresentativa di  frispetto a tali basi.\n≠15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#2": "15/12/203FunzioniDefinizioni: Siano Ae Bdue insiemi qualsiasi. Una funzione(o applicazione) f: A→Bè una legge che associa ad ogni elemento aÎAun elemento bÎB.L’insieme Asi dice insieme di partenzao dominiodi f. L’insieme Bsi dice insieme di arrivoo codominiodi f.L’elemento bviene indicato con f (a) è chiamato immaginedi a. L’insieme f (A) delle immagini degli elementi di Aviene chiamatoimmaginedi Aattraverso la funzione f(o anche immaginedi f ):Dato bÎB:  bÎf (A) se e solo seesiste aÎAtale che f (A) = b\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#20": "15/12/2021Matrice associata a un omomorfismoEsercizio:  f : M(2, 2, R) →M(2, 2, R) omomorfismo f (A) := A+ tAPer rappresentare f con una matrice dobbiamo fissare una base per M(2, 2, R) «di partenza» e una base per M(2, 2, R) «di arrivo».       In questo esempio prendiamo in entrambi i casi la base canonica. \n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#21": "15/12/2022Matrice associata a un omomorfismoEsercizio: Sia f : R3→R2l’omomorfismo f (x, y, z) = (x+ y, y+z)Per rappresentare frispetto alle basi canoniche di R3ed R2usiamo le immagini dei vettori e1:= (1, 0, 0), e2:= (0, 1, 0), e3:= (0, 0, 1)  espresse come combinazione lineare di  f1:= (1, 0),  f2:= (0, 1).La matrice Arappresentativa di frispetto alle basi canoniche è : \nLe colonnedi Adanno f (e1),  f (e2),  f (e3)  Le righedi Acorrispondono ai coefficientidi x, y, znelle espressioni (x+ y)e (y+ z)Tutto ciò vale soloquando rappresentiamo    frispetto alle basi canoniche!15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#22": "15/12/2023Matrice associata a un omomorfismoEsercizio: Sia f : R3→R2l’omomorfismo f (x, y, z) = (x+ y, y+z)Per rappresentare frispetto alle basi canoniche di R3ed R2usiamo le immagini dei vettori e1:= (1, 0, 0), e2:= (0, 1, 0), e3:= (0, 0, 1)  espresse come combinazione lineare di  f1’:= (1, 1),  f2’:= (2, −1).La matrice A’rappresentativa di  frispetto alle nuove basiè : \n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#23": "15/12/2024Matrice associata a un omomorfismoTeorema: Sia f: V→Wun omomorfismo di spazi vettoriali di dimensione finita. Siano fissate una base per Vformata dai vettori e1, e2, ... , ene una base per Wformata dai vettori f1,  f2, … , fm.     Sia Ala matrice rappresentativa di frispetto alle basi fissate.     Dato un vettore vdi Vpossiamo determinare f (v) come segue:1. Per prima cosa esprimiamo vcome combinazione lineare dei vettori della base e1, e2, … , en   :   v= b1 e1+ b2 e2+ … + bn en2. Calcoliamo poi il prodotto matriciale:3. Per ottenere f (v) basta ora calcolare la combinazione lineare:f (v) = c1  f1+ c2  f2+ … + cm  fm\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#24": "15/12/2025Omomorfismo associato a una matrice\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#25": "15/12/2026Omomorfismo associato a una matriceTeorema: Siano Ve Wdue spazi vettoriali di dimensione finita.   Sia data una base per Vformata dai vettori e1, e2, . . . , ene  una base per Wformata dai vettori f1, f2, . . . , fm. Allora esiste un unico omomorfismof: V→W, la cui matrice rappresentativa rispetto alle basi fissate è A di tipo M (m, n, R).\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#26": "15/12/2027Omomorfismo associato a una matriceDefinizione: Siano Ve Wdue spazi vettoriali di dimensione finita. Sia data una base per Vformata da e1, e2, … , ene una base per Wformata dai f1, f2, … ,  fm. Sia Auna matrice di tipo M (m, n, R): Si denota “omomorfismo associato ad Arispetto alle basi fissate”l’omomorfismo  f definito da:\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#27": "15/12/2028Omomorfismo associato a una matriceEsercizio: Sia  f : R3→R2l’omomorfismo associato alla matrice:Determinare f(x, y, z) per ogni (x, y, z) in R3.Calcoliamo le componenti di (x, y, z) rispetto alla base data di R3 :(x, y, z) = z(1, 1, 1) + (y− z) (1, 1, 0) + (x− y) (1, 0, 0)Per ottenere le componenti di f(x, y, z) rispetto alla base data di R2basta allora calcolare il prodotto di matrici:e1:= (1, 1, 1), e2:= (1, 1, 0), e3:= (1, 0, 0)f1:= (1, 1),  f2:= (1, −1)\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#28": "15/12/2029Omomorfismo associato a una matriceEsercizio: Sia  f : R2→R3definita da:  f (x, y) := (xy, x− y, 0)V ogliamo stabilire se  fè lineare. Troviamo le immagini di  fper i vettori della base canonica di R2f (1, 0) = (0, 1, 0),    f (0, 1) = (0, − 1, 0). Poi consideriamo l’unica applicazione lineareg : R2→R3definita dalle medesime condizioni g(1, 0) = (0, 1, 0),   g(0, 1) = (0, − 1, 0).La matrice rappresentativa di grispetto alle basi canoniche è : \nSi vede allora facilmente che:g(x,y) = (0, x − y,0)  da cui  f≠  gPer verifica mostriamo un vettore su cui le due funzioni  fe   gassumono valori diversi: f (1, 1) = (1, 0, 0),   g (1, 1) = (0, 0, 0). Segue la confermache  f≠ g.  Pertanto,  fnonè lineare.15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it29",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#29": "15/12/2030Omomorfismo associato a una matriceEsercizio: Sia  f : R2→R3definita da:  f (x, y) := (x+y,x−1,2x− y)V ogliamo stabilire se  fè lineare. Troviamo le immagini di  fper i vettori della base canonica di R2f (1, 0) = (1, 0, 2),    f (0, 1) = (1, − 1, − 1). Poi consideriamo l’unica applicazione lineareg : R2→R3definita dalle medesime condizioni g(1, 0) = (1, 0, 2),  g(0, 1) = (1, −1, −1).La matrice rappresentativa di grispetto alle basi canoniche è : Si vede allora facilmente che:g(x,y) = (x + y, − y, 2x − y)  da cui  f≠  gPer verifica mostriamo un vettore su cui le due funzioni  fe   gassumono valori diversi: f (2, 0) = (2, 1, 4),   g (2, 0) = (2, 0, 4). Segue la confermache  f≠ g.  Pertanto,  fnonè lineare.\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it30",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#3": "15/12/204Omomorfismi tra spazi vettorialiDefinizione: Sia f: V→Wun’applicazione tra due spazi Ve W. L’applicazione fsi dice omomorfismodi spazi vettoriali o applicazione lineare se valgono entrambele proprietà:1.f (u+ v) = f (u) + f (v)   per ogni uÎV, vÎV   [addizione]  2.f (k u)    = k f (u)           per ognik ÎR, uÎV[moltiplicazione]Esempio: Data f : R3→R2definita da:  f (x,y,z) := (x + y, y + z)Verifichiamo se fè un omomorfismo di spazi vettoriali:1.Presi due vettori u:= (x1, y1, z1) e v:= (x2, y2, z2) di R3f (u+ v)  = f((x1, y1, z1) + (x2,y2,z2)) =per la def. di somma in R3=  f (x1+ x2, y1+ y2, z1+ z2)  =  per la def. di f                       = (x1+ x2+ y1+ y2, y1+ y2+ z1+ z2)15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#30": "15/12/2031Omomorfismo associato a una matriceEsercizio: Sia  f : R2→R3definita da:  f (x, y) := (0, −x,2x − y)V ogliamo stabilire se  fè lineare. Troviamo le immagini di  fper i vettori della base canonica di R2f (1, 0) = (0, −1, 2),    f (0, 1) = (0, 0, −1). Poi consideriamo l’unica applicazione lineareg : R2→R3definita dalle medesime condizioni g(1, 0) = (0, −1, 2),  g(0, 1) = (0, 0, −1).La matrice rappresentativa di grispetto alle basi canoniche è : Si vede allora facilmente che:g(x,y) = (0, −x,2x − y)  da cui  f=  gPertanto,  fè lineare.\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it31",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#31": "15/12/2032Omomorfismo associato a una matriceOsservazione: L’esempio precedente ha in realtà validità generale!L’unica cosa che abbiamo sfruttato è che ciascuna delle espressioni 0, − x, 2x− yè un polinomio omogeneo di primo gradoin xe yoppure il polinomio nullo. [ Ricordiamo che un polinomio omogeneo di primo grado è un polinomio in cui tutti gli addendi sono monomi di grado 1 ]Esaminando gli esempi visti:2x− y+ 1 è un polinomio di primo grado non omogeneo,perché tra i suoi addendi c’è 1.xyè un polinomio omogeneo di secondo grado.15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it32",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#32": "15/12/2033OmomorfismiEsercizio: Stabilire se esistono omomorfismi f: R3→R2di spazi vettoriali tali che:f(1, 2, 1) = (0, 1)f (1, 0, 2) = (0, 1)f(2, 2, 1) = (2, 1)In caso affermativo stabilire se esiste uno solo di tali omomorfismi.I tre vettori (1, 2, 1), (1, 0, 2), (2, 2, 1) formano una base per R3 :\nEssendo i tre vettori una base, allora esiste esattamente un solo omomorfismo  fche soddisfa le condizioni date.15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it33",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#33": "15/12/2034OmomorfismiEsercizio: Stabilire se esistono omomorfismi  f: R3→R[x] di spazi vettoriali tali che:f (1, 3, 0) = xf(0, 1, 1) = x2f(1, 5, 2) = x+ x2In caso affermativo stabilire se esiste uno solo di tali omomorfismi.I vettori (1, 3, 0), (0, 1, 1) e (1, 5, 2) nonformano una base per R3 : \nInfatti: (1, 5, 2) = (1, 3, 0) + 2(0, 1, 1)f (1, 5, 2)  ≠f (1, 3, 0) + 2 f (0, 1, 1)f (1, 5, 2) = x+ x2f (1, 3, 0) + 2 f (0, 1, 1) = x+ 2x215/12/20Geometria e Combinatoria marcella.sama@uniroma3.it34",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#34": "15/12/2035OmomorfismiEsercizio: Sia f: R3→R2l’omomorfismo definito dalle condizioni  f (1, 0, 1) := (3, 1),   f (1, 1, 1) := (1, 2),   f (0, 0, 1) := (0, 1). Determinare la matrice rappresentativa di frispetto alle basi canoniche di R3e R2.\nLa matrice di frispetto alle basi canoniche è :15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it35",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#35": "15/12/2036OmomorfismiEsercizio: Sia f: R3→R2l’omomorfismo definito dalle condizioni  f (1, 0, 1) := (3, 1),   f (1, 1, 1) := (1, 2),   f (0, 0, 1) := (0, 1). Determinare la matrice rappresentativa di frispetto alle base di R3formata dai vettori v1:= (1, 0, 1), v2:= (1, 1, 1), v3:= (0, 0, 1)         e alla base di R2formata dai vettori w1:= (1, 1), w2:= (1, 2).\nLa matrice di frispetto alle basi assegnate è :\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#36": "15/12/2037OmomorfismiEsercizio: Stabilire se esiste un omomorfismo che verifica le seguenti condizioni e, in caso affermativo, dire se l’omomorfismo è unico.f: R3[x] → R2tale che:f (1 + x2) = (1, –2)f (2 + x+ x2) = (1, 0)I polinomi 1 + x2, 2 + x+ x2sono linearmente indipendenti. Se p3(x) è un qualsiasi vettore che insieme a 1 + x2e 2 + x+ x2forma una base di R3[x], allora per ogni vettore w3di R2esiste un unico omomorfismo f: R3[x] → R2tale che f (1 + x2) = (1, –2), f (2 + x+ x2) = (1, 0),   f(p3(x)) = w3.  Per l’arbitrarietà di w3abbiamo allora infiniti omomorfismi come quello cercato.15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it37",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#37": "15/12/2038OmomorfismiEsercizio: Determinare l’immagine del vettore (x, y, z, w) tramite l’omomorfismo  f: R4→ R3associato alla seguente matrice: e1:= (1, 2, 0, 0), e2:= (1, 0, 1, 0), e3:= (1, 3, 0, 0), e4:= (0, 0, 0, 1)f1:= (1, 2, 1), f2:= (1, 0, 2), f3:= (2, 2, 1)\nNotiamo che in questo sistema noi conosciamo x, y, z, we dobbiamo determinare a, b, c, d.\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it38",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#38": "15/12/2039OmomorfismiEsercizio: Determinare l’immagine del vettore (x, y, z, w) tramite l’omomorfismo  f: R4→ R3associato alla seguente matrice: e1:= (1, 2, 0, 0), e2:= (1, 0, 1, 0), e3:= (1, 3, 0, 0), e4:= (0, 0, 0, 1)f1:= (1, 2, 1), f2:= (1, 0, 2), f3:= (2, 2, 1)\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it39",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#39": "15/12/2040OmomorfismiEsercizio: Determinare l’immagine del vettore (x, y, z, w) tramite l’omomorfismo  f: R4→ R3associato alla seguente matrice: e1:= (1, 2, 0, 0), e2:= (1, 0, 1, 0), e3:= (1, 3, 0, 0), e4:= (0, 0, 0, 1)f1:= (1, 2, 1), f2:= (1, 0, 2), f3:= (2, 2, 1)\n15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it40",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#4": "15/12/205Omomorfismi tra spazi vettorialiDefinizione: Sia f: V→Wun’applicazione tra due spazi Ve W. L’applicazione fsi dice omomorfismodi spazi vettoriali o applicazione lineare se valgono entrambele proprietà:1.f (u+ v) = f (u) + f (v)   per ogni uÎV, vÎV   [addizione]  2.f (k u)    = k f (u)           per ognik ÎR, uÎV[moltiplicazione]Esempio: Data f : R3→R2definita da:  f (x,y,z) := (x + y, y + z)Verifichiamo se fè un omomorfismo di spazi vettoriali:1.Presi due vettori u:= (x1, y1, z1) e v:= (x2, y2, z2) di R3f (u+ v)  =  (x1+ x2+ y1+ y2, y1+ y2+ z1+ z2)f (u) + f (v) =  f (x1, y1, z1) + f (x2, y2, z2) =per la def.di f= (x1+ y1, y1+ z1) + (x2+ y2, y2+ z2) =per la def. di somma in R2 = (x1+ y1+ x2+ y2, y1+ z1+ y2+ z2)15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#5": "15/12/206Omomorfismi tra spazi vettorialiDefinizione: Sia f: V→Wun’applicazione tra due spazi Ve W. L’applicazione fsi dice omomorfismodi spazi vettoriali o applicazione lineare se valgono entrambele proprietà:1.f (u+ v) = f (u) + f (v)per ogni uÎV, vÎV   [addizione]  2.f (k u)    = k f (u)           per ognik ÎR, uÎV[moltiplicazione]Esempio: Data f : R3→R2definita da:  f (x,y,z) := (x + y, y + z)Verifichiamo se fè un omomorfismo di spazi vettoriali:1.Presi due vettori u:= (x1, y1, z1) e v:= (x2, y2, z2) di R3f (u+ v)  =(x1+ x2+ y1+ y2, y1+ y2+ z1+ z2)OKf (u) + f (v)=  f (x1, y1, z1) + f (x2, y2, z2) =per la def.di f= (x1+ y1, y1+ z1) + (x2+ y2, y2+ z2) =per la def. di somma in R2 =(x1+ y1+ x2+ y2, y1+ z1+ y2+ z2)OK15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#6": "15/12/207Omomorfismi tra spazi vettorialiDefinizione: Sia f: V→Wun’applicazione tra due spazi Ve W. L’applicazione fsi dice omomorfismodi spazi vettoriali o applicazione lineare se valgono entrambele proprietà:1.f (u+ v) = f (u) + f (v)   per ogni uÎV, vÎV   [addizione]  2.f (k u)    = k f (u)           per ognik ÎR, uÎV[moltiplicazione]Esempio: Data f : R3→R2definita da:  f (x,y,z) := (x + y, y + z)Verifichiamo se fè un omomorfismo di spazi vettoriali:2.Presi un vettore u:= (x,y,z) di R3e uno scalare kdi Rf(k u) = f(k(x,y,z)) =per la def. di prodotto in R3                 = f(k x, k y, k z) =per la def. di f= (k x+ k y, k y+ k z)15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#7": "15/12/208Omomorfismi tra spazi vettorialiDefinizione: Sia f: V→Wun’applicazione tra due spazi Ve W. L’applicazione fsi dice omomorfismodi spazi vettoriali o applicazione lineare se valgono entrambele proprietà:1.f (u+ v) = f (u) + f (v)   per ogni uÎV, vÎV   [addizione]  2.f (k u)    = k f (u)           per ognik ÎR, uÎV[moltiplicazione]Esempio: Data f : R3→R2definita da:  f (x,y,z) := (x + y, y + z)Verifichiamo se fè un omomorfismo di spazi vettoriali:2.Presi un vettore u:= (x,y,z) di R3e uno scalare kdi Rf(k u) = (k x+ k y, k y+ k z)kf (u)  =  kf(x,y,z) =per la def. di f=  k(x+ y, y+ z) =per la def. di prodotto in R2                =  (k x+ k y, k y+ k z)15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#8": "15/12/209Omomorfismi tra spazi vettorialiDefinizione: Sia f: V→Wun’applicazione tra due spazi Ve W. L’applicazione fsi dice omomorfismodi spazi vettoriali o applicazione lineare se valgono entrambele proprietà:1.f (u+ v) = f (u) + f (v)   per ogni uÎV, vÎV   [addizione]  2.f (k u)    = k f (u)per ognik ÎR, uÎV[moltiplicazione]Esempio: Data f : R3→R2definita da:  f (x,y,z) := (x + y, y + z)Verifichiamo se fè un omomorfismo di spazi vettoriali:2.Presi un vettore u:= (x,y,z) di R3e uno scalare kdi Rf(k u) =(k x+ k y, k y+ k z)OKkf (u)=  kf(x,y,z) =per la def. di f=  k(x+ y, y+ z) =per la def. di prodotto in R2                =(k x+ k y, k y+ k z)OK15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\13_Omomorfismo.pdf#9": "15/12/2010Omomorfismi tra spazi vettorialiDefinizione: Sia f: V→Wun’applicazione tra due spazi Ve W. L’applicazione fsi dice omomorfismodi spazi vettoriali o applicazione lineare se valgono entrambele proprietà:1.f (u+ v) = f (u) + f (v)per ogni uÎV, vÎV   [addizione]  2.f (k u)    = k f (u)per ognik ÎR, uÎV[moltiplicazione]Esempio: Data f : R3→R2definita da:  f (x,y,z) := (x + y, y + z)Verifichiamo se fè un omomorfismo di spazi vettoriali:1.Presi due vettori u:= (x1, y1, z1) e v:= (x2, y2, z2) di R3f (u+ v)  =f (u) + f (v)2.Presi un vettore u:= (x,y,z) di R3e uno scalare kdi Rf(k u) = kf (u)=> fè un omomorfismodi spazi vettoriali !15/12/20Geometria e Combinatoria marcella.sama@uniroma3.it10",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#0": "Geometria e Combinatoria marcella.sama@uniroma3.itL14: Immagine (27)Argomenti lezione:•Immagine di un omomorfismo•Calcolo dell’immagine•Esercizi",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#1": "15/12/202Immagine di un omomorfismoRicordiamo che, data un’applicazione tra insiemi f: A→B, l’immaginedi fè il sottoinsieme f (A) dell’insieme Bformato dalle immagini degli elementi di Atramite f . Vale a dire da tutti gli elementi bdi Bper cui esiste ain Atale che f (a) = b.Esempio: Sia  f: R2→R3definita da: f (x, y) := (x+2y, x+ y, x −y).L’immagine di  fsi determina tramite i w:= (a, b, c) di R3per cui esiste v:= (x, y) tale che f (v) = w. Cioè: (x+2y, x+y, x−y) = (a, b, c).Da cui il vettore vesiste se e solo se il seguente sistema è risolubile:\nmetodo di Gauss\nIl sistema è risolubile  se e solo se2a− 3b+ c= 0. Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#10": "15/12/2011Calcolo dell’immagineEsercizio: Prendiamo l’omomorfismo f: R3→R4 [x] definito da:f (a, b, c) := (2a +b +8c) + (3a −b +7c) x+ (−a −3c) x2+ (b+ 2c) x3Determinare una base per f (R3).La matrice A rappresentativa di frispetto alle basi canoniche è :La matrice ha rango 2: dim  f (R3) = 2. Poichè gli scalini sono in I e II posizione, una base per f (R3) è formata dall’immagine dei primi due vettori della base canonica di R3, cioè da f (1, 0, 0) e  f (0, 1, 0).Una base per f (R3) è formata dai vettori: 2 + 3x− x2e  1 − x+ x3.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#11": "15/12/2012Calcolo dell’immagineEsercizio: Stabilire se i seguenti omomorfismi sono suriettivi:\nPossiamo dire subito che  fnon è suriettivo (i.e. f (V) ≠W) : R3ha dimensione finita, mentre R[x] non ha dimensione finita.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#12": "15/12/2013Calcolo dell’immagineEsercizio: Stabilire se i seguenti omomorfismi sono suriettivi:\nLa matrice Arappresentativa di frispetto alle basi canoniche é:Si verifica facilmente che questa matrice ha rango 3 e, quindi, dim  f (R3) = 3. Pertanto  f  è suriettivo(i.e. f (V) = W).\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#13": "15/12/2014Calcolo dell’immagineEsercizio: Stabilire se i seguenti omomorfismi sono suriettivi:\nPossiamo dire subito che  fnon è suriettivo(i.e. f (V) ≠W) : infatti dim R3< dim M (2, 2, R).\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#14": "15/12/2015Calcolo dell’immagineEsercizio:  Sia  f: M (2, 2, R) →R2l’applicazione definita da:Mostrare che  fè un omomorfismo e stabilire se  f  è suriettivo.\nL’applicazione  f  è un omomorfismo visto che (a+ b)  e  (c+ d) sono polinomi omogenei di grado 1 in a, b, c, d. Nonescludiamo che fè suriettivo, perchè dim M (2, 2, R) ≥dim R2La matrice rappresentativa di  frispetto alle basi canoniche è:\nQuesta matrice ha rango 2.Dunque,  fè suriettivo.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#15": "15/12/2016Calcolo dell’immagineEsercizio: Mostrare che le seguenti condizioni definiscono un unico omomorfismo  f: R3→R2. Poi, stabilire se  f è suriettivo.\nI tre vettori (1, 2, 1), (1, 0, 1), (0, 0, 1) costituiscono una base per R3, abbiamo definito un unico omomorfismo f: R3→R2.dim R3≥ dim R2e quindi nonescludiamo che  fè suriettivo. La matrice rappresentativa di  frispetto alla base data di R3e dalla base canonica di R2è la seguente:\nQuesta matrice ha rango 2.Dunque,  fè suriettivo.110200111≠ 0\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#2": "15/12/203Immagine di un omomorfismoRicordiamo che, data un’applicazione tra insiemi f: A→B, l’immaginedi fè il sottoinsieme f (A) dell’insieme Bformato dalle immagini degli elementi di Atramite f . Vale a dire da tutti gli elementi bdi Bper cui esiste ain Atale che f (a) = b.Esempio: Sia  f: R2→R3definita da: f (x, y) := (x+2y, x+ y, x −y).L’immagine di  fsi determina tramite i w:= (a, b, c) di R3per cui esiste v:= (x, y) tale che f (v) = w. Cioè: (x+2y, x+y, x−y) = (a, b, c).Da cui il vettore vesiste se e solo se il seguente sistema è risolubile:\nmetodo di Gauss\nè un sottospazio vettorialedi R3Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#3": "15/12/204Immagine di un omomorfismo\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#4": "15/12/205Immagine di un omomorfismoDomanda: Se abbiamo un’applicazione non linearef: V→Wtra spazi vettoriali, cosa possiamo dire dell’immagine di f ?Risposta: Se abbiamo un’applicazione non lineare  f: V→Wtra spazi vett., non possiamo (a priori) dire nulla sull’immagine di  f  : •Né che sia un sottospazio vettoriale di W.•Né che non lo sia.Bisogna valutare caso per caso come è fatto il sistema risultante.Esempi: •Sia f: R2→R2l’applicazione non lineare f (x, y) := (x2, x+ y)Si può verificare che in questo caso  non è  un sottospazio di R2•Sia f: R2→R2l’applicazione non lineare f (x, y) := (xy, 2xy)Si può verificare che in questo caso  èun sottospazio di R2Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#5": "15/12/206Calcolo dell’immagineTeorema: Se  f: V→Wè un omomorfismo di spazi vettoriali e     se lo spazio vettoriale Vè generato dai vettori v1, v2, … , vn , allora f (V) è generato dai vettori  f (v1),  f (v2), … , f (vn).Dimostrazione: Dobbiamo mostrare che ogni wdell'immagine di fsi può esprimere come combin. lineare di  f (v1),  f (v2), ... , f (vn).Sappiamo che esiste un vettore vdi Vtale che w= f (v). Possiamo ora esprimere vcome combinazione lineare di v1, v2, … , vn :v= k1 v1+ k2 v2+ … + kn vnMa allora f (v) = k1f (v1) + k2  f (v2) + … + kn  f (vn)Poiché w= f (v) abbiamo dunque espresso wcome combinazione lineare dei vettori f (v1),  f (v2), … , f (vn), come volevamo.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#6": "15/12/207Calcolo dell’immagine•Abbiamo mostrato che f (v1), f (v2), … , f (vn) generano f (V), non tutto W(ciò è vero solose f (V) = W, ovvero fè suriettivo). •Notiamo poi che, anche nel caso in cui i vettori v1, v2, … , vnformano una base per V, non è detto che f (v1), f (v2), … , f (vn) formano una base per f (V). Contro-esempio: Sia dato l’omomorfismo f: R3→R[x] definito da:    f (a, b, c) := (a− b) + (b− c)x + (c− a) x2f (R3) è generato dalle immagini dei vettori di una base di R3. Presa la base canonica, f (R3) è generato dai polinomi:f (1, 0, 0) = 1 − x2,  f (0, 1, 0) = − 1 + x,  f (0, 0, 1) = − x +  x2.   Questi tre polinomi sono linearmente dipendenti:− x+ x2= − (1 − x2) − (− 1 + x)                                                                       Dunque, non formano una baseper f (R3).Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#7": "15/12/208Calcolo dell’immagineCorollario: Se f: V→Wè un omomorfismo di spazi vettoriali e sela dimensione di Vè finita, allora la dimensione di f (V) è finita e dim  f (V) ≤ dim V. (Invece non sappiamo nulla sulla dim W.) Dimostrazione: Se i vettori v1, v2, … , vnformano una base per V(e, dunque, dim V= n), allora f (V) è generato dagli nvettori f (v1),   f (v2), … , f (vn), e, pertanto, la sua dimensione è al più n. Osservazioni: Se f: V→Wè un omomorfismo di spazi vettoriali edim V< dim Wallora f nonpuò essere suriettivo (ovvero f (V) ≠W).Nel caso in cui dim V≥ dim Wnon possiamo dire nulla a priori: dobbiamo valutare caso per caso. Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#8": "15/12/209Calcolo dell’immagineEsercizio: Sia Vuno spazio vettoriale con una base formata dai vettori e1, e2, e3,e4. Sia Wun altro spazio vettoriale con una base formata dai vettorif1,  f2,  f3. Siaf: V→Wl’omomorfismo:f (e1) := f1+ f2+ f3 ;               f (e2) := f1+ 2 f2+ 3 f3 ;f (e3) := 3 f1+ 4 f2+ 5 f3 ;      f (e4) := − f2− 2 f3  .V ogliamo determinare una base per l’immagine di  f. Consideriamo la matrice Ale cui colonne danno le componenti di   f (e1),  f (e2),  f (e3),  f (e4) rispetto alla base formata da  f1,  f2,  f3 :Poiché gli scalini sono in I e II posizione troviamo che una base per f (V) è data dai vettori f (e1) e  f (e2), ovvero f1+f2+f3e   f1+2f2+3f3\nriduciamo  A a scalini\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\14_Immagine.pdf#9": "15/12/2010Calcolo dell’immagineTeorema: Sia f: V→Wun omomorfismo di spazi vettoriali di dimensione finita. Fissiamo una base per V, formata dai vettori    e1, e2, … , en , e una base per W, formata dai vettori f1,  f2, … ,  fm. Prendiamo la matrice Arappresentativa di  frispetto alle basi date.Risulta: dim f (V) = rk A. In particolare, abbiamo che fè suriettivo (i.e. f (V) = W) se e solo se rk A= dim W.Osservazioni: Per determinare una base dell’immagine di fnotiamo che le colonne della matrice Aforniscono le componenti rispetto alla base f1,  f2, … ,  fm dei vettori f (e1),  f (e2), … ,  f (en) che generano f (V). Possiamo quindi determinare una base di f (V) calcolando il rango rdella matrice Ae scegliendo opportunamente  rvettori tra  f (e1),  f (e2), … ,  f (en). Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#0": "15/12/201L15: Nucleo (28-29)Argomenti lezione:•Nucleo di un omomorfismo •Calcolo del nucleo •Cenni su isomorfismi •EserciziIl nucleodi un omomorfismo è il sottoinsieme dello spazio di partenza la cui immagine è il vettore nullo.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#1": "15/12/202Nucleo\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#10": "15/12/2011Calcolo del nucleoEsercizio: Sia l’omomorfismo f: V→Wdefinito dalle condizioni:  \nriducendo a scalini la matrice ACalcoliamo il nucleo tramite il sistema equivalenteGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#11": "15/12/2012Calcolo del nucleoEsercizio: Sia l’omomorfismo f: V→Wdefinito dalle condizioni:  \nriducendo a scalini la matrice A\nsoluzioni del sistemaPonendo t= 1 e u= 0 otteniamo la soluzione (−2, −1, 1, 0), ponendo invece t= 0 e u= 1 otteniamo la soluzione (−1, 1, 0, 1). Una baseper ker f : −2e1−1e2+ 1e3+ 0e4; −1e1+ 1e2+ 0e3+ 1e4Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#12": "15/12/2013Calcolo del nucleoEsercizio: Sia l’omomorfismo f: R3→R4[x] definito da: \nMatrice Arispettoalle basicanoniche\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#13": "15/12/2014Calcolo del nucleoEsercizio: Sia l’omomorfismo f: R3→R4[x] definito da: \nriducendo a scalini la matrice Asoluzioni del sistemaUna baseper  ker fè allora formata dal singolo vettore le cui componenti rispetto alla base canonica sono (−3, −2, 1). \ncerchiamo il nucleo\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#14": "15/12/2015Calcolo del nucleoTeorema: Sia  f: V→Wun omomorfismo di spazi vettoriali di dimensione finita. Fissiamo una base per Ve una base per W.Sia Ala matrice rappresentativa di  frispetto a tali basi. Risulta: dim ker f= dim V− rk A [dato chele soluzioni del sistema lineare omogeneo utilizzato peril calcolo del nucleo sono descritte da dim V–rk A parametri]\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#15": "15/12/2016Calcolo del nucleoTeorema: Sia  f: V→Wun omomorfismo di spazi vettoriali di dimensione finita. Fissiamo una base per Ve una base per W.Sia Ala matrice rappresentativa di  frispetto a tali basi. Risulta: dim ker f= dim V− rk A Teorema: Se  f: V→Wè un omomorfismo di spazi vettoriali e se V  ha dimensione finita, risulta: dim V= dim ker f+ dim f (V )[abbiamo visto in precedenza che dim f (V ) = rk A ]\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#16": "15/12/2017Calcolo del nucleoTeorema: Sia  f: V→Wun omomorfismo di spazi vettoriali di dimensione finita. Fissiamo una base per Ve una base per W.Sia Ala matrice rappresentativa di  frispetto a tali basi. Risulta: dim ker f= dim V− rk A Teorema: Se  f: V→Wè un omomorfismo di spazi vettoriali e se V  ha dimensione finita, risulta: dim V= dim ker f+ dim f (V )Teorema: Sia  f: V→Wun omomorfismo di spazi vettoriali. Risulta che fè iniettivose e solo se:  ker f= {0V}.[ fè iniettivoquando due vettori diversi ue v, con u≠ v, hanno immagini diverse, ovvero f (u) ≠ f (v) )Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#17": "15/12/2018Calcolo del nucleoTeorema: Sia  f: V→Wun omomorfismo di spazi vettoriali di dimensione finita. Fissiamo una base per Ve una base per W.Sia Ala matrice rappresentativa di  frispetto a tali basi. Risulta: dim ker f= dim V− rk A Teorema: Se  f: V→Wè un omomorfismo di spazi vettoriali e se V  ha dimensione finita, risulta: dim V= dim ker f+ dim f (V )Teorema: Sia  f: V→Wun omomorfismo di spazi vettoriali. Risulta che fè iniettivo(i.e. vettori diversi hanno immagini diverse) se e solo se:  ker f= {0V}.Notiamo che se fè iniettivo, allora dim V= dim  f (V ). Poichè  f (V ) è un sottospazio di W , abbiamo che dim V ≤ dim W.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#18": "15/12/2019Calcolo del nucleoNotiamo che se fè iniettivo, allora dim V= dim  f (V ). Poichè  f (V ) è un sottospazio di W , abbiamo che dim V ≤ dim W.Osservazione: Se  f: V→W èun omomorfismo di spazi vettoriali e dim V> dim Wallora l’omomorfismo  fnonpuò essere iniettivo.Nel caso in cui dim V≤ dim Wnon possiamo dire nulla a priori: dobbiamo valutare caso per caso. Criterio: Sia  f: V→ Wun omomorfismo tra spazi di dim. finita. Sia Ala matrice rappresentativa difrispetto a delle basi per Ve W. Sono allora equivalenti le seguenti condizioni:1.L’omomorfismo  fè iniettivo, 2.rk A= dim V, 3.dim f (V) = dim V.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#19": "15/12/2020Calcolo del nucleoEsercizio: Stabilire se i seguenti omomorfismi sono iniettivi:\nPossiamo dire subito che  fnonè iniettivo: infatti R[x] non ha dimensione finita,mentre R3ha dimensione finita.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#2": "15/12/203Definizione del vettore nulloTeorema: Se f: V→Wè un omomorfismo allora f (0v) = 0w .Esempio: Sia f: R3→R2  l’applicazione definita da:f (x,y, z) := (x+ y+ 3z , x+ 2y+ 4z−1). fnonè lineare (nonè un omomorfismo) dal momento chef (0, 0, 0) = (0, −1) ≠ (0, 0)Esempio: Sia f: R2→R2  l’applicazione  f (x, y) := (xy, x). Vediamo che f (0, 0) = (0, 0). Attenzione però:  fnonè un omomorfismo poiché, ad esempio,  f (0, 1) + f (1, 0) = (0, 0) + (0, 1) = (0, 1) , mentre          f ((0, 1) + (1, 0)) = f (1, 1) = (1, 1) Quindi anche se f (0v ) = 0w  nonè detto che fsia un omomorfismo!Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#20": "15/12/2021Calcolo del nucleoEsercizio: Stabilire se i seguenti omomorfismi sono iniettivi:\nPossiamo dire subito che  fnonè iniettivo: infatti dim M (2, 2, R) > dim R3.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#21": "15/12/2022Calcolo del nucleoEsercizio: Stabilire se i seguenti omomorfismi sono iniettivi:\nPoiché dim R3≤ dim M(2, 2, R) non possiamo escludere a priori che f sia iniettivo. Dobbiamo svolgere i dovuti calcoli. La matrice rappresentativa di frispetto alle basi canoniche è :\nQuesta matrice ha rango 3. Essendo rk A= dim V,segue fè iniettivo.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#22": "15/12/2023Isomorfismo\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#23": "15/12/2024IsomorfismiDefinizione: Un omomorfismo f: V→Wdi spazi vettoriali è dettoisomorfismose f è biiettivo,cioè se fè sia suriettivo che iniettivo. Due spazi vettoriali Ve Wsi dicono isomorfise esiste un isomorfismo f: V→W. Teorema: Se f: V→Wè un isomorfismo di spazi vettoriali e Vha dimensione finita, allora Wha dimensione finita e dim W= dim V.[ Infatti per  f suriettivo si ha:  f (V) = W mentre per  f iniettivo si ha: dim f (V) = dim V  ]Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#24": "15/12/2025IsomorfismiDefinizione: Un omomorfismo f: V→Wdi spazi vettoriali è dettoisomorfismose f è biiettivo,cioè se fè sia suriettivo che iniettivo.Due spazi vettoriali Ve Wsi dicono isomorfise esiste un isomorfismo f: V→W. Teorema: Se f: V→Wè un isomorfismo di spazi vettoriali e Vha dimensione finita, allora Wha dimensione finita e dim W= dim V.Teorema: Sia  f: V→Wun omomorfismo di spazi di dim. finita. Sia dim V= dim W= n.  Sia Ala matrice rappresentativa di  frispetto a delle basi per Ve W.  Sono equivalenti le condizioni:1.  fè suriettivo;    2.  fè iniettivo;    3. fè un isomorfismo (cioè è biiettivo);    4. det A≠ 0.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#25": "15/12/2026Calcolo di nucleo e immagineEsercizio: Sia  f: R3→R3l’omomorfismo definito dalle condizioni:\nDeterminare la matrice rappresentativa di f rispetto alla base canonica:\nmatrice rappresentativa\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#26": "15/12/2027Calcolo di nucleo e immagineEsercizio: Sia  f: R3→R3l’omomorfismo definito dalle condizioni:\nDeterminare la matrice rappresentativa di f rispetto alla base di R3formata dai vettori v1:= (1, 1, 1), v2:= (1, 1, 0), v3:= (1, 0, 0):\nmatrice rappresentativaGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#27": "15/12/2028Calcolo di nucleo e immagineEsercizio: Sia  f: R3→R3l’omomorfismo definito dalle condizioni:\nDeterminare la matrice rappresentativa di f rispetto alla base di R3formata dai vettori w1:= (1, 2, 1), w2:= (0, 1, 0), w3:= (0, 0, 2):Per prima cosa, determiniamo  f (1, 2, 1),  f (0, 1, 0),  f (0, 0, 2).\nPer trovare f (1, 2, 1), usiamo la matrice rappr. di frispetto alla base canonica.Analogamente si ha:  f (0, 1, 0) = (1, 2, 0),   f (0, 0, 2) = (0, –4, 2)Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#28": "15/12/2029Calcolo di nucleo e immagineEsercizio: Sia  f: R3→R3l’omomorfismo definito dalle condizioni:\nDeterminare la matrice rappresentativa di f rispetto alla base di R3formata dai vettori w1:= (1, 2, 1), w2:= (0, 1, 0), w3:= (0, 0, 2):Poi,decomponiamo i vettori trovati rispetto alla base data: \nmatrice rappresentativa\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#29": "15/12/2030Calcolo di nucleo e immagineEsercizio: Sia  f: R3→R3l’omomorfismo definito dalle condizioni:\nDeterminare il nucleo e l’immagine di  f:Sappiamo che f (R3) è generato dalle immagini dei vettori di una base.Ad esempio, f (R3) è generato da f (1, 1, 1),  f (1, 1, 0),  f (1, 0, 0), ovvero da (1, 0, 1), (1, 2, 0), (0, 0, 0). (1, 0, 1) e (1, 2, 0) sono linear. indip., e formano una baseper  f (R3).dim f (R3) = 2. Segue dim ker f= dim R3 –dim  f (R3) = 3–2 = 1.Poichè (1, 0, 0) Îker  f, questo vettore forma una baseper ker  f.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#3": "15/12/204Nucleo di un omomorfismoDefinizione: Se f: V→Wè un omomorfismo di spazi vettoriali chiamiamo nucleodi  f( kerf  ) l’insieme dei vettori di Vla cui immagine è il vettore 0w.  Dunque: ker f:= {  v∊V| f (v) = 0w}Esempio: Prendiamo l’omomorfismo f: R5→R[x] definito da:f (a, b, c, d, e) := (a− b+ c) + (b− c+ d) x+ (c− d+ e) x2Un vettore (a, b, c, d, e) appartiene a  ker fse e solo se: (a− b+ c) + (b−c+ d) x+ (c− d+ e) x2= 0. Ovvero se e solo se:Una baseper ker f  è formata da (−1, 0, 1, 1, 0) e (0, −1, −1, 0, 1). \nker fè l'insieme delle soluzioni di      un sistema omogeneo e, quindi, èun sottospazio vettorialedi R5.\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#4": "15/12/205Nucleo di un omomorfismoEsercizio: Sia f: M(2, 2, R) →M(2, 2, R) definito da f (A) := A+ tAPer determinare  ker  fpossiamo considerare la generica matricedi M(2, 2, R) e stabilire quando f (A) = 0.\nker fè un sottospazio vettorialedello spazio di partenza.ker fè formato dalle matrici Atali che A+ tA= 0 cioè le matrici antisimmetrichetali che tA= −A. Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#5": "15/12/206Nucleo di un omomorfismoTeorema: Il nucleodi un omomorfismo f: V→Wdi spazi vettoriali è un sottospazio vettorialedi V.Dimostrazione: Sappiamo che 0VÎker f ( f (0v) = 0w ). ker f≠ Ø.[ Per definizione di nucleosi ha:  ker f:= {  vin V| f (v) = 0w} ]\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#6": "15/12/207Nucleo di un omomorfismoTeorema: Il nucleodi un omomorfismo f: V→Wdi spazi vettoriali è un sottospazio vettorialedi V.Dimostrazione: Sappiamo che 0VÎker f ( f (0v) = 0w ). ker f≠ Ø.Sappiamo che f (v1) = 0we  f (v2) = 0we dobbiamo mostrare che      f (v1+ v2) = 0w . Dunque: f (v1+ v2) = f (v1) + f (v2) = 0w+ 0w= 0w .Ora dobbiamo mostrare che se vappartiene a ker f   ekè uno scalare, allora  k vappartiene a ker f . Sappiamo che f (v) = 0we  dobbiamo mostrare che f (k v) = 0w . Dunque f (k v) = k f (v) = k 0w= 0w. Abbiamo dimostrato che il nucleo di fè un sottospazio vett. di V.Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#7": "15/12/208Calcolo del nucleoProcedura: Dato un omomorfismo f: V→Wdi spazi vettoriali di dimensione finita. Data una base per V, formata dai e1, e2, . . . , en ,  e una base per W, formata dai f1,  f2, . . . ,  fm. Sia Ala matrice rappresentativa di frispetto alle basi date. Consideriamo ora un genericovettore vdi V: V ogliamo stabilire se vappartiene a ker f. \n•f1,  f2, … ,  fm sono linearm. indipend. (formano una base per W ) per cui vappartiene a  ker f  se e solo se  y1= y2= … = ym= 0.[ Per definizione di nucleosi ha:  ker f:= {  vin V| f (v) = 0w} ] \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#8": "15/12/209Calcolo del nucleoProcedura: Dato un omomorfismo f: V→Wdi spazi vettoriali di dimensione finita. Data una base per V, formata dai e1, e2, . . . , en ,  e una base per W, formata dai f1,  f2, . . . ,  fm. Sia Ala matrice rappresentativa di frispetto alle basi date. Consideriamo ora un genericovettore vdi V : V ogliamo stabilire se vappartiene a ker f. \n•f1,  f2, … ,  fm sono linearm. indipend. (formano una base per W ) per cui vappartiene a  ker f  se e solo se  y1= y2= … = ym= 0.•Partendo una soluzione (x1,x2, …, xn) del sistema lin. omogeneootteniamo un vettore del nucleo di  f  :  x1e1+ x2e2+ … + xnen  \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\15_Nucleo.pdf#9": "15/12/2010Calcolo del nucleoEsercizio: Sia l’omomorfismo f: V→Wdefinito dalle condizioni:  \nSia Vuno spazio vettor. con una base formata da e1, e2, e3,e4. Sia Wuno spazio vettor. con una base formata dai vettorif1,  f2,  f3. \nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#0": "23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it1L16: Endomorfismi (30)Argomenti lezione:•Introduzione•Endomorfismi •Cambiamento di base•Esercizi ",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#1": "23/12/202IntroduzioneObiettivo lezione:Studiare particolari omomorfismi di spazi vettoriali in cui lospazio di partenza e lo spazio di arrivo coincidono.Vedremo:Come rappresentare questi omomorfismi per mezzo di matrici.  Stabiliremo come variano le matrici rappresentative al variare delle basiscelte.Definizione: Dato uno spazio vettoriale V, un endomorfismodi Vè un omomorfismo  f: V→V.Segue che possiamo utilizzare per gli endomorfismi le stesse definizionie gli stessi risultativisti per gli omomorfismi.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#10": "23/12/2011Cambiamento di base\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#11": "23/12/2012Cambiamento di baseAbbiamo visto nel precedente esercizio che, cambiando base, cambiala matrice rappresentativa dell’endomorfismo. Ci chiediamo, in generale, come cambia tale matrice.Definizione: Sia Vuno spazio vettoriale di dimensione finita n.     Si considerino due basi di V: una base formata dai vettori e1 , e2 , … , enl’altra base formata dai vettori e1’, e2’, … , en’ La matrice Min M (n, n, R) la cui j-esima colonna è data dallecomponenti del vettore  ej’  rispetto alla base formata dai vettori    e1 , e2 , … , enè detta matrice di passaggio dalla base formata da e1 , e2 , … , enalla base formata da e1’, e2’, … , en’.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#12": "23/12/2013Cambiamento di baseEsercizio: Consideriamo di nuovo lo spazio vettoriale R3[x],     la sua base canonica: q1(x) := 1,  q2(x) := x,  q3(x) := x2e la base formata da p1(x) := 1 + x+ x2,  p2(x) := 1 + x,  p3(x) := 1.Per trovare la matrice Mdi passaggiodalla base canonica alla seconda base dobbiamo esprimere ciascuno dei p1(x), p2(x), p3(x)come combinazione linearedei q1(x), q2(x), q3(x). Abbiamo:p1(x) = 1q1(x) + 1q2(x) + 1q3(x)p2(x) = 1q1(x) + 1q2(x) + 0q3(x)p3(x) = 1q1(x) + 0q2(x) + 0 q3(x)\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#13": "23/12/2014Cambiamento di baseTeorema: La matrice di passaggio Mda una base di uno spazio vettoriale Va un’altra base è invertibile.Dimostrazione: Se Mè la matrice di passaggio dalla base formatadai vettori e1, e2, … , enalla base formata dai vettori e1’, e2’, …, en’, le colonne di Mdanno le componenti di e1’, e2’, …, en’ rispetto alla base formata dai vettori e1, e2, … , en. Sappiamo che il rango di Mè uguale alla dimensione dello spazio vettoriale generato dai vettori e1’, e2’, …, en’. Poiché i vettori e1’, e2’, …, en’ formano una base per Vabbiamo cherk M= dim V, ovvero det M≠0, ovvero Mè invertibile.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#14": "23/12/2015Cambiamento di baseTeorema: Date due basi di uno spazio vettoriale V sia Mla matrice di passaggio dalla prima base alla seconda base. La matrice di passaggio dalla seconda base alla prima base è allora M−1.Esempio: Consideriamo di nuovo lo spazio vettoriale R3[x],            la sua base canonica: q1(x) := 1,  q2(x) := x,  q3(x) := x2e la base formata da p1(x) := 1 + x+ x2,  p2(x) := 1 + x,  p3(x) := 1.La matrice Mdi passaggio dalla base canonica alla seconda base è:Domanda: Come si determina la matrice M’di passaggio dalla seconda base alla base canonica? 23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#15": "23/12/2016Cambiamento di baseEsempio (seguito): base canonica: q1(x) := 1,  q2(x) := x,  q3(x) := x2e base formata da p1(x) := 1 + x+ x2,  p2(x) := 1 + x,  p3(x) := 1.Domanda: Come si determina la matrice M’di passaggio dalla seconda base alla base canonica? Esprimiamo i polinomi della base canonica come combinazione lineare dei polinomi p1(x), p2(x), p3(x): •Poiché q1(x) = p3(x) allora q1(x) = 0p1(x) + 0p2(x) + 1p3(x). •Per decomporre q2(x) effettuiamo i seguenti passaggi:  x= h1(1 + x+ x2) + h2(1 + x) + h3(1) = (h1+h2+h3) + (h1+h2)x+ h1x2h1+h2+h3= 0, h1+h2= 1, h1= 0  da cui: h1= 0, h2= 1, h3= –1.•Per decomporre q3(x) effettuiamo i seguenti passaggi:  x2= h1(1 + x+ x2) + h2(1 + x) + h3(1) = (h1+h2+h3) + (h1+h2)x+ h1x2h1+h2+h3= 0, h1+h2= 0, h1= 1  da cui: h1= 1, h2= –1, h3= 0.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#16": "23/12/2017Cambiamento di baseEsprimiamo i polinomi della base canonica come combinazione lineare dei polinomi p1(x), p2(x), p3(x): •Poiché q1(x) = p3(x) allora q1(x) = 0p1(x) + 0p2(x) + 1p3(x). •Per decomporre q2(x) effettuiamo i seguenti passaggi:  x= h1(1 + x+ x2) + h2(1 + x) + h3(1) = (h1+h2+h3) + (h1+h2)x+ h1x2h1+h2+h3= 0, h1+h2= 1, h1= 0  da cui: h1= 0, h2= 1, h3= –1.•Per decomporre q3(x) effettuiamo i seguenti passaggi:  x2= h1(1 + x+ x2) + h2(1 + x) + h3(1) = (h1+h2+h3) + (h1+h2)x+ h1x2h1+h2+h3= 0, h1+h2= 0, h1= 1  da cui: h1= 1, h2= –1, h3= 0.\nAbbiamo M M’= I,cioè M’= M −1\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#17": "23/12/2018Cambiamento di baseTeorema: Sia fun endomorfismo di Vdi dimensione finita.            Sia Ala matrice rappresentativa di frispetto alla base formata dai vettori e1, e2, … , en. Sia A’ la matrice rappresentativa di frispetto alla base formata dai vettori e1’, e2’, …, en’. Sia Mla matrice di passaggio dalla base formata dai vettori e1, e2, … , enalla base formata dai vettori e1’, e2’, … , en’. Allora si ha: A’ = M−1 A MEsercizio: Prendiamo di nuovo l’endomorfismo f: R3 [x] →R3 [x] :f (a+ bx+ cx2) := (3a+ c) + (a+ b)x+ cx2. Calcolare A’ = M−1 A M.\nmatrice rappresentativa di frispetto alla base canonica\nmatrice rappresentativa di f  rispetto ad un’altra base\nmatrice dipassaggio23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#18": "23/12/2019Cambiamento di baseEsercizio: Sia  f: R2→R2l’endomorfismo che rispetto alla base canonica di R2[ovvero e1:= (1, 0),  e2:= (0, 1)] si rappresenta conla matrice V ogliamo determinare la matrice rappresentativa  A’di  frispetto alla base di R2formata dai vettori e1’ := (1, 2),  e2’ := (1, –1).Calcoliamo la matrice di passaggio dalla base canonica all’altra base:\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#19": "23/12/2020Cambiamento di baseEsercizio: Sia  f: R2→R2l’endomorfismo che rispetto alla base canonica di R2[ovvero e1:= (1, 0),  e2:= (0, 1)] si rappresenta conla matrice V ogliamo di nuovodeterminare la matrice rappresentativa  A’di  frispetto alla base di R2formata dai vettori e1’ := (1, 2),  e2’ := (1, –1).•Stavolta usiamo la definizione di matrice rappresentativa: \nLe componenti di e1’rispetto    alla base canonica sono (1, 2)Decomponiamo f (e1’)rispetto alla base formata   dai vettori e1’  e  e2’.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#2": "23/12/203Endomorfismo\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#20": "23/12/2021Cambiamento di baseEsercizio: Sia  f: R2→R2l’endomorfismo che rispetto alla base canonica di R2[ovvero e1:= (1, 0),  e2:= (0, 1)] si rappresenta conla matrice V ogliamo di nuovodeterminare la matrice rappresentativa  A’di  frispetto alla base di R2formata dai vettori e1’ := (1, 2),  e2’ := (1, –1).•Stavolta usiamo la definizione di matrice rappresentativa: Le componenti di e2’rispetto    alla base canonica sono (1, –1)Decomponiamo f (e2’)rispetto alla base formata   dai vettori e1’  e  e2’.\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#21": "23/12/2022Cambiamento di baseEsercizio: Sia  f: R2→R2l’endomorfismo che rispetto alla base canonica di R2[ovvero e1:= (1, 0),  e2:= (0, 1)] si rappresenta conla matrice V ogliamo di nuovodeterminare la matrice rappresentativa  A’di  frispetto alla base di R2formata dai vettori e1’ := (1, 2),  e2’ := (1, –1).•Stavolta usiamo la definizione di matrice rappresentativa: \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#22": "23/12/2023Cambiamento di baseIl teorema appena visto ci dice che se due matrici Ae Brappresentanolo stesso endomorfismo rispetto a basi diverse,allora esiste una matrice invertibile  Mtale che B = M−1A M .Definizione: Siano date due matrici Ae Bappartenenti a M (n, n, R).La matrice Bsi dice similealla matrice Ase e solo se esiste unamatrice Min GL(n, R) [insieme delle matrici invertibili di M (n, n, R)]tale che B= M−1 A M .Segue che le matrici rappresentative di uno stesso endomorfismo sono tutte simili fra loro.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#23": "23/12/2024EndomorfismiEsercizio: Sia fl’endomorfismo di R3definito dalle condizioni: \nDeterminare la matrice rappresentativa di  frispetto alla base formata da:\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#24": "23/12/2025EndomorfismiEsercizio: Sia fl’endomorfismo di R3definito dalle condizioni: \nDeterminare la matrice rappresentativa di  frispetto alla base canonica. \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#25": "23/12/2026EndomorfismiEsercizio: Sia fl’endomorfismo di R3definito dalle condizioni: \nDeterminare la matrice rappresentativa di  frispetto alla base canonica. \nPer evitare di calcolare M −1, possiamo (in alternativa) calcolare:\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#26": "23/12/2027EndomorfismiEsercizio: Sia  fl’endomorfismo di R4e Ala matrice rispetto alla base:\nDeterminare nucleo e immagine di  f\nGauss\nL’immaginedi  fè generata da :\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#27": "23/12/2028EndomorfismiEsercizio: Sia  fl’endomorfismo di R4e Ala matrice rispetto alla base:\nDeterminare nucleo e immagine di  f\nGaussIl nucleodi  fsi trova risolvendo il sistema omogeneo associato ad A :\nLe soluzioni sono (–2t, –t, –t, t)Per t= 1 si ha: (–2, –1, –1, 1) Una base del nucleo è: \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#3": "23/12/204EndomorfismiDefinizione: Sia f: V→V un endomorfismo di uno spazio vettoriale di dimensione finita. Fissata una base di V, formata dai vettori e1, e2, … , en , possiamo esprimere ciascun vettore f (ej) come combinazione linearedei vettori della base e1, e2, … , en : f (e1 ) = a11 e1+ a21 e2+...+ an1 enf (e2 ) = a12 e1+ a22 e2+...+ an2 en …  f (en ) = a1n e1+ a2n e2+…+ ann en La matrice A di M (n, n, R) è associata all’endomorfismo(ovvero è matrice rappresentativa di) f  rispetto alla base e1, e2, … , en .La j-esima colonna di Aè data dalle componenti del vettore f (ej) rispetto alla base formata da e1, e2, … , en . \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#4": "23/12/205EndomorfismiDefinizione: Sia Vuno spazio vettoriale di dimensione finita.        Sia data una base per Vformata dai vettori e1, e2, … , en .                Se Aè una matrice di M (n, n, R), chiamiamo endomorfismo associatoalla matrice Arispetto alle basi fissate l’omomorfismo f:f (e1) = a11 e1+ a21 e2+...+ an1 enf (e2) = a12 e1+ a22 e2+...+ an2 en …  f (en) = a1n e1+ a2n e2+…+ ann en Vale a dire l’endomorfismo  fla cui matrice rappresentativa rispetto alle basi assegnate è esattamentela matrice A.\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#5": "23/12/206EndomorfismiEsercizio: Consideriamo l'endomorfismo f: R3 [x] →R3 [x] :f (a+ bx+ cx2) := (3a+ c) + (a+ b) x+ c x2Se vogliamo ora determinare la matrice rappresentativa di  frispetto alla base canonica, dobbiamo determinare le immagini dei vettori della base canonica e decomporli rispetto alla base canonica stessa.  f (1)  =  3 + x=  31 + 1x+ 0x2f (x)  =         x= 01 + 1x+ 0x2 f (x2) = 1 + x2= 11 + 0x+ 1x2 La matrice rappresentativa di  frispetto alla base canonica è allora: \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#6": "23/12/207EndomorfismiEsercizio: Consideriamo l'endomorfismo f: R3 [x] →R3 [x] :f (a+ bx+ cx2) := (3a+ c) + (a+ b) x+ c x2Ora vogliamo rappresentare  frispetto a un’altra base, ovvero quella formata dai polinomi p1(x) := 1 + x+ x2,  p2(x) := 1 + x,  p3(x) := 1.Calcolando le immagini di tali vettori, si ha: f (p1(x)) = 4 + 2x+ x2.[ f ( p1(x) ) =  f( 1 + x+ x2 ) da cui a = b = c = 1 segue 4 + 2x+ x2 ] \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#7": "23/12/208EndomorfismiEsercizio: Consideriamo l'endomorfismo f: R3 [x] →R3 [x] :f (a+ bx+ cx2) := (3a+ c) + (a+ b) x+ c x2Ora vogliamo rappresentare  frispetto a un’altra base, ovvero quella formata dai polinomi p1(x) := 1 + x+ x2,  p2(x) := 1 + x,  p3(x) := 1.Calcolando le immagini di tali vettori, si ha: f (p1(x)) = 4 + 2x+ x2.Decomponiamo f (p1(x)) rispetto alla base: p1(x), p2(x), p3(x). f (p1(x)) = 4 + 2x+ x2  = 1(1 + x+ x2) + 1(1 + x) + 21Analogamente:  f (p2(x)) = 3 + 2x= 0(1 + x+ x2) + 2(1 + x) + 11f (p3(x)) =    3 + x= 0(1 + x+ x2) + 1(1 + x) + 21Dunque la matrice rappresentativa di  f  rispetto alla base formata dai polinomi p1(x), p2(x), p3(x) è  A’: \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#8": "23/12/209EndomorfismiEsercizio: Sia f: M(2, 2, R) →M(2, 2, R) l’endomorfismo:f (A) := A+ tA Determinare la matrice A rappresentativa di  frispetto alla base: \nA nonè una matrice diagonale23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\16_Endomorfismi.pdf#9": "23/12/2010EndomorfismiEsercizio: Sia f: M(2, 2, R) →M(2, 2, R) l’endomorfismo:f (A) := A+ tA Determinare la matrice A’rappresentativa di  frispetto alla base: \nA’ è una matrice diagonale23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it10",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#0": "23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it1L17: Autovalori e autovettori (31)Argomenti lezione:•Introduzione •Autovalori e autovettori •Polinomio caratteristico •Molteplicità di un autovalore •Autovalori e autovettori di matrici",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#1": "23/12/202IntroduzioneObiettivo: Individuare sotto quali specifiche condizioni un endomorfismo di uno spazio vettoriale si può rappresentare per mezzo di una matrice diagonale.Concetticheintrodurremooggi:•Autovalori e autovettori •Polinomio caratteristico •Molteplicità di un autovalore •Autovalori e autovettori di matrici23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#10": "23/12/2011Polinomio caratteristicoEsempio: Sia f: R4[x] →R4[x] l’endomorfismo che associa al polinomio a0+ a1 x+ a2 x2 + a3 x3il polinomio (a0+ a1) + (a1–a2+ a3) x+ (a0+ 2a1–a2) x2+ a3 x3 .Consideriamo la base canonica di R4[x] formata dai polinomi p1(x) := 1,   p2(x) := x,   p3(x) := x2,   p4(x) := x3. Rispetto a tale base l’endomorfismo fsi rappresenta con la matrice:\nV ogliamo ora determinare polinomi non nulli del tipo p(x) := a0+ a1 x+ a2 x2 + a3 x3tali che f (p(x)) = λp(x) per qualche λÎR.a0a1a2a323/12/20Geometria e Combinatoria marcella.sama@uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#11": "23/12/2012Polinomio caratteristicoEsempio (seguito): V ogliamo determinare polinomi non nulli del tipop(x) := a0+a1 x +a2 x2 +a3 x3 tali che f (p(x)) = λp(x) per qualche λÎR.=> p(x) è un autovettorese e solo se p(x) ≠0 ed esiste λÎRtale che:\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#12": "23/12/2013Polinomio caratteristicoEsempio (seguito): V ogliamo determinare polinomi non nulli del tipop(x) := a0+a1 x +a2 x2 +a3 x3 tali che f (p(x)) = λp(x) per qualche λÎR.=> p(x) è un autovettore se e solo se p(x) ≠0 ed esiste λÎRtale che:\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#13": "23/12/2014Polinomio caratteristicoEsempio (seguito): V ogliamo determinare polinomi non nulli del tipop(x) := a0+a1 x +a2 x2 +a3 x3 tali che f (p(x)) = λp(x) per qualche λÎR.=> p(x) è un autovettore se e solo se p(x) ≠0 ed esiste λÎRtale che:\nλ= x23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#14": "23/12/2015Polinomio caratteristicoEsempio (seguito): V ogliamo determinare polinomi non nulli del tipop(x) := a0+a1 x +a2 x2 +a3 x3 tali che f (p(x)) = λp(x) per qualche λÎR.=> p(x) è un autovettore se e solo se p(x) ≠0 ed esiste λÎRtale che:\nλ= xOsservazioni:•Il sistema, essendo omogeneo, è sempre risolubile.•det(A−λI) ≠ 0: il sistema (Crameriano) ha solola soluz. banale.•Stiamo cercando autovettori, ovvero vettori non nulli. Dunque ci interessano soluzioni non banalidi questo sistema omogeneo.•det(A−λI) = 0: rkAè minore del numero delle incognite e, quindi, esistono infinite soluzioni(incluse quelle non banali). 23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#15": "23/12/2016Polinomio caratteristicoEsempio (seguito): V ogliamo determinare polinomi non nulli del tipop(x) := a0+a1 x +a2 x2 +a3 x3 tali che f (p(x)) = λp(x) per qualche λÎR.=> p(x) è un autovettore se e solo se p(x) ≠0 ed esiste λÎRtale che:\nλ= xOsservazioni:•det(A−λI) = 0: rkAè minore del numero delle incognite e, quindi, esistono infinite soluzioni(incluse quelle non banali).•se λ= 1 si ha det(A−1I) = 0, il sistema ha soluzioni non banali, cioè 1 è autovaloredi  f•se λ= 2 si ha det(A−2I) ≠ 0, il sistema ha solamente la soluzione banale, cioè 2 non è autovaloredi  f23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#16": "23/12/2017Polinomio caratteristicoTeorema: Sia f: V→Vun endomorfismo di uno spazio vettoriale Vdi dimensione finita. Sia Ala matrice rappresentativa di  frispetto  a una base di V. Sia A’la matrice rappresentativa di  frispetto aun’altra base di V.  Allora si ha: det (A’ − xI ) = det (A− xI ). [Il teorema ci dice che gli autovalori di  fnondipendono A !cioè dalla base di Vscelta per rappresentare  f . ]\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#17": "23/12/2018Polinomio caratteristicoTeorema: Sia f: V→Vun endomorfismo di uno spazio vettoriale Vdi dimensione finita. Sia Ala matrice rappresentativa di  frispetto  a una base di V. Sia A’la matrice rappresentativa di  frispetto aun’altra base di V.  Allora si ha: det (A’ − xI ) = det (A− xI ). Dimostrazione:Sappiamo che esiste una matrice invertibile Mtale che A’= M –1AMdet (A’ − xI) = det (M –1AM− xI)                                       [A’= M –1AM]= det(M –1AM− M –1(xI ) M)                         [I= M –1I M]= det(M –1(A− xI) M)                            [prop. delle matrici]= detM –1det (A− xI) detM               [det (A B) = detA detB]= det (A− xI )                                      [det M –1= (det M )–1]23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#18": "23/12/2019Polinomio caratteristicoDefinizione: Sia f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita ne sia Ala matrice rappresentativa di frispettoa una base di V. Chiamiamo polinomio caratteristico di fil polinomio pf (x):= det (A−xI ) di grado nnell’incognita x.Teorema: Sia f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita. Allora gli autovaloridi fsono le radicidelpolinomio caratteristicodi f. Teorema: Sia f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita n. Sia Ala matrice rappresentativa di  frispetto a una base di V.  Se λè un autovaloredi  f, allora si ha:dim E(λ) = n−rk (A−λ I )23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#19": "23/12/2020Polinomio caratteristicoEsempio (seguito): Determiniamo autovalori e relativi autospazi.  Il polinomio caratteristico pf (x) := det (A−xI )è: \nAd esempio, sviluppiamo questo determinante rispetto all’ultima riga:\nCalcoliamo le radici delpolinomio caratt.  pf (x)  \nCalcoliamo il discriminante (∆ = b2 –4 a c) di  x2–x+ 1 : a= 1; b= –1; c= 1  segue  ∆ =1 –4 < 0  quindi  no radici realiLe radici di questo polinomio (ovvero gli autovalori di f ) sono 0 e 1. 23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#2": "23/12/203Autovalori e Autovettori\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#20": "23/12/2021Polinomio caratteristicoEsempio (seguito): Determiniamo autovalori e relativi autospazi.  Il polinomio caratteristico pf (x) := det (A−xI )è: \nLe radici di pf (x) (gli autovalori di f ) sono 0 e 1. \nL’autospazio relativo all’autovalore 1 è :\nQuindi  2 + x2+ x3costituisce una base di E(1)23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#21": "23/12/2022Polinomio caratteristicoEsempio (seguito): Determiniamo autovalori e relativi autospazi.  Il polinomio caratteristico pf (x) := det (A−xI )è: \nLe radici di pf (x) (gli autovalori di f ) sono 0e 1. L’autospazio relativo all’autovalore 0 è :Quindi  –1 + x+ x2costituisce una base di E(0)\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#22": "23/12/2023Polinomio caratteristicoProcedura: Per determinare l’autospazio E(λ) occorre innanzituttorisolvere il sistema lineare omogeneo associato alla matrice A− λ I.Poi, bisogna prendere i vettori che hanno queste soluzioni comecomponenti rispetto alla base considerata.Osservazioni pratiche: Per definizione, un autospazio E(λ) contiene vettori diversi dalvettore nullo. Se risolvendo il sistema omogeneo necessario alladeterminazione di E(λ) troviamo solo la soluzione banale, alloraquesto significa che abbiamo sbagliato a risolvere il sistemaoppureche il valore λnon è un autovalore(e quindi abbiamo sbagliato acalcolare il polinomio caratteristico o a determinare le sue radici). 23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#23": "23/12/2024Molteplicità di un autovaloreTeorema: Sia p(x) un polinomio nell’incognita x. Il numero reale aè radicedel polinomio p(x)  se e solo se   x− a   dividep(x).Definizione: Sia auna radice di un polinomio p(x). Allora a è radice di p(x) con molteplicitàmse   p(x) = (x− a) m  k(x)  con k(a) ≠ 0. In altri termini, aha molteplicità mse: (x− a) mdividep(x),  mentre (x− a)m+1non dividep(x). Esempio: \nLa radice 1 ha molteplicità 2.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#24": "23/12/2025Molteplicità di un autovaloreTeorema: Se un polinomio p(x) di grado nha le radici distinte a1, a2, …, ardi molteplicità rispettive  m1, m2, …, mr allora  m1+ m2+ … + mr≤ nEsempio (seguito): \nCerchiamo i divisori di 2 x3+ 6 x2+ 6 x + 4 4/2 = 2 quindi i possibili divisori sono: 1, –1, 2, –2Notiamo che 2 x3+ 6 x2+ 6 x + 4si annulla per x= –22 x3+ 6 x2+ 6 x + 4      x + 2–2 x3–4 x2                              2 x2+ 2 x + 22 x2+ 6 x + 4–2 x2–4 x2 x + 4–2 x  –42     6      6       42     22       0oppure tramite Ruffini :–2          –4   –4    –42 x2+ 2 x + 223/12/20Geometria e Combinatoria marcella.sama@uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#25": "23/12/2026Molteplicità di un autovaloreTeorema: Se un polinomio p(x) di grado nha le radici distinte a1, a2, …, ardi molteplicità rispettive  m1, m2, …, mr allora  m1+ m2+ … + mr≤ nEsempio (seguito): \nCalcoliamo il discriminante (∆ = b2 –4 a c) di  2x2+ 2x+ 2 : a= 2; b= 2; c= 2  segue  ∆ =8 –16 < 0  quindi  no radici realip(x) ha la radice 1 di molteplicità 2 e la radice –2 di molteplicità 1La somma delle molteplicità è 3 che è inferiore al grado di p(x)\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#26": "23/12/2027Molteplicità di un autovaloreDefinizione: Un polinomio p(x) di grado nsi dice totalmente riducibilese si può scrivere come prodotto di polinomi di I grado.Teorema: Sia p(x) un polinomio di grado ne siano a1, a2, …, arle radici distinte di p(x) aventi molteplicità rispettive m1, m2, …, mr .Il polinomio p(x) è totalmente riducibilese e solo sem1+ m2+ … + mr= n . Definizione: Sia f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita ne sia λun autovalore di f. Definiamo mf(λ) = mse λha molteplicità mcome radice del polinomio caratteristico di  f.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#27": "23/12/2028Molteplicità di un autovaloreDefinizione: Sia f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita ne sia λun autovalore di f. Definiamo mf(λ) = mse λha molteplicità mcome radice del polinomio caratteristico di  f.Teorema: Se λ1, λ2, …, λrsono gli autovalori distinti di unendomorfismo fdi uno spazio vettoriale di dimensione n, si ha:  mf  (λ1) + mf(λ2) + ... + mf  (λr)  ≤ n . In particolare  fha al piùnautovalori distinti.Il polinomio caratteristico di  fè totalmente riducibile se e solo semf (λ1) + mf  (λ2) + ... + mf  (λr) = n.  23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#28": "23/12/2029Molteplicità di un autovaloreTeorema: Sia f un endomorfismo di uno spazio vettoriale Vdi dimensione finita ne sia λun autovalore di f. Si ha:  1  ≤  dim E(λ)  ≤  mf(λ). \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it29",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#29": "23/12/2030Molteplicità di un autovaloreEsempio: Consideriamo l’endomorfismo di R5la cui matrice rappresentativa rispetto alla base canonica è la matrice A: \nV ogliamo determinare gli autovalori di fe le dimensioni dei relativi autospazi.\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it30",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#3": "23/12/204Autovalori e autovettoriEsempio: Sia  fl’endomorfismo di R2[x] definito daf (a+ bx) := 2b+ (–a+ 3b) xRispetto alla base canonica (p1(x) := 1 e p2(x) := x) di R2[x] questoendomorfismo si rappresenta con la matrice A: Se però consideriamo la base di R2[x] formata dai due polinomi   p1(x) := 1 + xe  p2(x) := 2 + x, vediamo che si ha: f (p1(x)) =  2 + 2x= 2 p1(x) + 0 p2(x) f (p2(x)) =  2 +   x= 0 p1(x) + 1 p2(x) La matrice rappresentativa di  frispetto alla base formata da p1(x)   e p2(x) è la matrice diagonaleA’:f (p1(x)) e f (p2(x)) sono multiplidi p1(x) e p2(x)!\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#30": "23/12/2031Molteplicità di un autovaloreEsempio: Consideriamo l’endomorfismo di R5la cui matrice rappresentativa rispetto alla base canonica è la matrice A: \nV ogliamo determinare gli autovalori di fe le dimensioni dei relativi autospazi.\nAutovalori: 3 di molteplicità 1, 0 di molteplicità 2, 2 di molteplicità 21 ≤dim E(3)  ≤  mf (3) = 1, pertanto dim E(3) = 1 ; 1≤  dim E(0)  ≤  mf(0) = 2\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it31",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#31": "23/12/2032Molteplicità di un autovaloreEsempio: Consideriamo l’endomorfismo di R5la cui matrice rappresentativa rispetto alla base canonica è la matrice A: \nV ogliamo determinare gli autovalori di fe le dimensioni dei relativi autospazi.\nAutovalori: 3 di molteplicità 1, 0 di molteplicità 2, 2 di molteplicità 21 ≤dim E(3)  ≤  mf (3) = 1, pertanto dim E(3) = 1 ; 1 ≤  dim E(0)  ≤  mf(0) = 2  ;   1≤  dim E(2)  ≤  mf(2) = 2\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it32",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#32": "23/12/2033Molteplicità di un autovaloreIl calcolo degli autovalori con le rispettive molteplicità può esserein alcuni casi semplificato: Teorema: Sia f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita. Supponiamo che esista una base per Vrispetto a cui  fsi rappresenta con una matrice triangolareA. Gli autovalori di  fsono gli elementi lungo la diagonale principaledi A: la molteplicità algebrica di ciascuno di essi è uguale alnumero di volteche compare sulla diagonale principale di A. [Dato che (A− xI)è una matrice triangolare: il suo determinante è dato dal prodotto degli elementi lungo la sua diagonale principale.] 23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it33",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#33": "23/12/2034Molteplicità di un autovaloreIl calcolo degli autovalori con le rispettive molteplicità può esserein alcuni casi semplificato: Teorema: Sia f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita. Supponiamo che esista una base per Vrispetto a cui  fsi rappresenta con una matrice triangolareA. Gli autovalori di  fsono gli elementi lungo la diagonale principaledi A: la molteplicità algebrica di ciascuno di essi è uguale alnumero di volteche compare sulla diagonale principale di A. Osservazione: Se λ è un autovalore di molteplicità 1 di unendomorfismo  f allora  dim E(λ) = 1. Un autovalore di molteplicità 1 viene detto semplice.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it34",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#34": "23/12/2035Autovalori e autovettori di matrici\nEsempio: Sia  fl’endomorfismo di R2[x] definito daf (a+ bx) := 2b+ (–a + 3b) xRispetto alla base canonica di R2[x] questoendomorfismo si rappresenta con la matrice p1(x) := 1 + xe  p2(x) := 2 + xsono autovettoridi  f.  Infatti: \n=\n=\nIl secondo vettore è due volte il primoIl secondo vettore è una volta il primo23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it35",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#35": "23/12/2036Autovalori e autovettori di matrici\nEsempio: Sia  fl’endomorfismo di R2[x] definito daf (a+ bx) := 2b+ (–a + 3b) xRispetto alla base canonica di R2[x] questoendomorfismo si rappresenta con la matrice p1(x) := 1 + xe  p2(x) := 2 + xsono autovettoridi  f.  Infatti: \n=\n=\nRicordiamo che la matrice Aè diagonalizzabilese Aè similea una matrice diagonale (cioè se esiste Minvertibile taleche  M –1A Mè diagonale)23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#36": "23/12/2037Autovalori e autovettori di matriciGeneralizziamo le proprietà:•Un autovettoredi Acon autovaloreλè un vettore colonna vnon nullo tale che  A v= λ v. •L’autospazioE(λ) è l’insieme dei vettori colonna vtali che Av= λv. •Il polinomio caratteristico di Aè  pA (x) := det (A− xI ). •La molteplicitàmA (λ) di un autovalore λè la sua molteplicità come radice del polinomio caratteristico di A.•Per ogni autovalore λdi una matrice Asi ha 1 ≤ dim E(λ) ≤ mA (λ).•Un autovalore λ èdetto semplicese la sua molteplicità è uguale a 1: la dimensione dell’autospazio relativo a un autovalore semplice è 1.•Gli autovalori di una matrice triangolareAsono gli elementi lungo la diagonale principale: la molteplicità algebrica di ciascuno di essi è pari al numero di volteche compare sulla diagonale principale di A.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it37",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#37": "23/12/2038Autovalori e autovettori di matriciEsercizio: Siano dati la matrice Ae il vettore v : \nDeterminare per quali valori di kil vettore vè autovettore di A, e relativamente a quale autovalore.\nIl vettore wè multiplo di vse e solo se 2 + 2k= 5, ovvero se k= 3/2.Dunque vè autovettoredi Ase e solo se k= 3/2.In tal caso vè autovettore relativamente all’autovalore5. \n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it38",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#4": "23/12/205Autovalori e autovettoriDefinizione: Un endomorfismo f: V→Vdi uno spazio vettoriale Vdi dimensione finita si dice diagonalizzabilese esiste (almeno) unabase di Vrispetto a cui  f  si rappresenta con una matrice diagonale.Teorema: Sia  fun endomorfismo di uno spazio vettoriale Vdidimensione finita. Data una base di Vformata da e1, e2, …, en,    la matrice rappresentativa di  frispetto a tale base è diagonalese e solo se f (ei ) è un multiplodi  eiper 1 ≤ i≤ n, ovvero se e solose esistono scalari λ1, λ2, …, λntali che  f (ei ) = λieiper 1 ≤ i≤ n. \nMatrice rappresentativadi  frispetto alla base formata dae1, e2, …, en 23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#5": "23/12/206Autovalori e autovettoriDefinizione: Sia f: V→Vun endomorfismo di uno spazio vett. V,v≠ 0 si dice autovettoredi fcon autovaloreλse si ha:  f (v) = λv.Quesito: Lo stesso vettore v≠ 0 può essere un autovettore di  frispetto a due autovalori diversi?No! Infatti: sia vautovettore di  fsia rispetto a due autovalori λe  μ.V ogliamo mostrare che λ= μ. Sappiamo che: f (v) = λ v;  f (v) = μv.Allora λ v= μv, cioè (λ −μ) v= 0. Poiché v≠ 0 abbiamo λ −μ= 0.Osservazione: Nella definizione abbiamo richiesto che v≠ 0. Infatti se v= 0 si ha  f (0) = 0 per qualunque numero reale λ .Dunque se nella definizione non avessimo richiesto v≠ 0,      ogni numero reale λsarebbe autovalore di  f!  23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#6": "23/12/207Autovalori e autovettoriEsercizio: Sia dato l’endomorfismo di R3definito daf (x, y, z) := (2x+ 2y+ z,  y,  0)Stabilire per ciascuno dei seguenti vettori se è un autovettore di f.In caso affermativo determinare l’autovalore corrispondente.v1:= (1, 0, 0)   =>  f (v1) = f (1, 0, 0) = (2, 0, 0) OK  =>  f (v1) = 2 v1v2:= (0, 1, 0)   =>  f (v2) = f (0, 1, 0) = (2, 1, 0) NO multiplo di v2v3:= (1, 0, –2) =>  f (v3) = f (1, 0, –2) = (0, 0, 0) OK => f (v3) = 0 v3v4:= (0, 0, 0)   =>  nonè un autovettore perchè, per definizione, un autovettore è diverso dal vettore nullo.23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#7": "23/12/208Autovalori e autovettoriTeorema: Sia f: V→Vun endomorfismo di uno spazio vettor. Ve sia λun autovalore di f.  L’insieme E(λ):= {vÎV|  f (v) = λ v}è un sottospaziovettoriale di V , detto autospaziodi  frelativo aλ. Def.: E(λ) è formato dagli autovettoridi  frelativi all’autovalore λe dal vettore nullo.  [Notiamo che, per definizione di autovettore v≠ 0, un autospaziononpuò mai essere costituito dal solo vettore nullo.]\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#8": "23/12/209Autovalori e autovettoriTeorema: Sia f: V→Vun endomorfismo di uno spazio vettor. Ve sia λun autovalore di f.  L’insieme E(λ):= {vÎV|  f (v) = λ v}è un sottospaziovettoriale di V , detto autospaziodi  frelativo aλ. Dimostrazione: E(λ) è non vuoto perché contiene il vettore nullo.Se v1e v2sono vettori di E(λ) allora anche la loro somma Îa E(λ)? Sappiamo che f (v1) = λv1 e  f (v2) = λv2e  dobbiamo mostrare che f (v1+ v2) = λ(v1+ v2).  Infatti:   f (v1+ v2) = f (v1) + f (v2) = λv1+ λv2 = λ(v1+ v2).Inoltre, dobbiamo mostrare che se vè un vettore di E(λ) mentre kèuno scalare, allora anche il prodotto k vappartiene a E(λ). Sappiamo che f (v) = λ v .Dobbiamo mostrare che  f (k v) = λ(k v).Infatti:   f (k v) = k f (v) = k λ v= λ (k v).   23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\17_Autovalori_Autovettori.pdf#9": "23/12/2010Polinomio caratteristico\n23/12/20Geometria e Combinatoria marcella.sama@uniroma3.it10",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#0": "23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it1L18: Diagonalizzazione (32)Argomenti lezione:•Introduzione •Diagonalizzazione",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#1": "23/12/202IntroduzioneObiettivo: Determiniamo un criterioper stabilire se unendomorfismo (o una matrice) è diagonalizzabile. In caso affermativo, descriviamo un procedimentoper trovare una base formata da autovettori dell’endomorfismo (o della matrice).Teorema: Un endomorfismo  fdi uno spazio vett. Vdi dim. finita è diagonalizzabilese e solo se esiste una base di Vformata daautovettori di  f .Osservazione: Questo teorema non ci dice come stabilire se esiste una base formata da autovettori, ne tantomeno come determinare esplicitamente una tale base (ammesso che esista).23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#10": "23/12/2011DiagonalizzazioneEsercizio: Consideriamo l’endomorfismo di R4definito da: \nLa matrice rappresentativa di  f  rispetto alla base canonica è : \nAutovalori: 0 di molt. 2,     3+√2 di molt. 1,     3–√2 di molt. 1Verifichiamo per l’autovalore 0 se la sua molteplicità coincide con la dimensione del relativo autospazio:\nfnonè diagonalizzabile !23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#11": "23/12/2012DiagonalizzazioneEsercizio: Sia dato l’endomorfismo  f: M(2, 2, R) →M(2, 2, R) :\nLa matrice rappresentativa Adi  frispetto alla base canonicaè : \n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#12": "23/12/2013DiagonalizzazioneEsercizio: Sia dato l’endomorfismo  f: M(2, 2, R) →M(2, 2, R) :\nCalcoliamo il discriminante (∆ = b2 –4 a c) di  x2–2x–3 : a= 1; b= –2; c= –3 segue  ∆ = 4 + 12 > 0 calcoliamo le radici:x= {–b+/–[radice quadrata ∆] } / 2 a=  1 +/–2 Quindi il polinomio caratteristico di  fè totalmente riducibile.Autovalori:  0 di molt. 2,   –1 di molt. 1,    3 di molt. 1\nfè diagonalizzabile ! f  si rappresenta con una matrice diagonale rispetto a una base23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#13": "23/12/2014DiagonalizzazioneMetodo per individuare se un endomorfismo è diagonalizzabile:Sia  f: V→Vun endomorfismo di uno spazio vett. Vdi dim. finita n.1. Scegliere e1, e2, ... , envettori che formano una basedi V. 2. Calcolare la matrice Arappresentativadi frispetto alla base scelta.3. Definire il polinomio(di grado n) caratt.di  f :pf(x) := det(A–xI ). 4. Risolvere det(A–xI ) = 0. Le soluzioni sono gli autovaloridi  f .    Se pf(x) nonè totalmente riducibile,  fnon èdiagonalizzabile, STOP.5. Per ciascun autovalore (di molteplicità > 1) verificare se la sua molteplicità e la dimensione del suo autospazio coincidono. 6. Se esiste almeno un autovalore λiper cui si ha  dim E(λi ) < m(λi ) allora fnon èdiagonalizzabile, STOP. 7. Se per tutti gli autovalori λisi ha  dim E(λi) = m(λi)                   allora  fèdiagonalizzabile, STOP. 23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#14": "23/12/2015DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto a una base :\nOgni autovalore è riportato lungo la diagonale principale un numero divolte uguale alla sua molteplicità.Calcoliamo una base rispetto a cui  fsi rappresenta con la matrice D:[ Dobbiamo trovare una base per ciascun autospazio e unirle. ][ Tale base deve essere formata da autovettori di  f . ] \n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#15": "23/12/2016DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto ad una base\nE(0):Calcoliamo una base rispetto a cui  fsi rappresenta con la matrice D:\n2 equazionilin. indipendenti\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#16": "23/12/2017DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto ad una base\nBase per E(0): E(0):Calcoliamo una base rispetto a cui  fsi rappresenta con la matrice D:\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#17": "23/12/2018DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto ad una base\nE(–1):\nCalcoliamo una base rispetto a cui  fsi rappresenta con la matrice D:rk (A–(–1) I ) = 33 equazionilin. indipendenti23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#18": "23/12/2019DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto ad una base\nBase per E(–1):E(–1):\nE(–1) = {–tE11 + tE12 | tÎR}\nCalcoliamo una base rispetto a cui  fsi rappresenta con la matrice D:\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#19": "23/12/2020DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto ad una base\nE(3):Base per E(3): Calcoliamo una base rispetto a cui  fsi rappresenta con la matrice D:\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#2": "23/12/203IntroduzioneEsempio: Sia f: R4[x] →R4[x] l’endomorfismo che associa al polinomio a0+ a1 x+ a2 x2 + a3 x3il polinomio (a0+ a1) + (a1–a2+ a3) x+ (a0+ 2a1–a2) x2+ a3 x3 .Consideriamo la base canonica di R4[x] formata dai polinomi p1(x) := 1,   p2(x) := x,   p3(x) := x2,   p4(x) := x3. Abbiamo calcolato le radici del pol. caratteristico pf (x) := det(A−xI)ovvero gli autovalori di fsono 0 e 1. Abbiamo mostrato che entrambi gli autospazi hanno dim. pari a 1.Domanda:fè diagonalizzabile ? Cioè esiste una base di R4[x] formata di autovettori di  f? Dato che dim E(0) = 1 e dim E(1) = 1, segue che i 4 polinomi p1(x), p2(x), p3(x),p4(x) nonpossono essere tutti autovettori. Dunque,  fnonè diagonalizzabile.   23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#20": "23/12/2021DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto ad una base\nBase per E(0): \nBase per E(–1):\nBase per E(3): Calcoliamo una base rispetto a cui  fsi rappresenta con la matrice D:;\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#21": "23/12/2022DiagonalizzazioneEsercizio(seguito):\nf  si rappresenta con la matrice diagonale D rispetto ad una base\nAbbiamo trovato una base  di  M(2, 2, R) formata da autovettori di  f  :\nSi può verificare che: \nLa matrice di passaggio Mdalla base canonica a questa base è : E(0):E(–1):E(3):\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#22": "23/12/2023DiagonalizzazioneEsercizio(seguito):\nAttenzione:f si rappresenta anche con la matrice diagonale DAbbiamo già trovato una base  di  M(2, 2, R) formata da autovettori di  f  :Si può verificare che: \nLa matrice di passaggio Ndalla base canonica a questa base è : \nE(0):E(–1):E(3):\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#23": "23/12/2024DiagonalizzazioneMetodo per la ricerca di una base di autovettori di un endomorfismo:Premessa: Sappiamo già che l’endomorfismo  fè diagonalizzabile. 1. Rispetto a una base opportuna di V,  fsi rappresenta con una matrice diagonaleD :  gli elementi lungo la diagonale principale sono gli autovaloridi  f, ciascuno riportato un numero di volte pari alla propria molteplicità (uguale alla dimensione dell’autospazio).2. Determinare una base per ciascun autospazioE(λi ): risolvere il sistema lineare: (A− λi I) X= 0. Il numero di vettori della base di Eideve coincidere con la dimensione del relativo autospazio. 3. Unire le basi di tutti gli autospazi per avere una base di Vformata da autovettoridi  f. 4. La matrice Mdi passaggio(da Aa D ) dalla base di partenza allabase di autovettori soddisfa la relazione:  D= M−1 A M  23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#24": "23/12/2025DiagonalizzazioneMetodo per la ricerca di una base di autovettori di un endomorfismo:Premessa: Sappiamo già che l’endomorfismo  fè diagonalizzabile. 4. La matrice Mdi passaggio(da Aa D ) dalla base di partenza allabase di autovettori soddisfa la relazione:  D= M−1 A M  Osservazioni sulle matrici Me D: •Mè la matrice le cui colonne danno le componenti dei vettori della base di autovettori rispetto alla base di partenza.•L’ordine in cui mettiamo gli autovalori lungo la diagonale di De l’ordine in cui scriviamo le colonne di Mdevono essere coerenti: se il k-esimo elemento lungo la diagonale di Dè un autovalore λi , allora la k-esima colonna di Mdeve dare le componenti di un autovettore relativo allo stesso autovaloreλi .23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#25": "23/12/2026DiagonalizzazioneEsercizio: Stabilire se la matrice se Aè diagonalizzabile :\nCalcoliamo il discriminante (∆ = b2 –4 a c) di  x2–x+1/4 : a= 1; b= –1; c= 1/4  segue ∆ = 1 –1 = 0  calcoliamo le radici:x= {–b+/–[radice quadrata ∆] } / 2 a=  1/2 pA(x) è totalmente riducibile, ovvero pA(x) = –x(x –1/2)2. Autovalori di A: 0 di molteplicità 1 e 1/2 di molteplicità 2.La dimensione dell’autospazio relativo a 0 è necessariamente 1. La dimensione dell’autospazio relativo a 1/2 è : 1 oppure 2 ? \n=  –x(x2–x+1/4 )\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#26": "23/12/2027DiagonalizzazioneEsercizio: Stabilire se la matrice se Aè diagonalizzabile :\nLa dimensione dell’autospazio relativo a 0 è necessariamente 1. La dimensione dell’autospazio relativo a 1/2 è : 1 oppure 2 ? \nSegue la matrice Aè diagonalizzabile ed è simile alla matrice D :\nDeterminiamo ora una matrice di passaggio da  Aa  D .23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#27": "23/12/2028DiagonalizzazioneEsercizio: Stabilire se la matrice se Aè diagonalizzabile :\nDeterminiamo ora una matrice di passaggio da  Aa  D .Per determinare E(0), risolviamo il sistema ( A− 0I ) X= 0Poichè 0 è autovalore semplice, la dim. dell’autospazio è 1 .Servono quindi 3 − 1 = 2 equazioni lin. indipendenti di A− 0I . (− t, t, 0) al variare di tin RUna base per E(0) è (− 1, 1, 0)23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#28": "23/12/2029DiagonalizzazioneEsercizio: Stabilire se la matrice se Aè diagonalizzabile :\nDeterminiamo ora una matrice di passaggio da  Aa  D .Per determinare E(1/2), risolviamo il sistema ( A− 1/2 I ) X= 0Sappiamo che l’autospazio è di dim. 2.   rk ( A− 1/2 I )= 1 .Serve quindi 3 − 2 = 1 equazione lin. indipendente di  A− 1/2 I . (− 3/2t + 1/2u, t, u) al variare di te uin R\nUna base per E(1/2)  è  (− 3/2, 1, 0)  e  (1/2, 0, 1) 23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it29",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#29": "23/12/2030DiagonalizzazioneEsercizio: Stabilire se la matrice se Aè diagonalizzabile :\nDeterminiamo ora una matrice di passaggio da  Aa  D .Una base per E(1/2) è  (− 3/2, 1, 0)  e  (1/2, 0, 1) Una base per E(0) è (− 1, 1, 0)Segue una base di R3formata da autovettori di Aè : \nE(0) E(1/2) 23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it30",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#3": "23/12/204DiagonalizzazioneTeorema: Sia  f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita ne siano 1, 2, …, sgli autovalori distinti di f . Se dim E(1) + dim E(2) + ... + dim E(s) < nallora  fnon è diagonalizzabile. In altre parole: affinchè  fsia diagonalizzabile è necessarioche:dim E(1) + dim E(2) + ... + dim E(s) = n Teorema: Sia  f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita ne siano 1, 2, …, sgli autovalori distinti di f . Prendiamo una base per ciascun autospazio: unendo tali basi siottengono dei vettori tra loro linearmente indipendenti.    Dunque, abbiamo degli autovettori linearmente indipendenti. In particolare:  dim E(1) + dim E(2) + … + dim E(s) ≤ n .23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#30": "23/12/2031DiagonalizzazioneEsercizio: Stabilire se il seguente  f: R3→R3è diagonalizzabile : In tal caso, trovare una base di R3formata da autovettori di  fe scrivere la matrice rappresentativa Adi  frispetto a tale base.\nrispetto alla base canonica:(1,0,0), (0,1,0), (0,0,1)\nPolinomio caratter. di fè totalmente riducibile. Autovaloridi f: 0 e 1.mf (0) = 2 ;  mf (1) = 1 \ndim E(0) < mf (0)f  nonè diagonalizzabile !23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it31",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#31": "23/12/2032DiagonalizzazioneEsercizio: Stabilire se la matrice Aa coeff. reali è diagonalizzabile :\nIn tal caso, determinare una matrice diagonale Dsimile ad Ae una matrice invertibile Mtale che D= M −1 A M \nsviluppo rispetto alla quarta riga\nsviluppo rispetto alla seconda riga\nIl polinomio caratteristico di Aè totalmente riducibile. Autovalori: 1 di molteplicità 2;  0 e 2 entrambi di molteplicità 1.23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it32",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#32": "23/12/2033DiagonalizzazioneEsercizio: Stabilire se la matrice Aa coeff. reali è diagonalizzabile :\nIn tal caso, determinare una matrice diagonale Dsimile ad Ae una matrice invertibile Mtale che D= M −1 A M Autovalori: 1 di molteplicità 2;  0 e 2 entrambi di molteplicità 1.\nSegue che Aè diagonalizzabile ed è simile alla matrice diagonale:Determiniamo una matrice di passaggio dalla matrice Aalla matrice D.23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it33",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#33": "23/12/2034DiagonalizzazioneEsercizio: Stabilire se la matrice Aa coeff. reali è diagonalizzabile :\nIn tal caso, determinare una matrice diagonale Dsimile ad Ae una matrice invertibile Mtale che D= M −1 A M \nDeterminiamo una matrice di passaggio dalla matrice Aalla matrice D.E(1):\nrk(A–1I ) = 2\n2 righe linear. indipendentidiA–1Ile cui soluzioni sono (0, t, 0, u) al variare di te uin Runa base per E(1) è :  v1:= (0, 1, 0, 0)  ,  v2:= (0, 0, 0, 1)23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it34",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#34": "23/12/2035DiagonalizzazioneEsercizio: Stabilire se la matrice Aa coeff. reali è diagonalizzabile :\nIn tal caso, determinare una matrice diagonale Dsimile ad Ae una matrice invertibile Mtale che D= M −1 A M \nDeterminiamo una matrice di passaggio dalla matrice Aalla matrice D.E(0):rk(A–0I ) = 33 righe linear. indip. di A–0Ile cui soluzioni sono (–t, 0, t, 0) al variare di tin Runa base per E(0) è :  v3:= (–1, 0, 1, 0)\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it35",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#35": "23/12/2036DiagonalizzazioneEsercizio: Stabilire se la matrice Aa coeff. reali è diagonalizzabile :\nIn tal caso, determinare una matrice diagonale Dsimile ad Ae una matrice invertibile Mtale che D= M −1 A M \nDeterminiamo una matrice di passaggio dalla matrice Aalla matrice D.E(2):rk(A–2I ) = 33 righe linear. indip. di A–2Ile cui soluzioni sono (t, 0, t, 0) al variare di tin Runa base per E(2) è :  v4:= (1, 0, 1, 0)\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#36": "23/12/2037DiagonalizzazioneEsercizio: Stabilire se la matrice Aa coeff. reali è diagonalizzabile :\nIn tal caso, determinare una matrice diagonale Dsimile ad Ae una matrice invertibile Mtale che D= M −1 A M \nDeterminiamo una matrice di passaggio dalla matrice Aalla matrice D.una base per E(2) è :  v4:= (1, 0, 1, 0)una base per E(1) è :  v1:= (0, 1, 0, 0)  ,  v2:= (0, 0, 0, 1)una base per E(0) è :  v3:= (–1, 0, 1, 0)\nv1v2v3v4\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it37",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#4": "23/12/205DiagonalizzazioneTeorema: Sia  f: V→Vun endomorfismo di uno spazio vett. Vdi dimensione finita ne siano 1, 2, …, sgli autovalori distinti di f . L’endomorfismo  fè diagonalizzabile se e solo se : dim E(1) + dim E(2) + ... + dim E(s) = nOsservazioni: •Sappiamo che per ciascun autovalore isi ha:  dim E(i)  ≤  mf(i)•dim E(1)+dim E(2) +...+ dim E(s) ≤ mf(1)+mf(2)+...+mf(s) ≤n•Affinchè  fsia diagonalizzabile è necessario che la somma dellemolteplicità degli autovalori sia uguale a n, ovvero il polinomiocaratteristico di  fdeve essere totalmente riducibile•Se anche per uno solo degli autovalori isi ha:  dim E(i)  <  mf(i)allorafnonè diagonalizzabile 23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#5": "23/12/206DiagonalizzazioneTeorema: Sia  f: V→Vun endomorfismo di uno spazio vett. Vdi dim. finita n. L’endomorsmo  fè diagonalizzabile se e solo sesono verificate entrambele seguenti condizioni: 1.il polinomio caratteristico di  fè totalmente riducibile;2.per ciascun autovalore idi  fsi ha:  dim E(i)  =  mf(i). Osservazioni: •Se λè un autovalore semplice allora dim E(λ) = 1.•La seconda condizione del teorema va quindi verificata solamente per gli autovalori di molteplicità almeno 2. 23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#6": "23/12/207DiagonalizzazioneTeorema: Sia  f: V→Vun endomorfismo di uno spazio vett. Vdi dim. finita n. L’endomorsmo  fè diagonalizzabile se e solo sesono verificate entrambele seguenti condizioni: 1.il polinomio caratteristico di  fè totalmente riducibile;2.per ciascun autovalore idi  fsi ha:  dim E(i)  =  mf(i). Calcolo di una base particolare: In tal caso una base di  Vformata da autovettori di  fsi ottiene  prendendo una base per ciascun autospazio e unendole. Rispetto a tale base:  fsi rappresenta con una matrice diagonale i cui elementi lungo la diagonale sono gli autovalori di  f,  ciascuno ripetuto un numero di volte pari alla sua molteplicità.23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#7": "23/12/208DiagonalizzazioneTeorema: Sia  fun endomorfismo di uno spazio vett. di dim. n. Se  f  ha nautovalori distinti allora  f  è diagonalizzabile.Esempio: Sia data la matrice Ae il suo polinomio caratteristico:  \nCalcoliamo il discriminante (∆ = b2 –4 a c) di  x2–6x+ 10 : a= 1; b= –6; c= 10 segue ∆ = 36 –40 < 0 quindi no radici reali Quindi il polinomio caratteristico di Anonè totalmente riducibile. Segue che la matrice Anonè diagonalizzabile.23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#8": "23/12/209DiagonalizzazioneEsercizio: Consideriamo l’endomorfismo di R4definito da: \nLa matrice rappresentativa di  f  rispetto alla base canonicaè : \n2x+ y2y+ z3y+ 2w2x+ 6y+ z+ 2\n23/12/20Geometria e Combinatora   marcella.sama@uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\18_Diagonalizzazione.pdf#9": "23/12/2010DiagonalizzazioneEsercizio: Consideriamo l’endomorfismo di R4definito da: \nLa matrice rappresentativa di  f  rispetto alla base canonica è : \nCalcoliamo il discriminante (∆ = b2 –4 a c) di  x2–6x+ 7 : a= 1; b= –6; c= 7  segue  ∆ = 36 –28 > 0 calcoliamo le radici:x= {–b+/–[radice quadrata ∆] } / 2 a=     3 +/–√2Quindi il polinomio caratteristico di  fè totalmente riducibile.Autovalori: 0 di molt. 2,     3+√2 di molt. 1,     3–√2 di molt. 123/12/20Geometria e Combinatora   marcella.sama@uniroma3.it10",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#0": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it1L1: Matrici (2-4) Argomenti lezione:•Definizione di Matrice•Matrici particolari•Operazioni tra matrici",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#1": "Definizione di matrice\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#10": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it11MatriciDefinizioni:Una matrice quadrata A= (aij) di ordine nsi dice triangolaresuperiorese tutti gli elementi che si trovano sottola diagonaleprincipale sono nulli.Una matrice quadrata A= (aij) è triangolare superiore se e solo  se aij= 0 per ogni i> jInsieme matrici triangolari superiori di ordine na coefficienti reali: \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#11": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it12MatriciInsieme matrici triangolari superioridi ordine na coefficienti reali:Osservazione: la definizione di matrice triangolare superiore nonimplica che aij≠ 0 per ogni i≤ jEsempio:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#12": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it13MatriciInsieme matrici triangolari inferioridi ordine na coefficienti reali:Þtutti gli elementi che si trovano al di sopradella diagonale = 0 Esempio: \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#13": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it14MatriciEsercizio di base (1): Determinare la matrice quadrata A:= (aij) di ordine 3 tale che aij:= max (0, i− j) \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#14": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it15MatriciEsercizio di base (2): Determinare la matrice quadrata A:= (aij) di ordine ntale che aij:= max (0, i− j) appartiene a Dobbiamo dimostrare che Aè una matrice triangolare inferiore, ovvero se j> i, allora aij= 0. Sia quindi j> i. Ricordiamo che  abbiamo aij= max(i− j, 0). Ma i− j< 0, perché j> i,da cui aij= 0.  \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#15": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it16MatriciDefinizione: Una matrice quadrata avente nulli tutti gli elementi non appartenenti alla diagonale principale si dice diagonale. Esempio: Insieme delle matrici diagonali di ordine na coefficienti reali:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#16": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it17MatriciDefinizione: Una matrice quadrata si dice simmetricase i suoi elementi in posizioni simmetriche rispetto alla diagonale principalesono uguali.Esempio: Insieme delle matrici simmetriche di ordine na coefficienti reali:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#17": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it18MatriciEsercizio di base: Dimostrare che ogni matrice diagonale è simmetrica, ovvero:Data una matrice A:=(aij)diagonale, si ha che aij= 0 se i≠ j. Dobbiamo dimostrare che aij= ajiper ogni coppia di indici ie j. Se i= j, ciò è ovvio. Se i≠ j, allora aij= 0 = aji(per la definizione di matrice diagonale). \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#18": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it19MatriciEsercizio di base (1): Determinare se la matrice quadrata A:= (aij)di ordine 3 tale che aij:= i+ jè simmetrica. \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#19": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it20MatriciEsercizio di base (2): Determinare se la matrice quadrata A:= (aij)di ordine ntale che aij:= i+ jè simmetrica. Dimostriamo che si ha  aij= ajiqualunque siano ie j . Poichè, per definizione, abbiamo  aij= i+ j , otteniamo:  aij= i+ j= j+ i= aji(proprietà commutativa della somma)Risposta: Sì!",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#2": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it3MatriciDef.: Una matrice a coefficienti reali a prighe e qcolonne è una tabella di numeri reali disposti su prighe e qcolonne\nI numeri pe qvengono detti dimensionidella matrice di tipo (p, q), ovvero della matrice con prighe e qcolonne\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#20": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it21Matrice traspostaDefinizione: Data una matrice A:= (aij) a prighe e qcolonne, si dice matrice traspostadi Ala matrice a qrighe e pcolonneavente come elemento di posto (j, i) l'elemento di posto (i, j) di AEsempio:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#21": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it22Proprietà della matrice traspostaEsempio: Calcolare la matrice trasporta di A\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#22": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it23Proprietà della matrice traspostaEsercizio di base: Dimostrare la seguente affermazioneAbbiamo cheQuesto significa che se i> jallora aij= 0.Sia                                            da cuiDobbiamo dimostrare che  quindi dimostrare che, se i> jallora bji= 0.Sia allora i> j. Abbiamo bji= aij= 0,vale a dire  \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#23": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it24Proprietà della matrice traspostaEsercizio di base: Dimostrare la seguente affermazioneDimostriamo il viceversa, ovvero:supponendo che se                                        allora  , ovvero bji= 0 per ogni i > jabbiamo aij= bji, quindi aij= 0 per ogni i> j, vale a dire\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#24": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it25Proprietà della matrice traspostaEsercizio di base: Dimostrare la seguente affermazioneAnalogamente si dimostra la seguente affermazione:Inoltre si può dimostrare che: •ogni matrice simmetrica coincide con la sua traspostaEsempio: \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#25": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it26Proprietà della matrice traspostaEsercizio di base: Dimostrare la seguente affermazioneAnalogamente si dimostra la seguente affermazione:Inoltre si può dimostrare che: •ogni matrice simmetrica coincide con la sua trasposta•una matrice quadrata è simmetrica se e solo se coincide con la propria trasposta, ovvero: \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#26": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it27Proprietà della matrice traspostaEsempio: Calcolare la matrice trasporta di A e la trasposta di\nIn generale, data una qualsiasi matricesi può dimostrare che  \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#27": "Operazioni tra matrici\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#28": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it29Matrice sommaObiettivo: Date matrici  Ae Bdefiniamo una matrice  A+ BN.B. L'operazione di addizione tra matrici verifica proprietà analoghe alle usuali proprietà dell'addizione tra numeri reali.Esempio: \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#29": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it30Matrice sommaDefinizione: Date due matrici A:= (aij)  e B:= (bij) entrambe di tipo (p, q), chiamiamo matrice somma A+ Bdi tipo (p, q) il cui elemento di posto (i, j) è dato dalla somma degli elementi di posto (i, j) delle matrici Ae B, ovvero:Osservazione: Si possono sommare solamente matrici che hanno lo stesso numero di righe e lo stesso numero di colonne, cioè matrici dello stesso tipo.\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#3": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it4MatriciDef.: Una matrice a coefficienti reali a prighe e qcolonne è una tabella di numeri reali disposti su prighe e qcolonne\nElemento(o coefficiente) di posto (i, j) della matrice Aè il numero reale sulla i-esima riga e sulla j-esima colonna di A\ni-esima riga j-esima colonna ",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#30": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it31Matrice somma\n-Aè chiamata la matrice opposta di A0 è chiamata la matrice nulla, in questo caso è di tipo (p, q)",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#31": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it32Matrice somma\n-Aè chiamata la matrice opposta di AEsempio:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#32": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it33Matrice somma\nLegge di semplicazione per l'addizione matriciale:Dimostrazione: Partiamo dal fatto che A+ C= B+ Cpoi                              A+ C+ (-C) = B+ C+ (-C)poichè si ha C+ (-C) = 0allora                         A+ 0 = B+ 0e sfruttando la proprietà della matrice nulla segue A= B",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#33": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it34Matrice somma\nEsempio:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#34": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it35Moltiplicazione per uno scalareDefinizione: Data una matrice Ae un numero reale k, indichiamo con kAla matrice di tipo (p, q)avente come elementi quelli della matrice Amoltiplicati per k. Studiamo le proprietà della moltiplicazione per uno scalare.Esempio: \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#35": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it36Moltiplicazione per uno scalare\nNOTA: Nella 6, lo 0 a sinistra è un numero, lo 0 a destra è una matrice di tipo (p,q)",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#36": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it37Matrice prodotto\nIl numero delle (q) colonne di Adeve essere uguale al numero delle (q) righe di BIl prodotto A• Bè una matrice a prighe (come A) e rcolonne (come B)",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#37": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it38Matrice prodotto\nEsempio:\nprodotto righe per colonne delle matrici  Ae  B4x22x3AB =4x3Non è possibile fare il prodotto di Be A",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#38": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it39Matrice prodotto\nprodotto righe per colonne delle matrici  Ae  BEsempio:AB =",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#39": "AB =\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it40Matrice prodotto\nprodotto righe per colonne delle matrici  Ae  BEsempio:",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#4": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it5MatriciEsempio: \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#40": "AB =\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it41Matrice prodotto\nprodotto righe per colonne delle matrici  Ae  BEsempio:",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#41": "AB =\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it42Matrice prodotto\nprodotto righe per colonne delle matrici  Ae  BEsempio:",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#42": "AB =\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it43Matrice prodotto\nprodotto righe per colonne delle matrici  Ae  BEsempio:",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#43": "AB =\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it44Matrice prodotto\nprodotto righe per colonne delle matrici  Ae  BEsempio:",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#44": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it45Matrice prodotto\n4 righe e 3 colonneEsempio:",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#45": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it46Matrice prodotto\nCalcolare AI = \nEsercizio di base:",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#46": "03/11/2047Matrice prodottoDeterminare quali dei seguenti prodotti sono definiti: AB  SI, perché il numero delle colonne di A(3) è uguale al numero delle righe di B (3).BA  NO, perché il numero delle colonne di B(3) non è uguale al numero delle righe di A (2).AA  NO, non è una matrice quadrataBB  SI, è una matrice quadrata (3, 3)  =>\nEsercizio di base:2x33x3\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it47",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#47": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it48Proprietà della moltiplicazioneL'operazione di moltiplicazione tra matrici verifica alcuneproprietà analoghe all'operazione di moltiplicazione tra numeriVale la proprietà associativadella moltiplicazione di matrici:Attenzione: ABè una matrice di tipo (p, r) e (AB)Cè di tipo (p, s). BCè una matrice di tipo (q, s) e A(BC) è di tipo (p, s). Dunque (AB)Ce A(BC) sono matrici dello stesso tipo.\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#48": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it49Proprietà della moltiplicazioneL'operazione di moltiplicazione tra matrici verifica alcuneproprietà analoghe all'operazione di moltiplicazione tra numeriValgono le proprietà distributivedelle operazioni matriciali:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#49": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it50Proprietà della moltiplicazioneL'operazione di moltiplicazione tra matrici verifica alcuneproprietà analoghe all'operazione di moltiplicazione tra numeri\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#5": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it6MatriciEsercizio di base: Determinare la matrice Adi tipo (2, 2) tale che aij:= i+ j\n1+11+22+12+2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#50": "03/11/2051Proprietà della moltiplicazione\nEsempio:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it51",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#51": "03/11/2052Proprietà della moltiplicazione\nEsempio:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it52",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#52": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it53Proprietà della moltiplicazioneAltre proprietà della moltiplicazione tra numeri reali nonsonovalide nel caso delle matrici. Nonvale la proprietà commutativa. Per esempio, se Ae Bsono due matrici tali che sia definito il prodotto AB, allora:•il prodotto BApotrebbe non essere definito;•il prodotto BApotrebbe essere definito ma avere dimensioni diverse da AB;=> Per poter fare entrambi i prodotti ABe BAdeve essere Adi tipo (p, q) e Bdi tipo (q, p). Da cui, si ha ABdi tipo (p, p)e BAdi tipo (q, q). Ha le stesse dimensioni solo sesi ha: p= q.",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#53": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it54Proprietà della moltiplicazioneAltre proprietà della moltiplicazione tra numeri reali nonsonovalide nel caso delle matrici. Nonvale la proprietà commutativa. Per esempio, se Ae Bsono due matrici tali che sia definito il prodotto AB, allora:•il prodotto BApotrebbe non essere definito;•il prodotto BApotrebbe essere definito ma avere dimensioni diverse da AB;•il prodotto BApotrebbe essere definito e avere le stesse dimensioni di AB(sono due matrici quadrate dello stesso ordine), ma non è detto che i prodotti ABe BAsono uguali.",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#54": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it55Proprietà della moltiplicazioneAltre proprietà della moltiplicazione tra numeri reali nonsonovalide nel caso delle matrici. Nonvale la proprietà commutativa. Per esempio, se Ae Bsono due matrici tali che sia definito il prodotto AB, allora:•il prodotto BApotrebbe essere definito e avere le stesse dimensioni di AB(sono due matrici quadrate dello stesso ordine), ma non è detto che i prodotti ABe BAsono uguali.Contro-Esempio: ",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#55": "03/11/2056Proprietà della moltiplicazioneAltre proprietà della moltiplicazione tra numeri reali nonsonovalide nel caso delle matrici. Nonvale la proprietà commutativa. Per esempio, se Ae Bsono due matrici tali che sia definito il prodotto AB, allora:•il prodotto BApotrebbe essere definito e avere le stesse dimensioni di AB(sono due matrici quadrate dello stesso ordine), ma non è detto che i prodotti ABe BAsono uguali.=> Ciò non implica che, date comunque Ae B, si ha sempreAB≠ BA.Definizione: Date due matrici quadrate Ae Bdello stesso ordine, Ae Bcommutanoo permutanose si ha AB= BA.03/11/20Geometria e Combinatoriasama@ing.uniroma3.it56",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#56": "03/11/2057Proprietà della moltiplicazioneDefinizione: Date due matrici quadrate Ae Bdello stesso ordine, AeBcommutanoo permutanose si ha AB= BA.Esempio: Siano Ae Bdue matrici quadrate dello stesso ordine.Stabilire se è vero o falso che qualunque siano Ae Bsi ha:\nDa cui è vero se e solose AB= BA, ovvero Ae Bcommutano!03/11/20Geometria e Combinatoriasama@ing.uniroma3.it57",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#57": "03/11/2058Proprietà della moltiplicazioneAltre proprietà della moltiplicazione tra numeri reali nonsonovalide nel caso delle matrici. Un'altra proprietà che nonvale è il principio di annullamento del prodotto: se Ae Bsono due matrici tali che AB= 0 non èdetto che almeno una delle matrici Ae Bsia nulla.Esempio:\nAB= 003/11/20Geometria e Combinatoriasama@ing.uniroma3.it58",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#58": "03/11/2059Proprietà della moltiplicazioneAltre proprietà della moltiplicazione tra numeri reali nonsonovalide nel caso delle matrici. Nonvale la legge di semplificazione del prodotto: se A, Be Csono matrici tali che AC= BCe  C≠ 0, non è detto che Asia uguale alla matrice B.Esempio:\nAB= 00B= 0AB= 0BA≠ 059Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#59": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it60Matrici e sistemiEsempio: Consideriamo il sistema S\nmatrice dei coefficienti di S",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#6": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it7MatriciDefinizioni:l'insieme delle matrici a coefficienti reali di tipo (p, q)una matrice a coefficienti reali a prighe e qcolonnel'insieme delle matrici quadratedi ordine na coefficienti realiIn una matrice quadrata A= (aij) di ordine n, gli elementi       a11,  a22, …, annsi dicono elementi della diagonale principale\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#60": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it61Matrici e sistemiEsempio: Consideriamo il sistema S\nmatrice colonna delle incognite di S",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#61": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it62Matrici e sistemiEsempio: Consideriamo il sistema S\nmatrice colonna dei termini noti di S",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#62": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it63Matrici e sistemiEsempio: Consideriamo il sistema S\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#63": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it64Matrici e sistemiEsempio: Consideriamo il sistema S\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#64": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it65Matrici e sistemiDato un sistema di pequazioni inq incognite:\nN.B. Risolvere il sistema Sè equivalente a determinare (se esistono) tuttele matrici Xtali che A X= B.\nAllora possiamo scrivere il sistema Snella forma :  A X= B",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#65": "Esempio (1): Data la matrice A:determinare tutte le matrici XÎM(2, 2, R) tali che AX= 0.\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it66Matrici e sistemi\nAX= 0 se e solo se:\nCiò avviene se e solo se a= b= c= d= 0, cioè se e solo se Xè la matrice nulla.\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#7": "Matrici particolari\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#8": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it9MatriciEsempio:La matrice Aè una matrice quadrata di ordine 3 Gli elementi sulla diagonale principale sono 2, 3, 6Inoltre tutti gli elementi di Ache si trovano sottola diagonaleprincipale, cioè gli elementi a21, a31, a32sono nulli. Per questa ragione la matrice Asi chiama triangolare superiore\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\1_Matrici.pdf#9": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it10MatriciDefinizioni:Una matrice quadrata A= (aij) di ordine nsi dice triangolaresuperiorese tutti gli elementi che si trovano sottola diagonaleprincipale sono nulli.Una matrice quadrata A= (aij) è triangolare superiore se e solo  se aij= 0 per ogni i> j\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#0": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it1L2: Determinante (5)Argomenti lezione:•Definizione •ProprietàStudiamo il determinante di una matricequadratae le sue proprietà",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#1": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it2Definizione determinante\nIl numero reale ad –bc è chiamato determinanteL'equazione lineare in un'incognitaax = bha un'unica soluzione se e solo se a≠ 0Il sistema di due equazioni lineari\nha un'unica soluzione se e solo se ad –bc ≠ 0.\n≠ 0≠ 0",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#10": "03/11/2011Proprietà determinanteFormula di sviluppo del determinante secondo la j-esima colonna:\nEsempio:  \n1103/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#11": "03/11/2012Proprietà determinanteSia Auna matrice quadrata.1. Se una riga o una colonna di Aha tutti i suoi elementi uguali a 0, allora det A= 0.2. Se Aè una matrice triangolare superiore o inferiore allora det Aè uguale al prodotto degli elementi della diagonale principale di A.Dimostrazione(punto 2):Se prendiamo una matrice triangolare superiore, tutti gli elementi della prima colonna di Atranne eventualmente a11sono nulli. det A= a11det A11Il minore A11è anch'esso una matrice triangolare superiore.Proseguendo in tal modo si ha: det A= a11a22… ann1203/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#12": "03/11/2013Proprietà determinanteSia Auna matrice quadrata.1. Se una riga o una colonna di Aha tutti i suoi elementi uguali a 0, allora det A= 0.2. Se Aè una matrice triangolare superiore o inferiore allora det Aè uguale al prodotto degli elementi della diagonale principale di A.\n1303/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#13": "03/11/2014Proprietà determinante\nEsempio:  \n1403/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#14": "03/11/2015Proprietà determinanteUna matrice quadrata con due righe (o due colonne) uguali hadeterminante nullo.Caso n= 2: Dimostrare che una matrice quadrata Adiordine 2con le righe uguali ha determinante nullo.\n1503/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#15": "03/11/2016Proprietà determinanteUna matrice quadrata con due righe (o due colonne) uguali hadeterminante nullo.Caso n= 3: Dimostrare che una matrice quadrata Adi ordine3con 2 righe uguali ha determinante nullo.1.La matrice Aha due righe uguali fra loro pertanto le matrici aggiunte degli elementi della riga rimanente sono matrici di ordine 2 aventi 2 righe uguali.2.Le matrici aggiunte hanno determinante nullo (vedi n = 2) . 3.Sviluppando il determinante di Arispetto alla riga rimanente troviamo quindi che il determinante è nullo. 1603/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#16": "03/11/2017Proprietà determinanteUna matrice quadrata con due righe (o due colonne) uguali hadeterminante nullo.Caso n= 2: Dimostrare che una matrice quadrata Adiordine 2con le righe uguali ha determinante nullo.Caso n= 3: Dimostrare che una matrice quadrata Adiordine 3con 2 righe uguali ha determinante nullo.… Ripetendo il ragionamento per matrici quadrate Adi ordine maggiore di ntroviamo quindi che det A= 0.1703/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#17": "03/11/2018Proprietà determinanteUna matrice quadrata con due righe (o due colonne) uguali hadeterminante nullo.Caso n= 2: Dimostrare che una matrice quadrata Adi ordine 2 con le righe uguali ha determinante nullo.Caso n= 3: Dimostrare che una matrice quadrata Adi ordine 3 con 2 righe uguali ha determinante nullo.… Ripetendo il ragionamento per matrici quadrate Adi ordine maggiore di ntroviamo quindi che det A= 0.Se invece Aha due colonne uguali basta osservare che in tal caso la trasposta di Aha due righe uguali e, pertanto, anche il dettA è nullo. Inoltre sappiamo che detA= dettA.1803/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#18": "03/11/2019Proprietà determinanteLa dimostrazione vista si chiama per induzione ovvero:Passo iniziale: si dimostra direttamente il caso iniziale dellaproprietà, ovvero il caso n= 2Passo induttivo: si suppone che la proprietà sia vera per gli interi minori di ne si utilizza questa supposizione per dimostrare la proprietà per l'intero n.Per la matrice quadrata con due righe/colonne uguali abbiamo:•supposto di aver dimostrato che matrici di ordine minore di ncon due righe/colonne uguali abbiano determinante nullo;•usato questa proprietà per provare che matrici di ordine ncon due righe/colonne uguali hanno determinante nullo.1903/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#19": "03/11/2020Proprietà determinanteSiano Ae Bdue matrici quadrate di ordine nche si ottengonouna dall'altra scambiando fra loro due righe. Allora det A= − det B(un'analoga proprietà vale per lo scambio di colonne).Caso n= 2:\n2003/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#2": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it3Definizione determinanteObiettivo: Definire e studiare le proprietà del determinante nel caso di sistemi di nequazioni in nincognite, aventi quindi le matrici dei coefficienti quadrate di ordine n. Tali sistemi hanno un'unica soluzione se e solo se il determinante della matrice dei coefficienti Aè diverso da 0Esempio:  \nDet Aè : \n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#20": "03/11/2021Proprietà determinanteSiano Ae Bdue matrici quadrate di ordine nche si ottengonouna dall'altra scambiando fra loro due righe. Allora detA= − detB.Dim.: Anche in questo caso la dimostrazione è per induzione.Passo induttivo: Sia allora n> 2 e supponiamo che le righe i1-esima e i2-esima di Ae Bsiano scambiate fra loro.Sviluppiamo il determinante sia di Ache di Brispetto a una qualsiasi riga diversa da i1-esima e i2-esima:•gli elementi di questa riga sono uguali fra loro nelle due matrici; •le matrici aggiunte di questi elementi sono matrici quadrateche si ottengono scambiando fra loro due righe; •per ipotesi di induzione si ha : detA= –detB.2103/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#21": "03/11/2022Proprietà determinanteTeorema (di Binet)\nEsempio:  \n–2322203/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#22": "03/11/2023Proprietà determinante\nEsempio:\n2303/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#23": "03/11/2024Proprietà determinanteData una matrice Adi ordine 2  e un numero reale ksi ha:\n2403/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#24": "03/11/2025Proprietà determinanteDate una matrice Ae una matrice Bcosì fatte:\nOsservando le due matrici, notiamo che le matrici aggiunte degli elementi della i-esima riga di Bsono uguali alle matrici aggiunte degli elementi della i-esima riga di A(Ai1= Bi1, Ai2= Bi2, etc.)  Segue che det B= k det A2503/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#25": "03/11/2026Proprietà determinante\nDimostrazione: •La matrice k Asi può ottenere dalla matrice Amoltiplicando la prima riga di Aper k, poi moltiplicando la seconda riga della matrice così ottenuta per ke così via per le nrighe di A. •Le matrici che via via otteniamo hanno determinante uguale al determinante della matrice precedente moltiplicato per k. •Dunque applicando nvolte il procedimento otteniamo:\n2603/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#26": "03/11/2027Proprietà determinanteSe una matrice quadrata Aha una riga che è multipla di un'altra,allora det A= 0. Un'analoga proprietà vale per le colonne.Esempio:  \nNotiamo che la quarta riga di Aè –3/2 la seconda riga(–3/2 volte la seconda riga equivale alla quarta riga). Dalla proposizione enunciata sopra abbiamo detA= 0.2703/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#27": "03/11/2028Proprietà determinanteEsercizio di base:\nDimostrare che nonesiste alcuna matrice quadrata Xdi ordine 3 tale che AX= I.\nSi dovrebbe avere, per ipotesi, che det (AX) = det I= 1. Ma, applicando il teorema di Binet, otteniamo:\n2803/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#3": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it4Definizione determinanteData una matrice quadrata di ordine n> 1 e un suo elemento aij, definiamo matrice aggiuntadi aijla matrice di ordine n>1 ottenuta da Acancellando la i-esima riga e la j-esima colonna. Indichiamo questa matrice con il simbolo AijEsempio:  \n•••",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#4": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it5Definizione determinanteSia Auna matrice quadrata di ordine n, chiamiamo determinantedi Ail numero det Acalcolato come segue:\n",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#5": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.it6Definizione determinanteSia Auna matrice quadrata di ordine n, è detto determinantedi Ail numero det Acalcolato come segue:\nEsempio:  \ndet",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#6": "03/11/207Definizione determinanteSia Auna matrice quadrata di ordine n, chiamiamo determinantedi Ail numero det Acalcolato come segue:\nDefinizione per induzioneo ricorrenza703/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#7": "03/11/208Proprietà determinante\nFormula di sviluppo del determinante secondo la i-esima riga:\nSuggerimento: scegliere le righe con il maggior numero di zeriEsempio (1):  \n803/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#8": "03/11/209Proprietà determinante\nFormula di sviluppo del determinante secondo la i-esima riga:Esempio (2):  \nSuggerimento: scegliere le righe con il maggior numero di zeri\n903/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\2_Determinante.pdf#9": "03/11/2010Proprietà determinanteFormula di sviluppo del determinante secondo la j-esima colonna:\nEsempio:  \n1003/11/20Geometria e Combinatoriasama@ing.uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#0": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.itL3: Matrice inversa (1,6)Argomenti lezione:•Matrice unità•Matrice inversa •Proprietà dell’inversa •Sistemi di equazioni lineari•Teorema di Cramer1",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#1": "Matrice Unità\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#10": "03/11/2011La matrice inversa è unicaData una matrice invertibile Asi chiama inversadi  Al’unicamatrice Btale che AB= BA= I.\nSe Aè una matrice invertibile e Be Csono due matrici taliche AB= BA= Ie AC= CA= I, allora B= C.\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#11": "03/11/2012Matrice inversaUna matrice quadrata Aè invertibile se e solo se det A≠0. In tal caso, detto nl’ordine di A, la matrice inversa di Asi calcola nel modo seguente:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#12": "03/11/2013Matrice inversaUna matrice quadrata Aè invertibile se e solo se det A≠0. In tal caso, detto nl’ordine di A, la matrice inversa di Asi calcola nel modo seguente:\nAttenzione:1.Consideriamo il determinante della matrice aggiunta Aji2.Lo dividiamo per il det A≠ 03.Lo moltiplichiamo per −1 elevato alla somma degli indici\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#13": "03/11/2014Matrice inversaEsempio (1):\ndet A≠ 0\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#14": "03/11/2015Matrice inversa\nAè invertibile !\nEsempio (2):\nbij= \n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#15": "03/11/2016Proprietà dell’inversaL'inversa di una matrice invertibile Aè una matrice invertibile.\nDimostrazione:Bisogna verificare che vale (def.): è per definizione l’inversa di A e quindi i due prodotti indicati sopra sono validi.Inoltre, dal teorema di Binet si ha:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#16": "03/11/2017Proprietà dell’inversaDate due matrici invertibili Ae Bdello stesso ordine, anche ilprodotto ABè invertibile e si ha:\nDimostrazione:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#17": "03/11/2018Proprietà dell’inversaData una matrice invertibile A, la sua trasposta è invertibile e si ha:\nDimostrazione:\nData una matrice invertibile A:Per la trasposta di un prodotto si ha:\nSegue:\nLa trasposta della matrice identità è la matrice identità, per cui:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#18": "03/11/2019Proprietà dell’inversa\nDimostrazione:Moltiplicando a sinistra ambo i membri per l’inversa di Asi ha:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#19": "03/11/2020Proprietà dell’inversa\nSi dimostra in modo analogo al precedente che: \n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#2": "03/11/203Matrice unitàProprietà dei numeri reali sull’inversodi un numero: dato un numero reale a≠0 esiste un numero reale btale che ab = 1. Tale numero bnon solo esiste, ma è anche unico. Studiamo la medesima proprietà per le matrici. Ci domandiamo:1.Esiste una matrice unità Icorrispondente al numero 1 ? 2.Data una matrice quadrataA, con A≠ 0, esiste una matrice quadrataB, tale che AB = I ?Risposte:1.Esiste la matrice unità I 2.Esiste la matrice quadrata Bse e solo se detA ≠ 003/11/20Geometria e Combinatoriasama@ing.uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#20": "Sistemi di equazioni lineari\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#21": "03/11/20Una equazione in una incognitaObiettivo:comedeterminareleeventualisoluzionidiunaequazionelineareinunaincognitaEsempidiequazionilineariinunaincognita:3x = 2   => SOLUZIONE  0x = 20x = 0\nGeometria e Combinatoriasama@ing.uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#22": "03/11/20Una equazione in una incognitaObiettivo:comedeterminareleeventualisoluzionidiunaequazionelineareinunaincognitaEsempidiequazionilineariinunaincognita:3x = 20x = 2 => NO SOLUZIONE0x = 0Geometria e Combinatoriasama@ing.uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#23": "03/11/20Una equazione in una incognitaObiettivo:comedeterminareleeventualisoluzionidiunaequazionelineareinunaincognitaEsempidiequazionilineariinunaincognita:3x = 20x = 20x = 0 => INFINITE SOLUZIONI,tale equazione si dice identicamente soddisfattaGeometria e Combinatoriasama@ing.uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#24": "03/11/20Una equazione in una incognitaEserciziodibase:Percheesistonoinfiniteequazionilineariinunaincognita?Ogni equazione lineare in una incognita può scriversi:ax= bEssa è determinata da due numeri qualsiasi: •il coefficiente adella x(incognita)•il termine noto bPoiché i numeri sono infiniti, le equazioni sono infinite.Geometria e Combinatoriasama@ing.uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#25": "03/11/20Una equazione in una incognitaOgni equazione lineare in un’incognita può scriversi:ax= bFissatiinumeriaeb,l'equazioneprecedentehasoluzioni?Quante?Procedimento: Isoliamo l’incognita x, ovvero moltiplichiamoambo i membri per l'inverso di a. Abbiamo due casi: 1.a≠ 0 =>                           => Abbiamo una sola soluzione2.a= 0 => primo membro è nullo, secondo membro? \nGeometria e Combinatoriasama@ing.uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#26": "03/11/20Una equazione in una incognitaOgni equazione lineare in un’incognita può scriversi:ax= b2. a= 0 => primo membro è nullo, secondo membro?•2.1Sotto caso b≠ 0 => I due membri dell'equazione sono diversi qualunque valore assuma la x. Quindi no soluzioni.•2.2Sotto caso b= 0 => I due membri dell'equazione sono uguali a 0, qualsiasi valore assuma la x. Infinite soluzioni.Geometria e Combinatoriasama@ing.uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#27": "03/11/20Due equazioni in due incogniteObiettivo:comedeterminareleeventualisoluzionididueequazionilineariindueincogniteEsempididueequazionilineariindueincognite:sistemaUna soluzione di un sistema Sè una coppia di numeri (h, k) che, sostituita nelle due equazioni alla coppia (x, y), da due uguaglianze \nGeometria e Combinatoriasama@ing.uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#28": "03/11/20Due equazioni in due incogniteLa coppia (-2, 3) è una soluzione del nostro sistema? No, ecco perché:\nGeometria e Combinatoriasama@ing.uniroma3.it29",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#29": "03/11/20Due equazioni in due incogniteLa coppia (8/3, 2/3) è una soluzione del nostro sistema? Sì, ecco perché:Come si è potuta determinare tale soluzione? Ci sono altre soluzioni a tale sistema?  \nGeometria e Combinatoriasama@ing.uniroma3.it30",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#3": "03/11/204Matrice unitàChiamiamo matrice unitào matrice identicadi ordine nlamatrice quadrata Inavente tutti gli elementi della diagonale principale uguali a 1 e tutti gli altri elementi uguali a 0 (la matrice identica è, quindi, una matrice diagonale).\ndet I= 1E’ una matrice triangolare.Il suo determinante è uguale al prodotto degli elementi della sua diagonale principale!03/11/20Geometria e Combinatoriasama@ing.uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#30": "03/11/20Due equazioni in due incogniteProcedimento per determinare le soluzioni al sistema S:1.Sottraendo alla seconda equazione la prima equazione, si ha:Da cui: y= 2/3 e sostituendo y= 2/3 nella prima equazione si ha x= 8/3 \nsistemaequivalentead S\nGeometria e Combinatoriasama@ing.uniroma3.it31",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#31": "03/11/20Due equazioni in due incognite\nSistema Sè equivalentea sistema S’  significa Sha le stesse soluzioni di S’ Trasformazioni possibili: •Sommare a un'equazione del sistema un'altra equazione moltiplicata per una costante (abbiamo sommato alla seconda equazione la prima equazione moltiplicata per –1)•Moltiplicare un'equazione per una costante non nulla (Per determinare yabbiamo moltiplicato la seconda equazione per 1/3 trovando y= 2/3)Geometria e Combinatoriasama@ing.uniroma3.it32",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#32": "03/11/20Due equazioni in due incognite\nSistema Sè equivalentea sistema S’  significa Sha le stesse soluzioni di S’ Trasformazioni possibili: •Possiamo anche scambiare tra loro due equazioni:\nsistemaequivalentea S (eS’)Geometria e Combinatoriasama@ing.uniroma3.it33",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#33": "03/11/20Due equazioni in due incogniteEserciziodibase:Determinareleeventualisoluzionidelsistema:Sommiamo alla II equazione la I equaz. moltiplicata per –3/2Il sistema ha quindi una sola soluzione: (x = 13/4, y = –7/4)  \nGeometria e Combinatoriasama@ing.uniroma3.it34",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#34": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.itDue equazioni in due incogniteIngeneraleabbiamoilseguentesistemadiequazioni:Vedremonelcorsoche:se ae–bd≠ 0, allora il sistema ha una sola soluzione(Cramer)se ae–bd= 0, allora il sistema ha nessuna soluzioneoppure hainfinite soluzioni(Rouché -Capelli) => situazione analoga al caso una equazione e una incognita!\n35",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#35": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.itMolte equazioni in molte incogniteObiettivo:comedeterminareleeventualisoluzionidiqequazionilineariinpincognite,peqnumeriinteripositiviPer indicare qincognite si usano i seguenti simboli:Abbiamo jè un numero intero tale che 1≤ j≤ q, dunquela j-esima incognita è indicata con il simbolo xjIndichiamo con il simbolo aijil coefficiente della j-esima incognita appartenente alla i-esima equazione:  aijxj\n36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#36": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.itMolte equazioni in molte incogniteSistemagenericodipequazioniinqincognite:\nIl numero aijcon 1 ≤ i≤ p, 1 ≤ j≤ q e il coefficiente dellaj-esima incognita appartenente alla i-esima equazione.\n37",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#37": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.itMolte equazioni in molte incogniteSistemagenericodipequazioniinqincognite:\nUnasoluzione del sistema S: q-upla di numeri reali (x1, x2, …, xq) che, sostituiti nelle equazioni del sistema S alle incognite (x1, x2, …, xq), danno delle identità. \n38",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#38": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.itMolte equazioni in molte incogniteMatricedeicoefficientidelsistema:Equivaleaunataletabellaconprigheeqcolonne\n39",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#39": "03/11/20Geometria e Combinatoriasama@ing.uniroma3.itMolte equazioni in molte incogniteConsideriamoalcunisistemiecalcoliamolematriciassociate:\nTale sistema non ha no soluzioniTale sistema ha infinitesoluzioni40",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#4": "03/11/205Matrice unitàChiamiamo matrice unitào matrice identicadi ordine nlamatrice quadrata Inavente tutti gli elementi della diagonale principale uguali a 1 e tutti gli altri elementi uguali a 0 (la matrice identica è, quindi, una matrice diagonale).\n?\ncon ncolonnecon nrighe03/11/20Geometria e Combinatoriasama@ing.uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#40": "Teorema di Cramer\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it41",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#41": "03/11/2042Teorema di CramerUn sistema di nequazioni lineari a coefficienti reali in nincognite:\nSe det A≠0, il sistema ammette una e una sola soluzionedata da:\nDimostrazione:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it42",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#42": "03/11/2043Teorema di CramerUn sistema di nequazioni lineari a coefficienti reali in nincognite:\nSe det A≠0, il sistema ammette una e una sola soluzionedata da:\ndove A(i) è la matrice ottenuta da Asostituendo la i-esimacolonna di Acon la colonna Bdei termini noti.\nUn sistema lineare di nequazioni in nincognite la cui matricedei coefficienti sia invertibilesi dice Crameriano.SOLUZIONE:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it43",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#43": "03/11/2044Teorema di CramerEsercizio (1):\ndet A= 1\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it44",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#44": "03/11/2045Teorema di CramerEsercizio (1):\ndet A= 1\nunica soluzione del sistemaCome controllo possiamo sostituire i valori di (x, y, z) = (2, 0, −1)nelle equazioni del sistema e verificare che sono tutte soddisfatte:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it45",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#45": "03/11/2046Teorema di CramerEsercizio (2): Determinare la soluzione del sistema: \ndet A= 36\nEsercizio:Verificareche la soluz.(x1, x2, x3, x4) trovata èammissibile.03/11/20Geometria e Combinatoriasama@ing.uniroma3.it46",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#5": "03/11/206Matrice unità\nDimostrazione:\nL'unico elemento ≠ 0 nella moltiplicazione per la i-esima riga è l’elemento della j-esima colonna (per Invale 1, per Avale aij). Dunque, l'elemento di posto (i, j) di AInè aij. Segue AIn= A. 03/11/20Geometria e Combinatoriasama@ing.uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#6": "03/11/207Matrice unità\nSi dimostra in modo analogo che:\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#7": "Matrice Inversa\n03/11/20Geometria e Combinatoriasama@ing.uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#8": "03/11/209Matrice inversaUna matrice quadrata Asi dice invertibilese esiste una matricequadrata Bdello stesso ordine di Atale che AB= BA= I. Indichiamo con GL(n, R) l'insieme delle matrici invertibilidi ordine n(anche note col nome Gruppo Lineare).Attenzione:•A≠ 0, altrimenti per qualunque Bavremmo AB = 0 e BA = 0. •det A≠ 0, altrimenti non esiste alcuna matrice Bt.c. AB = I, perché avremmo 1 = det I = det(AB) = det A det B= 0 det B= 0. 03/11/20Geometria e Combinatoriasama@ing.uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\3_Matrice_inversa.pdf#9": "03/11/2010La matrice inversa è unicaSe Aè una matrice invertibile e Be Csono due matrici taliche AB= BA= Ie AC= CA= I, allora B= C.Dimostrazione:Sappiamo che:  AB = BA = Ie   AC = CA = ICalcoliamo ora il prodotto CAB:CAB = (CA)B = IB = BD'altra parte:CAB = C(AB) = CI = CDunque CABè uguale sia a Bche a C.Segue Be Csono la stessa matrice.03/11/20Geometria e Combinatoriasama@ing.uniroma3.it10",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#0": "03/11/20Geometria e Combinatoria sama@ing.uniroma3.it1L4: Rango di una matrice (7-8)Argomenti lezione:•Definizione di rango •Proprietà del rango•Teorema dell’orlare•Teorema di Rouché-Capelli",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#1": "Rango di una matrice\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#10": "03/11/2011Definizione di rangoSe tutti i minori di Ahanno determinante nullo, allora rk A= 0Una matrice ha rango 0 se e solo se è la matrice nulla.Dimostrazione:Se A= 0 allora tutti i minori di qualsiasi ordine estratti da Asono nulli e hanno quindi determinante nullo. Dunque Aha rango 0.Se rk A= 0, tutti i minori estratti da Ahanno determinante = 0. Poiché il determinante di una matrice di ordine 1 è uguale all’unico elemento della matrice, si ha che tuttigli elementi di Asono nulli.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#11": "03/11/2012Proprietà del rangoPer ogni matrice Asi ha:  \nDimostrazione:I minori della trasposta di Asono, ovviamente, tutte e sole le matrici trasposte dei minori di A. Poiché una matrice quadrata e la sua trasposta hanno lo stesso determinante, si ha \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#12": "03/11/2013Proprietà del rangoSia Auna matrice. Se tutti i minori estratti da Adi un certo ordine fissato nhanno determinante nullo, allora tutti i minori estratti daAdi ordine più grandedi nhanno determinante nullo. Si ha rk A < nEsempio:\nVediamo i minori di ordine 3:\nTutti i minori di ordine 3 hanno determinante nullo, segue rk A< 303/11/20Geometria e Combinatoria sama@ing.uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#13": "03/11/2014Teorema dell’orlareDato un minore Bdi ordine ndi una matrice A, un minore Cdi Adi ordine n + 1 è detto orlatodi Bse Bè un minore di C. In altre parole, il minore Cè ottenuto dal minore Baggiungendo   ad esso un’altra riga e un’altra colonna di A. Esempio:\nminore Cminore B\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#14": "minore B\n03/11/2015Teorema dell’orlareTeorema dell'orlare: Sia Auna matrice e sia Bun suo minore condeterminante ≠ 0. Se tutti gli orlati di Bhanno determinante nullo, allora il rango della matrice Aè uguale all'ordine del minore B.Esempio:Invece di calcolare i minori di tutti i 16 minori di ordine 3, possiamo limitarci a considerare solo gli orlati di Bche sono 4.[ N.B. I 4 orlati si ottengono da: terza riga –seconda colonna,terza riga –quarta colonna, quarta riga –seconda colonna,  quarta riga –quarta colonna.] 03/11/20Geometria e Combinatoria sama@ing.uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#15": "minore B\n03/11/2016Teorema dell’orlareTeorema dell'orlare: Sia Auna matrice e sia Bun suo minore condeterminante ≠ 0. Se tutti gli orlati di Bhanno determinante nullo, allora il rango della matrice Aè uguale all'ordine del minore B.Esempio:Invece di calcolare i minori di tutti i 16 minori di ordine 3, possiamo limitarci a considerare solo gli orlati di Bche sono 4.… Facendo i calcoli si trova che hanno tutti determinante nullo:possiamo fermarci e affermare che rk A= 203/11/20Geometria e Combinatoria sama@ing.uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#16": "03/11/2017Teorema dell’orlareEsercizio di base: Calcoliamo il rango della matrice:\nConsideriamo il minore B1formato dalla prima riga e dalla prima colonna. Ovviamente il suo determinante è diverso da 0.Proseguiamo orlando B1. . .N.B. Bisogna orlare solamente se il minore B1ha det B1≠ 0 !B1\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#17": "03/11/2018Teorema dell’orlareEsercizio di base: Calcoliamo il rango della matrice:\nQuesto minore ha determinante nullo: dobbiamo proseguire.\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#18": "03/11/2019Teorema dell’orlareEsercizio di base: Calcoliamo il rango della matrice:Anche il minore così ottenuto ha determinante nullo: dobbiamo proseguire.\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#19": "03/11/2020Teorema dell’orlareEsercizio di base: Calcoliamo il rango della matrice:Questo minore (che chiameremo B2) ha determinante nonnullo: il rango di Aè almeno 2.Ora dobbiamo proseguire considerando gli orlati di B2. . .\nN.B. Bisogna orlare solamente se il minore B2ha det B2≠ 0 !B2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#2": "03/11/203Definizione di rangoSia Auna matrice di tipo (p, q). Sia nun numero intero positivotale che n≤ ped n≤ q. Unminoredi ordine ndi Aè una matrice che si ottiene scegliendo nrighe ed ncolonne di Ae prendendo gli elementi di Ache si trovano sia sulle righe che sulle colonne scelte. Esempio:\nI minori di ordine 1di Asono ovviamente le 12 matrici a una rigae una colonna formate dai 12 elementi di A.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#20": "03/11/2021Teorema dell’orlareEsercizio di base: Calcoliamo il rango della matrice:Þotteniamo un minore che ha determinante = 0\nB2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#21": "03/11/2022Teorema dell’orlareEsercizio di base: Calcoliamo il rango della matrice:\nÞotteniamo due minori che hanno determinante = 0Þpoiché B2non ha altri orlati,concludiamo che rk A= 2B2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#22": "03/11/2023Teorema dell’orlareEsercizio: Calcoliamo il rango della matrice col teorema dell’orlare:\n•Il minore B1formato dalla prima riga e dalla prima colonna è invertibile (detB1 ≠ 0). Quindi rk A≥ 1.•Determinante degli orlati di B1 : orlando B1con la seconda riga      e la seconda colonna otteniamo un minore B2con detB2 ≠ 0 B1\nB203/11/20Geometria e Combinatoria sama@ing.uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#23": "03/11/2024Teorema dell’orlareEsercizio: Calcoliamo il rango della matrice col teorema dell’orlare:\nB2•Consideriamo un orlato di B2. Se orliamo B2con la terza riga e la terza colonna otteniamo un minore con determinante nullo:\n•Consideriamo un altro orlato di B2. Se orliamo B2con la terza riga e la quarta colonna otteniamo un minore con determinante nullo:\nB2non ha altri orlati.rk A= 203/11/20Geometria e Combinatoria sama@ing.uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#24": "Teorema di Rouchè-Capelli\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#25": "03/11/2026Sistemi di equazioni lineariDal teorema di Cramer: un sistema di nequazioni in nincogniteha una sola soluzione se la matrice dei coefficienti è invertibileStudiamo ora il caso più generale in cui abbiamo:•Sistema di pequazioni lineari a coefficienti reali in qincognite•La matrice dei coefficienti del sistema non è invertibileSfrutteremo il calcolo del rango di due particolari matrici perindividuare se un sistema ha o non ha soluzioni.Inoltre, studieremo il metodo di Rouché-Capelli per il calcolo delle eventuali soluzioni del sistema (se il sistema è risolubile).03/11/20Geometria e Combinatoria sama@ing.uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#26": "03/11/2027DefinizioniSistema di pequazioni lineari a coefficienti reali in qincognite: \nmatrice dei coefficienti del sistemamatrice completa del sistema\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#27": "03/11/2028DefinizioniSistema di pequazioni lineari a coefficienti reali in qincognite: \nmatrice colonna dei termini notimatrice colonna delle incognite\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#28": "03/11/2029DefinizioniSistema di pequazioni lineari a coefficienti reali in qincognite: \nUnasoluzione del sistema risolubile Sè una q-upla di numeri reali che sostituiti nelle equazioni del sistema alle incognite danno delle identità. Indichiamo con il simbolo Sol (S) l’insieme delle soluzionidi S.\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it29",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#29": "03/11/2030DefinizioniSistema di pequazioni lineari a coefficienti reali in qincognite: \nOgni sistema Spuò essere scritto nella forma matriciale:\nUna soluzione del sistema Ssi scrive come la matrice colonna:\nPer verifica, se X0è sostituita in Salla matrice colonna X :\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it30",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#3": "03/11/204Definizione di rangoSia Auna matrice di tipo (p, q). Sia nun numero intero positivotale che n≤ ped n≤ q. Unminoredi ordine ndi Aè una matrice che si ottiene scegliendo nrighe ed ncolonne di Ae prendendo gli elementi di Ache si trovano sia sulle righe che sulle colonne scelte. Esempio:\nVediamo i minori di ordine 2:\nun minore di ordine 203/11/20Geometria e Combinatoria sama@ing.uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#30": "03/11/2031DefinizioniUn esempio: \nLa matrice Aè invertibile. Si tratta quindi di un sistema Crameriano. \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it31",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#31": "03/11/2032DefinizioniUn esempio: \nLa matrice Aè invertibile. Si tratta quindi di un sistema Crameriano. Esso è pertanto dotato di una sola soluzione. Svolgendo i calcoli: \nUn sistema Crameriano di nequazioni in nincognite ha la matricedei coefficienti Ae la matrice completa A’ambedue di rango n.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it32",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#32": "Il sistema non ha soluzioni: La prima e terza equazione sono evidentemente incompatibili\n03/11/2033DefinizioniUn altro esempio: \nrk A= 2rk A’= 3In questo caso il teorema dell’orlare nonci avrebbe avvantaggiato,perché l’unico orlato è anche l’unico minore di ordine 3 di A’.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it33",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#33": "03/11/2034Teorema di Rouché-CapelliTeorema di Rouché-Capelli: Sia Sun sistema lineare. Sia Ala matrice dei coefficienti del sistema Se sia  A’la matrice completa del sistema.   Il sistema Sè risolubile se e solo se rk A’= rk A\nrk A= 2rk A’= 3\nrk A= 2rk A’= 2Esempi:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it34",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#34": "03/11/2035Teorema di Rouché-CapelliSia Sun sistema lineare. Sia Ala matrice dei coefficienti diS e sia A’la matrice completa del sistema. Mostrare che rk A’≥ rk A.Dimostrazione:Sia ril rango di A: allora Apossiede almeno un minore di ordine rcon determinante non nullo. Sia Bun tal minore. Bè un minore anche di A’che ha, pertanto, rango almeno r.Inoltre, nel caso in cui il sistema è non risolubile si può dimostrareche rk A’= rk A + 1. [Se il sistema è risolubile già sappiamo che rk A’= rk A.] 03/11/20Geometria e Combinatoria sama@ing.uniroma3.it35",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#35": "03/11/2036Teorema di Rouché-CapelliSia Sun sistema lineare. Sia Ala matrice dei coefficienti di Se sia A’la matrice completa del sistema. Allora si ha: rk A’≥ rk A.Esempio (1):\nrk A= 22–2–3411= 06–1–223–2461= 069–1B2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#36": "03/11/2037Teorema di Rouché-CapelliSia Sun sistema lineare. Sia Ala matrice dei coefficienti di Se sia A’la matrice completa del sistema. Allora si ha: rk A’≥ rk A.Esempio (1):\nrk A= 2\ndeterminante = 0, ultima colonna * 2 = prima colonna\ndeterminante = 0, già noto (vedi A)\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it37",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#37": "03/11/2038Teorema di Rouché-CapelliSia Sun sistema lineare. Sia Ala matrice dei coefficienti di Se sia A’la matrice completa del sistema. Allora si ha: rk A’≥ rk A.Esempio (1):\nrk A= 2\ndeterminante = 0sistema risolubile !\nrk A’=2determinante = 0, già noto (vedi A)\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it38",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#38": "03/11/2039Teorema di Rouché-CapelliSia Sun sistema lineare. Sia Ala matrice dei coefficienti di Se sia A’la matrice completa del sistema. Allora si ha: rk A’≥ rk A.Esempio (2):\nrk A= 2\n11–121–3= 032–411–121–3= 053–7Non serve calcolare gli orlati di B2con la quarta colonna, perché è identica alla prima.B2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it39",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#39": "03/11/2040Teorema di Rouché-CapelliSia Sun sistema lineare. Sia Ala matrice dei coefficienti di Se sia A’la matrice completa del sistema. Allora si ha: rk A’≥ rk A.Esempio (2):\nrk A= 2determinante = 0\ndeterminante ≠003/11/20Geometria e Combinatoria sama@ing.uniroma3.it40",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#4": "03/11/205Definizione di rangoSia Auna matrice di tipo (p, q). Sia nun numero intero positivotale che n≤ ped n≤ q. Unminoredi ordine ndi Aè una matrice che si ottiene scegliendo nrighe ed ncolonne di Ae prendendo gli elementi di Ache si trovano sia sulle righe che sulle colonne scelte. Esempio:\nVediamo i minori di ordine 2:un altro minore di ordine 2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#40": "03/11/2041Teorema di Rouché-CapelliSia Sun sistema lineare. Sia Ala matrice dei coefficienti di Se sia A’la matrice completa del sistema. Allora si ha: rk A’≥ rk A.Esempio (2):\nrk A= 2determinante = 0sistema non risolubile !\ndeterminante ≠0rk A’≥303/11/20Geometria e Combinatoria sama@ing.uniroma3.it41",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#41": "03/11/2042Procedimento di Rouché-CapelliTeorema: Sia Sun sistema lineare risolubile, e siano Ae A’la matrice dei coefficienti di Se la matrice completa di S. Sia nil rango di A(e anche il rango di A’, dato che Sè risolubile) e sia Bun minore invertibile di Adi ordine n. Allora il sistema Sè equivalente al sistema ridottoSRche siottiene considerando solo le nequazioni di Scorrispondenti alle righe di B. Dunque: Sol(S) = Sol(SR)Il teorema ci dice che, scelte in modo opportuno nequazioni di S, le altre sono “conseguenza” di queste n.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it42",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#42": "03/11/2043Procedimento di Rouché-CapelliCome risolvere un sistema lineare Sdi pequazioni in qincognite.Sia Ala matrice del sistema Se sia A’la matrice completa di S.Esempio:  \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it43",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#43": "03/11/2044Procedimento di Rouché-Capelli1. Calcoliamo il rango della matrice A(sia nquesto rango) e scegliamo un minore Bdi Adi ordine ncon determinante ≠ 0;Esempio:\nFacendo i calcoli si trova che Aha rango 2 e che un minoredi ordine 2 con determinante non nullo è, e.g., il minore B: \ndetB ≠ 0\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it44",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#44": "03/11/2045Procedimento di Rouché-Capelli2. Calcoliamo il rango della matrice A’, ovvero calcoliamoi determinanti degli orlati di Bcon l'ultima colonna di A’(cioè con la colonna dei termini noti) e con tutte le possibili righe:Esempio:\ndeterminante = 0\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it45",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#45": "03/11/2046Procedimento di Rouché-Capelli2. Calcoliamo il rango della matrice A’, ovvero calcoliamoi determinanti degli orlati di Bcon l'ultima colonna di A’(cioè con la colonna dei termini noti) e con tutte le possibili righe:2.1Se tutti gli orlati così determinati hanno determinante nullo, allora rk A = rk A’e per il teorema di Rouché-Capelli il sistema è risolubile: passiamo al punto successivo;Esempio:\ndeterminante = 003/11/20Geometria e Combinatoria sama@ing.uniroma3.it46",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#46": "03/11/2047Procedimento di Rouché-Capelli2. Calcoliamo il rango della matrice A’, ovvero calcoliamoi determinanti degli orlati di Bcon l'ultima colonna di A’(cioè con la colonna dei termini noti) e con tutte le possibili righe:2.1Se tutti gli orlati così determinati hanno determinante nullo, allora rk A = rk A’e per il teorema di Rouché-Capelli il sistema è risolubile: passiamo al punto successivo;2.2 Invece, se anche uno solo di tali orlati ha determinante ≠ 0,      il rango di A’è diverso dal rango di A(è anzi esattamente uguale a 1 + rk A): il sistema non è risolubile e ci fermiamo.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it47",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#47": "03/11/2048Procedimento di Rouché-Capelli3. Consideriamo il sistema ridotto SRformato dalle nequazioni  i cui coefficienti concorrono a formare il minore B. Il sistema ridotto SRè equivalente al sistema S;Esempio:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it48",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#48": "03/11/2049Procedimento di Rouché-Capelli4. Portiamo a secondo membro (nel sistema SR) le q–nincognite i cui coefficienti nonconcorrono a formare il minore B;Esempio:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it49",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#49": "03/11/2050Procedimento di Rouché-Capelli5. Poniamo le incognite portate a secondo membro uguali a dei parametri h1, h2, . . . , hq–n(se q= na secondo membro non c’è alcuna incognita e quindi non occorre assegnare alcun parametro);Esempio:\nal variare dei parametrih (h1)e k(h2)\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it50",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#5": "03/11/206Definizione di rangoSia Auna matrice di tipo (p, q). Sia nun numero intero positivotale che n≤ ped n≤ q. Unminoredi ordine ndi Aè una matrice che si ottiene scegliendo nrighe ed ncolonne di Ae prendendo gli elementi di Ache si trovano sia sulle righe che sulle colonne scelte. Esempio:\nVediamo i minori di ordine 2:6 minori scegliendo le prime due righe di Ain tutti i modi possibili:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#50": "03/11/2051Procedimento di Rouché-Capelli6. Risolviamo il sistema parametrico Crameriano (di matrice B)nelle incognite rimaste a primo membro: otteniamo queste incognite in funzione dei parametri h1, h2, . . . , hq–n.Esempio:\nal variare dei parametrih (h1)e k(h2)\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it51",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#51": "03/11/2052Procedimento di Rouché-Capelli\nVerifica correttezza soluzione:\nPossiamo verificare che le soluzioni così trovate sono corrette sostituendo le espressioni trovate nelle equazioni del sistema S.Sostituendo, ad esempio, nella prima equazione di Stroviamo:\nOKAttenzione: Stiamo verificando solamente che le soluzioni  trovate siano effettivamente soluzioni, ma non stiamo verificando che siano tuttele soluzioni del sistema S.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it52",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#52": "03/11/2053Procedimento di Rouché-CapelliOsservazioni: •Il procedimento dipende da quale minore Bscegliamo.Cambiando minore Botterremo le stesse soluzionima parametrizzate in forma diversa.Esempio:\nC\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it53",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#53": "03/11/2054Procedimento di Rouché-CapelliOsservazioni: •Il procedimento dipende da quale minore Bscegliamo. Cambiando minore Botterremo le stesse soluzionima parametrizzate in forma diversa.Esempio:\nC\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it54",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#54": "03/11/2055Procedimento di Rouché-CapelliOsservazioni: •Sia Sun sistema risolubile di pequazioni in qincognite. Sia nil rango della matrice del sistema. Allora le soluzioni di Sdipendono da q–nparametri.•Cosa significa che “p” = n?Se p = n, quando consideriamo il sistema ridotto SRdobbiamo prendere nequazioni, quindi tutte le equazioni di S. Pertanto tutte le equazioni di Ssono necessarie.\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it55",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#55": "03/11/2056Procedimento di Rouché-CapelliOsservazioni: •Sia Sun sistema risolubile di pequazioni in qincognite. Sia nil rango della matrice del sistema. Allora le soluzioni di Sdipendono da q–nparametri. •Cosa significa che “q” = n?Se q = nquando consideriamo il sistema ridotto SRabbiamo un sistema di nequazioni in nincognite e la matrice del sistema ridotto contiene un minore di ordine ninvertibile.  Il sistema ridotto è pertanto Crameriano.Segue che il sistema Sha un’unica soluzione.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it56",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#56": "03/11/2057Procedimento di Rouché-CapelliCome risolvere un sistema lineare Sdi pequazioni in qincognite.Sia Ala matrice del sistema Se sia A’la matrice completa di S. Esercizio (1):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it57",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#57": "03/11/2058Procedimento di Rouché-CapelliEsercizio (1):1. Calcoliamo il rango della matrice A(sia nquesto rango) e scegliamo un minore Bdi Adi ordine ncon determinante ≠ 0;\nrk A = 2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it58",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#58": "03/11/2059Procedimento di Rouché-Capelli2. Calcoliamo il rango della matrice A’, ovvero calcoliamoi determinanti degli orlati di Bcon l'ultima colonna di A’(cioè con la colonna dei termini noti) e con tutte le possibili righe;Esercizio (1):\nrk A = rk A’ = 2Il sistema ammette quindi soluzioni.Poichè il rango della matrice del sistema A’è uguale al numero delle incognite (ovvero xe y), allora possiamo affermare che il sistema ha un’unica soluzione.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it59",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#59": "03/11/2060Procedimento di Rouché-Capelli3. Consideriamo il sistema ridotto SRformato dalle nequazioni  i cui coefficienti concorrono a formare il minore B. Il sistema ridotto SRè equivalente al sistema S;Esercizio (1):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it60",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#6": "03/11/207Definizione di rangoSia Auna matrice di tipo (p, q). Sia nun numero intero positivotale che n≤ ped n≤ q. Unminoredi ordine ndi Aè una matrice che si ottiene scegliendo nrighe ed ncolonne di Ae prendendo gli elementi di Ache si trovano sia sulle righe che sulle colonne scelte. Esempio:\nVediamo i minori di ordine 2:In totale abbiamo 18 minori di ordine 2:•6 minori scegliendo le prime due righe di Ain tutti i modi possibili.•6 minori scegliendo la prima riga e l’ultima riga di Ain tutti i modi possibili.•6 minori scegliendo le ultime due righe di Ain tutti i modi possibili.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#60": "03/11/2061Procedimento di Rouché-Capelli4. Portiamo a secondo membro (nel sistema SR) le q–nincognite i cui coefficienti nonconcorrono a formare il minore B;Esercizio (1):\nQuesto è un sistema Crameriano: non dobbiamo portare a secondo membro alcuna incognita.\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it61",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#61": "03/11/2062Procedimento di Rouché-Capelli5. Poniamo le incognite portate a secondo membro uguali a dei parametri h1, h2, . . . , hq–n(se q= na secondo membro non c’è alcuna incognita e quindi non occorre assegnare alcun parametro);Esercizio (1):\nQuesto è un sistema Crameriano: non dobbiamo portare a secondo membro alcuna incognita.Abbiamo q= n, quindi assegniamo alcun parametro.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it62",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#62": "03/11/2063Procedimento di Rouché-Capelli6. Risolviamo il sistema parametrico Crameriano (di matrice B)nelle incognite rimaste a primo membro: otteniamo queste incognite in funzione dei parametri h1, h2, . . . , hq–n.Esercizio (1):\nPossiamo risolvere il sistema tramite il teorema di Cramer e trovare l'unica soluzione di S:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it63",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#63": "03/11/2064Procedimento di Rouché-CapelliCome risolvere un sistema lineare Sdi pequazioni in qincognite.Sia Ala matrice del sistema Se sia A’la matrice completa di S. Esercizio (2):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it64",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#64": "03/11/2065Procedimento di Rouché-CapelliEsercizio (2):1. Calcoliamo il rango della matrice A(sia nquesto rango) e scegliamo un minore Bdi Adi ordine ncon determinante ≠ 0;\nrk A ≥ 2Il minore B2di Aè invertibile\nrk A ≥ 3Il minore B3di Aè invertibile\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it65",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#65": "03/11/2066Procedimento di Rouché-CapelliEsercizio (2):1. Calcoliamo il rango della matrice A(sia nquesto rango) e scegliamo un minore Bdi Adi ordine ncon determinante ≠ 0;\nrk A ≥ 2Il minore B2di Aè invertibile\nrk A ≥ 3Il minore B3di Aè invertibileDobbiamo orlare B3. L'unico orlato di B3è la matrice A stessa.Svolgendo i calcoli si verifica che il det A = 0, in quanto la terzacolonna coincide con la quarta colonna. Quindi rk A= 3.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it66",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#66": "03/11/2067Procedimento di Rouché-Capelli2. Calcoliamo il rango della matrice A’, ovvero calcoliamoi determinanti degli orlati di Bcon l'ultima colonna di A’(cioè con la colonna dei termini noti) e con tutte le possibili righe;Esercizio (2):\nSi ottiene un unico possibile minore:Facendo i calcoli si trova che il minore così ottenuto ha determinante nullo. Dunque rk A’= 3 = rk A, e il sistema ha soluzioni.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it67",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#67": "03/11/2068Procedimento di Rouché-Capelli3. Consideriamo il sistema ridotto SRformato dalle nequazioni  i cui coefficienti concorrono a formare il minore B. Il sistema ridotto SRè equivalente al sistema S;Esercizio (2):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it68",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#68": "03/11/2069Procedimento di Rouché-Capelli4. Portiamo a secondo membro (nel sistema SR) le q–nincognite i cui coefficienti nonconcorrono a formare il minore B;Esercizio (2):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it69",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#69": "03/11/2070Procedimento di Rouché-Capelli5. Poniamo le incognite portate a secondo membro uguali a dei parametri h1, h2, . . . , hq–n(se q= na secondo membro non c’è alcuna incognita e quindi non occorre assegnare alcun parametro);Esercizio (2):\nSoluzioni del  sistema S al variare del parametro h\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it70",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#7": "03/11/208Definizione di rangoSia Auna matrice di tipo (p, q). Sia nun numero intero positivotale che n≤ ped n≤ q. Unminoredi ordine ndi Aè una matrice che si ottiene scegliendo nrighe ed ncolonne di Ae prendendo gli elementi di Ache si trovano sia sulle righe che sulle colonne scelte. Esempio:\nVediamo i minori di ordine 3:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#70": "03/11/2071Procedimento di Rouché-Capelli6. Risolviamo il sistema parametrico Crameriano (di matrice B)nelle incognite rimaste a primo membro: otteniamo queste incognite in funzione dei parametri h1, h2, . . . , hq–n.Esercizio (2):\nSoluzioni del  sistema S al variare di h\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it71",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#71": "03/11/2072Procedimento di Rouché-CapelliCome risolvere un sistema lineare Sdi pequazioni in qincognite.Sia Ala matrice del sistema Se sia A’la matrice completa di S. Esercizio (3):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it72",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#72": "03/11/2073Procedimento di Rouché-CapelliEsercizio (3):1. Calcoliamo il rango della matrice A(sia nquesto rango) e scegliamo un minore Bdi Adi ordine ncon determinante ≠ 0;\nrk A ≥ 2Il minore B2di Aè invertibileDobbiamo considerare gli orlati di B2fino a che eventualmente ne troviamo uno con determinante ≠ 0. Facendo i calcoli (sono 6 orlati) si trova che hanno tutti determinante = 0, dunque Aha rango 2.[ In realtà, serve calcolare solo 4 orlati di B2(quarta/quinta colonna combinata a terza/quarta riga), perché le prime due colonne di Asono identiche. ]  03/11/20Geometria e Combinatoria sama@ing.uniroma3.it73",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#73": "03/11/2074Procedimento di Rouché-Capelli2. Calcoliamo il rango della matrice A’, ovvero calcoliamoi determinanti degli orlati di Bcon l'ultima colonna di A’(cioè con la colonna dei termini noti) e con tutte le possibili righe;Esercizio (3):\nAbbiamo 2 orlati di B2(terza e quarta riga) con l’ultima colonna di A’.Entrambi gli orlati hanno determinante nullo.Pertanto A’ha rango 2 e il sistema è risolubile.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it74",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#74": "03/11/2075Procedimento di Rouché-Capelli3. Consideriamo il sistema ridotto SRformato dalle nequazioni  i cui coefficienti concorrono a formare il minore B. Il sistema ridotto SRè equivalente al sistema S;Esercizio (3):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it75",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#75": "03/11/2076Procedimento di Rouché-Capelli4. Portiamo a secondo membro (nel sistema SR) le q–nincognite i cui coefficienti nonconcorrono a formare il minore B;Esercizio (3):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it76",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#76": "03/11/2077Procedimento di Rouché-Capelli5. Poniamo le incognite portate a secondo membro uguali a dei parametri h1, h2, . . . , hq–n(se q= na secondo membro non c’è alcuna incognita e quindi non occorre assegnare alcun parametro);Esercizio (3):\nSoluzioni del  sistema S al variare di h1 , h2 , h3\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it77",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#77": "03/11/2078Procedimento di Rouché-Capelli6. Risolviamo il sistema parametrico Crameriano (di matrice B)nelle incognite rimaste a primo membro: otteniamo queste incognite in funzione dei parametri h1, h2, . . . , hq–n.Esercizio (3):\nSoluzioni del  sistema S al variare di h1 , h2 , h3Risolvendo questo sistema: \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it78",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#78": "03/11/2079Sunto: Procedimento Rouché-Capelli\n6. Risolviamo il sistema parametrico Crameriano (di matrice B) nelle incogniterimaste a primo membro: otteniamo queste incognite in funzione dei parametri h1, h2, . . . , hq–n.Sia A(A’) la matrice (completa) del sistema Sdi pequazioni e qincognite. 1. Calcoliamo il rango della matrice A(sia nquesto rango) e scegliamo unminore Bdi Adi ordine ncon determinante ≠ 0;2. Calcoliamo il rango della matrice A’, ovvero calcoliamo i determinanti degli orlati di Bcon l'ultima colonna di A’e con tutte le possibili righe;3. Serk A = rk A’: Prendiamo il sistema ridotto SRformato dalle nequazionii cui coefficienti concorrono a formare il minore B. SRè equivalente a S;4. Portiamo a secondo membro (nel sistema SR) le q–nincognite i cuicoefficienti nonconcorrono a formare il minore B;5. Poniamo le incognite portate a secondo membro uguali a dei parametri h1, h2,. . . , hq–n(se q= na secondo membro non c’è  alcuna incognita e quindi non occorre assegnare alcun parametro);03/11/20Geometria e Combinatoria sama@ing.uniroma3.it79",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#8": "03/11/209Definizione di rangoI minori di una matrice sono sempre delle matrici quadrate, di cui possiamo quindi calcolare il determinante.Una matrice Aha rango(o caratteristica) rk Auguale ad nse:1. Esiste almeno un minore di Adi ordine ncon determinante ≠ 02. Tutti i minori di Adi ordine maggioredi nhanno determinante = 0Esempio:\nTale minore è invertibile, ovvero ha determinante ≠ 0.Il rango della matrice Aè maggiore o uguale a 2.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\4_Rango_di_una_matrice.pdf#9": "03/11/2010Definizione di rangoI minori di una matrice sono sempre delle matrici quadrate, di cui possiamo quindi calcolare il determinante.Una matrice Aha rango(o caratteristica) rk Auguale ad nse:1. Esiste almeno un minore di Adi ordine ncon determinante ≠ 02. Tutti i minori di Adi ordine maggioredi nhanno determinante = 0Esempio:\nTutti i quattro minori di Adi ordine 3 non sono invertibiliIl rango della matrice Aè quindi uguale a 2 :  rk A= 203/11/20Geometria e Combinatoria sama@ing.uniroma3.it10",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#0": "03/11/20Geometria e Combinatoria sama@ing.uniroma3.it1L5: Metodo di Gauss (9-10)Argomenti lezione:•Introduzione•Metodo di Gauss•Calcolo del determinante con Gauss•Calcolo del rango con Gauss•Matrice Inversa con Gauss –Jordan  ",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#1": "03/11/202Gauss: IntroduzioneProblema affrontato: Determinazione di eventuali soluzioni di sistemi di pequazioni in qincogniteApproccio visto finora: Metodo di Rouché-CapelliIdea di Rouché-Capelli: Determinare le soluzioni di un sistema parametrizzato basato su nequazioni ed nincognite in cui la matrice dei coefficienti è invertibile. Si utilizza il metodo di Cramer basato sul calcolo del determinante di matrici di ordine n. \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#10": "03/11/2011Gauss: MetodoAttenzione: ogni matrice quadrata a scalini è una matrice triangolare\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#11": "03/11/2012Gauss: MetodoAttenzione: ogni matrice quadrata a scalini è una matrice triangolare, però:nontutte le matrici triangolari sono matrici a scalini di altezza 1 !Contro-esempio:   \nNonè una matrice a scalini come le precedenti: perché ha uno scalino di altezza 2!03/11/20Geometria e Combinatoria sama@ing.uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#12": "03/11/2013Gauss: MetodoEsempio (4): \n–1 ( x+ 2y= 4 ) \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#13": "03/11/2014Gauss: MetodoEsempio (4): \nIl sistema Sha una sola soluzione:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#14": "03/11/2015Gauss: MetodoEsempio (5): \n–2 ( x+ 2y= 1 )\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#15": "03/11/2016Gauss: MetodoEsempio (5): \nQuesto sistema non ha soluzioni \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#16": "03/11/2017Gauss: MetodoIl metodo di Gauss fa uso delle seguenti tipologie di operazioniper trovare un sistema equivalente la cui matrice sia a scalini :•Se in un sistema Ssommiamo a una equazione un’altra equazione del sistema moltiplicata per una costante,            otteniamo un sistema S’equivalente al sistema S.•Se in un sistema Smoltiplichiamo un’equazione per unacostante non nulla, otteniamo un sistema S’equivalente ad S.•Se in un sistema S scambiamo tra loro due equazioni,    otteniamo un sistema S’equivalente ad S. 03/11/20Geometria e Combinatoria sama@ing.uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#17": "03/11/2018Gauss: MetodoEsercizio (1): \n–( x+ y+ z= 1 )\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#18": "03/11/2019Gauss: MetodoEsercizio (1):\n–( x+ y+ z= 1 )\n–( x+ y+ z= 1 )\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#19": "03/11/2020Gauss: MetodoEsercizio (1):\n–( x+ y+ z= 1 )\nIl sistema Sha    una sola soluzione:–( x+ y+ z= 1 )\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#2": "03/11/203Gauss: IntroduzioneLimiti del metodo visto finora: Il numero di operazioni necessarie per calcolare il determinante di una matrice di ordine naumenta molto velocemente al crescere di n. Applicazioni pratiche: Risolvere sistemi con alcune decine di equazioni e incognite. Metodo di Rouché-Capelli è poco efficiente.Metodo alternativo per un sistema di pequazioni in qincognite:Metodo di Gauss: rimpiazzareil sistema di cui si cercano le eventuali soluzioni con un sistema avente le stesse soluzioni         per il quale sia abbastanza agevole la loro determinazione.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#20": "03/11/2021Gauss: MetodoEsercizio (2):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#21": "03/11/2022Gauss: MetodoEsercizio (2):\n–1–2\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#22": "03/11/2023Gauss: MetodoEsercizio (2):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#23": "03/11/2024Gauss: MetodoEsercizio (2) -NOTE:\nHo due colonne uguali, già so che non ho una sola soluzioneInoltre se moltiplico per -2 la prima colonna e la sommo alla seconda, ottengo la terza 03/11/20Geometria e Combinatoria sama@ing.uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#24": "03/11/2025Operazioni elementariDue matrici Ae A’si dicono equivalenti per rigase è possibilepassare da una all’altra per mezzo di operazioni del tipo:•Sommare alla riga i-esima della matrice la riga j-esima moltiplicata per un numero reale k, con i≠j.•Scambiare tra loro due righe della matrice.Questi tipi di operazioni si dicono operazioni elementari di riga.Esempio:\n–2 (prima riga)\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#25": "03/11/2026Operazioni elementariDue matrici Ae A’si dicono equivalenti per rigase è possibilepassare da una all’altra per mezzo di operazioni del tipo:•Sommare alla riga i-esima della matrice la riga j-esima moltiplicata per un numero reale k, con i≠j.•Scambiare tra loro due righe della matrice.Questi tipi di operazioni si dicono operazioni elementari di riga.Esempio:\nscambio di righe\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#26": "03/11/2027Operazioni elementariDue matrici Ae A’equivalenti per riga soddisfano le proprietà:Proprietà riflessiva Ogni matrice Aè equivalente per riga a se stessa.(Infatti per passare daA ad A non è necessaria nessuna operazione.)Proprietà simmetrica Se una matrice Aè equivalente per riga a unamatrice A’allora A’è equivalente ad A.(Basta infatti ripercorre a ritroso ciascuna operazione elementare.)Proprietà transitiva Se una matrice Aè equivalente per riga a unamatrice A’e se A’è equivalente per riga a una matrice A’’ allora Aè equivalente a A’’.(Se passiamo da Aad A’e poi da A’ad A’’ siamo passati da Aad A’’.)03/11/20Geometria e Combinatoria sama@ing.uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#27": "03/11/2028Calcolo del determinanteAbbiamo dimostrato in precedenza la seguente proprietà: Siano Ae Bdue matrici quadrate di ordine nche si ottengonouna dall’altra scambiandofra loro due righe. Allora detA= − detB.Si può inoltre dimostrare che:Sia Auna matrice quadrata e sia A’la matrice ottenuta da Asommando alla riga i-esima la riga j-esima moltiplicata per k.Allora detA’ = detA.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#28": "03/11/2029Calcolo del determinanteSegue la seguente proprietà:Se Ae A’sono matrici quadrate equivalenti per riga, analizziamole operazioni elementari necessarie per passare da Aad A’ . Tra le operazioni elementari ci saranno:•un certo numero di operazioni di sommaa una riga di un’altra riga moltiplicata per un fattore (il determinante non cambia);•un certo numero mdi operazioni di scambiodi righe. Per lo scambio di righe (m> 0), si ha:Se nonsi ha scambio di righe (m= 0), allora det A’ = det A . \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it29",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#29": "03/11/2030Calcolo del determinantePer calcolare il determinante di una matrice quadrata A, operiamo nel seguente modo:1. Determiniamo, con il metodo di Gauss, una matrice a scalini A’equivalente per righe ad A, e contiamo il numero di scambi di riga che abbiamo operato per far ciò. Sia mquesto numero.Esempio:\nm = 1–1\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it30",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#3": "03/11/204Gauss: Esempio 1\nDeterminiamo x4dalla quarta equazione:     x4= 0Determiniamo x3dalla terza equazione:       x3= 1 / 4 Determiniamo x2dalla seconda equazione:  x2= 1 / 6 Determiniamo x1dalla prima equazione:      x1= –1 / 12 \nUnica soluzione di S: 03/11/20Geometria e Combinatoria sama@ing.uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#30": "03/11/2031Calcolo del determinantePer calcolare il determinante di una matrice quadrata A, operiamo nel seguente modo:2. Calcoliamo il determinante di A’semplicemente moltiplicando gli elementi della sua diagonale principale (dato che una matrice quadrata a scalini è triangolare superiore). Notare che detA’= 0se e solo se almeno uno di questi elementi si annulla.Esempio:\ndetA’= 503/11/20Geometria e Combinatoria sama@ing.uniroma3.it31",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#31": "03/11/2032Calcolo del determinantePer calcolare il determinante di una matrice quadrata A, operiamo nel seguente modo:3. Si ha quindi:Esempio:\nm = 1detA’= 5\n–1\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it32",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#32": "03/11/2033Calcolo del determinanteEsercizio (1):\nm = 2m = 1\n–(seconda riga)\ndet A = det A’ = 0\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it33",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#33": "03/11/2034Calcolo del rangoTeorema: Se Ae A’sono matrici equivalenti per riga, allora esse hanno ranghi uguali. In formule si ha:  rkA’  =  rkA .  (Notare che le matrici Ae A’possono anche essere non quadrate)Ci conviene trasformare A in A’ tramite operazioni elementari,        e poi calcolare il rango di A’  (ovvero di una matrice a scalini) ?Teorema: Il rango di una matrice a scaliniè uguale al numero degli scalini (cioè il numero di righe non nulle) della matrice.03/11/20Geometria e Combinatoria sama@ing.uniroma3.it34",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#34": "03/11/2035Calcolo del rangoTeorema: Il rango di una matrice a scaliniè uguale al numero degli scalini (cioè il numero di righe non nulle) della matrice.Esempio:\nLa matrice Aha 3 scalini\nPrendiamo il minore di Aformato dalle righe non nulle di Ae dalle tre colonne contenenti gli scalini03/11/20Geometria e Combinatoria sama@ing.uniroma3.it35",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#35": "03/11/2036Calcolo del rangoTeorema: Il rango di una matrice a scaliniè uguale al numero degli scalini (cioè il numero di righe non nulle) della matrice.Esempio:\nLa matrice Aha 3 scalini\nMatrice triangolare superiore, con det ≠ 0. Da cui: rk A≥ 3Ogni minore di Adi ordine 4 deve avere una riga tutta di 0          e ha pertanto determinante nullo. Segue che rk A= 3. 03/11/20Geometria e Combinatoria sama@ing.uniroma3.it36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#36": "03/11/2037Calcolo del rangoData una matrice A, ne calcoliamo il rango nel seguente modo:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it37",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#37": "03/11/2038Calcolo del rangoData una matrice A, ne calcoliamo il rango nel seguente modo:1. Determiniamo, con il metodo di Gauss, una matrice a scalini A’equivalente per righe ad A.Esempio:\n–2 1\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it38",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#38": "03/11/2039Calcolo del rangoData una matrice A, ne calcoliamo il rango nel seguente modo:1. Determiniamo, con il metodo di Gauss, una matrice a scalini A’equivalente per righe ad A.Esempio:\n2–1\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it39",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#39": "03/11/2040Calcolo del rangoData una matrice A, ne calcoliamo il rango nel seguente modo:1. Determiniamo, con il metodo di Gauss, una matrice a scalini A’equivalente per righe ad A.Esempio:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it40",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#4": "03/11/205Gauss: Esempio 2\nRicaviamo dalla terza equazione:            x3= 2 –x4Sostituiamo x3nella seconda equazione:     x2= –1Sostituiamo x2e x3nella prima equazione:   x1= 1 Assegniamo a x4il valore di un parametro hSoluzioni del sistema S: \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#40": "03/11/2041Calcolo del rangoData una matrice A, ne calcoliamo il rango nel seguente modo:2. Contiamo il numero di scalini di A’. Siano n . Si ha allora:rkA = rkA’ = nEsempio:\nrk A = rk A’’’  = 303/11/20Geometria e Combinatoria sama@ing.uniroma3.it41",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#41": "03/11/20Geometria e Combinatoria sama@ing.uniroma3.it42\nCalcolo del rangoEsercizio:\n-2-2-3\n-2-1rk = 4det = 36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#42": "03/11/2043Risolvere il sistemaEsercizio (2):\nscambio di righe\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it43",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#43": "03/11/2044Risolvere il sistemaEsercizio (2):\n1 (prima equ.)\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it44",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#44": "03/11/2045Risolvere il sistemaEsercizio (2):\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it45",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#45": "03/11/2046Risolvere il sistemaEsercizio (2):\n–1 (terza equ.)\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it46",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#46": "03/11/2047Risolvere il sistemaEsercizio (2):\nLe soluzioni del   sistema sono:\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it47",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#47": "03/11/2048Rango e determinanteEsercizio (3):\n–3/2 2–4\n–8\nA’’’ha 4 scalinirkA = rkA’’’  = 42. Contiamo il numero di scalini di A’. Siano n. Si ha allora: rkA = rkA’ = n–2 / 25detA = -4803/11/20Geometria e Combinatoria sama@ing.uniroma3.it48",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#48": "Anche l’inversa di una matrice può essere calcolata sfruttando il metodo di eliminazione di Gauss.𝐴𝐼⇒[𝐼|𝐴!\"]1.Si scrive una matrice a blocchi 𝑛×2𝑛𝐴𝐼;2.Si opera Gauss fino ad ottenere nel primo blocco I;3.La matrice nel secondo blocco è 𝐴!\".03/11/20Geometria e Combinatoria sama@ing.uniroma3.it49Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#49": "[𝐴|𝐼]=𝑎\"\"𝑎\"#𝑎#\"𝑎##…𝑎\"$…𝑎#$⋮⋮𝑎$\"𝑎$#⋱⋮…𝑎$$1001…0…0⋮⋮00⋱⋮…1𝑎′\"\"𝑎′\"#0𝑎′##…𝑎′\"$…𝑎′#$⋮⋮00⋱⋮…𝑎′$$𝛾′\"\"𝛾′\"#𝛾′#\"𝛾′##…𝛾′\"$…𝛾′#$⋮⋮𝛾′$\"𝛾′$#⋱⋮…𝛾′$$03/11/20Geometria e Combinatoria sama@ing.uniroma3.it50Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#5": "03/11/206Gauss: Esempio 3\nDalla seconda equazione ricaviamo:      x2= 2 –x3Sostituendo x2nella prima equazione:  x1= –1 + 2x3–x4Assegniamo a x3il valore di un parametro h1e assegniamo a x4il valore di un parametro h2Soluzioni del sistema S: \n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#50": "[𝐴|𝐼]=𝑎\"\"𝑎\"#𝑎#\"𝑎##…𝑎\"$…𝑎#$⋮⋮𝑎$\"𝑎$#⋱⋮…𝑎$$1001…0…0⋮⋮00⋱⋮…1𝑎′\"\"𝑎′\"#0𝑎′##…𝑎′\"$…𝑎′#$⋮⋮00⋱⋮…𝑎′$$𝛾′\"\"𝛾′\"#𝛾′#\"𝛾′##…𝛾′\"$…𝛾′#$⋮⋮𝛾′$\"𝛾′$#⋱⋮…𝛾′$$03/11/20Geometria e Combinatoria sama@ing.uniroma3.it51Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#51": "𝑎′\"\"𝑎′\"#0𝑎′##…0…0⋮⋮00⋱⋮…𝑎′$$𝛾′′\"\"𝛾′′\"#𝛾′′#\"𝛾′′##…𝛾′′\"$…𝛾′′#$⋮⋮𝛾′$\"𝛾′$#⋱⋮…𝛾′$$𝛾′′%&=𝛾′%&−'(!\")(#!)(!!03/11/20Geometria e Combinatoria sama@ing.uniroma3.it52Matrice Inversa\n−𝑎!\"#𝑎!##",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#52": "𝑎′\"\"𝑎′\"#0𝑎′##…0…0⋮⋮00⋱⋮…𝑎′$$𝛾\"\"((…(𝛾\"#((…(𝛾#\"((…(𝛾##((…(…𝛾\"$((…(…𝛾#$((…(⋮⋮𝛾′$\"𝛾′$#⋱⋮…𝛾′$$\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it53Matrice Inversa−𝑎!$%𝑎!%%",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#53": "𝑎′\"\"00𝑎′##…0…0⋮⋮00⋱⋮…𝑎′$$𝛾\"\"((…(𝛾\"#((…(𝛾#\"((…(𝛾##((…(…𝛾\"$((…(…𝛾#$((…(⋮⋮𝛾′$\"𝛾′$#⋱⋮…𝛾′$$\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it54Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#54": "𝑎′\"\"00𝑎′##…0…0⋮⋮00⋱⋮…1𝛾\"\"((…(𝛾\"#((…(𝛾#\"((…(𝛾##((…(…𝛾\"$((…(…𝛾#$((…(⋮⋮'$!%)!!%'#!%)!!%⋱⋮…'!!%)!!%\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it55Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#55": "1001…0…0⋮⋮00⋱⋮…1'$$%…%)$$%'$#%…%)$$%'#$%…%)##%'##%…%)##%…'$!%…%)$$%…'#!%…%)##%⋮⋮'$!%)!!%'#!%)!!%⋱⋮…'!!%)!!%\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it56Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#56": "𝐴!\"='$$%…%)$$%'$#%…%)$$%'#$%…%)##%'##%…%)##%…'$!%…%)$$%…'#!%…%)##%⋮⋮'$!%)!!%'#!%)!!%⋱⋮…'!!%)!!%\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it57Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#57": "Esempio:𝐴=4000002001112001\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it58Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#58": "Esempio:𝐴=1001000000001001\"+00−100100\"#!\"+100−11Divido la seconda riga per 203/11/20Geometria e Combinatoria sama@ing.uniroma3.it59Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#59": "Esempio:𝐴!\"=\"+00−100100\"#!\"+100−11\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it60Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#6": "03/11/207Gauss: Esempi 1, 2, 3\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#60": "Esercizio:𝐴=4221−110−1−2210110−11001000000001001\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it61Matrice Inversa\n+2-2-4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#61": "Esercizio:𝐴=10010−10102021−1−150001010−20010120−4\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it62Matrice Inversa\n+2-2-4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#62": "Esercizio:𝐴=10010−10102021−1−150001010−20010120−4\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it63Matrice Inversa-2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#63": "Esercizio:𝐴=10010−10100001−3−130001010−20−21−21600\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it64Matrice Inversa-2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#64": "Esercizio:𝐴=10010−10100001−3−130001010−20−21−21600\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it65Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#65": "Esercizio:𝐴=10010−10100001−3000001010−20−21−41616detA = 0 A non è invertibile03/11/20Geometria e Combinatoria sama@ing.uniroma3.it66Matrice Inversa",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#7": "03/11/208Gauss: Esempi 1, 2, 3\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it8",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#8": "03/11/209Gauss: Esempi 1, 2, 3\nSono tutte matrici a scalini :quattro scalini di altezza 1(nessun parametro)tre scalini di altezza 1(1 parametro: x4 = h1)due scalini di altezza 1(2 parametri: x3 = h1 ; x4 = h2)x1x2x3x4\n03/11/20Geometria e Combinatoria sama@ing.uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\5_Gauss.pdf#9": "03/11/2010Gauss: MetodoIl metodo di Gauss per la determinazione delle eventualisoluzioni di un sistema Sconsiste nel trasformare il sistema Sin un sistema S’ad esso equivalente avente la matrice dei coefficienti a scalini. Ecco alcune matrici a scalini ( ●è un num. ≠ 0; * num. qualsiasi):\nSistemaparametrico( x4= h1 )x4Sistemacon unicaSoluzione(se esiste)Sistemaparametrico( x2= h1 )x2Sistemaparametrico(x1= h1  ;x4= h2 )x1x403/11/20Geometria e Combinatoria sama@ing.uniroma3.it10",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#0": "Geometria e Combinatoria  marcella.sama@uniroma3.itL6: I vettori geometrici (11)Argomenti lezione:•Introduzione •Vettori del piano  •Addizione di vettori •Moltiplicare un vettore per uno scalare •Vettori dello spazio •Rette e piani per l’origine •Punto medio",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#1": "23/11/202IntroduzioneIntroduciamo il concetto di vettore di un piano. Definiamo le operazionidi addizione di due vettori e di moltiplicazione di un vettore per un numero reale.Notiamo che hanno le stesse proprietàdelle operazioni di addizione tra matrici e di moltiplicazione di una matrice per un numero reale. Estendiamo poi allo spaziole definizioni di vettore e delle loro operazioni e verifichiamo che queste ultime hanno proprietà simili alle proprietà dei vettori del piano. Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#10": "23/11/2011Addizione di vettoriProprietà del parallelogramma:1. In un parallelogramma i lati opposti hanno la stessa lunghezza2. Le diagonali di un parallelogramma si intersecano in un punto   Mche è detto il punto medio dei vertici opposti.\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#11": "23/11/2012Addizione di vettoriTorniamo ancora sulla definizione di parallelogramma.A partire da tre punti O, Ae B, cerchiamo il punto C \nDeterminiamo il punto medio Mdei punti Ae B.Determiniamo il punto Csimmetrico del punto Orispetto al punto medio M.\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#12": "23/11/2013Addizione di vettoriTorniamo ancora sulla definizione di parallelogramma.A partire da tre punti O, Ae B, cerchiamo il punto C Determiniamo il punto medio Mdei punti Ae B.Determiniamo il punto Csimmetrico del punto Orispetto al punto medio M.Il punto Cè allineato ai punti O, Ae B.\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#13": "23/11/2014Addizione di vettoriDefinizione: Dati quattro punti qualsiasi A, B, C, D, diciamoche essi formano un parallelogrammase il punto Cè il simmetrico del punto Arispetto al punto Mmedio di Be D.Se i punti A, B, Dnon sono allineati, otteniamo un parallelogramma nondegenereDA\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#14": "DBC23/11/2015Addizione di vettoriDefinizione: Dati quattro punti qualsiasi A, B, C, D, diciamoche essi formano un parallelogrammase il punto Cè il simmetrico del punto Arispetto al punto Mmedio di Be D.Se i punti A, B, Dsono allineati, otteniamo un parallelogramma degenere\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#15": "23/11/2016Addizione di vettoriRegola del parallelogramma: Dati i vettori v:= OAe w:= OB, poniamo per definizione v+ w:= OC, dove Cè l’unico punto del pianotale che OACBsia un parallelogramma.Cè il simmetrico di Orispetto al punto medio Mdi Ae B.→→→\nMMGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#16": "23/11/2017Addizione di vettoriTeorema: L’operazione di addizione tra vettori soddisfa:1.Proprietà associativa:Esempio: \nv+wu+v(u+v)+wu+(v+w)\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#17": "23/11/2018Addizione di vettoriTeorema: L’operazione di addizione tra vettori soddisfa:1.Proprietà associativa:2.Proprietà commutativa:3.Esistenza dello zero:\nO = BIl simmetrico (C) di Orispetto a Mè il punto Astesso.Segue: \nMP = A = C\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#18": "23/11/2019Addizione di vettoriTeorema: L’operazione di addizione tra vettori soddisfa:1.Proprietà associativa:2.Proprietà commutativa:3.Esistenza dello zero:4.Esistenza dell'opposto:\nIl vettoreOB è unico!→Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#19": "23/11/2020Moltiplicare un vettore per uno scalareDato un vettore v:= OPe uno scalare hinR, definiamo il vettore hvinV 2(O).Se v= 0 poniamo: h0 := 0 qualsiasi sia hin R.Se v≠ 0 i punti Oe P, sono distinti. Il punto Odelimita le due semirette r1(passante per P) e r2.   Poniamo: →\nOPr1r2\n•se h= 0 allora Q= O;•se h> 0 allora Qè il punto di r1tale che d(O, Q) = hd(O, P);•se h < 0 allora Qè il punto di r2tale che d(O, Q) = –hd(O, P).Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#2": "23/11/203Vettori del pianoConsideriamo un piano e fissiamo su esso una distanza con unità di misura U1U2. Fissiamo poi una volta per tutte un punto O del piano, che chiamiamo origine.Definizioni: Dato comunque un punto P, chiamiamo vettore applicatoin Odi verticePla coppia di punti Oe P. Indichiamo questo vettore con il simbolo OP. Diremo anche che il vettore OPha come origine il punto O.         →→OPGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#20": "23/11/2021Moltiplicare un vettore per uno scalareTeorema: L'operazione di moltiplicazione di un vettore per uno scalare verifica le proprietà:1 v= vper ogni vin V 2(O) Dimostrazione: Dato v:=OA, vogliamo determinare 1v. Per definizione di moltiplicazione di un vettore per uno scalare: se v= 0, si ha 1v= 0 e quindi 1v= v;se v= OA≠ 0, si ha O≠ A.r1è la semiretta delimitata da Oe passante per A. Il vettore 1vè uguale al vettore OB,dove B (= A) è il punto di r1tale che d(O, B) = 1 d(O, A). Quindi 1v= v.→→OA = Br1→Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#21": "OO23/11/2022Moltiplicare un vettore per uno scalareTeorema: L'operazione di moltiplicazione di un vettore per uno scalare verifica le proprietà:1 v= vper ogni vin V 2(O) h (k v) = (h k) vper ogni vin V 2(O), hin R, kin REsempio:PvP’P’’PvP’’kvh (kv)(h k) v\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#22": "OP = h (v1 + v2 )\n23/11/2023Moltiplicare un vettore per uno scalareTeorema: Le operazioni di addizione tra vettori e di moltiplicazione di un vettore per uno scalare verificano le seguenti proprietà,     dette proprietà distributive:\nEsempio:v1+v2OP’1  = h v1OP’2  = h v2→→OP= h v1+ h v2→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#23": "23/11/2024Moltiplicare un vettore per uno scalareEsercizio:  Verificare che   (–1) v= –vper ogni vin V 2(O)Dimostrazione: Dato v:= OAe dato –v := OB. Per definizione di moltiplicazione di un vettore per uno scalare: se v= 0, si ha –1v= 0. Inoltre –v = 0. Segue –1v= 0 = –v;se v= OA≠ 0, si ha O≠ A.r2è la semiretta delimitata da Oe non passante per A. Il vettore –1vè uguale al vettore OC,dove Cè il punto di r2tale che d(O, C) = | –1| d(O, A) = d(O, A). Quindi C= B. Segue –1v= –v.→→\nr2→= CGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#24": "23/11/2025Vettori dello spazioL’insieme V 3(O) dei vettori dello spazio con origine in O verificale seguenti proprietà di addizione tra vettori e di moltiplicazionedi un vettore per uno scalare [in modo analogo a V 2(O)]:1. Proprietà associativa:2. Proprietà commutativa:3. Esistenza dello zero:4. Esistenza dell'opposto: \nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#25": "23/11/2026Vettori dello spazioL’insieme V 3(O) dei vettori dello spazio con origine in O verificale seguenti proprietà di addizione tra vettori e di moltiplicazione di un vettore per uno scalare [in modo analogo a V 2(O)]:\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#26": "23/11/2027Rette e piani per l’origineOsservazioni:Se rè una retta passante per l'origine, Ae Bsono due punti di rallora il termine Cdel vettore OA+ OBè un punto di r.→→\nMrGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#27": "23/11/2028Rette e piani per l’origineOsservazioni:Se rè una retta passante per l'origine, Ae Bsono due punti di rallora il termine Cdel vettore OA+ OBè un punto di r.Se rè una retta passante per l'origine, Aè un punto di rallora il termine Ddel vettore k OA è un punto di r.→→→\nOADk OAr→OA→Esempio:\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#28": "23/11/2029Rette e piani per l’origineOsservazioni:Se rè una retta passante per l'origine, Ae Bsono due punti di rallora il termine Cdel vettore OA+ OBè un punto di r.Se rè una retta passante per l'origine, Aè un punto di rallora il termine Ddel vettore k OA è un punto di r.Analogamente:Se πè un piano passante per l'origine, Ae Bsono due punti di πallora il termine Cdel vettore OA+ OBè un punto di π.Se πè un piano passante per l'origine, Aè un punto di πallora il termine Ddel vettore k OA è un punto di π.→→→→→→Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#29": "23/11/2030Rette e piani per l’origineEsercizio: Sia runa retta non passante per l'origine. Siano Ae Bdue punti di r.Domanda 1: Il vettore OA+ OBha termine sulla retta r ? Il punto medio Mtra Ae Bappartiene alla retta r. Il simmetrico Cdi Orispetto a Msta sulla retta spassante per Oe M.Dunque Cnon appartiene alla retta r, perché l’unico punto in comune tra le rette re sè il punto M.→→s\nrGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#3": "23/11/204Vettori del pianoConsideriamo un piano e fissiamo su esso una distanza con unità di misura U1U2. Fissiamo poi una volta per tutte un punto O del piano, che chiamiamo origine.Definizioni: Dato comunque un punto P, chiamiamo vettore applicatoin Odi verticePla coppia di punti Oe P. Indichiamo questo vettore con il simbolo OP. Diremo anche che il vettore OPha come origine il punto O. Il simbolo V 2(O) indica l'insieme dei vettori applicati in O. Il vettore OOviene chiamato vettore nullo e indicato con il simbolo 0. Abbiamo quindi 0 = OO.→→→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#30": "23/11/2031Rette e piani per l’origineEsercizio: Sia runa retta non passante per l'origine. Siano Ae Bdue punti di r.Domanda 2: Se kè uno scalare, il vettore k OA ha termine su r ?→\ntrO ≠ Aperché Onon appartiene a r, Aappartiene a r.k OA è un punto della retta tpassante per Oe A:•per k= 1 si ottiene il vettore OAstesso;•per k≠ 1 si ottiene un vettore il cui termine è un punto di tdiverso da Ae, quindi, non appartenente a r.L’unico punto in comune tra re tè A.→→B\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#31": "23/11/2032Punto medioTeorema: Dati due punti Ae Bdel piano o dello spazio,                   il loro punto medio Mè dato dalla formula: Dimostrazione: Sia Cil punto tale che OC= OA + OB. Dobbiamo mostrare che OM= 1/2 OC.Sappiamo che il punto Cè il simmetrico di Orispetto al punto M. \n→→→→→\nMMGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#32": "23/11/2033Punto medioTeorema: Dati due punti Ae Bdel piano o dello spazio,                   il loro punto medio Mè dato dalla formula: Dimostrazione: Sia Cil punto tale che OC= OA + OB. Dobbiamo mostrare che OM= 1/2 OC.Sappiamo che il punto Cè il simmetrico di Orispetto al punto M. Distinguiamo due casi: •Se Mcoincide conO, allora Ccoincide conO, e OM = OC = 0. •Se Mnoncoincide con O, detta rla retta passante per Oe M,    per la definizione di prodotto scalare vettore, il termine del vettore2OMè esattamente il simmetrico di Mrispetto ad O:  2OM= OC. \n→→→→→→→→→→Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#4": "23/11/205Vettori del pianoEsempio:\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#5": "23/11/206Addizione di vettoriIntroduciamo in V 2(O) un’operazione di addizione di vettori:dati due vettori ve w, il vettore somma è definito come v+ w.Regola del parallelogramma: Dati i vettori v:= OAe w:= OB, poniamo per definizione v+ w:= OC, dove Cè l’unico punto del pianotale che OACBsia un parallelogramma.→→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#6": "23/11/207Addizione di vettoriIntroduciamo in V 2(O) un’operazione di addizione di vettori:dati due vettori ve w, il vettore somma è definito come v+ w.Regola del parallelogramma: Dati i vettori v:= OAe w:= OB, poniamo per definizione v+ w:= OC, dove Cè l’unico punto del pianotale che OACBsia un parallelogramma.Come si procede nel seguente caso? →→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#7": "23/11/208Addizione di vettoriTorniamo sulla definizione di parallelogramma.A partire da tre punti O, Ae B, cerchiamo il punto C Sia rla retta passante per Oe A.Sia sla retta passante per Oe B. Determiniamo la retta r’passante     per Be parallela alla retta r.Determiniamo la retta s’passante     per Ae parallela alla retta s.Determiniamo il punto Ccome intersezionedelle rette r’e s’.Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#8": "23/11/209Addizione di vettoriTorniamo sulla definizione di parallelogramma.A partire da tre punti O, Ae B, cerchiamo il punto C Osservazioni:Se O= Ala retta rè indeterminata Se O= Bla retta sè indeterminataSe le rette re ssono determinate (O≠ Ae O≠ B) ma coincidono,allora non possiamo determinare l’ intersezione tra le rette r’ed s’,  e quindi il punto C.Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\6_Vettori.pdf#9": "23/11/2010Addizione di vettoriTorniamo sulla definizione di parallelogramma.A partire da tre punti O, Ae B, cerchiamo il punto C Osservazioni:Se O= Ala retta rè indeterminata Se O= Bla retta sè indeterminataSe le rette re ssono determinate (O≠ Ae O≠ B) ma coincidono,allora non possiamo determinare l’ intersezione tra le rette r’ed s’,  e quindi il punto C.Ci si trova in questa situazione, quando i punti O, A, e Bsono allineati.\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#0": "Geometria e Combinatoria  marcella.sama@uniroma3.itL7: Combinazioni lineari di vettori geometrici (12)Argomenti lezione:•Introduzione•Combinazione lineare di vettori•Vettori linearmente indipendenti",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#1": "23/11/202Introduzione•Introduciamo il concetto di combinazione lineare di vettori(del piano o dello spazio) e di vettori linearmente indipendenti.•Dimostriamo poi che nel pianoesistono coppie di vettori linearmente indipendenti, ma non esistono più di due vettori linearmente indipendenti.•Si dimostra infine che nello spazioesistono terne di vettori linearmente indipendenti, ma non esistono più di tre vettori linearmente indipendenti.Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#10": "23/11/2011Vettori linearmente indipendentiTeorema: Dati comunque 2 vettori linearmente indipendentidi V 2(O), ogni vettore di V 2(O) è loro combinazione lineare.In altre parole: se v1:= OP1e v2:= OP2sono vettori di V 2(O) linearmente indipendenti, allora per ogni v:= OPin V 2(O),  esistono h1in R e h2in Rtali che:    v= h1v1+ h2v2.→→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#11": "23/11/2012Vettori linearmente indipendentiTeorema: Dati comunque 2 vettori linearmente indipendentidi V 2(O), ogni vettore di V 2(O) è loro combinazione lineare.Dimostrazione: Poniamo v1:= OP1, v2:= OP2, v:= OP. Poiché i vettori v1e v2sono linearmente indipendenti, i tre punti O, P1e P2nonsono allineati (vedi teorema precedente). OP’1PP’2è  un parallelogramma.→→→\nEsiste h1in Rtale che OP’1= h1v1Esiste h2in Rtale che OP’2= h2v2\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#12": "23/11/2013Vettori linearmente indipendentiTeorema: Dati comunque 3 vettori di V 2(O), essi sono linearmente dipendenti.Dimostrazione:Siano v1, v2e v3i tre vettori. Distinguiamo due casi:•Supponiamo v1e v2sono linearmente dipendenti. Allora esistono h1e h2non entrambi nullitali che h1v1 + h2v2= 0. Segue h1v1+ h2v2+ 0v3= 0 e i tre vettori sono lin. dipendenti;•Supponiamo v1e v2sono linearmente indipendenti. Allora esistono due numeri reali h1e h2tali che v3= h1v1 + h2v2Segue h1v1 + h2v2 + (–1)v3= 0 e i tre vettori sono lin. dipendenti.Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#13": "23/11/2014Vettori linearmente indipendentiTeorema: Dati comunque nvettori di V 2(O) conn≥3, essi sono linearmente dipendenti.Dimostrazione:Dobbiamo dimostrare che in V 2(O) non esistono più di due vettori che siano tra loro linearmente indipendenti.Sappiamo che 3 vettori in V 2(O) sono tra loro lin. dipendenti.Esistono quindi h1, h2e h3non tutti nullitali che:h1v1+ h2v2+ h3v3= 0Ora siano dati nvettori v1, v2, v3, v4, …, vn, con n> 3.Allora si ha: h1v1+ h2v2+ h3v3+ 0v4+ 0v5+ ... + 0vn= 0.Pertanto gli nvettori sono linearmente dipendenti. Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#14": "23/11/2015Vettori linearmente indipendentiNel caso di vettori dello spaziosi dimostrano i seguenti teoremi:Teorema: Siano v1:= OP1, v2:= OP2due vettori di V 3(O) linearmente indipendenti. L’insieme delle loro combinazioni lineari è formato da tutti e soli i vettori OPtali che il punto Pappartenga al piano passante per i punti non allineatiO, P1e P2.Teorema: Tre vettori v1:= OP1, v2:= OP2e v3:= OP3di V 3(O) sono linearmente dipendentise e solo se i punti O, P1, P2e P3sono complanari.→→→→→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#15": "23/11/2016Vettori linearmente indipendentiNel caso di vettori dello spaziosi dimostrano i seguenti teoremi:Teorema: Dati tre vettori linearmente indipendentidi V 3(O), ogni vettore di V 3(O) è loro combinazione lineare. In altre parole: se v1:= OP1, v2:= OP2e v3:= OP3sono vettori di V 3(O) linearmente indipendenti, allora per ogni v:= OPin V 3(O), esistono h1in R, h2in Re h3in Rtali che:v= h1 v1+ h2 v2+ h3 v3.Teorema: Dati comunque n≥ 4 vettori di V 3(O), essi sono linearmente dipendenti.→→→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#2": "23/11/203Combinazione lineare di vettoriSia nel piano che nello spazio, si ha la seguente definizione. Dati nvettori v1, v2, ... , vn, e dati nnumeri reali a1, a2, ... , an,il vettore v:= ∑!\"#$𝑎!𝑣!viene chiamato combinazione linearedei vettori v1, v2, ... , vncon coefficientia1, a2, ... , an. Esempio:\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#3": "23/11/204Combinazione lineare di vettoriDati comunque nvettori v1, v2, ... , vn, si ha: ∑!\"#$0𝑣!= 0Dimostrazione:Per definizione, abbiamo 0v= 0 per ogni vettore v. Pertanto 0v1+ 0v2+ ... + 0vn= 0 + 0 + ... + 0 = 0L’ultima uguaglianza si ottiene applicando ripetutamente la proprietà 0 + 0 = 0, caso particolare di v+ 0 = v con v= 0.\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#4": "23/11/205Vettori linearmente indipendentiSia nel piano che nello spazio, si ha la seguente definizione.Dati nvettori v1, v2, ... , vn, essi si dicono linearmentedipendentise esistono a1, a2, ... , ancoefficienti non tutti nullitali che ∑!\"#$𝑎!𝑣!= 0Dati nvettori v1, v2, ... , vn, essi si dicono linearmente indipendenti se l’uguaglianza ∑!\"#$𝑎!𝑣!= 0è verificata solamentenel caso in cui a1= a2= ... = an= 0. L’unica loro combinazione lineare uguale al vettore nullo è la combinazione lineare con tutti i coefficienti nulli.Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#5": "23/11/206Vettori linearmente indipendentiAnalizziamo il significato geometrico di indipendenza lineare.Teorema: Un vettore v1è linearmente dipendentese e solo se v1= 0.Dimostrazione:Supponiamo che v1= 0: Allora 1v1= 0 è una combinazione lineare di v1con coefficiente non nullouguale al vettore nullo.Dunque, per definizione, v1è linearmente dipendente.Viceversa, supponiamo che v1sia linearmente dipendente. Allora, per definizione, esiste h≠0 tale che h v1= 0. Moltiplicando ambo i membri per h–1si ha: h–1hv1= h–10. D’altra parte h–1h v1= 1v1= v1e   h–10 = 0.  Dunque v1= 0.Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#6": "23/11/207Vettori linearmente indipendentiAnalizziamo il significato geometrico di indipendenza lineare.Teorema: Dato un vettore del piano (o dello spazio) v:= OAlinearmente indipendente. Sia rla retta passante per Oe A.Dato un punto qualsiasi Pdella retta r, esiste un solo scalarehin Rtale che OP= h OA.Dimostrazione:Se P= Oallora OP= 0: basta porre h= 0.Se P≠O, sia d(P ,O) = kd(A,O):•Se Pin r1, OP= k OA, h= k.•Se Pin r2, OP= –kOA, h= –k.→→→OAr1r2r→→→→→Geometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#7": "OA23/11/208Vettori linearmente indipendentiAnalizziamo il significato geometrico di indipendenza lineare.Segue il teorema: Dato un vettore del piano (o dello spazio) v:= OAlinearmente indipendente. L’insieme delle combinazioni linearidi v, cioè l’insieme dei vettori hval variare di hin R, è formato da tutti e soli i vettori OPtali che il punto Pappartenga alla rettarpassante per i punti distinti Oe A.r→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#8": "23/11/209Vettori linearmente indipendentiTeorema: Due vettori v1:= OP1e v2:= OP2sono linearmentedipendentise e solo se i punti O, P1e P2sono allineati.Dimostrazione: Supponiamo che O, P1e P2sono allineati. Abbiamo due casi: •Se P1= Oallora v1= 0. Pertanto 1v1+ 0v2= 0.Dunque v1e v2sono linearmente dipendenti. •Se P1≠Oallora i punti P1e Oindividuano una retta r. Poiché O, P1e P2sono allineati il punto P2appartiene a r.Grazie al teorema precedente, esiste un hin Rtale che v2= h v1.Allora abbiamo: hv1+ (–1) v2= 0.Dunque v1e v2sono linearmente dipendenti.→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\7_Combinazione_lineare.pdf#9": "23/11/2010Vettori linearmente indipendentiTeorema: Due vettori v1:= OP1e v2:= OP2sono linearmentedipendentise e solo se i punti O, P1e P2sono allineati.Dimostrazione: Viceversasupponiamo che v1e v2sono linearmente dipendenti. Sappiamo che esistono due numeri reali he knon entrambi nulli taliche: h v1+ k v2= 0. Ad esempio, k≠0. Moltiplicando per k –1 si ha: k –1 hv1+ v2= 0Dunque: v2= –k –1hv1. Essendo v2multiplo di v1, il termine P2di v2appartiene a r.Pertanto esiste una retta rche contiene i punti O, P1e P2.→→\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#0": "23/11/201L8: Spazi vettoriali sui reali (11-14)Argomenti lezione:•Introduzione •Gli spazi vettoriali•Proprietà degli spazi vettoriali•Sottospazi vettoriali\nGeometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#1": "23/11/202Vettori del pianoConsideriamo un piano e fissiamo su esso una distanza con unità di misura U1U2. Fissiamo poi una volta per tutte un punto O del piano, che chiamiamo origine.Definizioni: Dato comunque un punto P, chiamiamo vettore applicatoin Odi verticePla coppia di punti Oe P. Indichiamo questo vettore con il simbolo OP. Diremo anche che il vettore OPha come origine il punto O.         →→OPIl simbolo V 2(O) indica l'insieme dei vettori applicati in O. Il vettore OOviene chiamato vettore nullo e indicato con il simbolo 0. Abbiamo quindi 0 = OO.23/11/202Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#10": "23/11/2011Esempi di spazi vettoriali Contro-esempio: Consideriamo R2con le operazioni cosi definite:•Le proprietà (1, 2, 3, 4) che coinvolgono solo l'operazione di addizione sono tutte verificate;•La proprietà 5 non è sempre verificataper il seguente motivo:se k= 1  e  a1≠ 0, si ha che 1(a1, a2) ≠ (0, a2).Pertanto R2con queste operazioni nonè uno spazio vettoriale. \n23/11/2011Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#11": "Proprietà degli Spazi Vettoriali\n23/11/2012Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#12": "23/11/2013Proprietà degli spazi vettorialiLegge di cancellazione della somma:Siano u, v, wvettori di uno spazio vettoriale V , u+ v= u+ wse e solo se v= wCome casi particolari abbiamo:  •u+ v= use e solo se   v= 0•u+ v= wse e solo se   v= w–uDimostrazione:Se v= wallora vale l’uguaglianza u + v= u+ w.Viceversa, se u + v= u + wallora col vettore –uotteniamo:–u+ (u+ v) = –u+ (u+ w)Per la proprietà associativa: (–u+ u) + v= (–u+ u) + wCioè 0 + v= 0 + w. Vale a dire v= w.23/11/2013Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#13": "23/11/2014Proprietà degli spazi vettorialiTeorema: In uno spazio vettoriale Vvalgono le proprietà:Dimostrazione:1. Utilizzando le proprietà che definiscono uno spazio vettorialeabbiamo:  0 v= (0 + 0) v= 0 v+ 0 vDalla legge di cancellazione segue allora che 0 v= 0.2. Calcolando il prodotto di uno scalare hper il vettore nullo,abbiamo:  h0 = h(0 + 0) = h0 + h0Dalla legge di cancellazione segue allora che h0 = 0.\n23/11/2014Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#14": "23/11/2015Proprietà degli spazi vettorialiTeorema: Sia vun vettore di uno spazio vettoriale V, e sia hin R.Se h v= 0 allora h= 0 oppure v= 0.Dimostrazione:Sappiamo che hv= 0. Dobbiamo mostrare che h= 0 oppure v= 0. Se h= 0 abbiamo finito, se h≠ 0 mostriamo che v= 0. Moltiplichiamo per h–1entrambi i membri di h v= 0 e otteniamo: h–1(h v) = h–10 h–1 ( hv) = (h–1 h) v= 1 v= vh–1 0 = 0Pertanto v= 0.23/11/2015Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#15": "23/11/2016Proprietà degli spazi vettorialiTeorema: Se vè un vettore di uno spazio vettoriale V, allora vale l’uguaglianza (–1) v= –v.Dimostrazione:Dobbiamo mostrare che (–1) vè l’oppostodel vettore v.Dobbiamo cioè mostrare che v+ (–1) v= 0. Allora: v+ (–1) v= 1v+ (–1) v= (1 + (–1)) v= 0 v= 0.\n23/11/2016Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#16": "Sottospazi vettoriali\n23/11/2017Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#17": "23/11/2018Sottospazi vettorialiObiettivo: Identificare se un sottoinsiemenon vuoto Edi unospazio vettoriale Vè esso stesso uno spazio vettoriale.Metodologia: •prima dobbiamo stabilire se abbiamo delle operazioni in E; •poi dobbiamo verificare se queste operazioni soddisfano le proprietà che definiscono uno spazio vettoriale.Definizione: Un sottoinsieme non vuoto Edi uno spazio vettoriale Vsi dice sottospaziovettoriale di Vse:\n23/11/2018Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#18": "23/11/2019Sottospazi vettoriali\nEsercizio: Sia Eil sottoinsieme di M(2, 2, R) formato dalle matrici che hanno almeno un elemento nullo. Per esempio:Domanda: Stabilire se Eè uno spazio vettoriale rispetto alle operazioni che possiamo indurre dallo spazio vettoriale M (2, 2, R).Contro-esempio: \nSegue che A + Bnon è necessariamente una matrice di E !Da cui Enonè uno spazio vettoriale rispetto a M (2, 2, R).23/11/2019Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#19": "Esercizio: Sia Eil sottoinsieme di M(2, 2, R) formato dalle matrici che hanno tutti gli elementi interi. Per esempio:Domanda: Stabilire se Eè uno spazio vettoriale rispetto alle operazioni che possiamo indurre dallo spazio vettoriale M (2, 2, R).Contro-esempio: Se moltiplichiamo la matrice di Aper uno scalare pari ad k= ½  non otteniamo una matrice di E !23/11/2020Segue che k A non è necessariamente una matrice di E !Da cui Enonè uno spazio vettoriale rispetto a M (2, 2, R).\nSottospazi vettoriali\n23/11/2020Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#2": "23/11/203Vettori dello spazioIn maniera analoga, l’insieme dei vettori dello spazio con origine in O viene definito con il simbolo V 3(O).\n23/11/203Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#20": "23/11/2021Sottospazi vettorialiDefinizione: Un sottoinsieme non vuoto Edi uno spazio vettoriale Vsi dice sottospaziovettoriale di Vse:Domanda: Il sottospazio Eè uno spazio vettoriale? Per rispondere alla domanda dobbiamo verificare tutte le proprietà.\n23/11/2021Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#21": "23/11/2022Sottospazi vettorialiDefinizione: Un sottoinsieme non vuoto Edi uno spazio vettoriale Vsi dice sottospaziovettoriale di Vse:Domanda: Il sottospazio Eè uno spazio vettoriale? Per rispondere alla domanda dobbiamo verificare tutte le proprietà.In particolaredobbiamo chiederci se nel sottoinsieme E diV:•Esiste l’elemento neutro: il vettore 0 in Vappartiene anche a E ?•Esiste l’elemento opposto: il vettore –uin Vappartiene anche a E ?\n23/11/2022Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#22": "23/11/2023Sottospazi vettorialiTeorema: Se Eè un sottospazio vettoriale di uno spazio vettoriale V,allora 0 è in Ee, per ogni vettore udi E, il vettore –uè in E. Da cui Eè esso stesso uno spazio vettoriale. Dimostrazione:•V ogliamo mostrare che 0 è in E. Sappiamo che se kè un numero reale e se vè un vettore di E,     allora si ha che k vappartiene a E. In particolare, ciò è vero se k= 0.Dunque:  0 vè in E, ma 0 v= 0, e quindi il vettore 0 è in E.•V ogliamo mostrare che se uè un vettore di E, allora –uè in E.Sappiamo che k uappartiene al sottospazio Equalunque sia k.In particolare, (–1) uappartiene a E. Poiché (–1) u= –uallora il vettore –uè in E. 23/11/2023Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#23": "23/11/2024Sottospazi vettorialiTeorema: Se Eè un sottospazio vettoriale di uno spazio vettoriale V,allora 0 è in Ee, per ogni vettore udi E, il vettore –uè in E. Da cui Eè esso stesso uno spazio vettoriale. Osservazione: Se un sottoinsieme Edi uno spazio vettoriale Vnoncontiene il vettore 0, allora Enonè un sottospazio vettoriale di V!Segue che dato un sottoinsieme Edi uno spazio vettoriale Vper mostrare che Eè uno spazio vettoriale dobbiamo innanzituttoverificare se il vettore 0 appartiene al sottoinsieme E!23/11/2024Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#24": "23/11/2025Sottospazi vettorialiEsercizio: Sia Eil sottoinsieme di M(2, 2, R) formato dalle matrici:al variare di ae bin R. Domanda: Eè un sottospazio vettoriale di M (2, 2, R) ?Notiamo che il sottoinsieme Enon contiene la matrice nulla. Dunque Enon è un sottospazio vettoriale.\nSegue che dato un sottoinsieme Edi uno spazio vettoriale Vper mostrare che Eè uno spazio vettoriale dobbiamo innanzituttoverificare se il vettore 0 appartiene al sottoinsieme E!\n23/11/2025Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#25": "23/11/2026Sottospazi vettorialiEsercizio: Sia Eil sottoinsieme di M(2, 2, R) formato dalle matrici:al variare di ae bin R.  Eè un sottospazio vettoriale di M (2, 2, R) ?\nChiaramente il sottoinsieme Eè non vuoto, perché contiene, ad esempio, la matrice nulla. Osservazione: Se a= bla matrice ha determinante nullo, e quindi non è invertibile. Ciò non implica nulla riguardo la matrice opposta!  Verifichiamo se Eè un sottospazio. 23/11/2026Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#26": "23/11/2027Sottospazi vettorialiEsercizio: Sia Eil sottoinsieme di M(2, 2, R) formato dalle matrici:al variare di ae bin R.  Eè un sottospazio vettoriale di M (2, 2, R) ?\nChiaramente il sottoinsieme Eè non vuoto, perchè contiene, ad esempio, la matrice nulla. Verifichiamo se Eè un sottospazio.Prese due matrici in E: \n(M+ N) appartiene a Ese e solo sea3:= a1+ a2 e  b3:= b1+ b2.\n23/11/2027Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#27": "23/11/2028Sottospazi vettorialiEsercizio: Sia Eil sottoinsieme di M(2, 2, R) formato dalle matrici:al variare di ae bin R.  Eè un sottospazio vettoriale di M (2, 2, R) ?\nChiaramente il sottoinsieme Eè non vuoto, perchè contiene, ad esempio, la matrice nulla. Verifichiamo se Eè un sottospazio.Analogamente, presa una matrice Min E: (kM) appartiene a Ese e solo sea4:= ka1e  b4:= kb1.\n23/11/2028Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#28": "23/11/2029Sottospazi vettorialiEsercizio: Sia Eil sottoinsieme di M(2, 2, R) formato dalle matrici:al variare di ae bin R.  Eè un sottospazio vettoriale di M (2, 2, R) ?\nChiaramente il sottoinsieme Eè non vuoto, perchè contiene, ad esempio, la matrice nulla. Verifichiamo se Eè un sottospazio.Riassumendo: (kM) appartiene a Ese e solo sea4:= ka1e  b4:= kb1.(M+ N) appartiene a Ese e solo sea3:= a1+ a2 e  b3:= b1+ b2.\n23/11/2029Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#29": "23/11/2030Sottospazi vettorialiEsercizio: Consideriamo lo spazio vettoriale M(n, n, R) delle matrici quadrate di ordine n. Vediamo alcuni esempi di suoi sottospazi: •il sottoinsieme TR(n) delle matrici triangolari superiori;Infatti:1. Tale sottospazio è non vuoto, perché la matrice nulla è una particolare matrice triangolare superiore;  2. La somma di due matrici triangolari superiori è una matrice triangolare superiore;3. Moltiplicando una matrice triangolare superiore per uno scalare   si ottiene una matrice triangolare superiore. Dunque TR(n) è un sottospazio vettoriale di M(n, n, R). 23/11/2030Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#3": "23/11/204IntroduzioneIniziamo lo studio degli spazi vettoriali sui reali:  L’insieme delle matrici M (p, q, R) con le operazioni di addizione tra matrici e di moltiplicazione di una matrice per uno scalare; L’insieme dei vettori V 2(O) o V 3(O) con le operazioni di addizione tra vettori e di moltiplicazione di un vettore per uno scalare.\n23/11/204Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#30": "23/11/2031Sottospazi vettorialiEsercizio: Consideriamo lo spazio vettoriale M(n, n, R) delle matrici quadrate di ordine n. Vediamo alcuni esempi di suoi sottospazi:•il sottoinsieme TR(n) delle matrici triangolari superiori;•il sottoinsieme TR(n) delle matrici triangolari inferiori;Si verifica analogamente a TR(n).  \n23/11/2031Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#31": "23/11/2032Sottospazi vettorialiEsercizio: Consideriamo lo spazio vettoriale M(n, n, R) delle matrici quadrate di ordine n. Vediamo alcuni esempi di suoi sottospazi:•il sottoinsieme TR(n) delle matrici triangolari superiori;•il sottoinsieme TR(n) delle matrici triangolari inferiori;•il sottoinsieme S(n, R) delle matrici simmetriche;1. Se Ae Bsono in S(n, R), allora A+Bappartiene a S(n, R)? Sappiamo che tA= Ae tB= B. Dimostriamo che t(A+ B) = A+B. Si ha t(A+ B) = t A+ t B= A+ B.2. Se Aappartiene a S(n, R) e kè uno scalare, allora k Aè in S(n, R)?Sappiamo che tA= A. Dobbiamo mostrare che t(k A) = k A. Si ha t(k A) = k  t A= k A. 23/11/2032Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#32": "23/11/2033Sottospazi vettorialiEsercizio: Consideriamo lo spazio vettoriale M(n, n, R) delle matrici quadrate di ordine n. Vediamo alcuni esempi di suoi sottospazi:•il sottoinsieme TR(n) delle matrici triangolari superiori;•il sottoinsieme TR(n) delle matrici triangolari inferiori;•il sottoinsieme S(n, R) delle matrici simmetriche;•il sottoinsieme A(n, R) delle matrici antisimmetriche (vale a dire delle matrici Atali che tA = –A);Si verifica analogamente a S(n, R). \n23/11/2033Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#33": "23/11/2034Sottospazi vettorialiConsideriamo ora un sistema Sdi pequazioni lineari in qincognite. Spuò essere scritto nella forma matriciale AX= B, dove:•la matrice dei coefficienti Aappartiene a M(p, q, R);•la matrice colonna dei termini noti Bappartiene a M(p, 1, R);•la matrice colonna delle incognite Xappartiene a M(q, 1, R);•le soluzioni di Sformano un sottoinsieme Sol(S) di M(q, 1, R). Domanda: Sol(S) è un sottospazio di M(q, 1, R) ?\n23/11/2034Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#34": "23/11/2035Sottospazi vettorialiConsideriamo ora un sistema Sdi pequazioni lineari in qincognite. Spuò essere scritto nella forma matriciale AX= B, dove:•la matrice dei coefficienti Aappartiene a M(p, q, R);•la matrice colonna dei termini noti Bappartiene a M(p, 1, R);•la matrice colonna delle incognite Xappartiene a M(q, 1, R);•le soluzioni di Sformano un sottoinsieme Sol(S) di M(q, 1, R). Domanda: Sol(S) è un sottospazio di M(q, 1, R) ?La matrice nulla appartiene a Sol(S) se e solo se A0 = B= 0 ! Definizione: Un sistema Sdi equazioni lineari si dice omogeneose i termini noti di tutte le equazioni del sistema Ssono nulli (B = 0).23/11/2035Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#35": "23/11/2036Sottospazi vettorialiDefinizione: I seguenti sottospazi vengono detti sottospazi banali: •Il sottoinsieme {0} (vettore nullo) dello spazio vettoriale V ; •Lo spazio vettoriale contenente tutto V(ovvero Vstesso) . Osservazione 1: Un sistema lineare omogeneo S: AX= 0 è sempre risolubile.Infatti rk A= rk A’. \n23/11/2036Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#36": "23/11/2037Sottospazi vettorialiDefinizione: I seguenti sottospazi vengono detti sottospazi banali: •Il sottoinsieme {0} (vettore nullo) dello spazio vettoriale V ; •Lo spazio vettoriale contenente tutto V(ovvero Vstesso) . Osservazione 1: Un sistema lineare omogeneo S: AX= 0 è sempre risolubile.AX= 0 ammette (almeno) la soluzione X= 0, detta soluzione banale. N.B. Un sistema omogeneo può avere altre soluzioni oltre alla banale.Osservazione 2: Un sistema S’non omogeneo noncontiene la soluzione banale.Infatti S’: AX= B≠ 0. 23/11/2037Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#37": "23/11/2038Sottospazi vettorialiDefinizione: I seguenti sottospazi vengono detti sottospazi banali: •Il sottoinsieme {0} (vettore nullo) dello spazio vettoriale V ; •Lo spazio vettoriale contenente tutto V(ovvero Vstesso) . Osservazione 1: Un sistema lineare omogeneo S: AX= 0 è sempre risolubile.AX= 0 ammette (almeno) la soluzione X= 0, detta soluzione banale. N.B. Un sistema omogeneo può avere altre soluzioni oltre alla banale.Osservazione 2: Un sistema S’non omogeneo noncontiene la soluzione banale.L'insieme delle soluzioni di S’ nonè quindi un sottospazio vettoriale! 23/11/2038Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#38": "23/11/2039Sottospazi vettorialiTeorema: Sia S: AX=0 un sistema lineare omogeneo di pequazioni in qincognite.L’insieme Sol(S) è un sottospazio vettoriale di M(q, 1, R).Dimostrazione: •Sol(S) ≠ ∅perché contiene la matrice nulla. •Siano ora X1e X2due elementi di Sol(S). Dobbiamo dimostrare che X1+ X2appartiene a Sol(S). X1e X2sono due soluzioni di S, quindi A X1= 0 e A X2= 0. Abbiamo che A (X1+ X2) = A X1+ A X2= 0 + 0 = 0.  •Dobbiamo dimostrare che  A(kX1) = 0. Abbiamo che A(kX1) = k (A X1) = k 0 = 0. 23/11/2039Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#39": "23/11/2040Sottospazi vettorialiEsercizio: Dimostrare che un sistema omogeneo S: AX= 0 ammette soluzioni non banali se e solo se il rk Aè minore del numero delle incognite.Soluzione: Il sistema Sammette soluzioni non banali se ha più di una soluzione. Sappiamo che le soluzioni di un sistema risolubile di matrice Adipendono da q–rk Aparametri, dove qè il numero delle incognite. Quindi il sistema Sha più di una soluzione se e solo se q–rk A> 0.23/11/2040Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#4": "Spazi Vettoriali\n23/11/205Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#40": "23/11/2041EserciziEsercizio: Sia Eil sottoinsieme di M (2, 2, R) formato dalleseguenti matrici, al variare di ain R:Stabilire se Eè un sottospazio vettoriale di M (2, 2, R). •Eè non vuoto.•Consideriamo due matrici MeN, calcoliamo la loro somma:\nDovremmo avere:  a3= a1+ a2;  a32= a12 + a22 In generale non è vero: se a1= a2= 1 allora a3= 2  ma  a32≠ 2 !Dunque Enonè un sottospazio vettoriale di M (2, 2, R).23/11/2041Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#5": "23/11/206Gli spazi vettoriali: EsempioConsideriamo l'insieme M(p, q, R) delle matrici a coefficienti reali aventi prighe e qcolonne. Abbiamo già visto le seguenti proprietà:\nPr. associativa della sommaEsistenza dello zeroEsistenza dell’oppostoPr. commutativa della somma\n23/11/206Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#6": "23/11/207Gli spazi vettorialiDefinizione di spazio vettoriale sui reali:Sia dato un insieme non vuoto V, i cui elementi sono vettori. Chiamiamo, invece, scalarii numeri reali in R. In Vsia definita un’operazione binaria interna, che data una coppia (v,w)in V associ un altro vettore in V(addizionev+ w). In Vsia definita un’operazione binaria esterna, che data una coppia (k, v) con vettore vin Ve scalare kin Rassoci un altro vettore in V(moltiplicazione kv). v + wè il vettore somma, mentre kvè il prodotto scalare vettore.23/11/207Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#7": "23/11/208Gli spazi vettorialiL’insieme Vviene detto spazio vettoriale su Rse sono verificate:\n23/11/208Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#8": "23/11/209Esempi di spazi vettoriali Con le 8 operazioni appena definite si definiscono spazi vettoriali:•M(p, q, R), V 2(O) e V 3(O); •L’insieme R in cui i numeri reali rivestono il ruolo sia di vettori che di scalari;•L’insieme delle n-uple di numeri reali (nè un numero naturale):•L’insieme R[x] dei polinomi a coefficienti reali nell'incognita x(le 8 operazioni appena viste si applicano all’addizione tra polinomi e alla moltiplicazione di uno scalare per un polinomio).\n23/11/209Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\8_Spazi_vettoriali.pdf#9": "23/11/2010Esempi di spazi vettoriali Contro-esempio: Consideriamo R2con le operazioni cosi definite:•Le proprietà (1, 2, 3, 4) che coinvolgono solo l'operazione di addizionesono tutte verificate;\n23/11/2010Geometria e Combinatoria marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#0": "23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it1L9: Generatori (12,15)Argomenti lezione:•Introduzione•Combinazione lineare di vettori•Vettori generatori di un sottospazio",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#1": "Sottospazio vettoriale\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it2",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#10": "23/11/2011Combinazione lineare di vettori Definizione: Dati dei vettori v1, v2, . . . , vrdi uno spazio vettoriale Ve degli scalari k1, k2, . . . , kr, una combinazione lineare dei vettori v1, v2, . . . , vra coefficienti k1, k2, . . . , krè ∑!\"#%𝑘!𝑣!.Esempio: Scrivere le combinazioni lineari delle matrici:Le loro combinazioni lineari sono tutte e sole le matrici del tipo:\nal variare di k1, k2, k3e k4in R.23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it11",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#11": "23/11/2012Combinazione lineare di vettori Definizione: Dati dei vettori v1, v2, . . . , vrdi uno spazio vettoriale Ve degli scalari k1, k2, . . . , kr, una combinazione lineare dei vettori v1, v2, . . . , vra coefficienti k1, k2, . . . , krè ∑!\"#%𝑘!𝑣!.Esempio: Scrivere le combinazioni lineari delle matrici:Le loro combinazioni lineari sono tutte e sole le matrici del tipo:\nal variare di k1, k2, k3e k4in R.23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it12",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#12": "23/11/2013Combinazione lineare di vettori Definizione: Dati dei vettori v1, v2, . . . , vrdi uno spazio vettoriale Ve degli scalari k1, k2, . . . , kr, una combinazione lineare dei vettori v1, v2, . . . , vra coefficienti k1, k2, . . . , krè ∑!\"#%𝑘!𝑣!.Esempio: Scrivere le combinazioni lineari delle matrici:Le loro combinazioni lineari sono tutte e sole le matrici del tipo:\nal variare di k1, k2, k3e k4in R.23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it13",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#13": "Generatori\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it14",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#14": "23/11/2015Vettori generatori di un sottospazioTeorema: L'insieme delle combinazioni linearidei v1, v2, . . . , vral variare dei coefficienti k1, k2, . . . , krè un sottospazio vettor. di Vchiamato sottospazio vettoriale generatodai vettori v1, v2, . . . , vr:L(v1, v2, . . . , vr):= ∑!\"#%𝑘!𝑣!𝑘!∈ℝ}.N.B. Tale sottospazio vettoriale contiene i vettori v1, v2, . . . , vred  è contenuto in tuttii sottospazi vett. di Vcontenenti v1, v2, . . . , vr.\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it15",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#15": "23/11/2016Vettori generatori di un sottospazioTeorema: L'insieme delle combinazioni linearidei v1, v2, . . . , vral variare dei coefficienti k1, k2, . . . , krè un sottospazio vettor. di Vchiamato sottospazio vettoriale generatodai vettori v1, v2, . . . , vr:L(v1, v2, . . . , vr):= ∑!\"#%𝑘!𝑣!𝑘!∈ℝ}.Dimostrazione: 1.Notiamo cheL(v1, v2, . . . , vr) contiene tutti i vettori v1, v2, ..., vrPer esempio si ha: v1= 1v1+ 0v2+ . . . + 0vr2.Sia Eun sottospazio vettoriale di Vcontenente v1, v2, ... , vr.  Allora: Econtiene tutti i vettori del tipo k1v1, k2v2, ... , krvr.         Econtiene tutte le loro combinazioni lineari: k1v1+k2v2+ ... +krvr. Quindi Eè generato daL(v1, v2, . . . , vr). 23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it16",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#16": "23/11/2017Vettori generatori di un sottospazioTeorema: L'insieme delle combinazioni linearidei v1, v2, . . . , vral variare dei coefficienti k1, k2, . . . , krè un sottospazio vettor. di Vchiamato sottospazio vettoriale generatodai vettori v1, v2, . . . , vr:L(v1, v2, . . . , vr):= ∑!\"#%𝑘!𝑣!𝑘!∈ℝ}.Dimostrazione: 3.Dimostriamo che L(v1, v2, ... , vr) è un sottospazio vettor. di VL(v1, v2, ... , vr) è non vuoto (e.g. contiene i vettori v1, v2, ... , vr) Dati   u= k1v1+ k2v2+ ... + krvre    v= h1v1+ h2v2+ ... + hrvr.Sia( u + v ) che ( k u ) appartengono a L(v1, v2, ... , vr). Infatti:     u + v= (k1 + h1) v1 + (k2 + h2) v2 + ... + (kr+ hr) vrk u= (kk1) v1+ (kk2) v2+ ... + (kkr) vr23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it17",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#17": "23/11/2018Vettori generatori di un sottospazioDefinizione: Se V= L(v1, v2, ... , vr) allora Vè generatodai vettori v1, v2, ... , vr .In altre parole, v1, v2, ... , vrsono generatoridi (o generano) V se e solo se ogni vettore di Vè combinazione lineare di v1, v2, ... , vr.Esempi: Data una retta rpassante per l’origine Odel piano o dello spazio,    e dato un punto Adella retta rdistinto da O, l'insieme dei vettoriOPcon Pin rè un sottospazio vettoriale generatodal vettore OA.Dati in V 2(O) due vettori OP1e OP2con O, P1e P2non allineati,i vettori OP1e OP2generanoV 2(O). →→→→→→23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it18",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#18": "23/11/2019Vettori generatori di un sottospazioDefinizione: Se V= L(v1, v2, ... , vr) allora Vè generatodai vettori v1, v2, ... , vr.In altre parole, v1, v2, ... , vrsono generatoridi (o generano) V se e solo se ogni vettore di Vè combinazione lineare di v1, v2, ... , vr.Esempi: Dati in V 3(O) due vettori OP1e OP2con O, P1e P2non allineati,i vettori OP1e OP2generanoil sottospazio di V 3(O) formato dai vettori OPcon Pappartenente al piano passante per O, P1e P2.V 3(O) è generatodai vettori OP1, OP2e OP3con O, P1, P2e P3non complanari.→→→→→→→→23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it19",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#19": "23/11/2020Vettori generatori di un sottospazioDefinizione: Se V= L(v1, v2, ... , vr) allora Vè generatodai vettori v1, v2, ... , vr.In altre parole, v1, v2, ... , vr sono generatoridi (o generano) V se e solo se ogni vettore di Vè combinazione lineare di v1, v2, ... , vr.Esempi: Consideriamo le seguenti tre matrici di M(2, 2, R): Il sottospazio vettoriale che esse generanoè quello formato dallematrici del tipo k1A1+ k2A2+ k3A3. Facendo i calcoli: 23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it20",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#2": "23/11/203IntroduzioneEsempio:  Consideriamo le seguenti tre matrici di M(2, 2, R):  \nQueste tre matrici appartengono al sottospazio vettoriale S(2, R) di M(2, 2, R) formato dalle matrici simmetriche.Queste tre matrici appartengono anche al sottospazio EdiM (2, 2, R) formato dalle matrici del tipo: Esistono altri sottospazi contenenti le tre matrici …Domanda: C’è un sottospazio più \"piccolo\" di tutti gli altri ?\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it3",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#20": "Definizione: Se V= L(v1, v2, ... , vr) allora Vè generatodai vettori v1, v2, ... , vr.In altre parole, v1, v2, ... , vrsono generatoridi (o generano) V se e solo se ogni vettore di Vè combinazione lineare di v1, v2, ... , vr.Esempi: Consideriamo i tre polinomi 1, x, x2. Vediamo che le loro combinazioni lineari sono i polinomi del tipo: k1+ k2 x+ k3 x2 .Dunque 1, x, x2generanoR3[x], ovvero il sottospazio vettoriale di R[x] formato dai polinomi di grado minore di 3. In maniera analoga si potrebbe verificare che 1, x, x2, . . . , xngeneranoRn+1[x].23/11/2021Vettori generatori di un sottospazio\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it21",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#21": "Esercizio: Sia Eil sottospazio vettoriale di R4generato dai vettori v1:= (3, 2, 2, 1), v2:= (1, 0, 1, –1) e v3:= (0, 1, 1, 2). Domanda: Il vettore v:= (0, 1, 0, 2) appartiene al sottospazio E? Dobbiamo stabilire se vè combinazione lineare di v1, v2e v3. Vale a dire se esistono tre scalari k1, k2e k3tali che:(0, 1, 0, 2) = k1(3, 2, 2, 1) + k2(1, 0, 1, –1) + k3(0, 1, 1, 2) \n23/11/2022Vettori generatori di un sottospazio\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it22",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#22": "Esercizio: Sia Eil sottospazio vettoriale di R4generato dai vettori v1:= (3, 2, 2, 1), v2:= (1, 0, 1, –1) e v3:= (0, 1, 1, 2). Domanda: Il vettore v:= (0, 1, 0, 2) appartiene al sottospazio E? Dobbiamo stabilire se vè combinazione lineare di v1, v2e v3. Vale a dire se esistono tre scalari k1, k2e k3tali che:(0, 1, 0, 2) = k1(3, 2, 2, 1) + k2(1, 0, 1, –1) + k3(0, 1, 1, 2) (0, 1, 0, 2) = (3k1+ k2,  2 k1+ k3,  2k1+ k2+ k3,  k1–k2+ 2k3)\n23/11/2023Vettori generatori di un sottospazio\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it23",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#23": "Esercizio: Sia Eil sottospazio vettoriale di R4generato dai vettori v1:= (3, 2, 2, 1), v2:= (1, 0, 1, –1) e v3:= (0, 1, 1, 2). Domanda: Il vettore v:= (0, 1, 0, 2) appartiene al sottospazio E? \n23/11/2024Vettori generatori di un sottospazio\nrkA= 3rkA’= 3vappartiene a Ese e solo se il sistema è risolubilecalcolare il rango di Ae A’e applicareRouché-Capelli23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it24",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#24": "Esercizio: Sia Eil sottospazio vettoriale di R4generato dai vettori v1:= (3, 2, 2, 1), v2:= (1, 0, 1, –1) e v3:= (0, 1, 1, 2). Domanda: Il vettore v:= (0, 1, 0, 2) appartiene al sottospazio E? \n23/11/2025Vettori generatori di un sottospazio\nrkA= 3rkA’= 3Abbiamo che rkA=rkA’.il sistema è risolubile!Quindi vappartiene aE23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it25",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#25": "Esercizio: Sia Eil sottospazio vettoriale di R4generato dai vettori v1:= (3, 2, 2, 1), v2:= (1, 0, 1, –1) e v3:= (0, 1, 1, 2). Domanda: Il vettore v:= (0, 1, 0, 2) appartiene al sottospazio E? \n23/11/2026Vettori generatori di un sottospazio\nrkA= 3rkA’= 3Le soluzioni del sistema: k1= 1/3, k2= –1, k3= 1/3(non ho parametri datoche rk A= qincognite) 23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it26",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#26": "Esercizio: Sia Eil sottospazio vettoriale di R4generato dai vettori v1:= (3, 2, 2, 1), v2:= (1, 0, 1, –1) e v3:= (0, 1, 1, 2). Domanda: Il vettore v:= (0, 1, 0, 2) appartiene al sottospazio E? \n23/11/2027Vettori generatori di un sottospazio\nrkA= 3rkA’= 3\nLe soluzioni del sistema:k1= 1/3, k2= –1, k3= 1/323/11/20Geometria e Combinatoria marcella.sama@uniroma3.it27",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#27": "Esercizio: Sia Eil sottospazio vettoriale di M(2, 3, R) generato da:Domanda: Le seguenti matrici appartengono a  E? \n23/11/2028Vettori generatori di un sottospazio\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it28",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#28": "Esercizio: Sia Eil sottospazio vettoriale di M(2, 3, R) generato da:\n23/11/2029Vettori generatori di un sottospazio\nConsideriamo una combinazione lineare delle quattro matrici:\nAappartiene a Ese e solo se il seguente sistema è risolubile: \nIl sistema nonè risolubile,Anonappartiene a E23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it29",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#29": "Esercizio: Sia Eil sottospazio vettoriale di M(2, 3, R) generato da:\n23/11/2030Vettori generatori di un sottospazio\nConsideriamo una combinazione lineare delle quattro matrici:\nBappartiene a Ese e solo se il seguente sistema è risolubile: \nFacendo i calcoli si ha:Il sistema è risolubile,Bappartiene a E23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it30",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#3": "23/11/204IntroduzioneEsempio:  Consideriamo le seguenti tre matrici di M(2, 2, R):  \nDomanda: C’è un sottospazio più \"piccolo\" di tutti gli altri ?Un Fdi M(2, 2, R) che contiene le tre matrici deve contenere:•le matrici del tipo k1A1, k2A2, k3A3con k1, k2e k3numeri reali;•le somme di questi multipli: k1A1+ k2A2+ k3A3; •i multipli delle matrici create, poi le loro somme, e così via.Segue l’insieme Fdelle matrici del tipo: k1A1+ k2A2+ k3A3Si dimostra che Fè un sottospazio vettoriale di M(2, 2, R). Ad esempio: A1si ottiene per k1= 1, k2= k3= 0.23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it4",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#30": "Osservazione: In generale non c’e alcun motivo perché dato un qualsiasi spazio vettoriale Vesistano v1, v2, ... , vrche generano V.Esempio: Si consideri lo spazio vettoriale dei polinomi R[x]. Consideriamo un numero finito di npolinomi f1[x], . . . , fn[x]. Essi non possono essere generatori di R[x], perchè tutte le combinazioni lineari di f1[x], . . . , fn[x] sono polinomi di grado minore o uguale a g(ovvero il massimo dei gradi di tali polinomi). Ma allora il polinomio xg+1, appartenente a R[x], nonè ottenibile come combinazione lineare di f1[x], . . . , fn[x]. Questi ultimi nonsono quindi generatori di R[x]. 23/11/2031Vettori generatori di un sottospazio\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it31",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#31": "Definizione: Diciamo che uno spazio vettoriale Vè finitamente generatose esistono v1, v2, ... , vrtali che V= L(v1, v2, ... , vr) .Esempi:•V 2(O) è finitamente generato;•V 3(O) è finitamente generato;•R[x] nonè finitamente generato. \n23/11/2032Vettori generatori di un sottospazio\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it32",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#32": "Esercizio (1): Dati r+1 vettori v1, v2, ..., vr, vr+1 di uno spazio vett. Vmostrare che L(v1, v2, ... , vr)  ÍL(v1, v2, ... , vr, vr+1). Dobbiamo mostrare che ogni combinazione lineare dei vettori v1, v2, ..., vrè anche combinazione lineare dei vettori v1, v2, ..., vr,vr+1.Infatti se vè combinazione lineare dei vettori v1, v2, ..., vr, esistono numeri reali k1, k2, . . . , krtali che:v= k1v1+ k2v2+ . . . + krvrv= k1v1+ k2v2+ . . . + krvr+ 0vr+1 Segue che vè combinazione lineare di v1, v2, ..., vr, vr+1. 23/11/2033Vettori generatori di un sottospazio\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it33",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#33": "Esercizio (2): Stabilire se le matrici S1e S2generano S(2, R).\n23/11/2034Vettori generatori di un sottospazio\nDobbiamo verificare se una qualsiasi matrice simmetrica Spuò essere espressa come combinazione lineare di S1e S2.Dobbiamo stabilire se esistono k1e k2tali che S= k1S1+ k2S2, cioè:\nDeterminiamo k1e k2se e solo se a= c.Dunque nontutte le matrici simmetriche sono combinazioni linearidi S1e S2. Perciò, S1e S2nongenerano S(2, R).23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it34",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#34": "Esercizio (3): Stabilire se le matrici S1, S2e S3generano S(2, R).\n23/11/2035Vettori generatori di un sottospazio\nDobbiamo verificare se una qualsiasi matrice simmetrica Spuò essere espressa come combinazione lineare di S1, S2e S3.Dobbiamo stabilire se esistono k1, k2e k3t.c. S= k1S1+ k2S2 + k3S3 :Basta porre: k1= b , k2= a , k3= c.Dunque tuttele matrici simmetriche sono combinazioni linearidi S1, S2e S3. Perciò, S1, S2e S3generanoS(2, R).\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it35",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#35": "Esercizio (4): Si consideri il sottospazio vettoriale di R[x] generato dai polinomi p1(x) := 3+2x,  p2(x) := x+x3,  p3(x) := 1+x+x2–x3.Stabilire se i polinomi  p(x) := 1 + 2x+ x3e   q(x) := –7 + 2x2+ 2x3appartengono al sottospazio E.Stabiliamo se p(x)è combinazione lineare di p1(x), p2(x) e p3(x), cioè:p(x) = k1p1(x) + k2p2(x) + k3p3(x) \n23/11/2036Vettori generatori di un sottospazio\nSvolgendo i calcoli si vede che questo sistema nonè risolubile. Perciò, p(x) nonappartiene al sottospazio E.23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it36",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#36": "Esercizio (4): Si consideri il sottospazio vettoriale di R[x] generato dai polinomi p1(x) := 3+2x,  p2(x) := x+x3,  p3(x) := 1+x+x2–x3.Stabilire se i polinomi  p(x) := 1 + 2x+ x3e   q(x) := –7 + 2x2+ 2x3appartengono al sottospazio E.Stabiliamo se q(x)è combinazione lineare di p1(x), p2(x) e p3(x), cioè:q(x) = k1p1(x) + k2p2(x) + k3p3(x) \n23/11/2037Vettori generatori di un sottospazio\nSvolgendo i calcoli si vede che questo sistema è risolubile. Perciò, q(x) appartiene al sottospazio E.\n–7 + 2x2+ 2x3  =–7 + 2x2+ 2x3  =\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it37",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#4": "23/11/205IntroduzioneEsempio:  Consideriamo le seguenti tre matrici di M(2, 2, R):  \nDomanda: C’è un sottospazio più \"piccolo\" di tutti gli altri ? Segue l’insieme Fdelle matrici del tipo: k1A1+ k2A2+ k3A3Dimostriamoche Fè un sottospazio vettoriale di M(2, 2, R). Prese le matrici A := k1A1+k2A2+k3A3e  B := h1A1+h2A2+h3A3Verifichiamo che:k A = k (k1A1+k2A2+k3A3) = (k k1) A1+ (k k2) A2 + (k k3) A3A + B= k1A1+ k2A2+ k3A3+ h1A1+ h2A2+ h3A3= (k1+h1 ) A1+ (k2 +h2 ) A2+ (k3+h3) A323/11/20Geometria e Combinatoria marcella.sama@uniroma3.it5",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#5": "Dipendenza e indipendenza lineare\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it6",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#6": "23/11/207Combinazione lineare di vettori Sia nel piano che nello spazio, si ha la seguente definizione.Dati nvettori v1, v2, ... , vn, essi si dicono linearmentedipendentise esistono a1, a2, ... , ancoefficienti non tutti nullitali che a1v1+ a2v2+ ... + anvn= 0Dati nvettori v1, v2, ... , vn, essi si dicono linearmente indipendenti se l’uguaglianza a1v1+ a2v2+ ... + anvn= 0è verificata solamentenel caso in cui a1= a2= ... = an= 0. L’unica loro combinazione lineare uguale al vettore nullo è la combinazione lineare con tutti i coefficienti nulli.23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it7",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#7": "23/11/208Combinazione lineare di vettoriSia nel piano che nello spazio, si ha la seguente definizione. Dati nvettori v1, v2, ... , vn, e dati nnumeri reali a1, a2, ... , an,il vettore v:= ∑!\"#$𝑎!𝑣!viene chiamato combinazione linearedei vettori v1, v2, ... , vncon coefficientia1, a2, ... , an. Esempio:\nGeometria e Combinatoria  marcella.sama@uniroma3.it",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#8": "23/11/209Combinazione lineare di vettori Definizione: Dati dei vettori v1, v2, . . . , vrdi uno spazio vettoriale Ve degli scalari k1, k2, . . . , kr, una combinazione lineare dei vettoriv1, v2, . . . , vra coefficienti k1, k2, . . . , krè   ∑!\"#%𝑘!𝑣!\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it9",
    "data_test\\rootfolder\\università\\GeometriaECombinatoria\\9_Generatori.pdf#9": "23/11/2010Combinazione lineare di vettori Definizione: Dati dei vettori v1, v2, . . . , vrdi uno spazio vettoriale Ve degli scalari k1, k2, . . . , kr, una combinazione lineare dei vettoriv1, v2, . . . , vra coefficienti k1, k2, . . . , krè ∑!\"#%𝑘!𝑣!.Esempio: Scrivere le combinazioni lineari delle matrici:Le loro combinazioni lineari sono tutte e sole le matrici del tipo:\n23/11/20Geometria e Combinatoria marcella.sama@uniroma3.it10",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nIntroduzione al Machine Learning  \n\u0000\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#1": "Introduzione al  \nmachine Learning\nIntuitivamente, un sistema è in grado di apprendere se, \nattraverso la sua attività, è in grado di migliorare le \nproprie prestazioni.\nNell’IA, il miglioramento delle prestazioni coincide in \ngenerale con l’acquisizione di nuove conoscenze.\n\u00002",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#10": "\u000011\nApplicazioni Finanziarie  \nApplicazioni in Medicina (e.g., rilevamento di tumori nelle \nscansioni cerebrali)\nRecommender Systems (e.g., raccomandare un prodotto a cui \nun cliente potrebbe essere interessato, sulla base di acquisti \npassati)\nRilevamento di frodi con carte di credito  \nRilevamento di pattern di accesso anomali a un sito Web\nSegnalazione automatica di commenti offensivi nei forum  \nIdentiﬁcazione di fake news\nEsempi di applicazione",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#11": "\u000012\nApplicazioni in vari campi dell’ingegneria (Civile, \nAeronautica, Telecomunicazioni, ecc. ecc.)\nMarketing (e.g., segmentazione dei clienti in base ai loro \nacquisti in modo da poter progettare una strategia di \nmarketing diversa per ogni segmento)  \nPrevisione dei ricavi della tua azienda per il prossimo anno, \nsulla base di varie metriche di performance\nCostruire un bot intelligente per un gioco  \nFar reagire la tua app ai comandi vocali  \nCreare un chatbot o un assistente personale\nEsempi di applicazione",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#12": "metodi di apprendimento\nApprendimento supervisionato\nRichiede che si apprenda una funzione partendo da esempi di input \ne output\nApprendimento non supervisionato\nRichiede di imparare a riconoscere pattern o schemi nell’input \nsenza alcuna indicazione speciﬁca dei valori di uscita.\nApprendimento per rinforzo\nL’agente apprende in base al rinforzo (ricompensa) ottenuto.\n\u000013",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#13": "Tipici problemi di  \nMachine Learning\nRegression \nClassiﬁcation  \nClusteringUna tipica classiﬁcazione dei problemi affrontati in ML  \nè la seguente:\n\u000014",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#14": "Applicazioni di  \nMachine Learning\nDEMO\n\u000015",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#2": "Introduzione al  \nmachine Learning\nQualsiasi cambiamento in un sistema che gli permetta di \navere prestazioni migliori la seconda volta, nella \nripetizione dello stesso compito o di un altro compito \ntratto dalla stessa popolazione.\n(Simon, 1984)\n\u00003",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#3": "Introduzione al  \nmachine Learning\nA computer program is said to learn from experience E \nwith respect to some class of tasks T and performance \nmeasure P, if its performance at tasks in T, as measured \nby P, improves with experience E .\n(Mitchell, 1997)\n\u00004",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#4": "Definizioni\nTask T : obiettivo del sistema\nGiocare a dama\nGuidare un autoveicolo\nRiconoscere parole pronunciate\nExperience E : Insieme di addestramento dal quale apprendere\nPartite giocate\nPercorsi\n.........\nPerformance measure P : misura della capacità di eseguire il \ntask\nNumero di partite vinte\nNumero di parole classiﬁcate correttamente\n\u00005",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#5": "Introduzione al  \nmachine Learning\nUn elemento fondamentale dell’apprendimento è la \ncapacità di valutare le proprie prestazioni, o almeno di \naccettare una valutazione dall’esterno.\nSenza una valutazione, infatti, non sarebbe possibile \nparlare di miglioramento.\nA sua volta, la valutazione delle prestazioni richiede la \ncapacità di accettare un certo tipo di informazioni \ndall’ambiente esterno.\n\u00006",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#6": "Progetti Rilevanti nello \nSviluppo del Machine Learning\n1989 : Guida autoveicolo - ALVINN system (Pomerlau, 1989)\n1995 : Classiﬁcazione nuove strutture astronomiche - NASA: classiﬁcazione \noggetti celesti (Fayyad et al., 1995)\n1992-95 : Backgammon - TD-Gammon (Tesauro, 1992, 1995): \napprendimento su 1 milione di partite giocate contro se stesso.\n2004 : DARPA introduce la “DARPA Grand Challenge”, una sﬁda per la \nguida autonoma di veicoli.\n2006 : Geoffrey Hinton dell’Università di Toronto introduce un algoritmo di \napprendimento veloce  per reti neurali artiﬁciali, che dà il via alla \nrivoluzione del Deep Learning.\n\u00007",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#7": "\u00008\n2006 : Netﬂix lancia la “Netﬂix Prize competition”, con una borsa di \nun milione di dollari, sﬁdando i gruppi di ricerca ad usare il Machine \nLearning per migliorare la accuracy del proprio Recommender \nSystem  di almeno il 10%. Un gruppo ha vinto il premio nel 2009.\n2010 : ImageNet lancia un concorso annuale - la “ImageNet Large \nScale Visual Recognition Challenge (ILSVRC)” - in cui i team \nutilizzano il Machine Learning per rilevare e classiﬁcare \ncorrettamente gli oggetti in un set di dati di immagini ampio e ben \ncurato. L’errore di classiﬁcazione migliora dal 25% nel 2011 a pochi \npunti percentuali nel 2015, grazie ai progressi nelle deep \nconvolutional neural networks.\nProgetti Rilevanti nello \nSviluppo del Machine Learning",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#8": "\u00009\n2011 : IBM Watson, un sistema di question-answering, batte i \ncampioni del gioco Jeopardy!   Brad Rutter e Ken Jennings. IBM \nWatson è ora utilizzato in diversi settori, tra cui l’assistenza sanitaria \ne la vendita al dettaglio. \n2014 : Facebook pubblica un lavoro su DeepFace, un sistema basato \nsu reti neurali artiﬁciali in grado di identiﬁcare volti con \nun’accuratezza del 97%, una prestazione al livello “umano”, che \nmigliora di circa il 27% le prestazioni di sistemi precedenti.\n2014 : Il Il consumo di energia per il raffreddamento dei Data Center \nè stato ridotto del 40% con un modello di Machine Learning:\nProgetti Rilevanti nello \nSviluppo del Machine Learning\nGao,\tJ.\t(2014) .\tMachine\tLearning\tApplica:ons\tfor\tData\tCenter\tOp:miza:on.\t Google\tResearch.  \nhCps://sta:c.googleusercontent.com/media/research.google.com/it//pubs/archive/42542.pdf",
    "data_test\\rootfolder\\università\\MachineLearning\\1-Introduzione ML-sbloccato.pdf#9": "\u000010\n2015 : AlphaGo di DeepMind batte il giocatore Fan Hui nel gioco del \nGo. Nel 2016 , batte Lee Sedol e, nel 2017 , batte Ke Jie.\n2017 : Il software per analizzare le immagini delle galassie sotto lenti \ngravitazionali è stato velocizzato di un fattore di 10 milioni  con un \nmodello di Machine Learning:\nProgetti Rilevanti nello \nSviluppo del Machine Learning\nHezaveh,\t Y.D.,\t Levasseur,\t I.P .,\t Marshall,\t P .J.\t (2017).\t Fast\t Automated\t Analysis\t of\t Strong\t\nGravita:onal\tLenses\twith\tConvolu:onal\tNeural\tnetworks.\t Nature ,\t548,\tpp.\t555-557.  \nhCps://arxiv.org/abs/1708.08842\nRecentemente David Patterson (Turing Award winner) e Jeff Dean (Google AI \nhead) hanno dichiarato l'alba di una \"età dell'oro\" per l'architettura dei computer \ngrazie al Machine Learning . ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nAlgoritmo K-NN\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#1": "Sommario\nRipasso su Information Retrieval \nAlgoritmo k-NN \nkd-trees per k-NN\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#10": "Modello Bag-of-Words\nUn problema che emerge in questa semplice rappresentazione è \nrelativa ai termini poco frequenti (“rare words”). \nIn effetti, in tale rappresentazione tutti i termini sono considerati \nugualmente importanti.  \nIn realtà certi termini hanno poca capacità discriminante ai ﬁni \ndella determinazione della rilevanza di un documento (e.g., \nquando ne calcoliamo la distanza rispetto ad un altro).  \nAd esempio, nel caso di una collezione di documenti relativi \nall’industria automobilistica, è piuttosto probabile avere il termine \n“automobile” in quasi ogni documento. \nTali termini dominerebbero dunque quelli più rari. \n \n11",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#11": "Modello TF-IDF\nUna rappresentazione alternativa che possiamo considerare è \nquella chiamata \n tf-idf\n. \nCome vedremo, questa rappresentazione enfatizza i termini \n“importanti”, individuati dalle seguenti caratteristiche: \n•\n appaiono frequentemente in un documento (“common locally”) \n•\n appaiono raramente nel corpus (“rare globally”) \n \n12",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#12": "Modello TF-IDF\nDeﬁniamo \n Document Frequency\n  (\ndf\n) per il termine \n t\n come il \nnumero di documenti nel corpus che contengono \n t\n. \nDeﬁniamo inoltre l’\n Inverse Document Frequency\n  come segue: \ndove N è la cardinalità del corpus. \n \n13idf t= logN\ndft",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#13": " \n14termine dft idft\ncar 18.165 1,65\nauto 6.723 2,08\ninsurance 19.241 1,62\nbest 25.235 1,5ESEMPIO: \nNella seguente tabella sono riportati alcuni esempi di valori df e idf \nrelativi alla collezione Reuters, costituita da 806.791 documenti:\nModello TF-IDF",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#14": "Modello TF-IDF\nIl tf-idf è deﬁnito come segue: \nIn sostanza il \n tf-idf\n per un termine \n t\n in un documento \n d\n assegna al \ntermine un peso nel documento che è: \n•\n molto elevato quando \n t\n è molto frequente in un piccolo numero \ndi documenti; \n•\n più basso quando il termine è poco frequente nel documento, \noppure quando è presente in molti documenti; \n•\n il più basso quando il termine compare in tutti i documenti. \n \n15tf-idf t,d=t f t,d ·idft",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#15": "Metriche\nVediamo ora come possiamo calcolare la distanza tra due \n item\n.\n \n16distanza( xi,xq)= |xi\u0000xq|\ndistanza( xi,xq)=q\n(xi[1]\u0000xq[1])2+···+(xi[d]\u0000xq[d])2\ndistanza( xi,xq)=q\n(xi\u0000xq)T·(xi\u0000xq)\nNel semplice caso di una dimensione possiamo deﬁnire la \nfunzione distanza come segue (Distanza Euclidea):\nNel caso di \n d \ndimensioni, la funzione \n distanza\n  può assumere la \nseguente forma (Distanza Euclidea):\nche possiamo riscrivere come segue, in forma matriciale:",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#16": "Metriche\n \n17distanza( xi,xq)=q\na1(xi[1]\u0000xq[1])2+···+ad(xi[d]\u0000xq[d])2\ndistanza( xi,xq)=q\n(xi\u0000xq)T·A ·(xi\u0000xq)\nNel caso in cui vogliamo pesare in modo diverso le varie \ndimensioni, possiamo usare una \n Scaled Euclidean distance\n :\nA=2\n664a10 ... 0\n0 a2 ... 0\n... ... ... ...\n00 ... a d3\n775\nche possiamo riscrivere come segue, in forma matriciale:\ndove:",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#17": "Cosine Similarity\nUna metrica largamente utilizzata per quantiﬁcare la similarità tra \ndue documenti \n x\ni \ne \nx\nq \nè la \n cosine similarity\n , che si avvale della \nrappresentazione vettoriale dei documenti: \n \n18sim(xi,xq)=xT\ni·xq\nkxik·kxqk\ndove il numeratore rappresenta il prodotto scalare tra i due vettori \ne il denominatore il prodotto tra i moduli dei due vettori. \nL’effetto del denominatore è dunque quello di normalizzare i \nvettori \n x\ni \ne \nx\nq \nottenendone i corrispondenti versori. Possiamo \ndunque riscrivere la precedente espressione come segue: \nsim(xi,xq)=ˆxT\ni·ˆxq",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#18": "Consideriamo ad esempio i documenti in ﬁgura a), rappresentati \nmediante i vari \n tf\n. La quantità: \n \n19Doc1 Doc2 Doc3\ncar 27 4 24\nauto 3 33 0\ninsurance 0 33 29\nbest 14 0 17Doc1 Doc2 Doc3\ncar 0,88 0,09 0,58\nauto 0,10 0,71 0\ninsurance 0 0,71 0,70\nbest 0,46 0 0,41\nCosine Similarity\nha i valori 30,56, 46,84 e 41,30 per Doc1, Doc2 e Doc3. \nApplicando la normalizzazione otteniamo la ﬁgura b): \na) \n b) kxk=vuutdX\nj=1x2\nj",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#19": "La similarità deﬁnita in precedenza corrisponde al coseno \ndell’angolo tra i due vettori: \n \n20θ\n01\n1sim(xi,xq)=xT\ni·xq\nkxik·kxqk= cos( ✓)\nˆxi\nˆxq\nCosine Similarity",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#2": "Document Retrieval\n \n3Diversamente dagli esseri umani,\nche presumibilmente …\nQuesto è un esempio di quello\nche gli informatici ….L’approccio ai sistemi simbolici\nnon è affatto ………\nPer rispondere a questa domanda\nl’approccio migliore …..\nProvate a pensare a come sarebbero ..Sfortunatamente, l’IA sta accelerando\nla sostituzione del capitale …..\nPotrebbe valere la pena correre dei\n rischi  ……\nIn precedenza, nel tentativo di\ndeﬁnire l’IA, …….Dopo la conferenza di Dartmouth,\nl’interesse …….\nPer affrontare la lettura occorre\n una certa dimestichezza …….\nNel testo si trovano inseriti  …….La relazione tra matrici ora\ndeﬁnita è riﬂessiva,  …….\nUna trasformazione può  …….\nPer questa ragione gran parte\ndegli psico-proseliti …….\nPer tutti costoro il massimo  …….Le ragioni per cui si è instaurata\nuna stretta connessione ….Il lettore può essere sorpreso  dalla\nquantità di spazio …..\nMolta della utilità di questo …….\nGalton si era convinto che i\ncaratteri mortali si ereditassero …..\nOltre a ciò Galton, memore dei  …….La situazione è tuttavia comple-\ntamente diversa …..\nUn’area in pieno sviluppo è  …..Storicamente, il modo più semplice\nper assistere i clienti …..\nAnche se quasi tutti i sistemi \nsperimentali  …..\nSupponiamo di avere disponibile un corpus di documenti: Come \npossiamo misurare la similarità tra di loro? Come possiamo \neffettuare ricerche? ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#20": "K-NN: Complessità della ricerca \n \n21Diversamente dagli esseri umani,\nche presumibilmente …\nQuesto è un esempio di quello\nche gli informatici ….L’approccio ai sistemi simbolici\nnon è affatto ………\nPer rispondere a questa domanda\nl’approccio migliore …..\nProvate a pensare a come sarebbero ..Sfortunatamente, l’IA sta accelerando\nla sostituzione del capitale …..\nPotrebbe valere la pena correre dei\n rischi  ……\nIn precedenza, nel tentativo di\ndeﬁnire l’IA, …….Dopo la conferenza di Dartmouth,\nl’interesse …….\nPer affrontare la lettura occorre\n una certa dimestichezza …….\nNel testo si trovano inseriti  …….La relazione tra matrici ora\ndeﬁnita è riﬂessiva,  …….\nUna trasformazione può  …….\nPer questa ragione gran parte\ndegli psico-proseliti …….\nPer tutti costoro il massimo  …….Le ragioni per cui si è instaurata\nuna stretta connessione ….Il lettore può essere sorpreso  dalla\nquantità di spazio …..\nMolta della utilità di questo …….\nGalton si era convinto che i\ncaratteri mortali si ereditassero …..\nOltre a ciò Galton, memore dei  …….Storicamente, il modo più semplice\nper assistere i clienti …..\nAnche se quasi tutti i sistemi \nsperimentali  …..La situazione è tuttavia comple-\ntamente diversa …..\nUn’area in pieno sviluppo è  …..\nIl calcolo delle distanze tra documenti può essere molto pesante \ncomputazionalmente quando N è molto elevato: ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#21": "K-NN: Complessità della ricerca \n \n22\nDato un \n query point\n , il costo della scansione su tutti i punti è: \n•\n O(N) per una query per 1-NN \n•\n O(N log k) per una query per k-NN \nPer rendere più efﬁciente la ricerca è possibile utilizzare una \nparticolare struttura dati, i \n KD-Trees\n . ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#22": "KD-Trees\n \n23\nPermette un’organizzazione strutturata degli item: \n•\n partiziona ricorsivamente i data point in “axis aligned boxes”. \nComporta un più efﬁciente pruning dello spazio di ricerca. \nOttiene buoni risultati in dimensioni “low-medium”. \nRiferimenti: \nBentley, J.L. “Multidimensional Binary Search Trees Used for Associative Searching”, in: \nCommunications of the ACM , 18(9), 1975, pp. 509-517.\nFriedman, J.H., Bentley, J.L., Finkel, R.A. “An Algorithm for Finding Best Matches in \nLogarithmic Expected Time”, in: ACM Transactions on Mathematical Software , 3(3), 1977, pp. \n209-226.",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#23": "KD-Trees\n \n24\nCostruzione dell’albero: \nData Point x[1] x[2]\n1 0,00 0,00\n2 1,00 4,31\n3 0,13 2,85\n… … …\nfeature 1feature 2",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#24": "KD-Trees\n \n25\nSplit relativo alla prima feature: \nData Point x[1] x[2]\n2 1,00 4,31\n… … …Data Point x[1] x[2]\n1 0,00 0,00\n3 0,13 2,85\n… … …x[1] > 0,5\n0,5x[1] ≤ 0,5",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#25": "KD-Trees\n \n26\nConsideriamo ora la parte sinistra: \nData Point x[1] x[2]\n2 1,00 4,31\n… … …Data Point x[1] x[2]\n1 0,00 0,00\n3 0,13 2,85\n… … …x[1] > 0,5\n0,5x[1] ≤ 0,5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#26": "KD-Trees\n \n27\nSplit relativo alla seconda feature: \nData Point x[1] x[2]\n3 0,13 2,85\n… … …Data Point x[1] x[2]\n1 0,00 0,00\n… … …x[2] > 0,1 x[2] ≤ 0,1\n0,1\n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#27": "KD-Trees\n \n28\nSi procede in tal modo ﬁno a completare l’albero: \n•split feature \n•split value \n•bounding box\nx[1] > 0,5 x[1] ≤ 0,5\nx[2] ≤ 0,1 x[2] > 0,1",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#28": "KD-Trees\n \n29\nEsempi di bounding box: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#29": "KD-Trees\n \n30\nEuristiche per effettuare le decisioni sugli splitting: \n•\n Scelta della dimensione (la più ampia, dim. alternate) \n•\n Valore della feature a cui effettuare lo split (mediana, centro \ndel box) \n•\n Condizione di terminazione (numero di punti sotto una \ndeterminata soglia, larghezza del box sotto una determinata \nsoglia)",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#3": "Nearest Neighbor\n \n4Diversamente dagli esseri umani,\nche presumibilmente …\nQuesto è un esempio di quello\nche gli informatici ….L’approccio ai sistemi simbolici\nnon è affatto ………\nPer rispondere a questa domanda\nl’approccio migliore …..\nProvate a pensare a come sarebbero ..Sfortunatamente, l’IA sta accelerando\nla sostituzione del capitale …..\nPotrebbe valere la pena correre dei\n rischi  ……\nIn precedenza, nel tentativo di\ndeﬁnire l’IA, …….Dopo la conferenza di Dartmouth,\nl’interesse …….\nPer affrontare la lettura occorre\n una certa dimestichezza …….\nNel testo si trovano inseriti  …….La relazione tra matrici ora\ndeﬁnita è riﬂessiva,  …….\nUna trasformazione può  …….\nPer questa ragione gran parte\ndegli psico-proseliti …….\nPer tutti costoro il massimo  …….Le ragioni per cui si è instaurata\nuna stretta connessione ….Il lettore può essere sorpreso  dalla\nquantità di spazio …..\nMolta della utilità di questo …….\nGalton si era convinto che i\ncaratteri mortali si ereditassero …..\nOltre a ciò Galton, memore dei  …….\nStoricamente, il modo più semplice\nper assistere i clienti …..\nAnche se quasi tutti i sistemi \nsperimentali  …..La situazione è tuttavia comple-\ntamente diversa …..\nUn’area in pieno sviluppo è  …..\nObiettivo: dato un documento \n x\nq\n, trovare l’articolo più simile nel \ncorpus di documenti disponibili: \ndocumento xqnearest neighbor",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#30": "KD-Trees\n \n31\nDato un query point (in verde), attraversiamo l’albero alla ricerca \ndel nearest neighbor. \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#31": "KD-Trees\n \n32\nPrima metà dell’area: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#32": "KD-Trees\n \n33\n.. e così via … \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#33": "KD-Trees\n \n34\nAbbiamo raggiunto la foglia che contiene il query point: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#34": "KD-Trees\n \n35\nCalcolo della distanza del NN tra i punti contenuti nella foglia: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#35": "KD-Trees\n \n36\nBacktrack e proviamo altri rami per ogni nodo visitato: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#36": "KD-Trees\n \n37\nValutiamo la distanza dal bounding box: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#37": "KD-Trees\n \n38\nLa distanza è minore di quella corrente, perciò visitiamo i \nsottoalberi (in questo caso le foglie). La prima ha distanza \nminore: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#38": "KD-Trees\n \n39\nBacktrack e visitiamo l’altra foglia: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#39": "KD-Trees\n \n40\nLa distanza dal bounding box è superiore alla minima, perciò \npossiamo potare il ramo: ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#4": "Algoritmo 1-NN\n \n5dist min = 1\nnearest doc = ;\nfor i=1,. . . ,N\n\u0000= distanza( xq,xi) ; distanza tra documento query ed o c u m e n t oi - e s i m o\nif\u0000<dist min\nnearest doc = xi; documento pi` u vicino corrente\ndist min = \u0000 ; distanza minima corrente\nreturn nearest doc\nInput: documento \n x\nq\n per la query e documenti \n x\n1\n , \nx\n2\n, … , \n x\nN \nOutput: documento \n x\ni\n più vicino (nearest_doc) a \n x\nq ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#40": "KD-Trees\n \n41\nBacktrack e proviamo altri rami per ogni nodo visitato: ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#41": "KD-Trees\n \n42\nLa distanza dal bounding box è superiore alla minima corrente, \nperciò possiamo potare il ramo: ",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#42": "KD-Trees\n \n43\nBacktrack e proviamo altri rami per ogni nodo visitato: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#43": "KD-Trees\n \n44\nLa distanza dal bounding box è superiore alla minima corrente, \nperciò possiamo potare il ramo: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#44": "KD-Trees\n \n45\nPruning complessivo: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#45": "Riferimenti\n \n46\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , Apogeo, 3a edizione, \n2015. \nMachine Learning: Clustering & retrieval\n , University of Washington - Coursera, \n2017. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. \nMurphy, K.P. \n Machine Learning - A Probabilistic Approach\n , The MIT Press, 2012.",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#5": " \n6Diversamente dagli esseri umani,\nche presumibilmente …\nQuesto è un esempio di quello\nche gli informatici ….L’approccio ai sistemi simbolici\nnon è affatto ………\nPer rispondere a questa domanda\nl’approccio migliore …..\nProvate a pensare a come sarebbero ..Sfortunatamente, l’IA sta accelerando\nla sostituzione del capitale …..\nPotrebbe valere la pena correre dei\n rischi  ……\nIn precedenza, nel tentativo di\ndeﬁnire l’IA, …….Dopo la conferenza di Dartmouth,\nl’interesse …….\nPer affrontare la lettura occorre\n una certa dimestichezza …….\nNel testo si trovano inseriti  …….La relazione tra matrici ora\ndeﬁnita è riﬂessiva,  …….\nUna trasformazione può  …….\nPer questa ragione gran parte\ndegli psico-proseliti …….\nPer tutti costoro il massimo  …….Le ragioni per cui si è instaurata\nuna stretta connessione ….Il lettore può essere sorpreso  dalla\nquantità di spazio …..\nMolta della utilità di questo …….\nGalton si era convinto che i\ncaratteri mortali si ereditassero …..\nOltre a ciò Galton, memore dei  …….\nStoricamente, il modo più semplice\nper assistere i clienti …..\nAnche se quasi tutti i sistemi \nsperimentali  …..La situazione è tuttavia comple-\ntamente diversa …..\nUn’area in pieno sviluppo è  …..\nObiettivo: dato un documento \n x\nq\n, trovare i k articoli più simili nel \ncorpus di documenti disponibili: \ndocumento xqk nearest neighbors\nk Nearest Neighbors",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#6": "Algoritmo k-NN\n \n7\nInput: documento \n x\nq\n per la query e documenti \n x\n1\n , \nx\n2\n, … , \n x\nN \nOutput: lista dei k documenti più vicini a \n x\nq \nlista kdist min = sort( \u00001,\u00002,..., \u0000k)\nlista knearest doc = sort( x1,x2,..., xk)\nfor i=k+1,. . . ,N\n\u0000= distanza( xq,xi) ; distanza tra documento query ed o c u m e n t oi - e s i m o\nif\u0000<lista kdist min[ k]\ninserisci \u0000in lista kdist min ; inserimento in lista ordinata\ninserisci xiin lista knearest doc ; inserimento in lista ordinata\nreturn lista knearest doc",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#7": "Criticità nella NN search\nPer effettuare una ricerca dei nearest neighbors occorre risolvere i \nseguenti problemi: \n•\nCome rappresentare gli item coinvolti (nel nostro esempio i \ndocumenti). \n•\nCome valutare la distanza tra gli item, ossia deﬁnire una metrica \nche consenta di calcolare la similarità tra i vari item. \n \n8",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#8": "Richiami su \nRappresentazione dei Documenti\nVediamo ora due possibili metodi per la rappresentazione dei \ndocumenti non strutturati: \n•\n bag of words \n•\n tf-idf \n (term frequency - inverse document frequency)  \n \n9",
    "data_test\\rootfolder\\università\\MachineLearning\\10-Algoritmo K-NN-sbloccato.pdf#9": "Modello Bag-of-Words\nIn questo modello è ignorato l’esatto ordine dei termini nel \ndocumento. \nViene preso in considerazione solo il numero di occorrenze (\n term \nfrequency\n : \ntf\n) di ogni termine nel documento. \nIn tal modo è possibile rappresentare ogni documento mediante \nun vettore di occorrenze: \n \n10Doc1 Doc2 Doc3\ncar 27 4 24\nauto 3 33 0\ninsurance 0 33 29\nbest 14 0 17",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nMachine Learning con Python: \nIntroduzione  \n\u0000\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#1": "Introduzione al \nMachine Learning con Python   \nTesto consigliato:\n\u00002Andreas C. Müller,  Sarah Guido\n“Introduction to Machine Learning with Python - A guide for Data Scientists”\nO’Reilly, 2017.\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#10": "\u000011\nCaricamento del  \nDataset IRIS\nIl nostro obiettivo è quello di realizzare un modello di \nmachine learning che apprenda, dagli esempi disponibili, \ncome classiﬁcare la specie di un nuovo ﬁore iris partendo \ndalle 4 misure relative ai suoi petali e sepali.\nIl dataset Iris è incluso in scikit-learn  nel modulo \ndatasets . E’ possibile caricarlo chiamando la funzione \nload_iris :\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#11": "\u000012\nL’oggetto iris restituito da load_iris  è un Bunch  object, ed \nè simile a un dizionario. Esso contiene chiavi e valori:\nDataset IRIS",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#12": "\u000013\nBunch objects\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#13": "\u000014\nIl valore associato alla chiave DESCR  è una descrizione \nsintetica del dataset. Vediamo i primi caratteri di tale \ndescrizione:\nDataset IRIS",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#14": "\u000015\nVediamo i valori associati alle chiavi target_names  e \nfeature_names :\nDataset IRIS",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#15": "\u000016\nI dati sono contenuti nei campi data e target . Vediamo il \ntipo e lo shape di data:\nDataset IRIS\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#16": "\u000017\nL’array target  contiene le specie di ciascuno dei ﬁori del \ndataset ( 0 per setosa , 1 per versicolor , 2 per virginica ): \nDataset IRIS\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#17": "I dati che useremo per il training e il test possono essere \nvisti come segue:\n\u000018\nData Set IRIS\nsepal \nlengthsepal \nwidthpetal \nlengthpetal \nwidthTarget \n(Iris species)\n5.9 3.0 4.2 1.5 1\n5.8 2.6 4.0 1.2 1\n6.8 3.0 5.5 2.1 2\n4.7 3.2 1.3 0.2 0\n6.9 3.1 5.1 2.3 2",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#18": "Esempi di valori delle feature presenti in data:\n\u000019\nData Set IRIS\nsepal \nlengthsepal \nwidthpetal \nlengthpetal \nwidthTarget \n(Iris species)\n5.9 3.0 4.2 1.5 1\n5.8 2.6 4.0 1.2 1\n6.8 3.0 5.5 2.1 2\n4.7 3.2 1.3 0.2 0\n6.9 3.1 5.1 2.3 2\nX",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#19": "Esempi di valori delle specie presenti in target :\n\u000020\nData Set IRIS\nsepal \nlengthsepal \nwidthpetal \nlengthpetal \nwidthTarget \n(Iris species)\n5.9 3.0 4.2 1.5 1\n5.8 2.6 4.0 1.2 1\n6.8 3.0 5.5 2.1 2\n4.7 3.2 1.3 0.2 0\n6.9 3.1 5.1 2.3 2\ny",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#2": "\u00003https://github.com/amueller/introduction_to_ml_with_python\nGli esempi di codice presentati nel libro sono disponibili \nnel seguente sito :\nIntroduzione al \nMachine Learning con Python   ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#20": "\u000021\nSuddivisione in \nTraining Set e Test Set \nscikit-learn  contiene una funzione che mescola il dataset \ndelle osservazioni disponibili e ne fa la suddivisione in  \ntraining set e test set . Si tratta della funzione:\ntrain_test_split\nQuesta funzione estrae il 75% degli esempi per formare il \ntraining set , costituito quindi dal 75% delle righe in data \ne le corrispondenti label in target .\nIl rimanente 25% degli esempi va a costituire il test set .",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#21": "\u000022\nIstruzioni per lo split:\nndarray da ripartire tra  \nX_train e X_testfunzione per lo split\nndarray da ripartire tra  \ny_train e y_test\nSuddivisione in \nTraining Set e Test Set ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#22": "\u000023\nShape delle variabili relative al training set e al test set:\nSuddivisione in \nTraining Set e Test Set ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#23": "\u000024\nIspezione dei Dati\nPrima di costruire un modello di machine learning è \nsempre opportuno ispezionare i dati disponibili, intanto \nper capire se il problema è risolvibile mediante tecniche \ndi machine learning.\nL’ispezione è utile anche per la individuazione di \neventuali anomalie, inconsistenze, ecc.\nUn ottimo metodo per effettuare tale ispezione è quello \ndi visualizzare i dati in questione (e.g., scatter plot).\nNella ﬁgura che segue viene rappresentato un “pair plot” \ndelle feature relativamente alle osservazioni del training \nset per il nostro esempio.",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#24": "\u000025\nIspezione dei Dati",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#25": "\u000026\nK-Nearest Neighbors \nCostruzione del Modello \nL’algoritmo k-NN  in scikit-learn  è implementato nella \nclasse KNeighborsClassiﬁer  nel modulo neighbors .\nPrima di usare il modello, dobbiamo istanziare la classe in \nun oggetto. In tal modo impostiamo i parametri del \nmodello.\nIl parametro più importante di KNeighborsClassiﬁer è il \nnumero del neighbors, che in questo caso impostiamo a 1:\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#26": "\u000027\nK-Nearest Neighbors \nCostruzione del Modello \nL’oggetto knn incapsula l’algoritmo che sarà utilizzato per \ncostruire il modello a partire dai dati di training, così \ncome l’algoritmo per fare le previsioni su nuovi data \npoints.\nNel caso di KNeighborsClassiﬁer  verrà semplicemente \nmemorizzato il training set.",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#27": "\u000028\nAddestramento del Modello\nPer addestrare il modello sul training set, scikit-learn  \nmette a disposizione il metodo ﬁt da chiamare \nsull’oggetto knn:\nmetodo per l’addestramento",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#28": "\u000029\nUso del Modello  \nper effettuare Previsioni \nDato un nuovo data point da classiﬁcare:\nfeatures del nuovo ﬁore da classiﬁcare ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#29": "\u000030\n… possiamo usare il metodo predict per la predizione \ndella sua specie:\nmetodo per la predizione\nUso del Modello  \nper effettuare Previsioni ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#3": "Testo consigliato:\n\u00004A. Géron\n“Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow”\nO’Reilly, 2019.\nIntroduzione al \nMachine Learning con Python   ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#30": "\u000031\nValutazione del Modello \nPer valutare il modello possiamo richiamare il metodo \npredict  su tutti gli esempi del test set:\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#31": "\u000032\nCalcolo del punteggio:\nValutazione del Modello ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#32": "\u000033\nSintesi della esercitazione\nDeﬁnizione di un task di machine learning (classiﬁcazione delle specie Iris \nmediante k-NN ).\nIndividuazione dei data points  (esempi, osservazioni) disponibili per \naddestrare il sistema (supervised learning task)\nSplit  del dataset, import  della classe, che poi è istanziata su un oggetto (setting \nparameters)\nFase di addestramento (metodo ﬁt) e valutazione  (metodo score ):\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#4": "\u00005\nScikit-learn  è uno dei package più popolari per il \nMachine Learning in Python.\nMette a disposizione efﬁcienti implementazioni di \nnumerosi algoritmi di ML e un gran numero di utili \nstrumenti per attività di pre- e post- processing relative a \ntask di ML.\nE’ possibile trovare la documentazione necessaria nel \nseguente sito: \nhttp://scikit-learn.org\nScikit-Learn ",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#5": "\u00006\nJupiter Notebook : ambiente interattivo per eseguire \ncodice nel browser.\nNumPy : è uno dei package fondamentali per il calcolo \nscientiﬁco in Python.\nLibrerie e Strumenti \npandas : libreria per data wrangling e analisi.\nmatplotlib : libreria per il plotting.\nmglearn : libreria di utility functions che gli autori (Müller \n& Guido) hanno scritto per il loro libro, in modo da non \n“intasare” il codice presentato con dettagli di plotting e di \ndata loading.",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#6": "Versioni linguaggio e librerie:\n\u00007\nAmbiente di Sviluppo \n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#7": "Cominciamo con un semplice esempio di classiﬁcation, \nutilizzando il data set Iris.\nQuesto è un famoso data set che contiene 150 esempi di \nﬁori iris, descritti da 4 features (lunghezza e larghezza di \npetali e sepali) e appartenenti ad una di tre specie \ndifferenti:\nIris setosa\nIris versicolor  \nIris virginica  \n\u00008\nUna prima applicazione:  \nClassificazione delle specie IRIS",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#8": "Ecco un esempio di ﬁori iris relativo alle tre specie:\n\u00009\nUna prima applicazione:  \nClassificazione delle specie IRIS\n",
    "data_test\\rootfolder\\università\\MachineLearning\\11-ML con Python - Introduzione-sbloccato.pdf#9": "\u000010\nUna prima applicazione:  \nClassificazione delle specie IRIS\nPetal\nSepal\nLa classiﬁcazione può essere fatta in base ai valori delle 4 \nfeatures, ossia lunghezza e larghezza dei petali e sepali:",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#0": "Intelligenza Artiﬁciale \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Regressione (Ex02)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#1": "Sommario\nDataset Better Life Index \nRichiami: Model Selection, Simple Linear Regression, Funzione di Costo \nLibreria Scikit-learn \nLinear Regression in Python \nEsempio: Dataset Diabete \nLinear Regression e Predizione \nEsercitazione su dataset Better Life Index",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#10": "Il modulo \n linear_model\n  di \nsklearn\n  implementa l'addestramento basandosi \nsu un modello lineare. La funzione di costo di default è la \n RSS\n. \nLa funzione \n ﬁt()\n prende come parametri due arrays \n X\n e \ny\n, effettua \nl'addestramento (o \n ﬁtting\n ) e memorizza i coefﬁcienti nella variabile \n coef_\n . \nAd esempio: \n>>> \nfrom \nsklearn \nimport\n linear_model \n>>> \nreg \n=\n linear_model\n .\nLinearRegression() \n>>> \nreg\n.\nfit([[\n0\n, \n0\n], [\n1\n, \n1\n], [\n2\n, \n2\n]], [\n0\n, \n1\n, \n2\n]) \nLinearRegression()  \n>>> \nreg\n.\ncoef_ \narray([0.5, 0.5]) \nNell'esempio si impiegano 2 valori (cioè 2 features) per punto, e la retta ha \n2 coefﬁcienti \n w\n1\n. Il valore di \n w\n0\n si ottiene con la variabile \n intercept_\n  del \nmodello. \nNota\n : l'underscore nel nome delle variabili indica che i valori sono ottenuti \ndurante l'addestramento, e perciò non sono iperparametri del modello.\nLinear Regression in Python\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#11": "Il modulo \n metrics\n  di \nsklearn\n  implementa varie misure di performance. \nhttps://scikit-learn.org/stable/modules/model_evaluation.html  \nNota: troviamo MSE ma non RSS.\nScikit-learn e le misure di performance\n12\n",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#12": "Un dataset diabete è un dataset \n toy \n(cioè utile per scopi didattici e per \ntestare il codice)  \n disponibile all'interno della libreria scikit-learn. \nURL: \n https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html   \nhttps://scikit-learn.org/stable/datasets/toy_dataset.html   \n442 istanze \n10 features reali normalizzate ove richiesto -.2 < x < .2 \nage age in years\nsex\nbmi body mass index\nbp average blood pressure\ns1 tc, total serum cholesterol\ns2 ldl, low-density lipoproteins\ns3 hdl, high-density lipoproteins\ns4 tch, total cholesterol / HDL\ns5 ltg, possibly log of serum triglycerides level\ns6 glu, blood sugar level\nTarget: intero nell'intervallo 25 - 346 che indica quanto la malattia sia \naccresciuta dopo 1 anno\nEsempio: dataset diabete\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#13": "Codice per impiegare il dataset: \nimport \nmatplotlib.pyplot  \nas \nplt \nimport \nnumpy \nas \nnp \nfrom \nsklearn \nimport\n datasets, linear_model \nfrom \nsklearn.model_selection  \nimport\n train_test_split \nfrom \nsklearn.metrics  \nimport \nmean_squared_error\n , \nr2_score  \n# Carico il dataset  \ndiabetes_X, diabetes_y \n = \ndatasets\n .\nload_diabetes\n (return_X_y\n =\nTrue\n) \n# Mantengo solo la terza feature  \ndiabetes_X \n =\n diabetes_X[:, \n np\n.\nnewaxis\n, \n2\n] \n# Suddivido il dataset in training/test 80/20%  \ndiabetes_X_train,diabetes_X_test,diabetes_y_train, diabetes_y_test \n =    \n  train_test_split(diabetes_X, diabetes_y,test_size=0.2) \n... \nEsercizio\n : completa il codice impiegando un modello lineare, \nvisualizzando il valore dei coefﬁcienti e l'errore MSE.\nEsempio Python: diabete (1)\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#14": "# Istanzia un modello di regressione lineare  \nregr \n= \nlinear_model\n .\nLinearRegression\n () \n# Addestramento con funzione di costo RMSE  \nregr\n.\nfit(diabetes_X_train, diabetes_y_train) \n# Ricava le predizioni sul test set  \ndiabetes_y_pred \n =\n regr\n.\npredict(diabetes_X_test) \n# Stampa i parametri del modello  \nprint\n(\n\"Coefficients: \n \\n\n\"\n, regr\n.\ncoef_) \n# Valuto il MSE  \nprint\n(\n\"Mean squared error: \n %.2f\n\" \n% \nmean_squared_error\n (diabetes_y_test, diabetes_y_pred)) \nplt\n.\nscatter\n(diabetes_X_test, diabetes_y_test, color\n =\n\"black\"\n) \nplt\n.\nplot\n(diabetes_X_test, diabetes_y_pred, color\n =\n\"blue\"\n, linewidth\n =\n3\n) \nplt\n.\nxticks\n(()) \nplt\n.\nyticks\n(()) \nplt\n.\nshow\n() \n# Coefficients:   [938.23786125]  \n# Mean squared error: 2548.07\nEsempio Python: diabete (2)\n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#15": "Il modulo \n linear_model\n  permette facilmente di fare predizione sui dati. \nLa funzione \n predict()\n  prende un array di istanze (una o più features) e \nricava il valore in base al modello addestrato. \nX_new = [[\n 22587\n]] \nprint\n(model.predict(X_new))\nLinear Regression e predizione\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#16": "Il problema consiste nel determinare i due parametri \n w\n. Chiaramente il \nmodello può solo approssimare la correlazione tra i valori.  \nSe facciamo più ipotesi, cioè creiamo più modelli, ci occorre una misura di \nperformance (o di costo) per confrontarli e scegliere il più adatto.\nEsempio: dataset Better Life Index (3)\n17PIL pro capiteLivello di soddisfazione\nPIL pro capitew0=8w1=-5×10-5\nw0=4w1=5×10-5 w0=0w1=2×10-5w0=?\nw1=?Livello di soddisfazione",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#17": "Accedi al seguente notebook dove i dati sono già caricati e formattati per gli step \nsuccessivi:  \nhttps://colab.research.google.com/drive/1apLqC0KAveOkkCT8JuO5kNQyj1GT5Hz9?usp=sharing   \nRisolvi i seguenti esercizi: \nEsercizio #1\n : crea e addestra un modello lineare con funzione di costo RSS \nEsercizio #2\n : visualizza i parametri del modello \nEsercizio #3\n : prendi tre campioni random dal dataset e ricava la predizione in base \nal modello addestrato \nEsercizio #4\n : calcola RSS MSE e RMSE valutando i tre campioni \nEsercizio #5\n : suddividi il dataset in input in train e test con un rapporto 80/20 \nEsercizio #6\n : addestra nuovamente il modello, e ricava RSS MSE e RMSE sui test set \nEsercizio #7\n : suddividi nuovamente il dataset ma con un rapporto 50/50. Valuta \nnuovamente le performance del modello e discuti eventuali differenze nei valori \nottenuti.\nEsercizio Python: Better Life Index\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#18": "Aurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017 \nAndreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016\nTesti di Riferimento\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#2": "Richiami\nLe tecniche di \n Regressione\n  ricadono nell'ambito dell'apprendimento \nModel-based\n , dove  \nSi costruisce un modello che rappresenta le caratteristiche dei dati in \ningresso (es. andamento).  \nTale modello verrà poi impiegato nella fase di \n predizione\n  su istanze in \ningresso distinte da quelle impiegate durante l'apprendimento.\n3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#3": "Datasets\nDurante le esercitazioni faremo uso di vari datasets, alcuni reali, altri \nsintetici che ci permetteranno di mettere in evidenza vari aspetti e \nproblematiche rilevanti nell'ambito dell'apprendimento automatico.\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#4": "Esempio: dataset Better Life Index (1)\nDataset che lega il benessere (life satisfaction) con indicatori giudicati essenziali \nnella vita quotidiana (es. salario, livello istruzione), suddivisi per nazione. \nhttps://stats.oecd.org/index.aspx?DataSetCode=BLI  \n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#5": "Esempio: dataset Better Life Index (2)\nValore del PIL (GDP) annuale \nhttps://www.imf.org/external/datamapper/NGDP_RPCH@WEO/OEMDC/ADVEC/WEOWORLD\n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#6": "Se prendiamo i due dataset e creiamo un join, possiamo mettere in \ncorrelazione due variabili, es. PIL pro capite e livello di soddisfazione \npercepito. \nSi nota come i due valori siano correlati, sebbene non esattamente, con un \nlegame lineare. \nPossiamo supporre che esista un modello \n lineare\n  (o \nordinary least squares\n ) \nche leghi la soddisfazione con il valore del PIL (fase di \n model selection\n ).\nRichiami: Model selection\n7\nPIL pro capiteLivello di soddisfazione\n",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#7": "Richiami: Simple Linear Regression Model\nI parametri del modello lineare sono \n w\n0\n e \nw\n1\n. \nAttenzione: non esiste un formalismo standard per rappresentare i \nparametri, a volte si impiega \n θ\n o altri simboli. \nI parametri del modello lineare sono \n w\n0\n e \nw\n1\n.  \nAdattando tali valori possiamo deﬁnire qualsiasi modello lineare.\n8yi=w0+w1xi+✏i\nˆyi=f(xi)=w0+w1xiy\nx\nPIL pro capiteLivello di soddisfazione",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#8": "Tipicamente is impiega una misura di costo basata sulla distanza tra valore \nesatto e valore determinato dal modello, come la \n Residual Sum of Squares  \n(RSS), sempre positiva, chiamata anche \n Sum of Squared Residuals\n  (SSR) o \nSum of Squared estimate of Errors\n  (SSE): \nValori prossimi allo \n 0\n indicano un modello ideale. \nLa \nMean Square Error \n (MSE), chiamata anche \n Mean Squared Deviation  \n(MSD), corrisponde alla RSS normalizzata sul numero di campioni.  \nÈ utile per valutare il modello ﬁnale dopo l'addestramento.\nRichiami: funzione di costo\n9RSS( w0,w1)=NX\ni=1(yi\u0000ˆyi)2=NX\ni=1[yi\u0000(w0+w1xi)]2",
    "data_test\\rootfolder\\università\\MachineLearning\\12-Ex_02 Esercitazione su Regressione-sbloccato.pdf#9": "È la libreria con licenza aperta (BSD) più conosciuta di machine learning in \nPython. La prima release risale al 2010.  \nhttps://scikit-learn.org/stable/   \nInclude gli algoritmi più popolari di classiﬁcazione, regressione, clustering \n(es. support-vector machines, random forests, gradient boosting, k-means e \nDBSCAN). \nAlcune parti del codice sono state scritte in modo altamente efﬁciente con \nvarie tecnologie (vedi Cython) \nÈ facilmente interfacciabile con altre librerie per la gestione e il calcolo \nnumerico di dati, es. NumPy (algebra lineare), SciPy (ottimizzazione, \nalgebra lineare, analisi dei segnali, etc) e Pandas.\nRichiami: la libreria Scikit-learn\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Regressione e Classiﬁcazione (Ex03)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#1": "Sommario\nRichiami: Classiﬁcazione e Regressione, overﬁtting, underﬁtting \n4 datasets: Forge, Wave, Wisconsin breast cancer, Boston housing \nClassiﬁcazione k-Neighbors e Scikit-learn, decision boundaries \nMisura R\n2 \nk-Neighbors regression e Scikit-learn \nEsercizi su linear, Ridge e LASSO regressioni su vari dataset",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#10": "Regression: Boston housing dataset\nIl dataset di 506 istanze mira a predire il costo degli immobili residenziali in \nvari quartieri di Boston nel 1970, impiegando 13 features (es. il tasso di \ncrimine, vicinanza al ﬁume, accessibilità alle autostrade). \nfrom \nsklearn.datasets \n import \nload_boston\nboston \n= \nload_boston\n ()\nprint\n(\n\"Data shape: {}\"\n .\nformat\n(\nboston\n.\ndata\n.\nshape\n))\n> Data shape: (506, 13)\nÈ possibile combinare due o più features creandone ulteriori non presenti nel \ndataset originale, attività che rientrano nella fase di \n feature engineering, \n dove \nsi identiﬁcano o costruiscono le caratteristiche salienti. \nIn questo esempio combiniamo 2 features alla volta: \nX\n, \ny \n= \nmglearn\n.\ndatasets\n .\nload_extended_boston\n ()\nprint\n(\n\"X.shape: {}\"\n .\nformat\n(\nX\n.\nshape\n))\n> X.shape: (506, 104)\nOra abbiamo 104 features, ottenute dalle 13 originali con tutte le 91 possibili \ncombinazioni di coppie.\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#11": "Classiﬁcazione k-Neighbors (k-NN)\nNel caso più semplice l'algoritmo k-NN considera solo 1 vicino (k=1), che \nrisulta il più vicino all'istanza su cui vogliamo esprimere una predizione. \nmglearn\n.\nplots\n.\nplot_knn_classification\n (\nn_neighbors\n =\n1\n)\nPer k=3: \nmglearn\n.\nplots\n.\nplot_knn_classification\n (\nn_neighbors\n =\n3\n)\nPer il forge dataset otteniamo:\n12\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#12": "Esercizio: Scikit-learn, k-NN e valutazione\nLa libreria Scikit-learn rende disponibile la classe \n KNeighborsClassiﬁer\n  per \ncreare modelli basati sull'algoritmo k-NN. \nLa funzione \n score\n ()\n valuta l'accuracy media, cioè il numero di label \ncorrettamente stimate rispetto al totale delle istanze valutate. \nfrom \nsklearn.neighbors \n import \nKNeighborsClassifier\nmodel \n= \nKNeighborsClassifier\n (\nn_neighbors\n =\n3\n)\nmodel\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Test set accuracy: {:.2f}\"\n .\nformat\n(\nclf\n.\nscore\n(\nX_test\n, \ny_test\n)))\nEsercizio\n : (1) prendere il dataset forge, (2) creare una partizione training/\ntest, (3) addestrare un classiﬁcatore KNeighborsClassiﬁer  \ne (4) valutarne \nl'accuratezza. \n13",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#13": "k-NN e decision boundaries\nIn presenza di 2 features è possibile rappresentare su un piano \n 2d\n la classe \nche verrebbe assegnata dal modello per ogni punto del piano, così da \nriconoscere il conﬁne tra una label e l'altra. Sfruttiamo la libreria \n mglearn\n : \nfig\n, \naxes \n= \nplt\n.\nsubplots\n (\n1\n, \n3\n, \nfigsize\n=\n(\n10\n, \n3\n))\nfor \nn_neighbors\n , \nax \nin \nzip\n([\n1\n, \n3\n, \n9\n], \naxes\n):\n \n clf \n= \nKNeighborsClassifier\n (\nn_neighbors\n =\nn_neighbors\n )\n.\nfit\n(\nX\n, \ny\n)\n  \nmglearn\n.\nplots\n.\nplot_2d_separator\n (\nclf\n, \nX\n, \nfill\n=\nTrue\n, \neps\n=\n0.5\n, \nax\n=\nax\n, \nalpha\n=.\n4\n)\n  \nmglearn\n.\ndiscrete_scatter\n (\nX\n[:, \n0\n], \nX\n[:, \n1\n], \ny\n, \nax\n=\nax\n)\n  \nax\n.\nset_title\n (\n\"{} neighbor(s)\"\n .\nformat\n(\nn_neighbors\n ))\n  \nax\n.\nset_xlabel\n (\n\"feature 0\"\n )\n  \nax\n.\nset_ylabel\n (\n\"feature 1\"\n )\naxes\n[\n0\n]\n.\nlegend\n(\nloc\n=\n3\n)\nQuali considerazioni possiamo fare?\n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#14": "k-NN e decision boundaries\n1-NN segue in modo migliore i dati di addestramento.  \nPer k > 1 crea un conﬁne più \"dolce\" e un modello più semplice.  \nCosa succede se k corrisponde al numero di istanze del dataset?\n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#15": "k-NN e decision boundaries\n1-NN segue in modo migliore i dati di addestramento.  \nPer k > 1 crea un conﬁne più \"dolce\" e un modello più semplice.  \nCosa succede se k corrisponde al numero di istanze del dataset?  \nTutte le istanze avrebbero lo stesso neighbors e le predizioni sarebbero \nsempre le stesse.\n16\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#16": "Esercizio: studio dell'accuracy\nColleziona le accuracy del classiﬁcatore KNeighborsClassiﬁer sul training \nset sia sul test set, al variare di k in [1,10], e valuta gli andamenti. \nfrom \nsklearn.datasets \n import \nload_breast_cancer\ncancer \n= \nload_breast_cancer\n ()\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nstratify\n =\ncancer\n.\ntarget\n, \nrandom_state\n =\n66\n)\ntraining_accuracy \n = \n[]\ntest_accuracy \n = \n[]\n...\nplt\n.\nplot\n(\nneighbors_settings\n , \ntraining_accuracy\n , \nlabel\n=\n\"training accuracy\"\n )\nplt\n.\nplot\n(\nneighbors_settings\n , \ntest_accuracy\n , \nlabel\n=\n\"test accuracy\"\n )\nplt\n.\nylabel\n(\n\"Accuracy\"\n )\nplt\n.\nxlabel\n(\n\"n_neighbors\"\n )\nplt\n.\nlegend\n()\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#17": "Esercizio: studio dell'accuracy\nCon k=1 si ha una accuracy massima per il training set. Con \n k\n più grandi la \ncomplessità del modello si riduce e l'accuracy decrementa. \nAl contrario, con \n k=1 \nl'accuracy sul test set è più bassa (circa 0.90), \nsintomo che il modello è \n troppo complesso\n . Allo stesso modo con \n k\n elevati \nl'accuracy \n non è soddisfacente \n poiché il  \nmodello è \n troppo semplice\n . \nPer questo dataset un valore ottimale si ottiene intorno a k=6. \nAttenzione:  solitamente i graﬁci non sono sempre così \n smooth\n .\n18\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#18": "k-neighbors regression\nRichiami: per \n k=1\n, il valore predetto corrisponde al valore associato \nall'istanza più vicina. Nel caso k > 1, si mediano i valori. \nmglearn\n.\nplots\n.\nplot_knn_regression\n (\nn_neighbors\n =\n1\n)\n19\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#19": "k-neighbors regression (2)\nRichiami: per \n k=1\n, il valore predetto corrisponde al valore associato \nall'istanza più vicina. Nel caso k > 1, si mediano i valori. \nmglearn\n.\nplots\n.\nplot_knn_regression\n (\nn_neighbors\n =\n3\n)\n20\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#2": "Richiami: classiﬁcazione e regressione\nFinora abbiamo visto problemi di regressione, dove si richiede di predire \nun valore numerico a partire da una istanza in ingresso. \nL'obiettivo della classiﬁcazione è assegnare una \n label\n ad una istanza in \ningresso.  \nSe le label sono due si parla di \n binary classiﬁcation\n , altrimenti \n multiclass\n . \nIl dataset \n iris\n è un esempio di multiclass classiﬁcation. \nUn modello ben addestrato mostra la capacità di generalizzare sui dati del \ntest set, e in fase di produzione. \nChiaramente se training set e test set hanno molte caratteristiche in \ncomune, allora ci aspettiamo che il modello addestrato, se ben progettato, \nsia anche accurato.\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#20": "Performance: la misura \n R\n2\n21\nR\n2\n è il \ncoefﬁciente di determinazione\n , e indica la porzione di varianza della \nvariabile y correttamente predetta dal modello (cioè dalle features impiegate), \nperciò è una misura di accuratezza. \nAssume valori in [0,1]. Per valori prossimi a \n 1\n il modello predice \naccuratamente il valore della variabile dipendente \n y\n in base al valore delle \nfeatures.  \nAd esempio: per \n R\n2\n=0.83, il 17% della variazione nei dati non è rappresentato dal \nmodello, o perché è dovuto al caso, o perché dipende da un features che non sono stata \nconsiderate. \nPer valori vicini a 0, il modello si comporta come un predittore che assume \nsempre il valor medio come output, perciò non tiene conto della varianza \nLa funzione \n score()\n  del modello Python valuta il valore \n R\n2\n sui dati in input.R2=1−RSS\n∑N\ni=1(yi−¯y)2=1−∑N\ni=1(yi−̂yi)2\n∑N\ni=1(yi−¯y)2=∑N\ni=1(̂yi−¯y)2\n∑N\ni=1(yi−¯y)2",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#21": "scikit-learn: k-neighbors regression\nLa classe KNeighborsRegressor implementa l'algoritmo di regressione k-neighbors. \nIl parametro \n n_neighbors\n  corrisponde a \n k\n. \nreg \n= \nKNeighborsRegressor\n (\nn_neighbors\n =\nk\n)\nEsercizio\n : completa il codice con la classe suddetta e valuta i graﬁci che ottieni:  \nfig\n, \naxes \n= \nplt\n.\nsubplots\n (\n1\n, \n3\n, \nfigsize\n=\n(\n15\n, \n4\n))\n# create 1,000 data points, evenly spaced between -3 and 3\nline \n= \nnp\n.\nlinspace\n (\n-\n3\n, \n3\n, \n1000\n)\n.\nreshape\n(\n-\n1\n, \n1\n)\n# make predictions using 1, 3, or 9 neighbors\nfor \nn_neighbors\n , \nax \nin \nzip\n([\n1\n, \n3\n, \n9\n], \naxes\n):\n...\nax\n.\nplot\n(\nline\n, \nreg\n.\npredict\n(\nline\n))\nax\n.\nplot\n(\nX_train\n, \ny_train\n, \n'^'\n, \nc\n=\nmglearn\n.\ncm2\n(\n0\n), \nmarkersize\n =\n8\n)\nax\n.\nplot\n(\nX_test\n, \ny_test\n, \n'v'\n, \nc\n=\nmglearn\n.\ncm2\n(\n1\n), \nmarkersize\n =\n8\n)\nax\n.\nset_title\n (\n\"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\"\n .\nformat\n(\nn_neighbors\n , \nreg\n.\nscore\n(\nX_train\n, \ny_train\n),\nreg\n.\nscore\n(\nX_test\n, \ny_test\n)))\nax\n.\nset_xlabel\n (\n\"Feature\"\n )\nax\n.\nset_ylabel\n (\n\"Target\"\n )\naxes\n[\n0\n]\n.\nlegend\n([\n\"Model predictions\"\n , \n\"Training data/target\"\n ,\n         \n \"Test data/target\"\n ], \nloc\n=\n\"best\"\n)\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#22": "scikit-learn: k-neighbors regression\nConsiderando più istanze durante la predizione si ottiene chiaramente una \ncurva più \n smooth\n .\n23\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#23": "k-NN nella pratica\nL'algoritmo ha due parametri principali: \n k\n e la \n misura di distanza\n . \nIn generale, si possono usare bassi valori per \n k\n (es. 5) sebbene occorra \nsperimentare il valore esatto in base al dataset.  \nLa \nmisura euclidea\n  si adatta bene in molti scenari. \nIl k-NN è spesso la scelta iniziale per la sua semplicità, ma in alcuni \ncontesti non è adatto: \nPer training set molto grandi (numero di istanze e/o features) che \ncausano tempi di predizione lenti, a meno di non precomputare \nl'output in una fase preliminare prima di impiegare l'algoritmo in \nproduzione. \nDataset sparsi, cioè con features spesso senza valore.\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#24": "Esercizio: linear regression e wave dataset\nEsercizio\n : applicare la \n linear regression\n  al wave dataset con 60 istanze. \nRicavare i parametri del modello. Valutare il valore \n R\n2\n.\n25",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#25": "Esercizio: linear regression e wave dataset\nEsercizio\n : applicare la \n linear regression\n  al wave dataset con 60 istanze. \nRicavare i parametri del modello. Valutare il valore \n R\n2\n. \nfrom \nsklearn.linear_model \n import \nLinearRegression\nX\n, \ny \n= \nmglearn\n.\ndatasets\n .\nmake_wave\n (\nn_samples\n =\n60\n)\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\nX\n, \ny\n, \nrandom_state\n =\n42\n)\nlr \n= \nLinearRegression\n ()\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"lr.coef_: {}\"\n .\nformat\n(\nlr\n.\ncoef_\n))\nprint\n(\n\"lr.intercept_: {}\"\n .\nformat\n(\nlr\n.\nintercept_\n ))\n> lr.coef_: [ 0.394]\n> lr.intercept_: -0.031804343026759746\nprint\n(\n\"Training set score: {:.2f}\"\n .\nformat\n(\nlr\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Test set score: {:.2f}\"\n .\nformat\n(\nlr\n.\nscore\n(\nX_test\n, \ny_test\n)))\n> Training set score: 0.67\n> Test set score: 0.66\nNota: coef_ è di tipo NumPy array, avendo dimensione pari al numero di \nfeatures per istanza. \nCosa possiamo dire con i valori di performance ottenuti?\n26",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#26": "Esercizio: linear regression e wave dataset\n> Training set score: 0.67\n> Test set score: 0.66\nSono valori piuttosto bassi.  \nI valori sul training e test set sono molto simili, sintomo di \n underﬁtting\n .  \nPer modelli lineari e dataset semplici (es. mono-dimensionali) esiste un \nrischio minore di fare overﬁtting data la semplicità del modello. \nEsercizio\n : prova lo stesso approccio con il Bostong housing dataset e \nconfronta le performance.\n27",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#27": "Esercizio: linear regression e wave dataset\nEsercizio\n : prova lo stesso approccio con il Bostong housing dataset e \nconfronta le performance.  \nX\n, \ny \n= \nmglearn\n.\ndatasets\n .\nload_extended_boston\n ()\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\nX\n, \ny\n, \nrandom_state\n =\n0\n)\nlr \n= \nLinearRegression\n ()\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Training set score: {:.2f}\"\n .\nformat\n(\nlr\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Test set score: {:.2f}\"\n .\nformat\n(\nlr\n.\nscore\n(\nX_test\n, \ny_test\n)))\n> Training set score: 0.95\n> Test set score: 0.61\nLa differenza tangibile tra training e test set è sintomo di overﬁtting. \nOccorre adattare il modello. \n28",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#28": "Richiami: Ridge regression\nNella \n ridge regression\n  si implementa un forma semplice di \nregolarizzazione\n , cioè tecniche per affrontare il problema del overﬁtting. \nI parametri \n w\n del modello lineare devono rispettare un vincolo \naggiuntivo: il valore assoluto dei singoli parametri deve essere piccolo.  \nIntuitivamente:\n  ogni feature può avere un effetto limitato sul valore \npredetto dal modello. \nPrende il nome di L2 regularization. \nLa funzione che rappresenta il costo nella ridge è la seguente:\n29\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#29": "Scikit-learn: Ridge regression\nLa classe \n Ridge\n  nel modulo sklearn.linear_model implementa la ridge \nregression: \nclf \n=\n Ridge(alpha\n =\n1.0\n) \nIl parametro \n λ\n che controlla il peso della regolarizzazione è deﬁnito \nmediante il parametro \n alpha,\n  che per default assume valore 1. \n30",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#3": "Richiami: overﬁtting e underﬁtting\nSupponiamo di avere il seguente dataset che rappresenta la possibilità che \nun cliente acquisti una barca in base a certe sue caratteristiche: \nSe guardi questi dati, che proﬁlo di cliente potenzialmente interessato a \ncomprare puoi identiﬁcare dalle features riportate?\n4\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#30": "Esercitazione: Ridge regression\nEsercizio\n : impiegare la ridge regression nel Boston housing dataset.  \nCosa ti aspetti dalle performance che ottieni rispetto alla linear regression \nsenza regolarizzazione? \nEsercizio\n : ricava gli score per \n λ\n pari a 0.1 e 10. Cosa ti aspetti? \nEsercizio\n : crea un graﬁco 2d dove visualizzi i parametri dei tre modelli \nridge\n , \nridge10\n  e \nridge01\n . Cosa ti aspetti nella distribuzione dei parametri? \nEsercizio\n : Come pensi che vari lo score \n R\n2\n, sul training e sul test set, al \nvariare del numero di istanze usate durante l'addestramento?\n31",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#31": "Richiami: LASSO\nNel modello \n LASSO\n  si impiega la \n L1-regularization\n , dove alcuni parametri \nassumono valore esattamente pari a 0, ignorando perciò alcune features. \nPuò essere interpretato come una sorta di \n feature selection\n , cioè un \nprocesso per selezionare le feature più rilevanti nel task in esame. \nIl vantaggio è avere un modello più semplice, più veloce da addestrare, che \nconsidera solo le features più rilevanti.\n32\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#32": "Scikit-learn: LASSO\nLa classe \n Lasso\n  nel modulo sklearn.linear_model implementa il modello \nLASSO:  \nclf \n=\n Lasso(alpha\n =\n1.0\n) \nIl parametro \n λ\n che controlla il peso della regolarizzazione è deﬁnito \nmediante il parametro alpha, che per default assume valore 1. \nEsercizio\n : impiegare LASSO nel Boston housing dataset. Cosa ti aspetti \ndalle performance che ottieni rispetto alla linear e Ridge regression?\n33",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#33": "Esercitazione: LASSO\nEsercizio\n : impiega LASSO nel Boston housing dataset \nEsercizio\n : prova a variare nuovamente \n λ\n per migliorare le performance. \n34",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#34": "Ridge e LASSO: considerazioni\nSolitamente si impiega la Ridge come primo approccio.  \nNel caso ci siano molte features, ma solo un sottoinsieme verosimilmente \nrilevanti, LASSO risulta la scelta migliore. \nLASSO inoltre produce modelli più semplici e più facilmente interpretabili \nrispetto a Ridge, utile per investigare il dataset nelle fasi iniziali. \nScikit-learn implementa la classe \n ElasticNet\n  che combina i due approcci e \nottiene ottime performance, ma con due iperparametri da impostare, uno \nper L1 e uno per L2 regularization.\n35",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#35": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n36",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#4": "Richiami: overﬁtting e underﬁtting (2)\nCliente di \n 45 anni o più\n , \nmeno di 3 ﬁgli\n  o \nnon divorziato\n . \nQuesta \"regola\" è al \n 100%\n  accurata. \nMa una regola sull'età del tipo età=66 OR 52 OR 53 OR 58 è altrettanto \naccurata. \nDobbiamo ricordarci che il modello dovrà funzionare altrettanto \naccuratamente su dati mai visti in precedenza. \nLe regole che abbiamo escogitato sembrano funzionare, ma sono troppo \nspeciﬁche per le istanze del nostro dataset. Se nel test set abbiamo istanze \nsimili, allora questo semplice modello può funzionare, ma non è detto che \nfunzioni anche in produzione.\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#5": "Richiami: overﬁtting e underﬁtting (3)\nIl nostro obiettivo è sempre trovare il modello più semplice che abbia \nbuone performance anche sul test set. \nCostruire un modello troppo complesso rispetto ai dati disponibili crea \noverﬁtting\n .  \nIl modello è troppo speciﬁco per le istanze nel training set ma non è capace di \ngeneralizzare sui dati nel test set. \nUn modello troppo semplice rispetto ai dati disponibili può creare \nfenomeni di \n underﬁtting\n , cioè scarse performance perﬁno nel training set \npoiché non riesce a catturare tutte le caratteristiche e legami tra le features.\n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#6": "4 datasets\nIntroduciamo 4 datasets utili per osservare come si comportano diversi \nalgoritmi di machine learning implementati in scikit-learn. \nForge dataset\n  (classiﬁcazione) \nwave dataset\n  (regressione) \nWisconsin Breast Cancer dataset\n  (classiﬁcazione) \nBoston housing dataset\n  (regressione)\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#7": "Classiﬁcazione: Forge dataset\n26 istanze, 2 features per istanza, e 2 classi: \n# codice per ottenere il dataset\nX\n, \ny \n= \nmglearn\n.\ndatasets\n .\nmake_forge\n ()\n# grafico le istanze\nmglearn\n.\ndiscrete_scatter\n (\nX\n[:, \n0\n], \nX\n[:, \n1\n], \ny\n)\nplt\n.\nlegend\n([\n\"Class 0\"\n , \n\"Class 1\"\n ], \nloc\n=\n4\n)\nplt\n.\nxlabel\n(\n\"First feature\"\n )\nplt\n.\nylabel\n(\n\"Second feature\"\n )\nprint\n(\n\"X.shape: {}\"\n .\nformat\n(\nX\n.\nshape\n))\n8\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#8": "Regressione: wave dataset\nSingola feature per istanza, singolo valore reale in output. \nX\n, \ny \n= \nmglearn\n.\ndatasets\n .\nmake_wave\n (\nn_samples\n =\n40\n)\nplt\n.\nplot\n(\nX\n, \ny\n, \n'o'\n)\nplt\n.\nylim\n(\n-\n3\n, \n3\n)\nplt\n.\nxlabel\n(\n\"Feature\"\n )\nplt\n.\nylabel\n(\n\"Target\"\n )\nPer dataset così piccoli è sempre utile studiare le caratteristiche delle \nistanze su graﬁci.\n9\n",
    "data_test\\rootfolder\\università\\MachineLearning\\13-Ex_03 Esercitazione su Regressione-sbloccato.pdf#9": "Classiﬁcazione: Wisconsin Breast Cancer dataset\nMisure cliniche associate a patologie tumorali, con label '\n benigno\n ' '\nmaligno\n '  \nfrom \nsklearn.datasets \n import \nload_breast_cancer\ncancer \n= \nload_breast_cancer\n ()\nprint\n(\n\"cancer.keys(): \\n{}\"\n .\nformat\n(\ncancer\n.\nkeys\n()))\n> cancer.keys():\n> dict_keys(['feature_names', 'data', 'DESCR', 'target', 'target_names'])\nprint\n(\n\"Shape of cancer data: {}\"\n .\nformat\n(\ncancer\n.\ndata\n.\nshape\n))\n> Shape of cancer data: (569, 30)\nprint\n(\n\"Sample counts per class:\\n{}\"\n .\nformat\n(\n{\nn\n: \nv \nfor \nn\n, \nv \nin \nzip\n(\ncancer\n.\ntarget_names\n , \nnp\n.\nbincount\n (\ncancer\n.\ntarget\n))}))\n> Sample counts per class: {'benign': 357, 'malignant': 212}\nprint\n(\n\"Feature names:\\n{}\"\n .\nformat\n(\ncancer\n.\nfeature_names\n ))\n> Feature names:\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n'mean smoothness' 'mean compactness' 'mean concavity'\n'mean concave points' 'mean symmetry' 'mean fractal dimension'\n'radius error' 'texture error' 'perimeter error' 'area error'\n'smoothness error' 'compactness error' 'concavity error'\n'concave points error' 'symmetry error' 'fractal dimension error'\n'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n'worst smoothness' 'worst compactness' 'worst concavity'\n'worst concave points' 'worst symmetry' 'worst fractal dimension']\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nClassiﬁcazione:  \nAlberi di Decisione\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#1": "Sommario\nIntroduzione ai Decision Trees \nEsempio di applicazione \nFeature split learning \nDecision Stump \nAlgoritmo greedy decision tree learning \nClassiﬁcazione mediante Decision Trees\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#10": "Learning Goal \n \n11\nIl nostro obiettivo è dunque quello di costruire un albero di \ndecisione che minimizzi il Classiﬁcation Error sui dati di \ntraining, calcolato mediante la metrica di qualità che \nabbiamo deﬁnita. \nPurtroppo questo è un task estremamente difﬁcile: \n•abbiamo un numero esponenziale di possibili alberi da considerare  \n•problema NP-hard \n•possiamo però utilizzare delle euristiche che funzionano bene in \npratica",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#11": "Algoritmo greedy  \ndecision tree learning \n1. Cominciamo da un albero “vuoto” e consideriamo tutti gli esempi \ndisponibili. \n2. Selezioniamo la feature “migliore” con la quale possiamo \npartizionare (split) i dati in base ai diversi valori che essa può assumere. \n3. Per ogni split: \n•\nSe non ci sono altre operazioni da fare, costruire foglia con la \nprevisione. \n•\nAltrimenti, continua la costruzione dell’albero a partire dallo \nsplit che stiamo considerando. \n \n12Vediamo informalmente come poter procedere per costruire un albero di \ndecisione:",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#12": "Algoritmo greedy  \ndecision tree learning \n1. Cominciamo da un albero “vuoto”. Consideriamo tutti gli esempi \ndisponibili. \n2. Selezioniamo la feature “migliore” con la quale possiamo \npartizionare (split) i dati in base ai vari valori che essa può assumere. \n3. Per ogni split: \n•\nSe non ci sono altre operazioni da fare, costruire foglia con la \nprevisione. \n•\nAltrimenti, continua la costruzione dell’albero a partire dallo \nsplit che stiamo considerando. \n \n13Primo problema:\nfeature\n selection",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#13": "Algoritmo greedy  \ndecision tree learning \n1. Cominciamo da un albero “vuoto”. Consideriamo tutti gli esempi \ndisponibili. \n2. Selezioniamo la feature “migliore” con la quale possiamo \npartizionare (split) i dati in base ai vari valori che essa può assumere. \n3. Per ogni split: \n•\nSe non ci sono altre operazioni da fare, costruire foglia con la \nprevisione. \n•\nAltrimenti, continua la costruzione dell’albero a partire dallo \nsplit che stiamo considerando. \n \n14Secondo problema:\nstopping conditions",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#14": "Algoritmo greedy  \ndecision tree learning \n1. Cominciamo da un albero “vuoto”. Consideriamo tutti gli esempi \ndisponibili. \n2. Selezioniamo la feature “migliore” con la quale possiamo \npartizionare (split) i dati in base ai vari valori che essa può assumere. \n3. Per ogni split: \n•\nSe non ci sono altre operazioni da fare, costruire foglia con la \nprevisione. \n•\nAltrimenti, continua la costruzione dell’albero a partire dallo \nsplit che stiamo considerando. \n \n15Chiamata ricorsiva:\nchiamata ricorsiva",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#15": "Predizioni con Decision Stump \n[feature selection]\n \n1622  18\nSicuroRoot node: relativo a tutte le osservazioni. \n22: output “Sicuro” \n18: output “Rischioso” \nOra dobbiamo selezionare una feature  \n(feature selection problem)",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#16": "Predizioni con Decision Stump \n[feature: Reputazione]\n \n17Reputazione22  18\nSicuroScarsa EccellenteSufﬁciente\n9    0 9    4 4    14Se scegliamo “Reputazione”:",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#17": "Decision Stump \n[feature: Reputazione]\n \n18SicuroReputazione22  18\nRischioso SicuroScarsa EccellenteSufﬁciente\n9    0 9    4 4    14\nset  ŷ = “majority value” ",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#18": "Decision Stump \n[feature: Reputazione]\n \n19SicuroReputazione22  18\nRischioso SicuroScarsa EccellenteSufﬁciente\n9    0 9    4 4    14\n0 errori 4 errori 4 errori",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#19": "Decision Stump \n[feature: Durata]\n \n20SicuroDurata22  18\nRischioso5 anni 3 anni\n16    4 6    14Se scegliamo “Durata”:\n4 errori 6 errori",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#2": "Alberi di Decisione \nVediamo ora un altro metodo per la classiﬁcazione, molto \nutile nella pratica. \nUn albero di decisione prende come ingresso un oggetto o \nuna situazione descritta da un insieme di attributi (features) e \nrestituisce una “decisione”. \nEffettua dunque una “classiﬁcazione” della situazione \npresentata in input.  \n \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#20": "Selezione della migliore feature \n[feature selection]\n \n21SicuroDurata22  18\nRischioso5 anni 3 anni\n16    4 6    14\nSicuroReputazione22  18\nRischioso SicuroScarsa EccellenteSufﬁciente\n9    0 9    4 4    14Dobbiamo deﬁnire un criterio per la scelta della migliore feature:\nvs.",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#21": "Selezione della migliore feature \n[feature selection]\n \n22SicuroDurata22  18\nRischioso5 anni 3 anni\n16    4 6    14\nSicuroReputazione22  18\nRischioso SicuroScarsa EccellenteSufﬁciente\n9    0 9    4 4    14\n0 errori 4 errori 4 errori 4 errori 6 erroriPer far questo consideriamo gli errori già visti in precedenza …….\nvs.",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#22": " \n23SicuroDurata22  18\nRischioso5 anni 3 anni\n16    4 6    14\nSicuroReputazione22  18\nRischioso SicuroScarsa EccellenteSufﬁciente\n9    0 9    4 4    14\n0 errori 4 errori 4 errori 4 errori 6 errori…… e usiamoli per calcolare il Classiﬁcation Error per ogni feature:\nvs.4+4\n22 + 18=0 .24+6\n22 + 18=0 .25\nScegliamo la feature con il Classiﬁcation Error più basso.\nSelezione della migliore feature \n[feature selection]",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#23": "Calcolo Classiﬁcation Error \nAbbiamo dunque diviso il calcolo del Classiﬁcation Error in \ndue fasi: \n1.\n Per ogni nodo relativo ad un sottoinsieme dei dati, ottenuto \nconsiderando uno dei possibili valori della feature \nd’interesse, assegniamo il valore della majority class del \nnodo (\n ŷ\n = “majority class”). \n2.\n Calcolo del Classiﬁcation Error considerando come  \npredizione per ogni nodo considerato quella assegnata nel \npasso precedente. \n \n24",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#24": "Algoritmo per  \nFeature Split Selection \nDato un sottoinsieme M di osservazioni disponibili (nodo \ndell’albero): \n• \n∀\n feature \n ɸ\nj\n(\nx\n): \n•\n Split dei dati M in  base ai valori della feature \n ɸ\nj\n(\nx\n). \n•\n Calcolo del Classiﬁcation Error per il Decision Stump \ndella feature \n ɸ\nj\n(\nx\n). \n•\n Scelta della feature \n ɸ\nj*\n(\nx\n) con il Classiﬁcation Error più \nbasso. \n \n25",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#25": "Tree Learning \n[recursive stump learning]\n \n26SicuroReputazione22  18\nScarsa EccellenteSufﬁciente\n9    0 9    4 4    14\ncostruire un decision \nstump con il sotto-\ninsieme dei dati in cui: \nReputazione = Sufﬁciente costruire un decision \nstump con il sotto-\ninsieme dei dati in cui: \nReputazione = Scarsa \nfoglia dell’alberoLa costruzione dell’albero si effettua come segue:",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#26": "Tree Learning \n[secondo livello]\n \n27SicuroReputazione22  18\nScarsa EccellenteSufﬁciente\n9     0 9     4 4    14\nDurata\nRischioso Sicuro3 anni 5 anni\n0     4 9     0",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#27": "Tree Learning \n[secondo livello]\n \n28SicuroReputazione22  18\nScarsa EccellenteSufﬁciente\n9     0 9     4 4    14\nDurata\nRischioso Sicuro3 anni 5 anni\n0     4 9     0Reddito\nRischiosoAlto Modesto\n0     9 4     5\naltro \ndecision \nstump",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#28": "Tree Learning \n[terzo livello]\n \n29Scarsa\n4    14\nReddito\nRischiosoAlto Modesto\n0     9 4     5\nSicuro RischiosoDurata\n3 anni 5 anni\n0     2 4     3",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#29": "Tree Learning \n[stopping conditions]\n \n30Scarsa\n4    14\nReddito\nRischiosoAlto Modesto\n0     9 4     5\nSicuro RischiosoDurata\n3 anni 5 anni\n0     2 4     3 stopping conditions?",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#3": "Esempio di applicazione \n[valutazione richiesta prestito]\n \n4Richiesta \nPrestitoModello di \nClassiﬁcazioneSicuro\nRischioso\nInput: xi(Output: y i = +1)\n(Output: y i = -1)\nVediamo un esempio di applicazione, relativo alla \nvalutazione di richieste di prestito da parte di un cliente alla \npropria banca:",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#30": "Stopping Conditions \nLa costruzione di un ramo dell’albero si ferma quando arriviamo \nad un nodo nel quale si veriﬁca una delle seguenti condizioni: \n1.\n Gli esempi relativi al nodo sono tutti di uno stesso tipo (e.g., \ntutti \nSicuro\n  o tutti \n Rischioso\n ): scegliamo come foglia il valore in \nquestione. \n2.\n Gli esempi relativi al nodo sono di tipo diverso, e non ci sono \npiù feature da considerare: scegliamo come foglia il majority \nvalue. \n3.\n Nel nodo non ci sono più esempi, ma c’è ancora qualche \nfeature non considerata nel percorso che porta a quel nodo: \nvalore di default (e.g., maggioranza nodo genitore). \n \n31",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#31": "Algoritmo greedy decision tree \nlearning \n1. Start da un nodo relativo a M esempi \n2. Feature Selection \n3. Per ogni split: \nif\n  Stopping Condition \nthen\n: costruire la foglia con la previsione \n ŷ \nelse\n:  decision_tree_learning(nodo relativo allo split) \n \n32decision_tree_learning (nodo) \nChiamata RicorsivaNon ci sono altre \noperazioni da fareSelezione Feature  \nper dividere i dati",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#32": "Algoritmo per fare previsioni \nmediante Decision Tree \nVediamo ora il semplice algoritmo che implementa la funzione \nT(\nx\n), ossia l’algoritmo che, a fronte di un ingresso \n x\ni\n, visita l’albero \ndi decisione costruito nella fase di training e fornisce in output una \nprevisione \n ŷ\ni\n: \n \n33 \nif\n tree_node corrente è una foglia \n         \n then\n: \nreturn\n  majority class dei punti relativi alla foglia \n        \n else\n: \n•\nnext_node = ﬁglio di tree_node il cui valore della feature \ncorrisponde all’input \n•\nreturn\n  predict(next_node, input) predict (tree_node, input) ",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#33": "Riferimenti\n \n34\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , 3a edizione, \nApogeo, 2015. \nMachine Learning: Classiﬁcation\n , University of Washington - Coursera, 2017. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. ",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#4": "Esempio di applicazione \nNel formulare questo problema come un problema di \napprendimento dobbiamo anzitutto decidere quali proprietà, \no attributi (features), sono disponibili per descrivere esempi \n(osservazioni) nel dominio. \nIn genere, alcune delle caratteristiche prese in considerazione \ni tali casi sono le seguenti: \n•\n  reputazione cliente (e.g., ha pagato regolarmente vecchi prestiti?) \n•\nreddito cliente \n•\ndurata prestito  \n•\n  altre informazioni personali (età, motivo per il prestito, ecc.)  \n \n5",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#5": "Decision Tree Classiﬁer \n \n6Sicuro Durata RedditoReputazioneStart\nSicuro RischiosoRischioso Rischioso Sicuro DurataScarsa Eccellente\nSufﬁciente\nAlto Modesto3 anni 5 anni\n3 anni 5 anniEsempio di decision tree per il problema in esame:",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#6": "Valutazione richiesta prestito \n \n7\nSicuro\nDurataRedditoReputazioneStart\nSicuro\nRischioso\nRischioso\nRischioso\nSicuroDurataScarsa\nEccellente\nSufﬁciente\nAlto\nModesto\n3 anni\n5 anni\n3 anni\n5 annixi = (Reputazione = Scarsa , Reddito = Alto, Durata = 5 anni)  ",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#7": " \n8Richiesta \nPrestitoModello di \nClassiﬁcazione\nInput: xiSicuro\nRischioso(Output: y i = +1)\n(Output: y i = -1)\nSicuro Durata RedditoReputazioneStart\nSicuro RischiosoRischioso Rischioso Sicuro DurataScarsa Eccellente\nSufﬁciente\nAlto Modesto3 anni 5 anni\n3 anni 5 anniRichiesta \nPrestitoSicuro\nRischioso(Output: y i = +1)\n(Output: y i = -1) Input: xi\nDecision Tree Model \nT(xi)",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#8": "Apprendimento albero dai dati\n \n9\nReputazione Durata Reddito yi\neccellente 3 anni alto Sicuro\nsufﬁciente 5 anni modesto Sicuro\nsufﬁciente 3 anni alto Rischioso\nscarso 5 anni alto Sicuro\nsufﬁciente 5 anni modesto Sicuro\nscarso 3 anni alto Rischioso\nscarso 5 anni modesto Rischioso\nsufﬁciente 3 anni alto Rischioso\neccellente 3 anni modesto SicuroT(xi)\nVediamo come sia possibile costruire (ossia apprendere) un decision \ntree a partire da un certo numero di osservazioni:  \nMinimizzazione  \nfunzione di costo",
    "data_test\\rootfolder\\università\\MachineLearning\\14-Classification - Decision Trees-sbloccato.pdf#9": "Metrica di Qualità \n[quality metric]\n \n10Errore =#previsioni errate\n#esempi\nLa metrica che si usa misura la frazione delle previsioni \nerrate fornite dall’albero:\nOvviamente: \n• miglior valore possibile: 0.0 \n• peggior valore possibile: ?",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nClassiﬁcazione:  \nAlgoritmo C4.5",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#1": "Algoritmi Induzione DT\nAlgoritmo Greedy Decision Tree Learning  \n•Scelta della migliore feature utilizzando come metrica il Classification Error  \nsui dati di training —> problema NP-Hard —> servono euristiche \nAlgoritmo C4.5\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#10": "Guadagno\nIntroduciamo ora il Guadagno (Gain)  di un attributo A \nCalcolo di Gain(S,A) per ciascun attributo A \n•Riduzione di Entropia attesa a seguito dell’ordinamento del \nset di istanze S basato su A \nScelta dell’attributo con il valore di Guadagno più elevato \ncome nodo dell’albero \nGain(S,A) = Entropy(S) – Expectation(A)  \n \n \n \ndove {S1 ... Si ... Sn} sono le partizioni di S secondo i valori \ndell’attributo A, n il numero di valori distinti di A, |Si| il \nnumero di istanze nella partizione Si e |S| il numero totale di \nistanze in S)( )( ),(\n1in\niiS EntropySSS Entropy AS Gain ∗ − = ∑\n=\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#11": "Scelta Nodo Radice\nSe Outlook è radice dell’albero ci \nsono 3 partizioni sulle istanze (S1 per \nSunny , S2 per Cloudy, S3 per Rainy ) \nS1 (Sunny) = {istanze 1,2,8,9,11} \n|S1| = 5 (di queste 5 istanze, i \nvalori per Play sono 3 No e 2 Yes) \nEntropy(S1) =  \n= -2/5 (log2 2/5) – 3/5 (log2 3/5) =        \n= -0.4 (-1.322) – 0.6 (-0.737) =  \n= 0.53 +0.44 = 0.97 \nAnalogamente si ottiene  \n Entropy(S2) = 0  \n Entropy(S3) = 0.97 \n 12",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#12": "Scelta Nodo Radice\nGain(S,Outlook) = Entropy(S) – Expectation(Outlook) = \n= Entropy(S) – [|S1|/|S| * Entropy(S1) + |S2|/|S| * Entropy(S2) +  \n+ |S3|/|S| * Entropy(S3)] = 0.94 – [5/14 * 0.97 + 4/14 * 0 + 5/14 * 0.97]  \nda cui si ottiene \nGain(S,Outlook) = 0.247 \nAnalogamente \nGain(S,Temperature) = 0.029 \nGain(S,Humidity) = 0.152 \nGain(S,Windy) = 0.048  \nIn conclusione Gain(S,Outlook)  è il guadagno più elevato e quindi \nOutlook dovrebbe essere scelto come radice dell’Albero di Decisione  \n13",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#13": "Scelta Nodi Successivi \nRipetiamo il procedimento per il ramo \n Sunny\n  …\ntemperatureoutlook\nrainy sunnycloudy\nhot mild cold?\n0    2 1    1 1    04    0\nwindyoutlook\nrainy sunnycloudy\nfalse?\ntrue\n1    2 1    14    0 humidityoutlook\nrainy sunnycloudy\nhigh?\nnormal\n0    3 2    04    0\n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#14": "Scelta Nodi Successivi \n… e per il ramo Rainy\nhumidityoutlook\nrainy sunnycloudy\nhigh normalMild High False Yes\nCool Normal False Yes\nCool Normal True No\nMild Normal False Yes\nMild High True NoNo Yes Yes \n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#15": "humidityoutlook\nrainy sunnycloudy\ntrue\nNo Yes\nYes normal\nDecision Tree \nwindy\nhigh\nNo Yes falseIn conclusione, si ottiene il seguente Albero di Decisione\n16\nNodi interni = test sugli attributi (feature) \nArchi uscenti = risultati dei test \nNodi foglia = etichette classe di appartenenza",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#16": "Scelta Nodo Radice \nLa selezione dell’attributo come nodo radice è eseguita \nvalutando il Guadagno di Informazione (Information Gain)  per \nciascun attributo e scegliendo quello che dà il valore maggioreQual è l’attributo migliore per essere nodo radice dell’albero?\noutlook\nrainy sunnycloudyhumidity\nlow hightemperature\ncold hotmildwindy\nfalse true\n2     \n34     \n04     \n23     \n13     \n23     \n46     \n12     \n26     \n23     \n3\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#17": "Algoritmo C4.5\nN.B. Pure: all instances in the subset fall in the same classSet di dati (tabella) attributo-valore\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#18": "Algoritmo C4.5\nSalvatore Ruggieri. 2000. Efficient C4.5.  Technical Report. University of \nPisa.  \nAbstract: We present an analytic evaluation  of the run-time behavior  of the \nC4.5 algorithm which highlights some efficiency improvements. We have \nimplemented a more efficient version of the algorithm, called EC4.5, that \nimproves on C4.5 by adopting the best among three strategies at each node \nconstruction. The first strategy uses a binary search of thresholds instead of \nthe linear search of C4.5. The second strategy adopts a counting sort method \ninstead of the quicksort of C4.5. The third strategy uses a main-memory \nversion of the RainForest algorithm for constructing decision trees. Our \nimplementation computes the same decision trees as C4.5 with a \nperformance gain of up to 5 times.\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#19": "Esercizio\nCreare l’Albero di Decisione (Indice) per la  \nPrevisione di Rischio per Richieste di Prestito\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#2": "Algoritmo C4.5\n J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning.   \nMorgan Kaufmann Publishers Inc., San Francisco, CA, USA. \n X. Wu, V. Kumar, J. R. Quinlan , J. Ghosh, Q. Yang, H. Motoda, G. J. \nMcLachlan, A. Ng, B. Liu, P. S. Yu, Z.-H. Zhou, M. Steinbach, D. J. \nHand, and D. Steinberg. 2007. Top 10 Algorithms in Data Mining.  \nKnowledge and Information Systems , Volume 14, Issue 1, December \n2007, Pages 1-37, Springer-Verlag New York, Inc. New York, NY, USA.  \nDOI=http://dx.doi.org/10.1007/s10115-007-0114-2  \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#20": "Esercizio\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#21": "Esercizio\n( ) )( )( ,)( )(log )(\n12\n1\nAn Expectatio S Entropy AS GainS EntropySSAn Expectatiop p S Entropy\nin\niiin\nii\n− =∗ =∗− =\n∑∑\n==\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#22": "Algoritmi Induzione DT\nAlgoritmo Greedy Decision Tree Learning  \n•Scelta della migliore feature utilizzando come metrica il Classification Error \nsui dati di training —> problema NP-Hard —> servono euristiche \nAlgoritmo C4.5  \n•Scelta della migliore feature utilizzando come metrica l’ Information Gain   \nsui dati di training —> problema risolvibile tramite strategia divide&conquer  \n23",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#23": "Algoritmi Induzione DT\nAlgoritmo Greedy Decision Tree Learning  \n•Scelta della migliore feature utilizzando come metrica il Classification Error \nsui dati di training —> problema NP-Hard —> servono euristiche \nAlgoritmo C4.5  \n•Scelta della migliore feature utilizzando come metrica l’ Information Gain   \nsui dati di training —> problema risolvibile tramite strategia divide&conquer \nAlgoritmo CART  \n24",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#24": "Algoritmo CART\nBreiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984). \nClassification and Regression Trees.  Monterey, CA: Wadsworth & \nBrooks/Cole Advanced Books & Software. \n25",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#25": "Algoritmo CART\nObiettivo: generare un Albero di Decisione da una Tabella di Dati  \nSi basa sul Gini Index (o Indice di Gini) \nIn corrispondenza di un certo nodo t dell’albero in costruzione, e rispetto alla \ncorrispondente partizione del dataset di training, si definisce l’Indice di Gini \ncome segue: \n \n      dove p(j/t) è la frequenza relativa (proporzione) della classe j  al nodo t  \nL’Indice di Gini misura l’ impurezza ( o disordine)  del dataset corrispondente a t \n•Massimo valore ( 1-1/n c, con nc=numero di classi equiprobabili) quando i \nrecord sono equamente distribuiti fra tutte le classi \n•Minimo valore (0) quando tutti i record appartengono a una sola classeGini(t)=1−∑\nj[p(j/t)]2\n26",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#26": "Indice di Gini\n           \nNel caso di una sola classe: \n                                                     \nNel caso di nc classi equiprobabili \n                                               \n \n       dove n è il numero di record del dataset al nodo tGini(t)=1−∑\nj[p(j/t)]2\nGini(t)=1−12=0\nGini(t)=1−∑\nj((n/nc)/n)2=1−∑\nj(1/nc)2=1−nc(1/nc)2=1−1/nc\n27",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#27": "Indice di Gini\n           \nC1=0, C2=6 —> P(C1)=0/6=0 , P(C2)=6/6=1  \n                             \nC1=1, C2=5 —> P(C1)=1/6 , P(C2)=5/6  \n                                    \nC1=2, C2=4 —> P(C1)=2/6 , P(C2)=4/6  \n                                    \nC1=3, C2=3 —> P(C1)=3/6=0.5 , P(C2)=3/6=0.5  \n                                   Gini(t)=1−∑\nj[p(j/t)]2\nGini(t)=1−P(C1)2−P(C2)2=1−0−1=0\nGini(t)=1−1/62−5/62=0.278\nGini(t)=1−2/62−4/62=0.444\nGini(t)=1−0.52−0.52=0.500\n28",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#28": "Algoritmo CART\n29\nCriterio di Splitting: Minimizzare l’Indice di Gini della suddivisione  \nQuando un nodo t è suddiviso in k partizioni (figli), la qualità della suddivisione è \ncalcolata come:  \n \n \n \n \ndove  \n   ni = numero di record della partizione (figlio) i \n   n = numero di record del dataset al nodo t \n   n i/n = peso dei vari Gini(i) \nDato il dataset associato al nodo t, si sceglie l’attributo che fornisce il più piccolo \nGini split(t) per partizionare il dataset \n•E’ necessario enumerare tutti i possibili punti di splitting per ciascun attributo, \novverosia tutte le possibili partizioni   \n Ginisplit=k\n∑\ni=1ni/n*Gini(i)",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#29": "Algoritmi Induzione DT\nAlgoritmo Greedy Decision Tree Learning  \n•Scelta della migliore feature utilizzando come metrica il Classification Error \nsui dati di training —> problema NP-Hard —> servono euristiche \nAlgoritmo C4.5  \n•Scelta della migliore feature utilizzando come metrica l’ Information Gain   \nsui dati di training —> problema risolvibile tramite strategia divide&conquer \nAlgoritmo CART  \n•Scelta della migliore feature utilizzando come metrica il Gini Index   \nsui dati di training —> problema risolvibile tramite strategia divide&conquer \n30",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#3": "Algoritmo C4.5\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#4": "Algoritmo C4.5\nObiettivo: generare un Albero di Decisione da una Tabella di Dati  \nSviluppato da J. R. Quinlan nel 1993 come estensione dell’ Algoritmo ID3   \nL’Albero ottenuto può essere usato per la classificazione, per cui \nl’Algoritmo C4.5  è spesso indicato come Statistical Classifier \nBasato sulla Teoria dell’Informazione (Claude E. Shannon, A Mathematical \nTheory of Communication , 1948) \nStrategia “divide and conquer” (suddivisione del problema in \nsottoproblemi più semplici e loro risoluzione ricorsiva):  \n•Scelta di uno degli attributi come nodo radice \n•Creazione ramo per ciascun valore di quell’attributo \n•Suddivisione delle istanze lungo i rami \n•Ripetizione del processo per ciascun ramo finché tutti le istanze nel ramo hanno \nla stessa classe di appartenenza (si dice che tutti i sottoalberi sono “puri”)  \nAssunzione di fondo: quanto più semplice  è l’albero che classifica le \nistanze, tanto meglio  è\n 5",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#5": "Entropia\nIntroduciamo il concetto di Entropia (Entropy) [(dal greco antico ἐν   \nen, \"dentro\", e τροπή  tropé, “trasformazione\")] \nEntropia in Meccanica Statistica: grandezza interpretata come \nmisura del disordine presente in un sistema fisico qualsiasi, \nincluso - come caso limite - l’universo \nEntropia in Teoria dell’Informazione: quantità di incertezza o \ninformazione presente in un segnale aleatorio \n•Primo Teorema di Shannon (Codifica di Sorgente): “Una \nsorgente casuale d’informazione non può essere rappresentata \ncon un numero di bit (da cui la base 2 del logaritmo) inferiore \nalla sua entropia, cioè alla sua autoinformazione media.”  \nTale teorema ha quindi un’implicazione in termini di \nrappresentazione dati, in quanto l’Entropia può essere \ninterpretata anche come la minima complessità descrittiva di \nuna variabile aleatoria, ovvero il limite inferiore della \ncompressione dei dati \n 6",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#6": "Tabella di Dati\n7\n",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#7": "Decision Tree\n8humidityoutlook\nrainy sunnycloudy\ntrue\nNo Yes\nYes normalwindy\nhigh\nNo Yes false\nNodi interni = test sugli attributi (feature) \nArchi uscenti = risultati dei test \nNodi foglia = etichette classe di appartenenza",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#8": "Entropia\nIntro duciamo il concetto di Entropia (Entropy) di un set di istanze  \nS è un set di istanze (i.e., record della tabella) \nA è una feature (Play nell’esempio) \n{S1 ... Si ... Sn} sono le partizioni  di S secondo gli n valori che può \nassumere A (“Yes” e “No”  nell’esempio) \n{p1 ... pi ... pn} sono le proporzioni  di {S1 ... Si ... Sn} in S \nSi definisce Entropia di S la seguente grandezza\n( ) ∑\n=∗− =n\nii i p p S Entropy\n12log )(\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\15-Alberi C4.5-sbloccato.pdf#9": "Entropia\nNel caso dell’esempio \nS è il set di 14 istanze \nL’obiettivo è classificare le istanze secondo i valori della \nfeature Play, ossia “Yes” e “No”  \nLa proporzione delle istanze con valore “Yes” è 9 su 14 \n(9/14=0.64) \nLa proporzione delle istanze con valore “No” è 5 su 14 \n(5/14=0.36) \nL’Entropia misura l’ impurezza di S e in questo caso vale  \nEntropy(S)= - 0.64 (log2 0.64) – 0.36 (log2 0.36)=  \n= - 0.64 (- 0.644) – 0.36 (- 1.474) = 0.41 + 0.53 = 0.94 \n10",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nClassiﬁcation:  \nBoosting",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#1": "Sommario\nIntroduzione \nEnsemble Learning \nBoosting \nAdaBoost\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#10": "Ensemble Classiﬁer \nL’idea è quella di considerare un certo numero di classiﬁcatori \nche, a fronte di un input, forniscono una loro previsione: \n \n11\nOgni classiﬁcatore esprime un voto in base al valore della \nfeature relativa. 1\nSicuroReputazione\nRischioso SicuroScarsa EccellenteSufﬁciente\nRischioso3 anni 5 anniDurata\nSicuro Rischiosocattive buoneCondizioni di \nmercato\nSicuroInput: xi = (Reputazione = Scarsa, Durata = 5 anni, Condizioni di Mercato = cattive)\nf1(xi) = -1 f2(xi) = +1 f3(xi) = -12 3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#11": "Ensemble Model \nI vari voti espressi dai classiﬁcatori sono combinati insieme \ncome segue per formulare la previsione ﬁnale: \n \n12\nSe il segno è positivo la previsione vale +1, se è negativo vale -1. \nQuesto è un semplice esempio di Ensemble Classiﬁer. \nSi segnala l’importanza dei pesi w\n i\n, che devono essere \nindividuati mediante un processo di training. F(xi) = sign[w 1 * f 1(xi) + w 2 * f 2(xi) + w 3 * f 3(xi)]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#12": "Riepilogo sugli \nEnsemble Classiﬁer \nObiettivo: \n•\n predire un output \n ŷ\n (+1 o -1 nell’esempio) a partire da un \ninput \n x \n \n13\nApprendimento dell’Ensemble Model: \n• Classiﬁers: f 1(x), f2(x), …, f T(x) \n• Coefﬁcienti: ŵ1, ŵ2, …, ŵT \nPredizione: \nˆy= sign[TX\nt=1ˆwtft(x)]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#13": "Boosting \nxi\n(N esempi)Dati di Training\n(1° classiﬁcatore)Apprendimento\nPredizionef1(xi)\nŷi = sign[f 1(xi)]\nConsideriamo un problema di apprendimento automatico per  \nla classiﬁcazione: \n \n14",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#14": " \n15Credito Reddito yi\nA 130K $ Sicuro\nB 80K $ Rischioso\nC 110K $ Rischioso\nA 110K $ Sicuro\nA 90K $ Sicuro\nB 120K $ Sicuro\nC 30K $ Rischioso\nC 60K $ Rischioso\nB 95K $ Sicuro\nA 60K $ Sicuro\nA 98K $ SicuroApprendimento di un \nDecision Stump \n≤100K$ >100K$Reddito",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#15": " \n16Credito Reddito yi\nA 130K $ Sicuro\nB 80K $ Rischioso\nC 110K $ Rischioso\nA 110K $ Sicuro\nA 90K $ Sicuro\nB 120K $ Sicuro\nC 30K $ Rischioso\nC 60K $ Rischioso\nB 95K $ Sicuro\nA 60K $ Sicuro\nA 98K $ SicuroApprendimento di un \nDecision Stump \n≤100K$ >100K$Reddito\n3     1",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#16": " \n17Credito Reddito yi\nA 130K $ Sicuro\nB 80K $ Rischioso\nC 110K $ Rischioso\nA 110K $ Sicuro\nA 90K $ Sicuro\nB 120K $ Sicuro\nC 30K $ Rischioso\nC 60K $ Rischioso\nB 95K $ Sicuro\nA 60K $ Sicuro\nA 98K $ SicuroApprendimento di un \nDecision Stump \n≤100K$ >100K$Reddito\n3     1\nSicuro",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#17": " \n18Credito Reddito yi\nA 130K $ Sicuro\nB 80K $ Rischioso\nC 110K $ Rischioso\nA 110K $ Sicuro\nA 90K $ Sicuro\nB 120K $ Sicuro\nC 30K $ Rischioso\nC 60K $ Rischioso\nB 95K $ Sicuro\nA 60K $ Sicuro\nA 98K $ SicuroApprendimento di un \nDecision Stump \n≤100K$ >100K$Reddito\n4     3 3     1\nSicuro",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#18": " \n19Credito Reddito yi\nA 130K $ Sicuro\nB 80K $ Rischioso\nC 110K $ Rischioso\nA 110K $ Sicuro\nA 90K $ Sicuro\nB 120K $ Sicuro\nC 30K $ Rischioso\nC 60K $ Rischioso\nB 95K $ Sicuro\nA 60K $ Sicuro\nA 98K $ SicuroApprendimento di un \nDecision Stump \n≤100K$ >100K$Reddito\nSicuro4     3 3     1\nSicuro",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#19": "L’esempio precedente ci mostra che il decision stump non è \nriuscito a catturare adeguatamente le informazioni dal numero \nlimitato di dati disponibili. \n \n20\nQuello che fa il Boosting è considerare il decision stump, lo \nvaluta, vede come classiﬁca i vari punti, e addestra un \nsuccessivo decision stump (un successivo classiﬁcatore) con il \nquale si focalizza soprattutto sui punti dove il precedente \nclassiﬁcatore era debole. Boosting: focus sugli “hard points” ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#2": "Boosting question \n \n3\nCan a set of weak learners be combined to create a stronger learner? \n(Kearns e Valiant, 1988, 1989)\nSì!!  —>  Boosting  \n(Schapire, 1990)\n",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#20": " \n21xi\n(N esempi)Dati di Training\n(1° classiﬁcatore)Apprendimento\nPredizione\nValutazione\nIndividuazione\npunti critici\n(2° classiﬁcatore)Apprendimentoyif1(xi)\n… e così viaŷiBoosting:  focus sugli “hard points” \nl’algoritmo di apprendimento \nfocalizza l’attenzione \nsui punti “critici”",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#21": "Apprendimento su Dati Pesati \n[weighted data]\nL’idea è quella di dare maggiore attenzione ai data points \nritenuti maggiormente importanti: \n•\n ogni data point (\n x\ni\n, y\ni\n) è pesato mediante un \n α\ni \n•\n più il punto è ritenuto importante, più è elevato il peso \n α\ni  \n•\n l’algoritmo di apprendimento rimane lo stesso \n \n22",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#22": " \n23Credito Reddito yi Peso α\nA 130K $ Sicuro 0.5\nB 80K $ Rischioso 1.5\nC 110K $ Rischioso 1.2\nA 110K $ Sicuro 0.8\nA 90K $ Sicuro 0.6\nB 120K $ Sicuro 0.7\nC 30K $ Rischioso 3\nC 60K $ Rischioso 2\nB 95K $ Sicuro 0.8\nA 60K $ Sicuro 0.7\nA 98K $ Sicuro 0.9≤100K$ >100K$Redditopeso α incrementato per i punti  \nerroneamente classiﬁcati\nApprendimento su Dati Pesati \n[weighted data]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#23": " \n24Credito Reddito yi Peso α\nA 130K $ Sicuro 0.5\nB 80K $ Rischioso 1.5\nC 110K $ Rischioso 1.2\nA 110K $ Sicuro 0.8\nA 90K $ Sicuro 0.6\nB 120K $ Sicuro 0.7\nC 30K $ Rischioso 3\nC 60K $ Rischioso 2\nB 95K $ Sicuro 0.8\nA 60K $ Sicuro 0.7\nA 98K $ Sicuro 0.9≤100K$ >100K$Reddito\n2     1.2\nSicuropeso α incrementato per i punti  \nerroneamente classiﬁcati\nApprendimento su Dati Pesati \n[weighted data]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#24": " \n25Credito Reddito yi Peso α\nA 130K $ Sicuro 0.5\nB 80K $ Rischioso 1.5\nC 110K $ Rischioso 1.2\nA 110K $ Sicuro 0.8\nA 90K $ Sicuro 0.6\nB 120K $ Sicuro 0.7\nC 30K $ Rischioso 3\nC 60K $ Rischioso 2\nB 95K $ Sicuro 0.8\nA 60K $ Sicuro 0.7\nA 98K $ Sicuro 0.9Rischioso≤100K$ >100K$Reddito\n3     6.5 2     1.2\nSicuropeso α incrementato per i punti  \nerroneamente classiﬁcati\nApprendimento su Dati Pesati \n[weighted data]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#25": "Apprendimento su Dati Pesati \n[weighted data]\nTale approccio comporta che: \n•\n Ogni punto i (\n x\ni\n, y\ni\n) conta come \n α\ni\n punti.  \n•\n L’algoritmo di apprendimento rimane lo stesso. \nL’apprendimento su dati pesati non è solo relativo ai decision \nstumps. \nEsso si può applicare a molti algoritmi di Machine Learning  \n•\n ad esempio, nel gradient ascent per la logistic regression: \n \n26w(t+1)\nj w(t)\nj+⌘·NX\ni=1\u0000j(xi){I[yi= +1]\u0000P(y=+ 1 |xi,w(t))}\n↵i w(t+1)\nj w(t)\nj+⌘·NX\ni=1\u0000j(xi){I[yi= +1]\u0000P(y=+ 1 |xi,w(t))}\n",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#26": "Boosting   \nAlgoritmo Greedy per l’apprendimento di “ensemble” dai \ndati. \nSi avvale di weak learners usati come black box.  \nWeak Learning Assumption\n : ciascun weak learner deve \navere prestazioni migliori di un classiﬁcatore “random”. \nNel Boosting i base classiﬁers sono addestrati in sequenza.  \nPer migliorare le prestazioni di un weak learner, l’algoritmo \ndeve poter manipolare i dati in ingresso, altrimenti si \nottengono sempre gli stessi risultati \n \n27",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#27": "Boosting framework \nStep principali: \n \n28xi(N esempi)Dati di Training\n(1° classiﬁcatore)Apprendimento\nPredizione\n(2° classiﬁcatore e ŵ)Apprendimentof1(xi)\n… e così viaŷi = sign[f1(xi)]\nŷi = sign[ŵ1*f1(xi) + ŵ 2*f2(xi)]Individuazione\npunti critici\ne ricalcolo pesi\nPredizioneŵ, f2(xi)weighted data\nIdea del Boosting: aggiungere via via \nnuovi classiﬁcatori ottimizzando i pesi \nper focalizzarsi sui punti critici per poi \napprendere i coefﬁcienti dei diversi \nclassiﬁcatori. ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#28": "AdaBoost \n[Adaptive Boosting]\nProposto da Yoav Freund e Robert E. Schapire nel 1996. \nI due autori hanno vinto il Gödel Prize nel 2003. \nAlgoritmo estremamente utile e facile da implementare.  \n \n29Freund, Y ., Schapire, R. E. “A Short Introduction to Boosting”, in: J ournal of Japanese Society \nfor Artiﬁcial Intelligence , 14(5), 1999, pp. 771-780. Riferimenti:\nSchapire, R.E., Freund, Y . Boosting - Foundations and Algorithms . The MIT Press, 2012. Freund,  Y .,  Schapire,  R.  E.  “Experiments  with  a  new  Boosting  Algorithm”,  in:  Thirteenth  \nInternational Conference on Machine Learning , 1996, pp. 148-156. ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#29": "AdaBoost \n[Adaptive Boosting]\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n(\nminimizza funzione di costo\n ) \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n30ˆy= sign[TX\nt=1ˆwtft(x)]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#3": "Introduzione al Boosting \nIl Boosting è una potente tecnica per combinare molti \nclassiﬁcatori “di base” (detti anche “weak learners”) per produrre \nuna forma di comitato le cui prestazioni sono di gran lunga \nmigliori di ciascuno dei classiﬁcatori. \nOriginariamente progettato per risolvere problemi di \nclassiﬁcazione, può anche essere esteso alla regressione \n(Friedman, 2001).  \n \n4",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#30": "Evidenziamo qui il processo di training dei vari classiﬁcatori, basato su \nuna forma pesata dei punti del training set (linee rosse). \nOgni peso dipende dalle prestazioni del precedente classiﬁcatore (linee \nverdi)\n \n31f1(x) fT(x){↵(1)\ni} {↵(T)\ni}\nf2(x)······\n······{↵(2)\ni}\nˆy= sign[TX\nt=1ˆwtft(x)]\nBoosting framework ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#31": "Dobbiamo risolvere i seguenti due problemi: \n1. come calcolare il coefﬁciente \n ŵ\nt  \n(qual è la mia \n“ﬁducia” in f\n t\n(\nx\n) ?) \n2. come ricalcolare i pesi \n α\ni \n(individuare i punti “critici”) \n \n32\nAdaBoost \n[Adaptive Boosting]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#32": "Il peso \n ŵ\nt\n rappresenta un “grado di ﬁducia” nella \n f\nt\n. Pertanto:\n \n33\n1° problema: \nCalcolo del coefﬁciente \n ŵ\nt \nft(x) buona?\nŵt elevato ŵt bassosi no\nUna funzione è considerata “buona” se ha un basso training error \nVediamo come misurare l’errore nel caso di dati “pesati” (“weighted \nclassiﬁcation error”) ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#33": "Weighted Classiﬁcation Error \nLa misura di un errore pesato è simile a quella di un errore calcolato \nsu dati non pesati. \n \n34\nVediamo un semplice esempio: \nData point i yi αi ŷi risultato\n1 +1 1.2 +1\n 👍\n2 -1 0.5 +1\n 👎\n3 -1 0.7 -1\n 👍\n… … … …\npeso previsioni corrette 1.9\npeso previsioni errate 0.5",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#34": "Weighted Classiﬁcation Error \nPeso totale degli errori = \n \n35NX\ni=1↵iI[ˆyi 6=yi]\nNX\ni=1↵i\n Peso totale di tutti i data points = \nL’errore “pesato” misura la frazione del peso degli errori: \nweighted error =peso totale degli errori\npeso totale di tutti i data points=PN\ni=1↵iI[ˆyi 6=yi]\nPN\ni=1↵i\nMiglior valore: 0.0    Peggior valore: random classiﬁer ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#35": "Calcolo del coefﬁciente \n ŵ\nt \n[per il classiﬁcatore f\n t\n(\nx\n)]\nLa formula usata in \nAdaBoost è la seguente:\n \n36   \nsui training dataŵt\n0.01 (1 - 0.01)/0.01 = 99 +2.3\n0.5 (1 - 0.5)/0.5 = 1 0\n0.99 (1 - 0.99)/0.99 = 0.01 -2.31\u0000weighted error (ft)\nweighted error (ft)weighted error (ft)ˆwt=1\n2ln⇣\n1\u0000weighted error( ft)\nweighted error( ft)⌘\nVediamo un esempio: \nft(x) buona?si\nno",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#36": "2° problema: Ricalcolo pesi alfa\nCome sappiamo, dobbiamo focalizzarci soprattutto sui data point \ndove la funzione commette errori: \n \n37↵i ⇢↵i·e\u0000ˆwtseft(xi)=yi\n↵i·eˆwt seft(xi)6=yift(xi) classiﬁca \n bene xi?\ndecrementa αisi no\nincrementa αi\n",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#37": "2° problema: Ricalcolo pesi alfa \nVediamo un esempio:\n \n38↵i ⇢↵i·e\u0000ˆwtseft(xi)=yi\n↵i·eˆwt seft(xi)6=yi\n  ft(xi) = y i ? ŵtmoltiplicare α i per: implicazioni\nSI (corretto) +2.3 0.1 diminuisci l’importanza del punto\nSI (corretto) 0 1 mantieni la stessa importanza\nNO (errore) +2.3 9.98 aumenta l’importanza del punto\nNO (errore) 0 1 mantieni la stessa importanza",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#38": "Normalizzazione pesi alfa \nLa normalizzazione dei pesi \n α\ni\n è suggerita dal fatto che:  \n•\n se la funzione sbaglia spesso la classiﬁcazione di \n x\ni\n, il peso \n α\ni \ntende ad assumere valori molto alti \n•\n se la funzione prevede spesso correttamente la classiﬁcazione \ndi \nx\ni\n, il peso \n α\ni \ntende ad assumere valori molto bassi \nTutto ciò può causare instabilità numerica dopo varie iterazioni. \nSi normalizza come segue in modo tale che, dopo ogni iterazione, \nla somma dei pesi \n α\ni\n risulti sempre uguale ad 1:\n \n39↵i ↵iPN\nj=1↵j",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#39": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n40ˆy= sign[TX\nt=1ˆwtft(x)]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#4": "Introduzione al Boosting \nSuo impatto per il Machine Learning: \napproccio di default per molti task di computer vision (e.g., \nface detection) \nnumerose applicazioni nell’industria \nvince molte “ML competitions” (Kaggle, KDD Cup, ecc.): \n•   \nmalware classiﬁcation \n•\ncredit fraud detection \n•\nsales forecasting \n•\nHiggs boson detection, ecc., ecc.  \nSi basa sul concetto di Ensamble Learning  \n \n5",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#40": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n41ˆy= sign[TX\nt=1ˆwtft(x)]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#41": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n42ˆy= sign[TX\nt=1ˆwtft(x)]ˆwt=1\n2ln⇣\n1\u0000weighted error( ft)\nweighted error( ft)⌘",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#42": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n43ˆy= sign[TX\nt=1ˆwtft(x)]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#43": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n44ˆy= sign[TX\nt=1ˆwtft(x)]↵i ⇢↵i·e\u0000ˆwtseft(xi)=yi\n↵i·eˆwt seft(xi)6=yi",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#44": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n45ˆy= sign[TX\nt=1ˆwtft(x)]",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#45": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n46ˆy= sign[TX\nt=1ˆwtft(x)]↵i ↵iPN\nj=1↵j",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#46": "AdaBoost\nfor\n i = 1, 2, …, N: \nα\ni \n= 1\\N \nfor\n t = 1, 2, … T: \n•\n apprendi f\n t\n(\nx\n) con i pesi \n α\ni \n•\n calcola il coefﬁciente \n ŵ\nt  \n•\n ricalcola i pesi \n α\ni \n•\n normalizza i pesi \n α\ni \nCalcola la predizione ﬁnale: \n \n47ˆy= sign[TX\nt=1ˆwtft(x)]↵i ↵iPN\nj=1↵j↵i ⇢↵i·e\u0000ˆwtseft(xi)=yi\n↵i·eˆwt seft(xi)6=yiˆwt=1\n2ln⇣\n1\u0000weighted error( ft)\nweighted error( ft)⌘",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#47": "Riferimenti \n \n48Freund, Y ., Schapire, R. E. “A Short Introduction to Boosting”, in: J ournal of Japanese \nSociety for Artiﬁcial Intelligence , 14(5), 1999, pp. 771-780. \nSchapire, R.E., Freund, Y . Boosting - Foundations and Algorithms . The MIT Press, 2012. Freund,  Y .,  Schapire,  R.  E.  “Experiments  with  a  new  Boosting  Algorithm”,  in: \nThirteenth  International Conference on Machine Learning , 1996, pp. 148-156. \nFriedman, J.H. “Greedy Function Approximation: A Gradient Boosting Machine”, in: \nAnnals of Statistics , 29(5), 2001, pp. 1189-1232. Schapire, R.E.  “The Strength of Weak Learnability”, in: Machine Learning , 5(2), 1990, \npp. 197–227.\nMachine Learning: Classiﬁcation, University of Washington - Coursera, 2017.",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#5": "Weak Classiﬁers\nL’idea è quella di partire da Simple (o Base o Weak) \nClassiﬁers, come ad es.:  \n \n6SicuroReputazione\nRischioso SicuroScarsa EccellenteSufﬁciente+\n++++-\n--\n-+\n+\nLogistic Regression \ncon semplici featuresShallow \nDecision TreeDecision Stump\nEssi in genere sono caratterizzati da bassa varianza (scarso \noverﬁtting) ma alto bias. ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#6": "Andamento Errori  \ne Bias-Variance Trade-off\n \n7\nL’andamento del training error e del true error per la classiﬁcation è in \ngenere il seguente:\nDobbiamo come al solito considerare il trade-off tra bias e variance.True Error\nTraining Error\nModel ComplexityClassiﬁcation\nError\n(Weak Learner)",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#7": " \n8\nUn approccio per migliorare un classiﬁcatore può essere quello di \naggiungere più features al classiﬁcatore, ad es.: \n•\n logistic regression: polinomio di grado più elevato, cercando di \nevitare l’overﬁtting \n•\n decision trees: aumentare la profondità dell’albero \nNel Boosting si fa qualcosa di diverso: si parte da un insieme di weak \nclassiﬁers i cui risultati sono opportunamente combinati per ottenere \nuno strong classiﬁer.\nIntroduzione al Boosting ",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#8": "Ensemble Classiﬁer \nAlla base del Boosting c’è l’idea dell’Ensemble Classiﬁer, che \nora vedremo. \nConsideriamo un weak classiﬁer, ad esempio un Decision \nStump:  \n \n9\nEsso, a fronte del valore della feature d’interesse, restituisce \nun risultato (+1 o -1). SicuroReputazione\nRischioso SicuroScarsa EccellenteSufﬁciente\nInput: xi\nOutput: ŷ = f( xi)",
    "data_test\\rootfolder\\università\\MachineLearning\\16-Classification - BoostingAdaBoost-sbloccato.pdf#9": "Ensemble Classiﬁer \nL’idea è quella di considerare un certo numero di classiﬁcatori \nche, a fronte di un input, forniscono una loro previsione: \n \n101\nSicuroReputazione\nRischioso SicuroScarsa EccellenteSufﬁciente\nRischioso3 anni 5 anniDurata\nSicuro Rischiosocattive buoneCondizioni di \nmercato\nSicuroInput: xi = (Reputazione = Scarsa, Durata = 5 anni, Condizioni di Mercato = cattive)\n2 3",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nClassiﬁcazione:  \nOverﬁtting e Regularization\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#1": "Sommario\nIntroduzione \nOverﬁtting nella Classiﬁcazione \nRegolarizzazione \nL2 Penalty \nL1 Penalty (sparse solutions)\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#10": "Funzione di Qualità \nnel caso L\n 2\n Penalty\n \n11\nQuesto è il caso in cui usiamo la somma dei quadrati ( L2 \nRegularization ). \nLa funzione che rappresenta la qualità totale nel caso della \nlogistic regression ( L2 regularized logistic regression ) è la \nseguente:    \ndove il parametro λ (tuning parameter ) serve per bilanciare i \ndue termini.    \n                                           Qualit` a totaleL2=l n L(w)\u0000\u0000·kwk2\n2\n<latexit sha1_base64=\"3dst1017rZDOv7BwzXBseJyVJLg=\">AAAC0nicbVFNbxMxEJ1dvkr4aIAjF4sICQ5Eu6ESXJAq4MCBQys1aaUkRF5n2pra65U9S1tWOSCu/EEOSPwUxts9pC2W7Jl5b55n7CkqowNl2e8kvXHz1u07G3d79+4/eLjZf/R4ElztFY6VM84fFDKg0SWOSZPBg8qjtIXB/eLkQ+T3v6EP2pV7dF7h3MqjUh9qJYkh13cwAwKEMz4b2IUaJBjQHP1lZsHW8Y4YworjBj7zOWJfwDve62oDZYtH1LKG4BhUq42qFbxgvOD7DCwZOWXkZZv9qlUYzrTML9nGWLEXa19wE67iu+jyHets7C/29qWzvUV/kA2zdonrTt45A+jWzqL/Z7Z0qrZYkjIyhGmeVTRvpCetDK56szpgJdWJPMIpu6W0GOZNO4eVeF4HSU5U6IU2ogVxXdFIG8K5LTjTSjoOV7kI/o+b1nT4dt7osqoJSxULkTbYFgrKax4wiqX2SCRj5yh0KZT0kgi9FlIpBmueePyP/OrrrzuT0TB/PRztbg2233c/swFP4RlPMIc3sA2fYAfGoJKPydckJJTupd/TH+nPi9Q06TRP4NJKf/0De4+6Ww==</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#11": " \n12Vediamo cosa accade a fronte di diversi valori del parametro λ: \nSe λ = 0: \nci riconduciamo alla vecchia soluzione, ossia massimizzazione \ndel likelihood( w) → ŵMLE \nSe λ → ∞: \nper soluzioni dove ŵ ≠ 0, il costo totale → - ∞ \nl’unica soluzione per massimizzare la qualità è: ŵ = 0 \nSe 0 < λ < ∞: \n0<kˆwk2\n2<kˆwMLEk2\n2\n<latexit sha1_base64=\"GbN8PXIxssjt4tSdrVsj583mbQM=\">AAACT3icdVBNSyNBFOyJ3/Erq0cvjUHwFGaioAcP4rLgQUHBRCEThzedpzbp+aD7ja408+P2J+zRg2ev7mlv4kycg0YtaCiq6vFeV5gqach1H5zaxOTU9MzsXH1+YXFpufFjpWuSTAvsiEQl+iIEg0rG2CFJCi9SjRCFCs/D4c/SP79FbWQSn9F9iv0IrmN5JQVQIQWNnsv3uK+6qIn7YaIG1r8Bsnd5/iYGtp1fFu+7VGB9wt9kj49+jY/Ug0bTbbkj8M/Eq0iTVTgJGo/+IBFZhDEJBcb0PDelvgVNUijM635mMAUxhGvsFTSGCE3fjkrI+UZmgBKeouZS8ZGI7ycsRMbcR2GRjIBuzLhXil95vYyudvtWxmlGGItyEUmFo0VGaFm0i3wgNRJBeTlyGXMBGohQSw5CFGJW1F324Y3//jPptlveVqt9ut3cP6iamWVrbJ1tMo/tsH12yE5Yhwn2hz2xZ/bP+ev8d15qVbTmVGSVfUBt7hUB/7VE</latexit>\nFunzione di Qualità \nnel caso L\n 2\n Penalty",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#12": " \n13Come già visto nel caso della Regressione, per la \ndeterminazione del parametro λ non usiamo mai il Test Set. Ci \navvaliamo invece: \ndel Validation Set , se abbiamo a disposizione un \nnumero sufﬁcientemente elevato di osservazioni; \ndella Cross-Validation , se abbiamo a disposizione un \nnumero limitato di osservazioni. \nScelta del Parametro di Tuning \n λ",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#13": "Bias-Variance Tradeoff\n \n14\nParametro λ elevato: \nhigh bias, low variance  (e.g., ŵ = 0 per λ = ∞) \nParametro λ piccolo: \nlow bias, high variance  (e.g., maximum likelihood (MLE) \nﬁt per polinomi di grado elevato per λ = 0) Il parametro λ controlla la complessità del modello: ",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#14": "L\n2\n Regularization \nEsempio\n \n15\nVediamo l’effetto della L 2 regularization nel caso visto in \nprecedenza (caso con 20 features):\nRegularization:\nRange coefﬁcienti: \nDecision boundary:",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#15": "Gradient Ascent \ncon la L\n 2\n Regularization \n \n16\nCome è noto, nell’algoritmo Gradient Ascent dobbiamo \naggiornare il vettore dei pesi w come segue:\nw(t+1) w(t)+↵·rQualit` a totaleL2(w(t))\n<latexit sha1_base64=\"TzTwdt9I3sAyHV9gN7z7vrC1WrU=\">AAAC0nicfVFNb9NAEB2bj5bw0QBHLisipFaVIjtUKscKOHDg0EpNWykJ0XgzaZeuvdbuGChWDogrf5BDpf4Uxk4O0AJjaefNvDc7452stCZwkvyM4lu379xdW7/Xuf/g4aON7uMnR8FVXtNQO+v8SYaBrCloyIYtnZSeMM8sHWfnbxr++BP5YFxxyBclTXI8LczcaGRJua6DMWTgwMIMavgMC/ggfhMYtiGFLYmVKCwQzCWH4OVzolP/rFvWbLcKFL6EM/FNpEXrRNHgQnKZsEuG5f4vctZwAFVbZSS6EmYq3rWdmxkWEtfwXs6B4M3/zrAFHehMu72kn7SmboJ0BXqwsv1p93I8c7rKqWBtMYRRmpQ8qdGz0ZYWnXEVqER9jqc0ElhgTmFSt3tYqBdVQHaqJK+MVW2Sfq+oMQ/hIs9EmSOfhetck/wbN6p4/mpSm6KsmArdNGJjqW0UtDeyYFIz44kZm8lJmUJp9MhM3ijUWpKVbLx5j/T6398ER4N++rI/ONjp7b1evcw6PIPn8rYp7MIevIN9GIKO3kYfoxBxfBh/jb/F35fSOFrVPIU/LP7xC+iqucA=</latexit>\nDobbiamo dunque calcolare il gradiente della funzione di \nqualità totale ( L2 regularized log-likelihood ):\nQualit` a totaleL2=l n L(w)\u0000\u0000·kwk2\n2\n<latexit sha1_base64=\"3dst1017rZDOv7BwzXBseJyVJLg=\">AAAC0nicbVFNbxMxEJ1dvkr4aIAjF4sICQ5Eu6ESXJAq4MCBQys1aaUkRF5n2pra65U9S1tWOSCu/EEOSPwUxts9pC2W7Jl5b55n7CkqowNl2e8kvXHz1u07G3d79+4/eLjZf/R4ElztFY6VM84fFDKg0SWOSZPBg8qjtIXB/eLkQ+T3v6EP2pV7dF7h3MqjUh9qJYkh13cwAwKEMz4b2IUaJBjQHP1lZsHW8Y4YworjBj7zOWJfwDve62oDZYtH1LKG4BhUq42qFbxgvOD7DCwZOWXkZZv9qlUYzrTML9nGWLEXa19wE67iu+jyHets7C/29qWzvUV/kA2zdonrTt45A+jWzqL/Z7Z0qrZYkjIyhGmeVTRvpCetDK56szpgJdWJPMIpu6W0GOZNO4eVeF4HSU5U6IU2ogVxXdFIG8K5LTjTSjoOV7kI/o+b1nT4dt7osqoJSxULkTbYFgrKax4wiqX2SCRj5yh0KZT0kgi9FlIpBmueePyP/OrrrzuT0TB/PRztbg2233c/swFP4RlPMIc3sA2fYAfGoJKPydckJJTupd/TH+nPi9Q06TRP4NJKf/0De4+6Ww==</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#16": "Gradient Ascent \ncon la L\n 2\n Regularization \n \n17\nNell’algoritmo l’aggiornamento dei pesi possiamo farlo per \nogni componente w j:\nw(t+1)\n0 w(t)\n0+↵·@Qualit` a totaleL2(w(t))\n@w0\nw(t+1)\n1 w(t)\n1+↵·@Qualit` a totaleL2(w(t))\n@w1\n······ ·····················\nw(t+1)\nj w(t)\nj+↵·@Qualit` a totaleL2(w(t))\n@wj\n······ ·····················\nw(t+1)\nD w(t)\nD+↵·@Qualit` a totaleL2(w(t))\n@wD\n<latexit sha1_base64=\"DNhBsTO/Fo42FgAscBocdQBJt+Y=\">AAAHvXic3VVLb9NAEJ4GiEt4pXDksiKiKlRK7XCAGxXkwIFDK5G2Up1G680k3dQvvGtCZOWHckDiwA9hdm1BmxQkokpFbGTvvL7sN/NJ3iANpdKu+2WtduPmrbqzfrtx5+69+w+aGw8PVJJnAnsiCZPsKOAKQxljT0sd4lGaIY+CEA+Ds7cmf/gJMyWT+IOepdiP+DiWIym4plCyUfsODfAhAIQxSIihIOsj7Rwy+nGYwXOYU80UBpRzyT6hfQs0bIMHz8hnsEmPDyEhRxQvkQkhyswyskRtWxQnXAqntBtPwJCQmmxmeY0sB0E4n6rMP2tiaTAmounEz/QuYB9yG5XkfaPMgPbEcjGs5pbBe3p3yN6y/SaUGVJ0usDLPKbfy0781cncdsAsZ7+ySs5ljbfynLy/ntOfpsSucU7ezzmZGTXOsVYXelC2/82l2KJ/FV7JpOQ3WVmjyZVqdH0KTf5xhborK9T9TxTqVgoZbZC+ysPffp8bg2bLbbt2sWXDq4wWVGtv0PzqDxORRxhrEXKljj031f2CZ1qKEOcNP1eYcnHGx3hMZswjVP3CXjlz9jRXXCcsxYzJkNkgnkcUPFJqFgVUGXF9qhZzJnhZ7jjXo1f9QsZprjEW5iAtQ7QHKZFJusuQDWWGWnPDHJmMmeAZ1xozybgQFMzpcjPz8Ba7XzYOOm3vRbuz32ntvqkmsw6P4Qlp5MFL2IV3sAc9EPWdeq9+Uh84rx10QicuS2trFeYRXFjO9AeM558S</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#17": "Gradient Ascent \ncon la L\n 2\n Regularization \n \n18\nLa derivata parziale della funzione di qualità totale rispetto al \ntermine generico w j è la seguente:\nComponente MLE Componente L 2 Penalty@Qualit` a totaleL2(w(t))\n@wj= derivata parziale[ j]\u00002\u0000w(t)\nj\n<latexit sha1_base64=\"qwKQWDz8o+RHfQa5roWAERrRYkA=\">AAAC/HicbVI7b9RAEB47PMLxOkJJs+KEFApO9gUJGqQIGgqKROKSSHfmtF7PhU3WD+2OE4J1VPwEWqjoorT8FwokfgqzjosjyUqz8+033zy83rQy2lEU/Q7ClWvXb9xcvdW7fefuvfv9B2s7rqytwrEqTWn3UunQ6ALHpMngXmVR5qnB3fTwjY/vHqF1uize00mFSS73Cz3XShJTZf8r9GAKc7AgQUHDuGJkgUCzNy1DgPCJ9wa2oW5Zzae/HJmxL9k8h7DgcwPveB8xXud4ylEDGbPHzHxgv87qp4y9LUBc2fG4rXPQKrzmFdvyFBkjy+oj1vvefo7zGp+7Gn6WCVdIOPMZ24g1hiM5T5SxF22Pg+WJZv1BNIzaJS6DuAMD6NbWrP9nmpWqzrEgZaRzkziqKGmkJa0MLnrT2mEl1aHcxwnDQubokqb9YwvxpHaSSlGhFdqIlsTljEbmzp3kKStzSR/dxZgnr4pNapq/TBpdVDVhoXwj0gbbRk5ZzU8BRaYtEkk/OQpdCCWtJEKrhVSKyZrfRo/vI7749ZfBzmgYbwxH288Hm6+7m1mFR/CYbzWGF7AJb2ELxqCCIvgWfA9+hF/Cn+FpeHYuDYMu5yH8t8Jf/wA0VMWQ</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#18": "Gradient Ascent \ncon la L\n 2\n Regularization \n \n19\nQuesta è la versione dell’algoritmo:\nw(1)= 0 (oppure lo inizializziamo in modo casuale)\nt=1\nwhilekrQualit` a totaleL2(w(t))k2>✏\nfor j=0,1,. . . ,D\nderivata parziale[ j]=NX\ni=1\u0000j(xi){I[yi= +1]\u0000P(y=+ 1 |xi,w(t))}\nw(t+1)\nj w(t)\nj+↵⇤(derivata parziale[ j]\u00002\u0000w(t)\nj)\nt t+1\n<latexit sha1_base64=\"2zh0FCvD+/2VMpyTakbuiEO167M=\">AAAGWXicnVTLbhMxFL0taSjl1dIlmxEVqC+iTBCCTVEFLEBCqJXoQ8qEyHGc1O28NPa0TUO+kC9ACIlfYAsbjh0zbdoGKsayfX1f5/rY41YaSqWr1S8Tk9dKU+Xr0zdmbt66fefu7Ny9bZXkGRdbPAmTbLfFlAhlLLa01KHYTTPBolYodloHr4x951BkSibxB91LRSNi3Vh2JGcaqmRuskYBtUhQlyTF1CdGIaQu5GUa0Aw9svYE2jasR9B9xLxIPi1B9mgNvYoe2K6R6Rij8UgoRcspg85DfILRYEg6QR/inDg5Kqyeldt2zWFRyGB8hcMLLJKpywPOEN93+ovV7iGniR0UFYa0jXVmYwPgMfiHGEer33SoEqvvsDQxJ+jMZWvC5x3Gms28OIYjbWteGkE1kbWinhd2FuBJ2UoTy8DpXvagT4HK4dMHzxV6Cjly8aeYHUjZmV16tF+czaplyIwV24z0+i8o/hmEU0batn5Jh/AzPBhOTExWnKbhpQ7chkM28eb0IrtnCZ3vmHmPOUC0OR1j28d6lMNjx7G0/JlMfXqL7L1Cu0YryGewHqNvIL7ndJ/GZlr95zkNrsDKUVHzn+iV4m8ILA8dy1CGlsD7soih94qNMNwZLpjlbNndp/9l3rBRc5WY/6qFeDa2hqUr3jU9dm/a7cNHFnOTY8vtuVekObtQrVTt510UfCcskPs2mrPfgnbC80jEmodMqbpfTXWjzzIteSgGM0GuRMr4AeuKOsSYRUI1+vY5HHgPc8V04qUi82ToWaU4G9FnkVK9qAXPiOk9dd5mlJfZ6rnuPG/0ZZzmWsTcAGkZCgukeCbxzgqvLTOhNTOVC0/GHmcZ01pk0mOcQ5nj4TV8+Od3f1HYrlX8J5XaZm1h/aVjZpru0wP7+j6jdXqDe79FvPS59KP0s/Rr6mt5ojxdnhm6Tk64mHka+crzvwEKslZZ</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#19": "Funzione di Qualità \nnel caso L\n 1\n Penalty\n \n20\nQuesto è il caso in cui usiamo la somma dei valori assoluti \nper la penalty ( L1 Regularization ). E’ in genere chiamata \n“sparse logistic regression ”. \nLa funzione che rappresenta la qualità totale nel caso della \nlogistic regression ( L1 regularized logistic regression ) è la \nseguente:    \ndove il parametro λ (tuning parameter ) serve per bilanciare i \ndue termini.                                              Qualit` a totaleL1=l n L(w)\u0000\u0000·kwk1\n<latexit sha1_base64=\"kVQvjmORbVIpZXR5YRGRGxnE/j0=\">AAACzHicbVFNb9QwEJ2Ej5bla4EjF4sVEhxYJS1SuYAquHBAqJXYbaXd1cpxpq1V24nsCVBFufIfOSDxUxinOWxbRrI98948z9hT1EYHyrLfSXrr9p27W9v3RvcfPHz0ePzk6TxUjVc4U5Wp/HEhAxrtcEaaDB7XHqUtDB4V558if/QdfdCV+0YXNa6sPHX6RCtJDFVjA0sgQPjJewuH0IAEA5qjv8ys+ax4RQyh47iFL7zn7At4z2tTbcD1eEQtawjOQPXaqOrgFeMF32egZOQHI6/77De9wnCmZb7kM8aKvVj7kptzFT9EV+/YZGN/sbcRjNbjSTbNehM3nXxwJjDYwXr8Z1lWqrHoSBkZwiLPalq10pNWBrvRsglYS3UuT3HBrpMWw6rtJ9CJl02QVIkavdBG9CBuKlppQ7iwBWdaSWfhOhfB/3GLhk7erVrt6obQqViItMG+UFBe82hRlNojkYydo9BOKOklEXotpFIMNjzr+B/59dffdOY703x3unP4drL/cfiZbXgOL3h2OezBPnyGA5iBSj4kZWITl35NKW3T7jI1TQbNM7hi6a9/TBq4og==</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#2": "Metriche di Qualità \n[quality metric]\n \n3Errore =#previsioni errate\n#esempi\nUna metrica che si usa misura la frazione delle previsioni errate \nfornite:\nmiglior valore possibile: 0.0\nAccuracy =#previsioni corrette\n#esempi\n<latexit sha1_base64=\"NDG+msvJ/lTBIqDvf5G5eA2dVhE=\">AAACPXicbVA9TxtBEN0jBIiTEBPKNCusSKmsO4JEGiRIGtIRCQOSz7LmhjEZsfeh3TmEdbrfxE/Ir0hBAanoEG1a9oyF+HrV03vzdnZeUhh2Eobnwcyr2ddz8wtvWm/fvV/80F76uOfy0iL1MDe5PUjAkeGMesJi6KCwBGliaD85/tH4+ydkHefZrowLGqRwlPGIEcRLw/bPWOhUqi3E0gKOa72h45FnVdy5c/xrJ9zEOR5ibi2JUF3f2+QoLbiuW61huxN2wwn0cxJNSUdNsTNsX8SHOZYpZYIGnOtHYSGDCqwwGqpbcemoADyGI+p7mkFKblBNTq7159KB5Logq9noiUgPExWkzo3TxE+mIL/dU68RX/L6pYy+DSrOilIow2aRsKHJIoeWfZekD7mpAZqfk+ZMI1jwtVjWgOjF0pfb9BE9vf452VvtRl+7q7/WOpvfp80sqE9qRX1RkVpXm2pb7aieQnWm/qpL9S/4E1wF18HN3ehMMM0sq0cI/t8C0Zew6Q==</latexit>\nUn’altra metrica possibile misura la frazione delle previsioni \ncorrette:\nmiglior valore possibile: 1.0",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#20": " \n21Anche in questo caso vediamo cosa accade a fronte di diversi valori \ndel parametro λ: \nSe λ = 0: \nci riconduciamo alla soluzione standard, ossia massimizzazione \ndel likelihood( w) → ŵMLE \nSe λ → ∞: \nper soluzioni dove ŵ ≠ 0, il costo totale → - ∞ \nl’unica soluzione per massimizzare la qualità è: ŵ = 0 \nSe 0 < λ < ∞:  \nsi va verso soluzioni “sparse”, in cui vari w j sono uguali a zero.\nFunzione di Qualità \nnel caso L\n 1\n Penalty",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#21": "Pesi nella regolarizzazione \n \n22\nNelle ﬁgure seguenti riportiamo un esempio di andamento \ndei pesi w j al variare di 𝜆 per i due tipi di penalty:\nL2 Penalty\n L1 Penaltyλ λ ",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#22": "Riferimenti\n \n23\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , 3a edizione, \nApogeo, 2015. \nMachine Learning: Classiﬁcation\n , University of Washington - Coursera, 2017. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. ",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#3": " \n4\nOverﬁtting \nApprendimento della Decision Boundary \nj 𝜱j wj\n0 1 0.23\n1 x{1} 1.12\n2 x{2} -1.07\nx[1] x[1]x[2] x[2]\nData Points dell’esempio Decision Boundary:\n0.23 + 1.12 x[1] - 1.07 x[2] = 0Score( x) < 0 Score( x) > 0",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#4": " \n5\nj 𝜱j wj\n0 1 1.68\n1 x{1} 1.39\n2 x{2} -0.59\n3 x{1}^2 -0.17\n4 x{2}^2 -0.96\nx[2]\nx[1] x[1]x[2]\nData Points dell’esempio\nOverﬁtting \nApprendimento della Decision Boundary \n1.68 + 1.39 x[1] - 0.59 x[2] - 0.17 x[1]^2  - 0.96 x[2]^2 = 0Score( x) < 0 Score( x) > 0\nDecision Boundary:",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#5": " \n6\nj 𝜱j wj\n0 1 21.6\n1 x{1} 5.3\n2 x{2} -42.7\n3 x{1}^2 -15.9\n4 x{2}^2 -48.6\n5 x{1}^3 -11.0\n6 x{2}^3 67.0\n… … …\n11 x[1]^6 0.8\n12 x[2]^6 -8.6\nx[2]\nx[1] x[1]x[2]\nData Points dell’esempio\nOverﬁtting \nApprendimento della Decision Boundary \nScore( x) < 0 Score( x) > 0\nI valori assoluti di vari coefﬁcienti \nwj sono aumentati(chiaro overﬁtting)Decision Boundary",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#6": " \n7\nx[2]\nx[1]x[2]\nx[1]\nData Points dell’esempio\nOverﬁtting \nApprendimento della Decision Boundary \nScore( x) < 0 Score( x) > 0\n(overﬁtting ancora più evidente)j 𝜱j wj\n0 1 8.7\n1 x{1} 5.1\n2 x{2} 78.7\n… … …\n11 x{1}^6 -7.5\n12 x{2}^6 3803\n13 x{1}^7 21.1\n14 x{2}^7 -2406\n… … …\n39 x[1]^20 -2*10^-8\n40 x[2]^20 0.03\nI valori assoluti di vari coefﬁcienti \nwj sono aumentati ancora di piùDecision Boundary",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#7": "Andamento Errori  \ne Bias-Variance Trade-off\n \n8\nL’andamento del training error e del true error per la \nclassiﬁcation è in genere il seguente:\nDobbiamo come al solito considerare il trade-off tra bias e varianza.True Error\nTraining Error\nModel ComplexityClassiﬁcation\nError\nDato un modello con parametri ŵ, si ha overﬁtting \nse esiste un modello con i parametri stimati w’ tale \nche: \n 1. training error( ŵ) < training error(w’) \n 2. true error( ŵ) > true error(w’) \nŵ w’\n",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#8": "Regularization \nnella Classiﬁcazione\n \n9\nL’idea è quella di limitare il valore assoluto dei coefﬁcienti w i \ndeﬁnendo come segue la funzione di qualità totale (da \nmassimizzare nella fase di training): \nQualità_totale  = misura del “ﬁt” - misura grandezza coefﬁcienti\nPer misura del “ﬁt” intendiamo una funzione come la MLE. \nLa misura dei coefﬁcienti possiamo deﬁnirla in vari modi. ",
    "data_test\\rootfolder\\università\\MachineLearning\\17-Classification - Overfitting e Regularization-sbloccato.pdf#9": "Misura dei Coefﬁcienti \n \n10\nSomma dei valori:                                                  \nSomma dei valori assoluti ( L1 norm ): \nSomma dei quadrati (quadrato della L2 norm ): w0+w1+w2+···+wD\n|w0|+|w1|+|w2|+···+|wD|=DX\nj=0|wj|,kwk1\n👍\n👎\n👍\nw2\n0+w2\n1+w2\n2+···+w2\nD=DX\nj=0w2\nj,kwk2\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Regressione e Classiﬁcazione (Ex04)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#1": "Sommario\nEsercizi su Linear models per la classiﬁcazione",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#10": "LinearLogistic: Breast cancer dataset\nEsercizio\n : visualizza i parametri del modello. Cosa ti aspetti? \nplt\n.\nplot\n(\nlogreg\n.\ncoef_\n.\nT\n, \n'o'\n, \nlabel\n=\n\"C=1\"\n)\nplt\n.\nplot\n(\nlogreg100\n .\ncoef_\n.\nT\n, \n'^'\n, \nlabel\n=\n\"C=100\"\n)\nplt\n.\nplot\n(\nlogreg001\n .\ncoef_\n.\nT\n, \n'v'\n, \nlabel\n=\n\"C=0.001\"\n )\nplt\n.\nxticks\n(\nrange\n(\ncancer\n.\ndata\n.\nshape\n[\n1\n]), \ncancer\n.\nfeature_names\n , \nrotation\n =\n90\n)\nplt\n.\nhlines\n(\n0\n, \n0\n, \ncancer\n.\ndata\n.\nshape\n[\n1\n])\nplt\n.\nylim\n(\n-\n5\n, \n5\n)\nplt\n.\nxlabel\n(\n\"Coefficient index\"\n )\nplt\n.\nylabel\n(\n\"Coefficient magnitude\"\n )\nplt\n.\nlegend\n()\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#11": "LinearLogistic: Breast cancer dataset\nRegolarizzazioni alte spingono i parametri a 0.  \nIn alcuni casi (\n mean perimeter\n ), per C=100 e C=1 il coefﬁciente è negativo, \npositivo per C=0.001.  \n12\nAttenzione: \n È sbagliato \npensare che i parametri \nsuggeriscano quali features \ndeterminino direttamente la \nclasse (tumore maligno o \nmeno). L'importanza della \nfeature dipende strettamente \ndal modello preso in \nconsiderazione.",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#12": "LinearLogistic: Breast cancer dataset\nEsercizio\n : cerca di interpretare meglio l'importanza delle feature \nimpiegando la L1 regularization. \n13",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#13": "LinearLogistic: Breast cancer dataset\nEsercizio\n : cerca di interpretare meglio l'importanza delle feature impiegando la L1 \nregularization. \nfor \nC\n, \nmarker \nin \nzip\n([\n0.001\n, \n1\n, \n100\n], [\n'o'\n, \n'^'\n, \n'v'\n]):\nlr_l1 \n= \nLogisticRegression\n (\nC\n=\nC\n, \npenalty\n=\n\"l1\"\n)\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\"\n .\nformat\n(\nC\n, \nlr_l1\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\"\n .\nformat\n(\nC\n, \nlr_l1\n.\nscore\n(\nX_test\n, \ny_test\n)))\nplt\n.\nplot\n(\nlr_l1\n.\ncoef_\n.\nT\n, \nmarker\n, \nlabel\n=\n\"C={:.3f}\"\n .\nformat\n(\nC\n))\nplt\n.\nxticks\n(\nrange\n(\ncancer\n.\ndata\n.\nshape\n[\n1\n]), \ncancer\n.\nfeature_names\n , \nrotation\n =\n90\n)\nplt\n.\nhlines\n(\n0\n, \n0\n, \ncancer\n.\ndata\n.\nshape\n[\n1\n])\nplt\n.\nxlabel\n(\n\"Coefficient index\"\n )\nplt\n.\nylabel\n(\n\"Coefficient magnitude\"\n )\nplt\n.\nylim\n(\n-\n5\n, \n5\n)\nplt\n.\nlegend\n(\nloc\n=\n3\n)\n> Training accuracy of l1 logreg with C=0.001: 0.91\n> Test accuracy of l1 logreg with C=0.001: 0.92\n> Training accuracy of l1 logreg with C=1.000: 0.96\n> Test accuracy of l1 logreg with C=1.000: 0.96\n> Training accuracy of l1 logreg with C=100.000: 0.99\n> Test accuracy of l1 logreg with C=100.000: 0.98\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#14": "LinearLogistic: Breast cancer dataset\nL'effetto della L1 regularization dipende dal valore del parametro, in modo \nsimile al caso della regressione. \n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#15": "Linear models per la classiﬁcazione multiclass\nAlcuni modelli non si adattano facilmente al caso multiclass.  \nUn approccio piuttosto semplice è il\n  one-vs-rest:\n  si creano vari modelli, \ndove ogni modello si addestra a riconoscere una speciﬁca classe. Durante \nla predizione vengono valutati tutti i modelli e quello con score più alto \ndetermina la classe in output. \nChiaramente si avranno un insieme di parametri da addestrare per ogni \nclasse. \n16",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#16": "Linear models per la classiﬁcazione multiclass\nEsempio\n : creiamo un dataset con 2 features e 3 classi e impieghiamo il \nLinear SVM per la classiﬁcazione. Il dataset è creato seguendo una \ndistribuzione gaussiana. \nfrom \nsklearn.datasets \n import \nmake_blobs\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n42\n)\nmglearn\n.\ndiscrete_scatter\n (\nX\n[:, \n0\n], \nX\n[:, \n1\n], \ny\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\nplt\n.\nlegend\n([\n\"Class 0\"\n , \n\"Class 1\"\n , \n\"Class 2\"\n ])\n17\n",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#17": "Linear models per la classiﬁcazione multiclass\nlinear_svm \n = \nLinearSVC\n ()\n.\nfit\n(\nX\n, \ny\n)\nprint\n(\n\"Coefficient shape: \"\n , \nlinear_svm\n .\ncoef_\n.\nshape\n)\nprint\n(\n\"Intercept shape: \"\n , \nlinear_svm\n .\nintercept_\n .\nshape\n)\n> Coefficient shape: (3, 2)\n>Intercept shape: (3,)\nOgni riga di \n _coef\n  rappresenta il vettore dei parametri per una delle 3 \nclassi, e le colonne sono le 2 features. \n intercept_\n  è un array 1d che \nmemorizza l'intercetta per ogni classe. \nmglearn\n.\ndiscrete_scatter\n (\nX\n[:, \n0\n], \nX\n[:, \n1\n], \ny\n)\nline \n= \nnp\n.\nlinspace\n (\n-\n15\n, \n15\n)\nfor \ncoef\n, \nintercept\n , \ncolor \nin \nzip\n(\nlinear_svm\n .\ncoef_\n, \nlinear_svm\n .\nintercept_\n ,\n[\n'b'\n, \n'r'\n, \n'g'\n]):\nplt\n.\nplot\n(\nline\n, \n-\n(\nline \n* \ncoef\n[\n0\n] \n+ \nintercept\n ) \n/ \ncoef\n[\n1\n], \nc\n=\ncolor\n)\nplt\n.\nylim\n(\n-\n10\n, \n15\n)\nplt\n.\nxlim\n(\n-\n10\n, \n8\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\nplt\n.\nlegend\n([\n'Class 0'\n , \n'Class 1'\n , \n'Class 2'\n , \n'Line class 0'\n , \n'Line class 1'\n ,\n'Line class 2'\n ], \nloc\n=\n(\n1.01\n, \n0.3\n))\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#18": "Linear models per la classiﬁcazione multiclass\nOgni istanza etichettata con la classe 0 è al di sopra della \n boundary  \ndeﬁnita dal classiﬁcatore per la class 0. Le istanze delle altre classi al di \nsotto. Stessa cosa per la classe 1 e 2, e i relativi classiﬁcatori. \nCosa succede se una istanza si trova nel triangolo centrale? \n19\n",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#19": "Linear models per la classiﬁcazione multiclass\nCosa succede se una istanza si trova nel triangolo centrale? \nLa classe associata corrisponde alla linea più vicina. \n20\n",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#2": "Richiami: Linear models per la classiﬁcazione\nI modelli lineari possono essere impiegati per la classiﬁcazione con \ndecision boundary\n  che rappresento linee, piani o iperpiani.  \nNella logisitic regression si impiega un modello lineare tradizionale il cui \noutput è valutato da una funzione \n logistic\n  (s\nigmoid function\n ) che restituisce \nun valore in [0,1] ed indica la probabilità di appartenenza ad una certa \nclasse (> 0.5) o meno (< 0.5). \nIl \ngradient descent\n  è impiegato per minimizzare la funzione di costo. \nI due algoritmi di classiﬁcazione lineari più famosi sono la \n logistic \nregression\n , e i \nlinear support vector machine\n  (o Linear SVM) che vedremo \nin seguito. \nScikit-learn fornisce la classe \n linear_model.LogisticRegression\n  e \nsvm.LinearSVC\n  che implementa i due algoritmi.\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#20": "Linear models per la classiﬁcazione multiclass\nIl seguente codice mostra le regioni associate alle predizioni \nmglearn\n.\nplots\n.\nplot_2d_classification\n (\nlinear_svm\n , \nX\n, \nfill\n=\nTrue\n, \nalpha\n=.\n7\n)\nmglearn\n.\ndiscrete_scatter\n (\nX\n[:, \n0\n], \nX\n[:, \n1\n], \ny\n)\nline \n= \nnp\n.\nlinspace\n (\n-\n15\n, \n15\n)\nfor \ncoef\n, \nintercept\n , \ncolor \nin \nzip\n(\nlinear_svm\n .\ncoef_\n, \nlinear_svm\n .\nintercept_\n ,\n                            [\n 'b'\n, \n'r'\n, \n'g'\n]):\nplt\n.\nplot\n(\nline\n, \n-\n(\nline \n* \ncoef\n[\n0\n] \n+ \nintercept\n ) \n/ \ncoef\n[\n1\n], \nc\n=\ncolor\n)\nplt\n.\nlegend\n([\n'Class 0'\n , \n'Class 1'\n , \n'Class 2'\n , \n'Line class 0'\n , \n'Line class 1'\n ,\n      \n'Line class 2'\n ], \nloc\n=\n(\n1.01\n, \n0.3\n))\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#21": "Linear models per la classiﬁcazione multiclass\n22\n",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#22": "Considerazioni sul tuning\nGli iperparametri C e \n λ\n sono solitamente campionati su scala logaritmica. \nSe si assume che il dataset contenga solo alcune feature rilevanti si può \ntestare la L1 regularization, altrimenti si tende a preferire la L2.  \nLa L1 regularization è utile anche per dare una interpretazione del modello.   \nI modelli lineari sono veloci da addestrare, anche su dati sparsi. \nPer dataset con >100.000 istanze si può impiegare il parametro \n solver='sag'  \nnella LogisticRegression e Ridge, che rende l'apprendimento più veloce. \nAltre opzioni sono SGDClassiﬁer e SGDRegressor che implementano \nversioni più scalabili dei relativi algoritmi. \nI parametri possono indicare quali feature siano più rilevanti, nei casi in cui \nle feature sono indipendenti tra loro. \nI modelli lineari hanno buone performance quando il numero di features è \ngrande rispetto al numero di istanze. Sono impiegati anche su grandi dataset \nperché spesso gli altri modelli non sono facilmente addestrabili.\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#23": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#3": "Linear models per la classiﬁcazione: forge dataset\nEsercizio\n : impiega il forge dataset e la LogisticRegression per la \nclassiﬁcazione. Valuta le performance. \nfrom \nsklearn.linear_model \n import \nLogisticRegression\nX\n, \ny \n= \nmglearn\n.\ndatasets\n .\nmake_forge\n ()\n...\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#4": "Linear models per la classiﬁcazione: forge dataset\nEsercizio\n : impiega il forge dataset e la LogisticRegression per la \nclassiﬁcazione. Valuta le performance. \nfrom \nsklearn.linear_model \n import \nLogisticRegression\nfrom \nsklearn.svm \n import \nLinearSVC\nX\n, \ny \n= \nmglearn\n.\ndatasets\n .\nmake_forge\n ()\nfig\n, \naxes \n= \nplt\n.\nsubplots\n (\n1\n, \n2\n, \nfigsize\n=\n(\n10\n, \n3\n))\nfor \nmodel, \nax \nin \nzip\n([\nLinearSVC\n (), \nLogisticRegression\n ()], \naxes\n):\nclf \n= \nmodel\n.\nfit\n(\nX\n, \ny\n)\nmglearn\n.\nplots\n.\nplot_2d_separator\n (\nclf\n, \nX\n, \nfill\n=\nFalse\n, \neps\n=\n0.5\n,\nax\n=\nax\n, \nalpha\n=.\n7\n)\nmglearn\n.\ndiscrete_scatter\n (\nX\n[:, \n0\n], \nX\n[:, \n1\n], \ny\n, \nax\n=\nax\n)\nax\n.\nset_title\n (\n\"{}\"\n.\nformat\n(\nclf\n.\n__class__\n .\n__name__\n ))\nax\n.\nset_xlabel\n (\n\"Feature 0\"\n )\nax\n.\nset_ylabel\n (\n\"Feature 1\"\n )\naxes\n[\n0\n]\n.\nlegend\n()\nNota: \n Impieghiamo entrambe le implementazioni, anche se l'algoritmo SVM \nlo vedremo in dettaglio in seguito. \n5",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#5": "Linear models per la classiﬁcazione: forge dataset\nIn questo dataset la decision boundary è rappresentata da una retta. \n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#6": "Scikit-learn: Logistic regression\nIn Scikit-learn, la Logistic regression impiega la \n L2\n di default. \nIl parametro \n penalty\n  speciﬁca quale regolarizzazione impiegare {‘l1’, ‘l2’, \n‘elasticnet’, ‘none’}.  \nIl parametro \n C\n indica il peso della regolarizzazione (valori bassi \nrappresentano una regolarizzazione maggiore), il default è C=1.0  \n7",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#7": "Richiami: Linear models per la classiﬁcazione\nAl variare di \n C\n si può notare l'effetto sul dataset considerato. \nCon alta regolarizzazione (C basso) il modello sbaglia a classiﬁcare 2 \nistanze cercando di considerare la \"maggioranza\" durante la scelta della \ndecision boundary. \nPer valori più alti di C la retta si inclina dando più importanza ai 2 punti. \nUn punto rimane comunque non classiﬁcato, ed è impossibile considerarlo \ncon una semplice linea retta. \n8\n",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#8": "LinearLogistic: Breast cancer dataset\nEsercizio\n : valuta la \n linera logistic \n nel caso del Breast cancer dataset, \nconsiderando valori di C pari a 0.01, 1 e 100. Commenta i risultati ottenuti \nin termini di accuracy e potenziali fenomeni di over o underﬁtting.\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\18-Ex_04 Esercitazione Linear models per la classificazione-sbloccato.pdf#9": "LinearLogistic: Breast cancer dataset\nEsercizio\n : valuta la linera logistic nel caso del Breast cancer dataset, \nconsiderando valori di C pari a 1, 100 e 0.01. \nfrom \nsklearn.datasets \n import \nload_breast_cancer\ncancer \n= \nload_breast_cancer\n ()\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nstratify\n =\ncancer\n.\ntarget\n, \nrandom_state\n =\n42\n)\nlogreg \n= \nLogisticRegression\n (C=1)\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Training set score: {:.3f}\"\n .\nformat\n(\nlogreg\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Test set score: {:.3f}\"\n .\nformat\n(\nlogreg\n.\nscore\n(\nX_test\n, \ny_test\n)))\nTraining set score: 0.953\nTest set score: 0.958\nTraining set score: 0.972\nTest set score: 0.965\nTraining set score: 0.934\nTest set score: 0.930\nPer C=1 c'è un probabile underﬁtting. Per C=100 (modello più complesso) \nmigliorano le performance. Per C=0.01 si incrementa l'underﬁtting iniziale.\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Decision Trees (Ex05)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#1": "Sommario\nRichiami \nscikit-learn e decision trees \nVisualizzazione \nFeature importance \nDecision trees e regressione \nPruning",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#10": "scikit-learn e decision trees\ntree \n= \nDecisionTreeClassifier\n (\nmax_depth\n =\n4\n, \nrandom_state\n =\n0\n)\ntree\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Accuracy on training set: {:.3f}\"\n .\nformat\n(\ntree\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Accuracy on test set: {:.3f}\"\n .\nformat\n(\ntree\n.\nscore\n(\nX_test\n, \ny_test\n)))\n> Accuracy on training set: 0.988\n> Accuracy on test set: 0.951\nPiù bassa sul training, ma migliora (meno overﬁtting) sul test.\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#11": "scikit-learn: visualizzare i decision trees\nLa funzione \n export_graphviz\n  del modulo \n tree\n permette di visualizzare \nl'albero. Salva un ﬁle .dot che può essere importato per la visualizzazione. \nfrom \nsklearn.tree \n import \nexport_graphviz\nexport_graphviz\n (\ntree\n, \nout_file\n =\n\"tree.dot\"\n , \nclass_names\n =\n[\n\"malignant\"\n , \n\"benign\"\n ], \nfeature_names\n =\ncancer\n.\nfeature_names\n , \nimpurity\n =\nFalse\n, \nfilled\n=\nTrue\n)\nimport \ngraphviz\nwith \nopen\n(\n\"tree.dot\"\n ) \nas \nf\n:\ndot_graph \n = \nf\n.\nread\n()\ngraphviz\n .\nSource\n(\ndot_graph\n )\nVisualizzare il \"comportamento\" di un algoritmo di ML è molto utile per \nspiegarne l'output (\n explaination\n ), in questo caso anche ai non-esperti.\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#12": "scikit-learn: visualizzare i decision trees\nL'albero generato:\n13\nsamples  indica il numero di \nistanze, value  la rispettiva \nsuddivisione in base alle label, \nclass  la classe majoriy.",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#13": "scikit-learn: visualizzare i decision trees\nUn altro modo per esplorare i decision trees è assegnare una misura di \nimportanza\n  alle feature in base al funzionamento dell'algoritmo. \nLa variabile feature_importances_ del modello è un array con valori in [0,1] \ndove 1 indica \"predice perfettamente in valore target\". \nprint\n(\n\"Feature importances:\\n{}\"\n .\nformat\n(\ntree\n.\nfeature_importances_\n ))\nOut[62]:\n[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.01\n0.048 0. 0. 0.002 0. 0. 0. 0. 0. 0.727 0.046\n0. 0. 0.014 0. 0.018 0.122 0.012 0. ]\ndef \nplot_feature_importances_cancer\n (\nmodel\n):\nn_features \n = \ncancer\n.\ndata\n.\nshape\n[\n1\n]\nplt\n.\nbarh\n(\nrange\n(\nn_features\n ), \nmodel\n.\nfeature_importances_\n , \nalign\n=\n'center'\n )\nplt\n.\nyticks\n(\nnp\n.\narange\n(\nn_features\n ), \ncancer\n.\nfeature_names\n )\nplt\n.\nxlabel\n(\n\"Feature importance\"\n )\nplt\n.\nylabel\n(\n\"Feature\"\n )\nplot_feature_importances_cancer\n (\ntree\n)\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#14": "Decision trees e feature importance\nUn altro modo per esplorare i decision trees è assegnare una misura di \nimportanza\n  alle feature in base al funzionamento dell'algoritmo. \nLa variabile \n feature_importances_\n  del modello è un array con valori in \n[0,1] dove 1 indica \"predice perfettamente in valore target\". È valutata \nmediante la metrica GINI. \nprint\n(\n\"Feature importances:\\n{}\"\n .\nformat\n(\ntree\n.\nfeature_importances_\n ))\nOut[62]:\n[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.01\n0.048 0. 0. 0.002 0. 0. 0. 0. 0. 0.727 0.046\n0. 0. 0.014 0. 0.018 0.122 0.012 0. ]\ndef \nplot_feature_importances_cancer\n (\nmodel\n):\nn_features \n = \ncancer\n.\ndata\n.\nshape\n[\n1\n]\nplt\n.\nbarh\n(\nrange\n(\nn_features\n ), \nmodel\n.\nfeature_importances_\n , \nalign\n=\n'center'\n )\nplt\n.\nyticks\n(\nnp\n.\narange\n(\nn_features\n ), \ncancer\n.\nfeature_names\n )\nplt\n.\nxlabel\n(\n\"Feature importance\"\n )\nplt\n.\nylabel\n(\n\"Feature\"\n )\nplot_feature_importances_cancer\n (\ntree\n)\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#15": "Decision trees e feature importance\nSi può notare come worst radius è la feature più discriminante. Questo \nindica anche che l'albero è ben costruito avendo questa feature in cima. \nPuoi dire che una \n feature\n  con bassa importance è poco discriminante?\n16\n",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#16": "Esempio: feature importance\ntree \n= \nmglearn\n.\nplots\n.\nplot_tree_not_monotone\n ()\ndisplay\n(\ntree\n)\nEsercizio: \n In questo esempio come costruiresti l'albero di decisione?\n17\nX[0]X[1]",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#17": "Esempio: feature importance\ntree \n= \nmglearn\n.\nplots\n.\nplot_tree_not_monotone\n ()\ndisplay\n(\ntree\n)\nIn questo esempio, l'informazione rilevante è contenuta in X[1]. Infatti non \npossiamo dire che un valore alto per la feature X[0] identiﬁca la classe 0, e \nuno basso la classe 1. L'albero effettivamente impiega la feature corretta.\n18\nX[0]X[1]",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#18": "scikit-learn: decision tree per la regressione\nLa classe DecisionTreeRegressor impiega lo stesso algoritmo in ambito di \nregressione \nimport \npandas \nas \npd\nram_prices \n = \npd\n.\nread_csv\n (\n\"data/ram_price.csv\"\n )\nplt\n.\nsemilogy\n (\nram_prices\n .\ndate\n, \nram_prices\n .\nprice\n)\nplt\n.\nxlabel\n(\n\"Year\"\n)\nplt\n.\nylabel\n(\n\"Price in $/Mbyte\"\n )\n19\nhttps://github.com/amueller/introduction_to_ml_with_python/blob/master/data/ram_price.csvSu scala logaritmica per le y si può \nipotizzare una relazione lineare",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#19": "scikit-learn: decision tree per la regressione\nConfrontiamo i decision trees con un modelli lineare, con l'accortezza di \nconvertire i dati in valori logaritmo altrimenti il modello lineare non può \nfunzionare. \nfrom \nsklearn.tree \n import \nDecisionTreeRegressor\n# use historical data to forecast prices after the year 2000\ndata_train \n = \nram_prices\n [\nram_prices\n .\ndate \n< \n2000\n]\ndata_test \n = \nram_prices\n [\nram_prices\n .\ndate \n>= \n2000\n]\n# predict prices based on date\nX_train \n = \ndata_train\n .\ndate\n[:, \nnp\n.\nnewaxis\n]\n# we use a log-transform to get a simpler relationship of data to target\ny_train \n = \nnp\n.\nlog\n(\ndata_train\n .\nprice\n)\ntree \n= \nDecisionTreeRegressor\n ()\n.\nfit\n(\nX_train\n, \ny_train\n)\nlinear_reg \n = \nLinearRegression\n ()\n.\nfit\n(\nX_train\n, \ny_train\n)\n# predict on all data\nX_all \n= \nram_prices\n .\ndate\n[:, \nnp\n.\nnewaxis\n]\npred_tree \n = \ntree\n.\npredict\n(\nX_all\n)\npred_lr \n = \nlinear_reg\n .\npredict\n(\nX_all\n)\n# undo log-transform\nprice_tree \n = \nnp\n.\nexp\n(\npred_tree\n )\nprice_lr \n = \nnp\n.\nexp\n(\npred_lr\n)\nCosa ti aspetti?\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#2": "Richiami: Decision Trees\nImpiegati spesso per la classiﬁcazione e regressione.  \nIn sintesi creano una albero di nodi if/else che porta ad una certa \ndecisione.  \nSi può rappresentare come un albero dove le foglie contengono la risposta.\n3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#20": "scikit-learn: decision tree per la regressione\nplt\n.\nsemilogy\n (\ndata_train\n .\ndate\n, \ndata_train\n .\nprice\n, \nlabel\n=\n\"Training data\"\n )\nplt\n.\nsemilogy\n (\ndata_test\n .\ndate\n, \ndata_test\n .\nprice\n, \nlabel\n=\n\"Test data\"\n )\nplt\n.\nsemilogy\n (\nram_prices\n .\ndate\n, \nprice_tree\n , \nlabel\n=\n\"Tree prediction\"\n )\nplt\n.\nsemilogy\n (\nram_prices\n .\ndate\n, \nprice_lr\n , \nlabel\n=\n\"Linear prediction\"\n )\nplt\n.\nlegend\n()\nIl modello lineare approssima con una retta. Il decision tree è molto più \naccurato nella predizione. \nMa che succede dopo l'ultima data presente nel dataset?\n21\n",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#21": "scikit-learn: decision tree per la regressione\nplt\n.\nsemilogy\n (\ndata_train\n .\ndate\n, \ndata_train\n .\nprice\n, \nlabel\n=\n\"Training data\"\n )\nplt\n.\nsemilogy\n (\ndata_test\n .\ndate\n, \ndata_test\n .\nprice\n, \nlabel\n=\n\"Test data\"\n )\nplt\n.\nsemilogy\n (\nram_prices\n .\ndate\n, \nprice_tree\n , \nlabel\n=\n\"Tree prediction\"\n )\nplt\n.\nsemilogy\n (\nram_prices\n .\ndate\n, \nprice_lr\n , \nlabel\n=\n\"Linear prediction\"\n )\nplt\n.\nlegend\n()\nIl modello lineare approssima con una retta. Il decision tree è molto più \naccurato nella predizione. \nAttenzione: \n L'algoritmo decision tree non è in grado di fare predizioni su \nnuovi dati con la data oltre a quella contenuta nel dataset. \n22\n",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#22": "scikit-learn: decision tree e pruning\nPer limitare l'overﬁtting e la complessità, solitamente è sufﬁciente \nimpiegare una tecnica di pre-pruning con uno dei seguenti parametri: \nmax_depth\n , \nmax_leaf_nodes\n ,  o.  \nmin_samples_leaf\nNota: \n min_samples_leaf indica il minimo numero di istanze per foglia. \nEsercizio\n : prova ad addestrare nuovamente il decision trees sul breast \ncancer dataset impostano a turno uno di questi valori e valutare le \nvariazioni di accuracy.\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#23": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#3": "Richiami: Decision Trees\nIn ML, ogni \"domanda\" in un nodo è chiamata comunemente \n test\n, ed è \nspesso codiﬁcata con feature su domini continui, ad esempio: \nla feature \n i\n è maggiore del valore \n a\n? \nL'algoritmo si focalizza nello scegliere le sequenze if/else che portano ad \nuna riposta più velocemente, ovvero sono più \n informative\n  per la variabile \ntarget.\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#4": "Dataset two_moons\nToy dataset generato da scikit-learn \nsklearn.datasets.make_moons(\n n_samples=100\n , \n*\n, \nshuﬄe=True\n , \nnoise=None\n , \nrandom_state=None\n )\nOgni istanza ha 2 valori. \nAd esempio, per 75 istanze otteniamo: \nPer la profondità 0 dell'albero, quale test immagineresti?\n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#5": "Decision Trees su two_moons dataset\nDepth = 1 \nDepth = 2 \n...\n6\ndove [2,32] indica che 2 istanze appartengono \nalla classe 1 e 32 alla classe 2\nSe in una foglia ci sono istanze appartenenti \nad una sola classe allora la foglia si chiama \npure.",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#6": "Richiami: Decision Trees\nOgni test considera una singola feature, perciò la relativa decisione è \nrappresentata come una asse parallelo ad uno degli assi. \nNella predizione, una volta arrivati ad una foglia, si assegna la classe target \nche appare più spesso nella regione associata. In modo simile per la \nregressione si opera una media dei valori delle istanze nella regione. \nPer dataset grandi, creare foglie pure è molto dispendioso in termini di \nrisorse computazione e può creare fenomeni di overﬁtting. \nSi possono implementare tecniche di early stopping limitando la \nprofondità dell'albero (\n pre-pruning\n ), oppure rimuovere o fondere foglie \nche contengono poca informazione (\n post-pruning\n  o \npruning\n )\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#7": "scikit-learn e decision trees\nLa classe \n DecisionTreeClassiﬁer\n  del modulo \n DecisionTreeRegressor  \nimplementa l'algoritmo. \nEsercizio\n : prova ad impiegarlo nel dataset Breast Cancer e valuta l'accuracy \nsul training e test set \nfrom \nsklearn.tree \n import \nDecisionTreeClassifier\ncancer \n= \nload_breast_cancer\n ()\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nstratify\n =\ncancer\n.\ntarget\n, \nrandom_state\n =\n42\n)\n...\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#8": "scikit-learn e decision trees\nLa classe \n DecisionTreeClassiﬁer\n  del modulo \n DecisionTreeRegressor  \nimplementa l'algoritmo. \nEsercizio\n : prova ad impiegarlo nel dataset Breast Cancer e valuta l'accuracy \nsul training e test set \nfrom \nsklearn.tree \n import \nDecisionTreeClassifier\ncancer \n= \nload_breast_cancer\n ()\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nstratify\n =\ncancer\n.\ntarget\n, \nrandom_state\n =\n42\n)\ntree \n= \nDecisionTreeClassifier\n (\nrandom_state\n =\n0\n)\ntree\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Accuracy on training set: {:.3f}\"\n .\nformat\n(\ntree\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Accuracy on test set: {:.3f}\"\n .\nformat\n(\ntree\n.\nscore\n(\nX_test\n, \ny_test\n)))\n> Accuracy on training set: 1.000\n> Accuracy on test set: 0.937\nTi aspettavi una accuracy del 100%?\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\19-Ex_05 Esercitazione su Classificazione Decision Trees-sbloccato.pdf#9": "scikit-learn e decision trees\n> Accuracy on training set: 1.000\n> Accuracy on test set: 0.937\nTi aspettavi una accuracy del 100%? Sì, per come funziona l'algoritmo \nl'albero cresce ﬁno a creare foglie pure che rappresentazione perfettamente \nl'appartenenza delle istanze alle relative label.  \nL'accuracy sul test set è leggermente inferiore ai modelli lineari (95% ca). \nEsercizio\n : Prova a impostare una profondità col parametro \n max_depth  \ndurante la costruzione dell'oggetto DecisionTreeClassiﬁer. Cosa ti aspetti \nsulle due accuracy?\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#0": "Machine Learning \nAnno Accademico 2021 - 2022 \n  \nRichiami di Matematica",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#1": "Sommario\nRichiami sulle Funzioni Convesse \nFunzioni di più Variabili (Derivate Parziali) \nGradiente di una Funzione \nAlgoritmo di Gradient Descent \nCenni di Calcolo delle Probabilità\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#10": "Funzioni Convesse \n 11\nwg(w)\nv wg(w)\ng(v)\n..g(v)+rg(v)T(w\u0000v)g(v)+dg(v)\ndw(w\u0000v)\ncaso  \nmonodimensionale\nPer una funzione convessa g(\n w\n) differenziabile, il “piano” tangente giace sempre al \ndi sotto del graﬁco della funzione:\ng(w)\u0000g(v)+rg(v)T(w\u0000v)",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#11": "Funzioni di più Variabili\n \n12",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#12": "Derivate Parziali di \nFunzioni di più Variabili \n 13g(w0+\u0000w0,w1)\u0000g(w0,w1)\n\u0000w0\n<latexit sha1_base64=\"+SgtUNqNwOjhFnklqk6DFaFtyzI=\">AAACZXicbVHJTsMwEHXCXrayiAsHLCokEFAlgARHBBw4gkShUlNVEzMtFs6CPaGqonwn4sCZr0AiKT10YU7P772ZsZ/9WElDjvNp2VPTM7Nz8wulxaXlldXy2vqjiRItsCYiFem6DwaVDLFGkhTWY40Q+Aqf/NfrQn96R21kFD5QL8ZmAJ1QtqUAyqlW+c1raxBpZ9+LclsxJe1mrdTJ+CH3blAR8G5xPOKjBjc74Md8su0fX5YOD8pa5YpTdfrFJ4E7ABU2qLtW+ct7jkQSYEhCgTEN14mpmYImKRRmJS8xGIN4hQ42chhCgKaZ9qPJ+F5igCIeo+ZS8T6Jwx0pBMb0Aj93BkAvZlwryP+0RkLti2YqwzghDEWxiKTC/iIjtMwzQP4sNRJBcXPkMuQCNBChlhyEyMkk/4RSnoc7/vpJ8HhSdU+rJ/dnlcurQTLzbJvtsn3msnN2yW7ZHasxwT7YjzVrzVnf9rK9aW/9WW1r0LPBRsre+QVEDroO</latexit>0< |\u0000w0|<\u0000\n<latexit sha1_base64=\"u356CZIh7btrlBj4nRu25R6OhAY=\">AAACG3icbVC7TgJBFJ3FF+ILtbSZSIxWZBdNtLAwamGJiTwSIOTucMEJs4/M3NWYDZ/gJ/gVtlrZGVsLC//FATFR8FQn55ybe8/1YyUNue6Hk5mZnZtfyC7mlpZXVtfy6xtVEyVaYEVEKtJ1HwwqGWKFJCmsxxoh8BXW/P7Z0K/doDYyCq/oLsZWAL1QdqUAslI7v+vyY95UNkK8eY6KgN+2U3fwo1nTyF4A7XzBLboj8GnijUmBjVFu5z+bnUgkAYYkFBjT8NyYWilokkLhINdMDMYg+tDDhqUhBGha6ajQgO8kBijiMWouFR+J+HsihcCYu8C3yQDo2kx6Q/E/r5FQ96iVyjBOCEMxXERS4WiREVraysg7UiMRDC9HLkMuQAMRaslBCCsm9nU5+w9vsv00qZaK3n6xdHlQODkdfybLttg222MeO2Qn7IKVWYUJds8e2RN7dh6cF+fVefuOZpzxzCb7A+f9C3UZoAU=</latexit>\nConsideriamo una funzione                 di 2 variabili deﬁnita in un campo A.\ng(w0,w1)\n<latexit sha1_base64=\"0nuB/rhxp/IDPQ5Yo6JeFVwMZ9U=\">AAAB/XicbVDLSsNAFJ3UV62vqks3g0WoICWpgi6LblxWsA9IQ5hMb+vQyYOZG0sJxa9wqyt34tZvceG/mMQstHpWh3Pu5Z57vEgKjab5YZSWlldW18rrlY3Nre2d6u5eV4ex4tDhoQxV32MapAiggwIl9CMFzPck9LzJVeb37kFpEQa3OIvA8dk4ECPBGaaSPa5PXfOETl3r2K3WzIaZg/4lVkFqpEDbrX4OhiGPfQiQS6a1bZkROglTKLiEeWUQa4gYn7Ax2CkNmA/aSfLIc3oUa4YhjUBRIWkuws+NhPlaz3wvnfQZ3ulFLxP/8+wYRxdOIoIoRgh4dgiFhPyQ5kqkXQAdCgWILEsOVASUM8UQQQnKOE/FOC2nkvZhLX7/l3SbDeu00bw5q7Uui2bK5IAckjqxyDlpkWvSJh3CSUgeyRN5Nh6MF+PVePseLRnFzj75BeP9CxbnlII=</latexit>\nche si chiama rapporto incrementale parziale rispetto a       della     , perché in \nesso consideriamo incrementata soltanto la      , mantenendo inalterata la      .\nw0\n<latexit sha1_base64=\"sgHalUL2H+5/2ELVE3iy+BBsRNs=\">AAAB9XicbVC7TsNAEDyHVwivACXNiQiJKrIDEpQRNJRBkIeURNH6sgmnnB+6WxNFVj6BFio6RMv3UPAv2MYFJEw1mtnVzo4bKmnItj+twsrq2vpGcbO0tb2zu1feP2iZINICmyJQge64YFBJH5skSWEn1Aieq7DtTq5Tv/2I2sjAv6dZiH0Pxr4cSQGUSHfTgT0oV+yqnYEvEycnFZajMSh/9YaBiDz0SSgwpuvYIfVj0CSFwnmpFxkMQUxgjN2E+uCh6cdZ1Dk/iQxQwEPUXCqeifh7IwbPmJnnJpMe0INZ9FLxP68b0eiyH0s/jAh9kR4iqTA7ZISWSQfIh1IjEaTJkUufC9BAhFpyECIRo6SUUtKHs/j9MmnVqs5ZtXZ7Xqlf5c0U2RE7ZqfMYReszm5YgzWZYGP2xJ7ZizW1Xq036/1ntGDlO4fsD6yPb7Blkic=</latexit>\ng\n<latexit sha1_base64=\"M1D3T4qT4zLethYLQWkyJspQDuA=\">AAAB83icbVC7TsNAEDyHVwivACXNiQiJKrIDEpQRNJSJRB5SYkXnyyaccj5bd3tIkZUvoIWKDtHyQRT8C7ZxAQlTjWZ2tbMTxFIYdN1Pp7S2vrG5Vd6u7Ozu7R9UD4+6JrKaQ4dHMtL9gBmQQkEHBUroxxpYGEjoBbPbzO89gjYiUvc4j8EP2VSJieAMU6k9HVVrbt3NQVeJV5AaKdAaVb+G44jbEBRyyYwZeG6MfsI0Ci5hURlaAzHjMzaFQUoVC8H4SR50Qc+sYRjRGDQVkuYi/N5IWGjMPAzSyZDhg1n2MvE/b2Bxcu0nQsUWQfHsEAoJ+SHDtUgbADoWGhBZlhyoUJQzzRBBC8o4T0WbVlJJ+/CWv18l3Ubdu6g32pe15k3RTJmckFNyTjxyRZrkjrRIh3AC5Ik8kxfHOq/Om/P+M1pyip1j8gfOxzdtWJF0</latexit>\nw0\n<latexit sha1_base64=\"sgHalUL2H+5/2ELVE3iy+BBsRNs=\">AAAB9XicbVC7TsNAEDyHVwivACXNiQiJKrIDEpQRNJRBkIeURNH6sgmnnB+6WxNFVj6BFio6RMv3UPAv2MYFJEw1mtnVzo4bKmnItj+twsrq2vpGcbO0tb2zu1feP2iZINICmyJQge64YFBJH5skSWEn1Aieq7DtTq5Tv/2I2sjAv6dZiH0Pxr4cSQGUSHfTgT0oV+yqnYEvEycnFZajMSh/9YaBiDz0SSgwpuvYIfVj0CSFwnmpFxkMQUxgjN2E+uCh6cdZ1Dk/iQxQwEPUXCqeifh7IwbPmJnnJpMe0INZ9FLxP68b0eiyH0s/jAh9kR4iqTA7ZISWSQfIh1IjEaTJkUufC9BAhFpyECIRo6SUUtKHs/j9MmnVqs5ZtXZ7Xqlf5c0U2RE7ZqfMYReszm5YgzWZYGP2xJ7ZizW1Xq036/1ntGDlO4fsD6yPb7Blkic=</latexit>\nw1\n<latexit sha1_base64=\"SMv9N8e7smYx2+Jzvx4lA9b269Q=\">AAAB9XicbVC7TsNAEFyHVwivACXNiQiJKrIDEpQRNJRBkIeURNH5sgmnnB+6WxNFVj6BFio6RMv3UPAv2MYFJEw1mtnVzo4bKmnItj+twsrq2vpGcbO0tb2zu1feP2iZINICmyJQge643KCSPjZJksJOqJF7rsK2O7lO/fYjaiMD/55mIfY9PvblSApOiXQ3HTiDcsWu2hnYMnFyUoEcjUH5qzcMROShT0JxY7qOHVI/5pqkUDgv9SKDIRcTPsZuQn3uoenHWdQ5O4kMp4CFqJlULBPx90bMPWNmnptMepwezKKXiv953YhGl/1Y+mFE6Iv0EEmF2SEjtEw6QDaUGol4mhyZ9JngmhOhlowLkYhRUkop6cNZ/H6ZtGpV56xauz2v1K/yZopwBMdwCg5cQB1uoAFNEDCGJ3iGF2tqvVpv1vvPaMHKdw7hD6yPb7H0kig=</latexit>\nsi ha                                 e si può considerare il seguente rapporto incrementale:\n(w0+\u0000w0,w1)2A\n<latexit sha1_base64=\"MtP2/MU/+3Vkqc9/VVk+7SdRHu8=\">AAACLXicbVDLSgNBEJz1bXxFPXoZDIKihN0oqLf4OHiMYFTIhtA7tjpk9sFMr0GW/RY/wa/wqicPgnr1N5yNOfiqU01VFz3VQaKkIdd9cYaGR0bHxicmS1PTM7Nz5fmFUxOnWmBTxCrW5wEYVDLCJklSeJ5ohDBQeBZ0Dwr/7Aa1kXF0QrcJtkO4iuSlFEBW6pR3V/3Y+kU86+WdzM35OvcPURHwXvHc4D8HvHyN+zLie51yxa26ffC/xBuQChug0Sm/+RexSEOMSCgwpuW5CbUz0CSFwrzkpwYTEF24wpalEYRo2lm/Ys5XUgMU8wQ1l4r3RfyeyCA05jYM7GQIdG1+e4X4n9dK6XKnnckoSQkjUSwiqbC/yAgtbXXkF1IjERQ/R267C9BAhFpyEMKKqT1myd7D+93+LzmtVb3Nau14q1LfH1xmgi2xZbbKPLbN6uyINViTCXbHHtgje3LunWfn1Xn/Gh1yBplF9gPOxyfAg6f7</latexit>\nSia                    .   Esiste allora un intorno circolare di centro       e \nopportuno raggio 𝜎, contenuto in A. Ne segue che, se:\nP\n<latexit sha1_base64=\"Vg7Q1rv1rOgh7aYWrlRKdHDIMjA=\">AAAB/3icbVC7TsNAEDzzDOEVoKQ5ESFRRXZAgjKChjJI5CElVnS+bMIp57O5WyNFVgq+ghYqOkTLp1DwL5yNC0iYajSzo92dIJbCoOt+OkvLK6tr66WN8ubW9s5uZW+/baJEc2jxSEa6GzADUihooUAJ3VgDCwMJnWBylfmdB9BGROoWpzH4IRsrMRKcoZX8fmTNLJs2Z3RQqbo1NwddJF5BqqRAc1D56g8jnoSgkEtmTM9zY/RTplFwCbNyPzEQMz5hY+hZqlgIxk/zo2f0ODEMIxqDpkLSXITfiZSFxkzDwE6GDO/MvJeJ/3m9BEcXfipUnCAoni1CISFfZLgW9mGgQ6EBkWWXAxWKcqYZImhBGedWTGw9ZduHN//9ImnXa95prX5zVm1cFs2UyCE5IifEI+ekQa5Jk7QIJ/fkiTyTF+fReXXenPef0SWnyByQP3A+vgGy5pat</latexit>\nP⌘(w0,w1)2A\n<latexit sha1_base64=\"JoT9dMpAcV9D5gyTs2M1a0HUaxE=\">AAACLnicbVDLSgNBEJz1bXxFPXoZDEIECbtRUDz5uHiMYBIhG8LspBMHZx/O9EbCkn/xE/wKr3oSPASvfoaz64IarVNNVTc9VV4khUbbfrOmpmdm5+YXFgtLyyura8X1jYYOY8WhzkMZqmuPaZAigDoKlHAdKWC+J6Hp3Z6nfnMASoswuMJhBG2f9QPRE5yhkTrFYzc0drqd1EbUhbtYDGj5W7wfdew9+uvt7FJXBPS0UyzZFTsD/UucnJRIjlqnOHa7IY99CJBLpnXLsSNsJ0yh4BJGBTfWEDF+y/rQMjRgPuh2kmUc0Z1YMwxpBIoKSTMRfm4kzNd66Htm0md4oye9VPzPa8XYO2onIohihICnh1BIyA5proQJDrQrFCCy9OdATXbOFEMEJSjj3IixabNg+nAm0/8ljWrF2a9ULw9KJ2d5Mwtki2yTMnHIITkhF6RG6oSTB/JEnsmL9Wi9WmPr/Wt0ysp3NskvWB+fwZ2pFA==</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#13": " 14\nSe esiste determinato e ﬁnito il seguente limite:\nlim\n\u0000w0!0g(w0+\u0000w0,w1)\u0000g(w0,w1)\n\u0000w0\n<latexit sha1_base64=\"JRgIcRMvmpBtpSOD2ft2HVySLE8=\">AAACh3icbVHLTsMwEHTCu7wKHDlgUSEVASUBBBx5HTiCRAGpqaKN2RYL5yF7Q4WifAVfx4EP4UYSeqCUPY1nZzzJOEiUNOQ4H5Y9MTk1PTM7V5tfWFxarq+s3ps41QLbIlaxfgzAoJIRtkmSwsdEI4SBwofg5bLcP7yiNjKO7ugtwW4I/Uj2pAAqKL/+7ikZ+pl3hYqAD/zMybmnZf+ZQOt4wMtjT4PI+k0vLi4qc7JBXul2+G/bLh8VuPk23+Pjtn90+Uh+7tcbTsupho8DdwgabDg3fv3Te4pFGmJEQoExHddJqJuBJikU5jUvNZiAeIE+dgoYQYimm1Xl5XwrNUAxT1BzqXhF4m9HBqExb2FQKEOgZ/N3V5L/7Top9U67mYySlDASZRBJhVWQEVoWHSB/khqJoPxy5DLiAjQQoZYchCjItHimWtGH+/fvx8H9Qcs9bB3cHjXOLobNzLJ1tsmazGUn7IxdsxvWZoJ9WRtW09q25+x9+9g+/ZHa1tCzxkbGPv8G8XPF8g==</latexit>Derivate Parziali di \nFunzioni di più Variabili \nla funzione   si dice parzialmente derivabile rispetto a       nel punto            .\ng\n<latexit sha1_base64=\"M1D3T4qT4zLethYLQWkyJspQDuA=\">AAAB83icbVC7TsNAEDyHVwivACXNiQiJKrIDEpQRNJSJRB5SYkXnyyaccj5bd3tIkZUvoIWKDtHyQRT8C7ZxAQlTjWZ2tbMTxFIYdN1Pp7S2vrG5Vd6u7Ozu7R9UD4+6JrKaQ4dHMtL9gBmQQkEHBUroxxpYGEjoBbPbzO89gjYiUvc4j8EP2VSJieAMU6k9HVVrbt3NQVeJV5AaKdAaVb+G44jbEBRyyYwZeG6MfsI0Ci5hURlaAzHjMzaFQUoVC8H4SR50Qc+sYRjRGDQVkuYi/N5IWGjMPAzSyZDhg1n2MvE/b2Bxcu0nQsUWQfHsEAoJ+SHDtUgbADoWGhBZlhyoUJQzzRBBC8o4T0WbVlJJ+/CWv18l3Ubdu6g32pe15k3RTJmckFNyTjxyRZrkjrRIh3AC5Ik8kxfHOq/Om/P+M1pyip1j8gfOxzdtWJF0</latexit>\nw0\n<latexit sha1_base64=\"sgHalUL2H+5/2ELVE3iy+BBsRNs=\">AAAB9XicbVC7TsNAEDyHVwivACXNiQiJKrIDEpQRNJRBkIeURNH6sgmnnB+6WxNFVj6BFio6RMv3UPAv2MYFJEw1mtnVzo4bKmnItj+twsrq2vpGcbO0tb2zu1feP2iZINICmyJQge64YFBJH5skSWEn1Aieq7DtTq5Tv/2I2sjAv6dZiH0Pxr4cSQGUSHfTgT0oV+yqnYEvEycnFZajMSh/9YaBiDz0SSgwpuvYIfVj0CSFwnmpFxkMQUxgjN2E+uCh6cdZ1Dk/iQxQwEPUXCqeifh7IwbPmJnnJpMe0INZ9FLxP68b0eiyH0s/jAh9kR4iqTA7ZISWSQfIh1IjEaTJkUufC9BAhFpyECIRo6SUUtKHs/j9MmnVqs5ZtXZ7Xqlf5c0U2RE7ZqfMYReszm5YgzWZYGP2xJ7ZizW1Xq036/1ntGDlO4fsD6yPb7Blkic=</latexit>\n(w0,w1)\n<latexit sha1_base64=\"Hc3a+9D9w2RpqKTtwd9bdjillWY=\">AAACGHicbVC7TsNAEDyHVwgvAyXNiYAUJBTZAQnKCBrKIJGHlFjR+bIJJ84P3a1BkeUf4BP4Clqo6BAtHQX/gm1SkISpRjM72p11Qyk0WtaXUVhYXFpeKa6W1tY3NrfM7Z2WDiLFockDGaiOyzRI4UMTBUrohAqY50pou3eXmd++B6VF4N/gOATHYyNfDAVnmEp986DSC1I/i8cPST+2kmM6rdjJUd8sW1UrB50n9oSUyQSNvvndGwQ88sBHLpnWXdsK0YmZQsElJKVepCFk/I6NoJtSn3mgnThvk9DDSDMMaAiKCklzEf4mYuZpPfbcdNJjeKtnvUz8z+tGODx3YuGHEYLPs0UoJOSLNFciLQ10IBQgsuxyoMKnnCmGCEpQxnkqRunfSuk/7Nn286RVq9on1dr1abl+MflMkeyRfVIhNjkjdXJFGqRJOHkkz+SFvBpPxpvxbnz8jhaMSWaXTMH4/AEzF6Cm</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#14": " 15\nSupponiamo ora che la funzione g sia parzialmente derivabile rispetto a w\n 0 \nin ogni punto del campo A.  \nPer ogni punto di A resta ben determinato il corrispondente valore della \nderivata parziale rispetto a w\n 0\n. \nNasce così in A una nuova funzione di due variabili w\n 0\n, w\n 1\n che si chiama \nderivata parziale rispetto a \n w\n0\n della funzione g \n e si denota ad esempio come \nsegue:Derivate Parziali di \nFunzioni di più Variabili \n@g\n@w0\n<latexit sha1_base64=\"VQx8u7rzFuG6k3KXt2wTraBjelE=\">AAACI3icbZC5TsNAEIbX3IQrQEmzIkKiCjYgQRlBQxkkApFiyxpvJmHF+tDumEOWH4NH4ClooaJDNBR5F2wTiXOqX98/s7PzB4mShmz73ZqYnJqemZ2bry0sLi2v1FfXzk2caoEdEatYdwMwqGSEHZKksJtohDBQeBFcHZf+xTVqI+PojO4S9EIYRnIgBVCB/PqOO9AgMjcBTRJU5hLeUjbM87z2BW/8zC4I57zm1xt2066K/xXOWDTYuNp+feT2Y5GGGJFQYEzPsRPysvJloTCvuanBBMQVDLFXyAhCNF5WHZbzrdQAxTxBzaXiFcTvExmExtyFQdEZAl2a314J//N6KQ0OvUxGSUoYiXIRSYXVIiO0LBJD3pcaiaD8OXIZcQEaiFBLDkIUMC0iLPNwfl//V5zvNp295u7pfqN1NE5mjm2wTbbNHHbAWuyEtVmHCXbPHtkTe7YerBfr1Xr7bJ2wxjPr7EdZow9TRaVm</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#15": " 16Derivate Parziali di \nFunzioni di più Variabili \nsupposto determinato e ﬁnito.\nAnalogamente si deﬁnisce la derivata parziale rispetto a      , nel punto    ,  \ncome il limite\nw1\n<latexit sha1_base64=\"SMv9N8e7smYx2+Jzvx4lA9b269Q=\">AAAB9XicbVC7TsNAEFyHVwivACXNiQiJKrIDEpQRNJRBkIeURNH5sgmnnB+6WxNFVj6BFio6RMv3UPAv2MYFJEw1mtnVzo4bKmnItj+twsrq2vpGcbO0tb2zu1feP2iZINICmyJQge643KCSPjZJksJOqJF7rsK2O7lO/fYjaiMD/55mIfY9PvblSApOiXQ3HTiDcsWu2hnYMnFyUoEcjUH5qzcMROShT0JxY7qOHVI/5pqkUDgv9SKDIRcTPsZuQn3uoenHWdQ5O4kMp4CFqJlULBPx90bMPWNmnptMepwezKKXiv953YhGl/1Y+mFE6Iv0EEmF2SEjtEw6QDaUGol4mhyZ9JngmhOhlowLkYhRUkop6cNZ/H6ZtGpV56xauz2v1K/yZopwBMdwCg5cQB1uoAFNEDCGJ3iGF2tqvVpv1vvPaMHKdw7hD6yPb7H0kig=</latexit>\nP\n<latexit sha1_base64=\"RzOqhOyvtYvEmUKPvym5XynVJro=\">AAAB/nicbVC7TsNAEDyHVwivACXNiQiJKrIDEpQRNJRBIg8pjqLzZRNOOZ+tuzVSZEXiK2ihokO0/AoF/8LZuICEqUYzO9rdCWIpDLrup1NaWV1b3yhvVra2d3b3qvsHHRMlmkObRzLSvYAZkEJBGwVK6MUaWBhI6AbT68zvPoA2IlJ3OIthELKJEmPBGVrJ9yNrZtm0NR9Wa27dzUGXiVeQGinQGla//FHEkxAUcsmM6XtujIOUaRRcwrziJwZixqdsAn1LFQvBDNL85jk9SQzDiMagqZA0F+F3ImWhMbMwsJMhw3uz6GXif14/wfHlIBUqThAUzxahkJAvMlwL+y/QkdCAyLLLgQpFOdMMEbSgjHMrJradiu3DW/x+mXQade+s3rg9rzWvimbK5Igck1PikQvSJDekRdqEk5g8kWfy4jw6r86b8/4zWnKKzCH5A+fjG1b5loM=</latexit>\nlim\n\u0000w1!0g(w0,w1+\u0000w1)\u0000g(w0,w1)\n\u0000w1\n<latexit sha1_base64=\"AAg1+BW2nr0op9Oj8OQeUTZZmyE=\">AAACiHicjVFNTxsxEPVuaaHpB6E99jIiqkT6Ee0CEvSGoAeOIDWAlI1Ws2YSLLwfsmeJkLX/on+uh/6RnvCmORDgwJye38ybZz9nlVaWo+hPEL5Yeflqde11583bd+/XuxsfzmxZG0lDWerSXGRoSauChqxY00VlCPNM03l2fdT2z2/IWFUWv/i2onGO00JNlET2VNr9nWiVpy75SZoRZqmLG0iMml4xGlPOIPLHiUHppltJ6Re1Pm7WpC5qvsEy45Vf4f6iPnyH58j6zZJ/00m7vWgQzQseg3gBemJRJ2n3b3JZyjqngqVGa0dxVPHYoWElNTWdpLZUobzGKY08LDAnO3bz9Br4XFvkEioyoDTMSbqvcJhbe5tnfjJHvrIPey35VG9U82R/7FRR1UyFbI1YaZobWWmUD4HgUhlixvbmBKoAiQaZyShAKT1Z+39q84gfvv4xONsexDuD7dPd3sHhIpk18Ulsii0Riz1xII7FiRgKKf4FEPSDL2EnjMK98Mf/0TBYaD6KpQoP7wB2TsYJ</latexit>\nE se avviene che tale derivata esista in ogni punto                , resta ivi \ndeﬁnita una nuova funzione delle variabili            che si chiama la derivata \nparziale rispetto a       della                  e si indica ad esempio come segue:\n(w0,w1)2A\n<latexit sha1_base64=\"tqi9PhFGSNkmscV0Id0WgsEszQ0=\">AAACBHicbVC7TsNAEDyHVwgvAyXNiQgpSCiyAxKUARrKIJGHlFjW+bIJp5wfulsniqK0fAUtVHSIlv+g4F+wjQsITDWa2dXOjhdJodGyPozC0vLK6lpxvbSxubW9Y+7utXQYKw5NHspQdTymQYoAmihQQidSwHxPQtsbXad+ewxKizC4w2kEjs+GgRgIzjCRXNOsTFzrhE5c+5j2REAvXbNsVa0M9C+xc1ImORqu+dnrhzz2IUAumdZd24rQmTGFgkuYl3qxhojxERtCN6EB80E7syz5nB7FmmFII1BUSJqJ8HNjxnytp76XTPoM7/Wil4r/ed0YBxfOTARRjBDw9BAKCdkhzZVIKgHaFwoQWZocaPI7Z4ohghKUcZ6IcdJRKenDXvz+L2nVqvZptXZ7Vq5f5c0UyQE5JBVik3NSJzekQZqEkzF5JE/k2XgwXoxX4+17tGDkO/vkF4z3L2CgljI=</latexit>\nw0,w1\n<latexit sha1_base64=\"vH1j0jv1BCwogwzrtRh5C1k41l0=\">AAAB+nicbVC7TsNAEDzzDOEVoKQ5ESFRoMgOSFBG0FAGiTykxLLOl0045Xy27tZEkclP0EJFh2j5GQr+Bdu4gISpRjO72tnxIykM2vantbS8srq2Xtoob25t7+xW9vbbJow1hxYPZai7PjMghYIWCpTQjTSwwJfQ8cfXmd95AG1EqO5wGoEbsJESQ8EZplJ34tmndOI5XqVq1+wcdJE4BamSAk2v8tUfhDwOQCGXzJieY0foJkyj4BJm5X5sIGJ8zEbQS6liARg3yfPO6HFsGIY0Ak2FpLkIvzcSFhgzDfx0MmB4b+a9TPzP68U4vHQToaIYQfHsEAoJ+SHDtUiLADoQGhBZlhyoUJQzzRBBC8o4T8U4baac9uHMf79I2vWac1ar355XG1dFMyVySI7ICXHIBWmQG9IkLcKJJE/kmbxYj9ar9Wa9/4wuWcXOAfkD6+MbgT2TrA==</latexit>\nw1\n<latexit sha1_base64=\"SMv9N8e7smYx2+Jzvx4lA9b269Q=\">AAAB9XicbVC7TsNAEFyHVwivACXNiQiJKrIDEpQRNJRBkIeURNH5sgmnnB+6WxNFVj6BFio6RMv3UPAv2MYFJEw1mtnVzo4bKmnItj+twsrq2vpGcbO0tb2zu1feP2iZINICmyJQge643KCSPjZJksJOqJF7rsK2O7lO/fYjaiMD/55mIfY9PvblSApOiXQ3HTiDcsWu2hnYMnFyUoEcjUH5qzcMROShT0JxY7qOHVI/5pqkUDgv9SKDIRcTPsZuQn3uoenHWdQ5O4kMp4CFqJlULBPx90bMPWNmnptMepwezKKXiv953YhGl/1Y+mFE6Iv0EEmF2SEjtEw6QDaUGol4mhyZ9JngmhOhlowLkYhRUkop6cNZ/H6ZtGpV56xauz2v1K/yZopwBMdwCg5cQB1uoAFNEDCGJ3iGF2tqvVpv1vvPaMHKdw7hD6yPb7H0kig=</latexit>\ng(w0,w1)\n<latexit sha1_base64=\"0nuB/rhxp/IDPQ5Yo6JeFVwMZ9U=\">AAAB/XicbVDLSsNAFJ3UV62vqks3g0WoICWpgi6LblxWsA9IQ5hMb+vQyYOZG0sJxa9wqyt34tZvceG/mMQstHpWh3Pu5Z57vEgKjab5YZSWlldW18rrlY3Nre2d6u5eV4ex4tDhoQxV32MapAiggwIl9CMFzPck9LzJVeb37kFpEQa3OIvA8dk4ECPBGaaSPa5PXfOETl3r2K3WzIaZg/4lVkFqpEDbrX4OhiGPfQiQS6a1bZkROglTKLiEeWUQa4gYn7Ax2CkNmA/aSfLIc3oUa4YhjUBRIWkuws+NhPlaz3wvnfQZ3ulFLxP/8+wYRxdOIoIoRgh4dgiFhPyQ5kqkXQAdCgWILEsOVASUM8UQQQnKOE/FOC2nkvZhLX7/l3SbDeu00bw5q7Uui2bK5IAckjqxyDlpkWvSJh3CSUgeyRN5Nh6MF+PVePseLRnFzj75BeP9CxbnlII=</latexit>\n@g\n@w1\n<latexit sha1_base64=\"hykhrxvVEe3yRLuSCDFkAUXCvfY=\">AAACFXicbVC7TgJBFJ3FF+ILtbQZJSZWZBdNtCTaWGIijwQIuTtccMLsIzN3NWSztZ/gV9hqZWdsrS38F3eRRAVPdeac+5h73FBJQ7b9YeUWFpeWV/KrhbX1jc2t4vZOwwSRFlgXgQp0ywWDSvpYJ0kKW6FG8FyFTXd0kfnNW9RGBv41jUPsejD05UAKoFTqFfc7Aw0i7oSgSYKKh0ny87jrOUnSK5bssj0BnyfOlJTYFLVe8bPTD0TkoU9CgTFtxw6pG2czhcKk0IkMhiBGMMR2Sn3w0HTjySkJP4wMUMBD1FwqPhHxd0cMnjFjz00rPaAbM+tl4n9eO6LBWTeWfhgR+iJbRFLhZJERWqYZIe9LjUSQ/Ry59LkADUSoJQchUjFKQyukeTiz18+TRqXsHJcrVyel6vk0mTzbYwfsiDnslFXZJauxOhPsnj2yJ/ZsPVgv1qv19l2as6Y9u+wPrPcvW36gVg==</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#16": " 17\nOsserviamo che, mentre per le funzioni di una variabile la derivabilità in un \npunto implica la continuità in tale punto, non sussiste il fatto analogo per le \nfunzioni di due variabili.Derivate Parziali di \nFunzioni di più Variabili \nPossono cioè in un punto esistere le due derivate parziali senza che la \nfunzione g sia continua in esso.\nTutte le considerazioni fatte ﬁno ad ora sulle funzioni di due variabili si \nestendono immediatamente al caso delle funzioni di più di due variabili:\ng(w0,w1,...,w n)= g(w)\n<latexit sha1_base64=\"iUBK7+RA0FYb7PAax0+9XxEzqiQ=\">AAACIXicbVDLSsNAFJ34tr6iLt0MFqGClqQKuhGKblxWsK3QljKZXuvgZBJmbiwS8hV+gl/hVlfuxJ2I/+IkduHrbO6Zc19zTxBLYdDz3pyJyanpmdm5+dLC4tLyiru61jJRojk0eSQjfREwA1IoaKJACRexBhYGEtrB9Umeb9+ANiJS53gbQy9kQyUuBWdopb67O6yM+qmX7VAbfBu6gwhN8VLZNj2iw0o3iOQgHWXbpVLfLXtVrwD9S/wxKZMxGn33w87jSQgKuWTGdHwvxl7KNAouISt1EwMx49dsCB1LFQvB9NLirIxuJYZhRGPQVEhaiPC9I2WhMbdhYCtDhlfmdy4X/8t1Erw87KVCxQmC4vkiFBKKRYZrYf0COhAaEFn+c6BCUc40QwQtKOPciok1MPfD/339X9KqVf29au1sv1w/HjszRzbIJqkQnxyQOjklDdIknNyRB/JInpx759l5cV6/Sieccc86+QHn/RPB2qGW</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#17": "Gradiente\n \n18",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#18": "Gradiente di una Funzione \n 19\nIl gradiente di una funzione è una diretta generalizzazione della nozione di \nderivata per una funzione a più variabili.\nrg(w)=2\n6666666664@g(w)\n@w0\n@g(w)\n@w1\n···\n@g(w)\n@wn3\n7777777775\n<latexit sha1_base64=\"06VfDqyZDj5rj/n4a5yMYNZXbn4=\">AAADBnicpVI9j9QwEHXCxx3L1y6UNBYrpKNZJQcSNCedoKE8JPbupPVqNXFmc9Y5TmRPuFtZ6fkVtFDRIVr+BgX/BWeJBOxSwcjF6L158zxjZ7VWjpLkWxRfuXrt+s7ujcHNW7fv3B2O7h27qrESp7LSlT3NwKFWBqekSONpbRHKTONJdv6y40/eonWqMm9oVeO8hMKopZJAAVqMopEwkGkQhJfki3ZPZJXO/UX7mB9woXFJM85FhoUyHqyFVetlywc8hFhakF7UYEmB9tsd2vYXe7HwSdu2XIhw/kmebshlXpH7j34m9BsINHk/FxdWFWc0HyyG42SSrINvJ2mfjFkfR4vhd5FXsinRkNTg3CxNapr7zkpqDCaNwxrkORQ4C6mBEt3cr9+u5Y8aB1TxGi1Xmq9B/F3hoXRuVWahsgQ6c5tcB/6NmzW0fD73ytQNoZGdESmNayMnrQqfAnmuLBJBd3PkynAJFojQKg5SBrAJv6TbR7o5/XZyvD9Jn0z2Xz8dH77oN7PLHrCHbI+l7Bk7ZK/YEZsyGV1G76MP0cf4Xfwp/hx/+VkaR73mPvsj4q8/AIJp+C8=</latexit>\ng(w0,w1,...,w n)= g(w)\n<latexit sha1_base64=\"iUBK7+RA0FYb7PAax0+9XxEzqiQ=\">AAACIXicbVDLSsNAFJ34tr6iLt0MFqGClqQKuhGKblxWsK3QljKZXuvgZBJmbiwS8hV+gl/hVlfuxJ2I/+IkduHrbO6Zc19zTxBLYdDz3pyJyanpmdm5+dLC4tLyiru61jJRojk0eSQjfREwA1IoaKJACRexBhYGEtrB9Umeb9+ANiJS53gbQy9kQyUuBWdopb67O6yM+qmX7VAbfBu6gwhN8VLZNj2iw0o3iOQgHWXbpVLfLXtVrwD9S/wxKZMxGn33w87jSQgKuWTGdHwvxl7KNAouISt1EwMx49dsCB1LFQvB9NLirIxuJYZhRGPQVEhaiPC9I2WhMbdhYCtDhlfmdy4X/8t1Erw87KVCxQmC4vkiFBKKRYZrYf0COhAaEFn+c6BCUc40QwQtKOPciok1MPfD/339X9KqVf29au1sv1w/HjszRzbIJqkQnxyQOjklDdIknNyRB/JInpx759l5cV6/Sieccc86+QHn/RPB2qGW</latexit>\nData la seguente funzione:\ndeﬁniamo gradiente di g il vettore le cui componenti sono le derivate \nparziali della funzione:",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#19": " \n20\nw0w1ŵ\nŵ0ŵ1gradiente:\nijrg(w)=@g(w)\n@w0i+@g(w)\n@w1j\n<latexit sha1_base64=\"UbdTBP63qiqYyxQKEMwdHSMgLt8=\">AAACkniclVFNT9tAEF27UGigEKC3XlaNkIBKkQ1IICQkKJceOIDUAFIcRePNJCys19buGIpW+1f6v3rgv9QOkfgIl77T03sz83Zn0kJJS1H0Nwg/zMx+nJv/1FhY/Ly03FxZvbB5aQR2RK5yc5WCRSU1dkiSwqvCIGSpwsv09qT2L+/QWJnrX/RQYC+DkZZDKYAqqd/8k2hIFSSEv8mN/EaS5mrg7v0mP+TJ0IBwSQGGJCg3XeP9s3vfd5H3nj+Z0jd4he//PSN+nnHjeaPfbEXtaAw+TeIJabEJzvrNx2SQizJDTUKBtd04Kqjn6gCh0DeS0mIB4hZG2K2ohgxtz4336Pl6aYFyXqDhUvGxiC87HGTWPmRpVZkBXdu3Xi2+53VLGu73nNRFSahFHURS4TjICiOrAyEfSINEUL8cudRcgAEiNJKDEJVYVher9xG//f00udhuxzvt7fPd1tGPyWbm2Vf2jW2wmO2xI/aTnbEOE8FMsBXsBLvhl/AgPA5PnkrDYNKzxl4hPP0HqF7LxA==</latexit>\nW(t)gGradiente di una Funzione ",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#2": "Richiami sulle \nFunzioni Convesse\n \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#20": " \n21\nw0w1ŵ\nŵ0ŵ1\nij\nrg(w)=@g(w)\n@w0i+@g(w)\n@w1j\n<latexit sha1_base64=\"UbdTBP63qiqYyxQKEMwdHSMgLt8=\">AAACkniclVFNT9tAEF27UGigEKC3XlaNkIBKkQ1IICQkKJceOIDUAFIcRePNJCys19buGIpW+1f6v3rgv9QOkfgIl77T03sz83Zn0kJJS1H0Nwg/zMx+nJv/1FhY/Ly03FxZvbB5aQR2RK5yc5WCRSU1dkiSwqvCIGSpwsv09qT2L+/QWJnrX/RQYC+DkZZDKYAqqd/8k2hIFSSEv8mN/EaS5mrg7v0mP+TJ0IBwSQGGJCg3XeP9s3vfd5H3nj+Z0jd4he//PSN+nnHjeaPfbEXtaAw+TeIJabEJzvrNx2SQizJDTUKBtd04Kqjn6gCh0DeS0mIB4hZG2K2ohgxtz4336Pl6aYFyXqDhUvGxiC87HGTWPmRpVZkBXdu3Xi2+53VLGu73nNRFSahFHURS4TjICiOrAyEfSINEUL8cudRcgAEiNJKDEJVYVher9xG//f00udhuxzvt7fPd1tGPyWbm2Vf2jW2wmO2xI/aTnbEOE8FMsBXsBLvhl/AgPA5PnkrDYNKzxl4hPP0HqF7LxA==</latexit>W(t)gGradiente di una Funzione ",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#21": " \n22w0w1gDerivata Direzionale \nnPw1\nw0\nQ\nw0 + 𝛼𝜌w1 + 𝛽𝜌\ngradiente:\nrg(w)=@g(w)\n@w0i+@g(w)\n@w1j\n<latexit sha1_base64=\"UbdTBP63qiqYyxQKEMwdHSMgLt8=\">AAACkniclVFNT9tAEF27UGigEKC3XlaNkIBKkQ1IICQkKJceOIDUAFIcRePNJCys19buGIpW+1f6v3rgv9QOkfgIl77T03sz83Zn0kJJS1H0Nwg/zMx+nJv/1FhY/Ly03FxZvbB5aQR2RK5yc5WCRSU1dkiSwqvCIGSpwsv09qT2L+/QWJnrX/RQYC+DkZZDKYAqqd/8k2hIFSSEv8mN/EaS5mrg7v0mP+TJ0IBwSQGGJCg3XeP9s3vfd5H3nj+Z0jd4he//PSN+nnHjeaPfbEXtaAw+TeIJabEJzvrNx2SQizJDTUKBtd04Kqjn6gCh0DeS0mIB4hZG2K2ohgxtz4336Pl6aYFyXqDhUvGxiC87HGTWPmRpVZkBXdu3Xi2+53VLGu73nNRFSahFHURS4TjICiOrAyEfSINEUL8cudRcgAEiNJKDEJVYVher9xG//f00udhuxzvt7fPd1tGPyWbm2Vf2jW2wmO2xI/aTnbEOE8FMsBXsBLvhl/AgPA5PnkrDYNKzxl4hPP0HqF7LxA==</latexit>n\nij",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#22": "Derivata Direzionale \n 23\nSi può dimostrare che la derivata direzionale secondo n è:\n@g\n@n=↵·@g\n@w0+\u0000·@g\n@w1\n<latexit sha1_base64=\"+Er6gFVcrheB4cs8jHX92VkksPI=\">AAACdXicjVHBbtNAEF2btqSBlgBSOSCkFSkIqVKw00gth0oVvXAMUpNWii1rvJmkq67Xq91xUWX5wGdy4MwncK2dBkFpqZjT2/fe7Oy+SY2SjoLgm+c/WFlde9habz96vLH5pPP02djlhRU4ErnK7WkKDpXUOCJJCk+NRchShSfp+VGjn1ygdTLXx3RpMM5gruVMCqCaSjpfo5kFUUYGLElQ5byqfh90VfEDHoEyZ8AjMc2J32f/kgR1ww6PUqT/84dV1U463aDXH3zo7+3z2yDsBYvqsmUNk873aJqLIkNNQoFzkzAwFJfNpUJh1Y4KhwbEOcxxUkMNGbq4XERV8TeFA8q5Qcul4gsS/+woIXPuMktrZwZ05v7WGvIubVLQbD8upTYFoRbNIJIKF4OcsLLeAfKptEgEzcuRS80FWCBCKzkIUZNFvZQmj1+f5v8G434v3O31Pw+6hx+XybTYS/aavWMh22OH7BMbshET7Ie34W15L7yf/it/2397bfW9Zc9zdqP891efxMJr</latexit>\nDetto \n n\n il versore della direzione \n n\n, la    è il prodotto scalare dei due \nvettori:@g\n@n\n<latexit sha1_base64=\"g3oVOo0czFVRrXmUUHD+5Uu0VMQ=\">AAACFHicdVC7TsNAEDzzDOEVoKQ5ESFRWXaIBHQRNJRBIoCURNH62IQT57N1t0aKLLd8Al9BCxUdoqWn4F+wQxDvqeZm9nE7QaykJc97cSYmp6ZnZktz5fmFxaXlysrqiY0SI7AlIhWZswAsKqmxRZIUnsUGIQwUngaXB4V/eoXGykgf0zDGbggDLftSAOVSr8I7fQMi7cRgSIJKB1n2+dBZVu5Vqp5bq+/Vdnb5b+K73ghVNkazV3ntnEciCVGTUGBt2/di6qbFSKEwK3cSizGISxhgO6caQrTddHRJxjcTCxTxGA2Xio9E/NqRQmjtMAzyyhDowv70CvEvr51Qf7ebSh0nhFoUi0gqHC2ywsg8IuTn0iARFD9HLjUXYIAIjeQgRC4meWZFHh9H8//JSc31t93aUb3a2B8nU2LrbINtMZ/tsAY7ZE3WYoJds1t2x+6dG+fBeXSe3ksnnHHPGvsG5/kN2TSgHQ==</latexit>\nrg·n\n<latexit sha1_base64=\"HmMzMEElHE9OTkg5/sPKXqpLV5A=\">AAACF3icdVC7TsNAEDzzDOEVoKQ5ESFRRU6IlNAhaCiDRCBSHEXryyacOJ+tuzUCWfkAPoGvoIWKDtFSUvAv2CZIPKe50cys9nb8SElLrvvqTE3PzM7NFxaKi0vLK6ultfVTG8ZGYFuEKjQdHywqqbFNkhR2IoMQ+ArP/IvDzD+7RGNlqE/oOsJeACMth1IApVK/VPY0+Ar4iHtiEBL3CK/IHyb5KynR43ExTbmVWn2v1mjy36RacXOU2QStfunNG4QiDlCTUGBtt+pG1EvAkBQKx0UvthiBuIARdlOqIUDbS/Jjxnw7tkAhj9BwqXgu4teJBAJrrwM/TQZA5/anl4l/ed2Yhs1eInUUE2qRLSKpMF9khZFpS8gH0iARZD9HLjUXYIAIjeQgRCrGaW1ZH59H8//Jaa1S3a3Ujuvl/YNJMwW2ybbYDquyBttnR6zF2kywG3bH7tmDc+s8Ok/O80d0ypnMbLBvcF7eAZzfoGg=</latexit>\ncioè è la componente del gradiente sulla retta orientata \n n\n. Questo signiﬁca \nche la derivata direzionale della funzione \n g \nè massima secondo la \ndirezione e verso del vettore gradiente.\nSi può avere una visione globale di tutte queste possibili derivate, \ncollegando al punto \n P \nil gradiente della funzione g in tale punto.",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#23": " \n24w0w1gDerivata Direzionale \nnPw1\nw0\nQ\nw0 + 𝛼𝜌w1 + 𝛽𝜌\ngradiente:\nrg(w)=@g(w)\n@w0i+@g(w)\n@w1j\n<latexit sha1_base64=\"UbdTBP63qiqYyxQKEMwdHSMgLt8=\">AAACkniclVFNT9tAEF27UGigEKC3XlaNkIBKkQ1IICQkKJceOIDUAFIcRePNJCys19buGIpW+1f6v3rgv9QOkfgIl77T03sz83Zn0kJJS1H0Nwg/zMx+nJv/1FhY/Ly03FxZvbB5aQR2RK5yc5WCRSU1dkiSwqvCIGSpwsv09qT2L+/QWJnrX/RQYC+DkZZDKYAqqd/8k2hIFSSEv8mN/EaS5mrg7v0mP+TJ0IBwSQGGJCg3XeP9s3vfd5H3nj+Z0jd4he//PSN+nnHjeaPfbEXtaAw+TeIJabEJzvrNx2SQizJDTUKBtd04Kqjn6gCh0DeS0mIB4hZG2K2ohgxtz4336Pl6aYFyXqDhUvGxiC87HGTWPmRpVZkBXdu3Xi2+53VLGu73nNRFSahFHURS4TjICiOrAyEfSINEUL8cudRcgAEiNJKDEJVYVher9xG//f00udhuxzvt7fPd1tGPyWbm2Vf2jW2wmO2xI/aTnbEOE8FMsBXsBLvhl/AgPA5PnkrDYNKzxl4hPP0HqF7LxA==</latexit>n\nij",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#24": " 25\nLa proprietà citata in precedenza del vettore gradiente, ossia il fatto che il \ngradiente fornisce direzione della pendenza più ripida, è alla base di \nalgoritmi di Ricerca Locale che operano in spazi continui.\nTali algoritmi si dividono in due classi principali: \n•\nAlgoritmi a Salita più Ripida (Hill-Climbing) \n•\nAlgoritmi a Discesa del Gradiente (Gradient Descent)\nAlgoritmo Gradient Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#25": " \n26\nw0w1g\nŵ\nŵ0ŵ1\nij\nW(t+1)W(t)\n- 𝛼 * gradiente\nAlgoritmo Gradient Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#26": "Algoritmo Gradient Descent\n \n27w(1)= 0 (oppure lo inizializziamo in modo casuale)\nt=1\nwhilekrg(w(t))k2>✏\nw(t+1) w(t)\u0000↵⇤rg(w(t))\nt t+1\n<latexit sha1_base64=\"C1wbNEwaiOAK1Ma17wL9dfyBIzQ=\">AAADRnichVJNbxMxEPWmfJTw1cKRy4iIKmlFtBuE4AKq4MKxlUhaKQ6R15kmVr32yp6ltKv9X/wE/gIHuMKJG+KKN42qpEViDtZ4Zt574/GkuVae4vhr1Fi7dv3GzfVbzdt37t67v7H5YOBt4ST2pdXWHabCo1YG+6RI42HuUGSpxoP0+G2dP/iIzitr3tNpjqNMTI06UlJQCI03o32e4lSZUmg1NdtVc4unVk/Kk+pD2U46FbyCGDhwwk9Utm2eFw5BW1BGnamAOQtnVl8hsxMLUvhCaAw4zqG5BRTwSfAvaGdKY1UT6gE6Am5EqsWCflq1l8SpU3XOq8Zlr4a8Bo65V9qac8KZz4XEMpFZyC7hduq2ucYjEs7ZE1jlhKfAhc5nAmAb/qe/IhR3n9dStMxNsANJk6OZXAywOd5oxd14bnDVSRZOiy1sb7zxjU+sLDI0JLXwfpjEOY1K4UjJMK0mLzyGDo7FFIfBNSJDPyrnf1/Bk8ILspCjA6VhHsRlRCky70+zNFRmgmb+cq4O/is3LOjo5ahUJi8IjayFKPzdXMhLp8JSIUyUQyJRd471DkjhBBE6BULKECzCltXzSC6//qoz6HWTZ93efq+1+2YxmXX2iD1mbZawF2yXvWN7rM9k9Dn6Hv2Ifja+NH41fjf+nJc2ogXmIVuxNfYXr1EImg==</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#27": "Algoritmo Gradient Descent\n \n28\nFunzione non convessa di due variabili:",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#28": "Richiami di Probabilità\n \n29",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#29": "Variabili Aleatorie\nLe quantità di interesse che sono determinate dal risultato di \nun esperimento casuale sono dette \n variabili aleatorie\n . \nPoiché il valore di una variabile aleatoria è determinato \ndall’esito di un esperimento, possiamo assegnare delle \nprobabilità ai suoi valori possibili. \nEsempi di v.a.: risultato del lancio di un dado, risultato del \nlancio di una moneta, ecc.\n \n30",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#3": "Insiemi Convessi \n 4Un insieme C in uno spazio vettoriale è convesso  se, comunque si scelgano \ndue punti v e w appartenenti a C, il segmento che unisce i due punti \nappartiene a C.\nPiù formalmente:\nUn insieme C in uno spazio vettoriale è convesso  se, ∀ v, w ∈ C, e ∀ λ ∈ [0, 1], si \nha:\n\u0000v+( 1\u0000\u0000)w2C",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#30": "Valore Atteso\nIl concetto di Valore Atteso è uno dei più importanti concetti \nin tutta la teoria della probabilità. \nSia X una variabile aleatoria discreta che può assumere i \nvalori x\n 1\n, x\n2\n, …, x\n N\n. Il Valore Atteso di X è il numero: \n \n31E[X],NX\ni=1[xi·P(X=xi)]",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#31": "Valore Atteso\n \n32\nSi tratta della media pesata dei valori possibili di X, usando \ncome pesi le probabilità che tali valori vengano assunti da X. \nPer questo E[X] è anche detto \n media\n  di X (termine che però è \nsconsigliabile), oppure \n aspettazione\n  (\nexpectation\n ). ",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#32": "Valore Atteso\nSia X il punteggio che si ottiene lanciando un dado non \ntruccato. Quanto vale E[X]?\n \n33E[X]=1 ·1\n6+2 ·1\n6+3 ·1\n6+4 ·1\n6+5 ·1\n6+6 ·1\n6=7\n2=3.5\nESEMPIO: lancio di un dado",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#33": "Valore Atteso\nSi noti che in questo esempio il valore atteso di X non è uno \ndei possibili valori che X può assumere.  \nPerciò, anche se E[X] è chiamato \n valore atteso\n  di X, ciò non \nvuole affatto dire che noi ci attendiamo di vedere questo \nvalore, ma piuttosto che ci aspettiamo che sia il limite a cui \ntende il punteggio medio del dado su un numero crescente di \nripetizioni.\n \n34\nESEMPIO: lancio di un dado",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#34": "Valore Atteso\nESEMPIO: Indicator Function\n \n35E[I]=1 ·P(I= 1) + 0 ·P(I= 0) = P(I= 1) = P(A)\nSe I[A] è la funzione indicatrice di un evento A, ossia se:\n     allora:\nQuindi il valore atteso della indicator function di un evento è \nla probabilità di quest’ultimo.I[A],8\n<\n:1 se A si veriﬁca\n0 se A non si veriﬁca",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#35": "Valore Atteso\nProprietà di E\n \n36\nSi riportano qui di seguito alcune proprietà della funzione E (a e \nb sono variabili aleatorie):\nE[a+b]= E[a]+E[b]\nE[k·a]= k·E[a] (k costante)\nE[a·b]= E[a]·E[b]( aebi n d i p e n d e n t i )",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#36": "Varianza\n \n37\nSia X una variabile aleatoria con media \n μ\n. La varianza di X è la \nquantità:\nVar(X),E[(X\u0000µ)2]",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#37": "Varianza\n \n38\nEsiste una formula alternativa per la varianza, che si ricava in \nquesto modo:\nossia:\nVar(X)=E[X2]\u0000E[X]2Var(X),E[(X\u0000µ)2]=\n=E[X2\u00002µX+µ2]=\n=E[X2]\u00002µ·E[X]+µ2=\n=E[X2]\u0000µ2",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#4": "Insiemi Convessi \n 5Vediamolo nel caso a due dimensioni:L’espressione: \ncorrisponde dunque ai punti appartenenti al  segmento che unisce i due punti \nv e w, al variare di λ ∈ [0, 1].\u0000v+( 1\u0000\u0000)w",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#5": " \n6wv\nλv + (1-λ)wλ(v - w)\nλv + (1- λ)w = w + λ(v-w)\nλ = 1\nλ = 0Insiemi Convessi \n[caso a due dimensioni]\nλ > 1\nλ < 0λ = 0.6\nC",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#6": " 7Vediamo ora alcuni esempi di insiemi convessi e non convessi nel caso di \nspazio a due dimensioni:\nsi ?\nsi\n noInsiemi Convessi \n[caso a due dimensioni]",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#7": " 8Vediamo ora alcuni esempi di insiemi convessi e non convessi nel caso di \nspazio a due dimensioni:\nsi no\nsi\n noInsiemi Convessi \n[caso a due dimensioni]\n",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#8": "Funzioni Convesse \n 9con\u00002[0,1]\nwg(w)\nv wg(w)\ng(v)\n.. .\n..\u0000g(v)+( 1\u0000\u0000)g(w)\ng(\u0000v+( 1\u0000\u0000)w)\n\u0000v+( 1\u0000\u0000)wg(\u0000v+( 1\u0000\u0000)w)\u0000g(v)+( 1\u0000\u0000)g(w)Sia C un insieme convesso. Una funzione                     si dice convessa se, per \nogni v e w appartenenti al suo dominio di deﬁnizione, vale la seguente proprietà:g:C!R\ng:R!R",
    "data_test\\rootfolder\\università\\MachineLearning\\2-Richiami di Matematica-sbloccato.pdf#9": "Deﬁnizione di Funzione  \nStrongly Convex\n \n10\nUna funzione \n g\n è detta \n λ\n-strongly convex\n  se, per ogni \n w\n, \nv\n e \nα \n∈\n (0, 1), si ha:\nOvviamente, ogni funzione convessa è 0\n -strongly convex.g(↵v+( 1\u0000↵)w)↵g(v)+( 1 \u0000↵)g(w)\u0000\u0000\n2↵(1\u0000↵)kv\u0000wk2\nwg(w)\nv wg(w)\ng(v)\n.. .\n..\u0000\u0000\n2↵(1\u0000↵)kv\u0000wk2\n↵v+( 1\u0000↵)wg(↵v+( 1\u0000↵)w)↵g(v)+( 1\u0000↵)g(w)",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Ensembles di Decision Trees (Ex 06)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#1": "Sommario\nEnsembles \nRandom Forests \nGradient boosted regression trees",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#10": "Scikit-learn: Random forests e breast cancer dataset\nEsercizio\n : crea un RF per il dataset breast cancer con 100 alberi, e valuta \nl'accuracy nel training e test set, confrontandola con quella ottenuta con \nun singolo DT.\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#11": "Scikit-learn: Random forests e breast cancer dataset\nEsercizio\n : crea un RF per il dataset breast cancer con 100 alberi, e valuta \nl'accuracy nel training e test set, confrontandola con quella ottenuta con \nun singolo DT. \nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nrandom_state\n =\n0\n)\nforest \n= \nRandomForestClassifier\n (\nn_estimators\n =\n100\n, \nrandom_state\n =\n0\n)\nforest\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Accuracy on training set: {:.3f}\"\n .\nformat\n(\nforest\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Accuracy on test set: {:.3f}\"\n .\nformat\n(\nforest\n.\nscore\n(\nX_test\n, \ny_test\n)))\nAccuracy on training set: 1.000\nAccuracy on test set: 0.972\nL'accuracy è più alta rispetto al modello lineare e al DT. \nÈ possibile fare un tuning con i parametri max_features e l'approccio \npruning, ma su alcuni dataset i valori di default possono essere già \nsufﬁcienti.\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#12": "Scikit-learn: Random forests e breast cancer dataset\nCosa ti aspetti dalla feature importance ottenuta mediando i valori dei \nsingoli trees?  \nplot_feature_importances_cancer\n (\nforest\n)\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#13": "Scikit-learn: Random forests e breast cancer dataset\nCosa ti aspetti dalla feature importance ottenuta mediando i valori dei \nsingoli trees?  \nplot_feature_importances_cancer\n (\nforest\n)\nIl valore aggregato ha più variabilità e tendenzialmente è più accurato. Il \nmodello considera più features dando meno importanza alle singole (es. \nworst radius\n )\n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#14": "Considerazioni sui Random forests (1)\nI RF sono modelli di ML molto utilizzati essendo versatili, non richiedono \nlunghe fasi di tuning degli iperparametri e il rescaling dei dati. \nD'altro canto se hai bisogno di una rappresentazione compatta, il singolo \nDT è la soluzione migliore. È impossibile interpretare il valore di centinaia \no più DT, soprattutto se hanno profondità elevate. \nLe implementazione dei RF possono essere facilmente parallelizzate su più \nCPU. Il parametro \n n_jobs\n  imposta il numero di core da impiegare (un \nvalore pari a -1 indica l'uso di tutti i core). \nL'approccio random nei RF rende i modelli generati sugli stessi dati anche \nmolto diversi tra loro. Se vuoi ottenere risultati riproducibili, imposta il \nparametro \n random_state\n . \nI RF non mostrano buone prestazioni su dati sparsi e/o con molte features, \nes. dati testuali. I modelli lineare sono da preferire.\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#15": "Considerazioni sui Random forests (2)\nUn parametro elevato di n_estimators solitamente migliora le performance, \nma richiede più tempo e memoria per il training. \nUna indicazione per il parametro \n max_features\n  è impostarlo pari a \nsqrt(n_features)\n  per la classiﬁcazione, e \n log2(n_features)\n  per la \nregressione.\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#16": "Ensembles: Gradient boosted regression trees\nI \nGradient boosted regression trees\n  (\ngradient boosting machines\n ) \nGBRT  \nsono un approccio di \n ensembles\n , e possono essere impiegate sia per la \nclassiﬁcazione sia per la regressione.  \nA differenza dei RF, gli alberi sono costruiti in modo sequenziale, dove \nogni albero tenta di risolvere i problemi mostrati in precedenza. \nL'algoritmo è basato sull'approccio \n boosting\n  visto in precedenza.  \nAl posto dell'elemento casuale, è impiegato l'approccio pre-pruning. Gli \nalberi prodotti non sono profondi (tipicamenti depth da 1 a 5), e questo \nrende il modello più compatto e veloce nelle predizioni. \nI singoli alberi sono modelli \n semplici\n  (in ML sono spesso chiamati \n weak \nlearners\n ) che producono buone performance su alcune istanze dei dati. \nRispetto ai RF sono più sensibili alla scelta degli iperparametri, ma possono \nprodurre risultati migliori, per questo sono spesso impiegati in scenari reali.\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#17": "Ensembles: Gradient boosted regression trees\nOltre al pre-pruning e al numero di alberi (\n n_estimators\n ), un altro \niperparametro fondamentale è il \n learning_rate,\n  che controlla quanto un \nalbero deve correggere gli errori prodotti dal precedente. Un valore elevato \ngenera modelli più complessi. Allo stesso modo, un valore elevato di \nn_estimators\n  incrementa la complessità e può ridurre gli errori commessi. \nIn scikit-learn, la classe \n GradientBoostingClassiﬁer\n  implementa gli GBRT. \nNel caso del Breast cancer dataset, con 100 alberi, con profondità max pari \na 3 e un learning rate pari a 0.1: \nfrom \nsklearn.ensemble \n import \nGradientBoostingClassifier\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nrandom_state\n =\n0\n)\ngbrt \n= \nGradientBoostingClassifier\n (\nrandom_state\n =\n0\n)\ngbrt\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Accuracy on training set: {:.3f}\"\n .\nformat\n(\ngbrt\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Accuracy on test set: {:.3f}\"\n .\nformat\n(\ngbrt\n.\nscore\n(\nX_test\n, \ny_test\n)))\nAccuracy on training set: 1.000\nAccuracy on test set: 0.958\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#18": "Gradient boosted regression trees\nOtteniamo una accuracy pari al 100%, potrebbe indicare un possibile \noverﬁtting.  \nEsercizio\n : prova ad incrementare il pre-pruning o ridurre il learning rate.\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#19": "Gradient boosted regression trees\nEsercizio\n : prova ad incrementare il pre-pruning o ridurre il learning rate. \ngbrt \n= \nGradientBoostingClassifier\n (\nrandom_state\n =\n0\n, \nmax_depth\n =\n1\n)\ngbrt\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Accuracy on training set: {:.3f}\"\n .\nformat\n(\ngbrt\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Accuracy on test set: {:.3f}\"\n .\nformat\n(\ngbrt\n.\nscore\n(\nX_test\n, \ny_test\n)))\nAccuracy on training set: 0.991\nAccuracy on test set: 0.972\ngbrt \n= \nGradientBoostingClassifier\n (\nrandom_state\n =\n0\n, \nlearning_rate\n =\n0.01\n)\ngbrt\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Accuracy on training set: {:.3f}\"\n .\nformat\n(\ngbrt\n.\nscore\n(\nX_train\n, \ny_train\n)))\nprint\n(\n\"Accuracy on test set: {:.3f}\"\n .\nformat\n(\ngbrt\n.\nscore\n(\nX_test\n, \ny_test\n)))\nAccuracy on training set: 0.988\nAccuracy on test set: 0.965\nEntrambi gli approcci riducono la complessità e l'accuracy sul training set. \nIn questo scenario, ridurre la profondità migliora maggiormente le \nperformance.\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#2": "Ensembles\nIn ML, l'\n ensembles\n  un approccio che combina più modelli di ML per \ncreare un nuovo modello più soﬁsticato, che potenzialmente aggrega i \nbeneﬁci dei singoli modelli.  \nEsistono vari approcci di ensembles. Due approcci basati sui decision trees \n(\nDT\n) si sono dimostrati molto adatti in vari domini: \nRandom forests \nGradient boosted decision trees.\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#20": "Gradient boosted regression trees\nAvendo impiegato 100 alberi, è poco pratico visualizzare le decision \nboundaries di ognuno, ma possiamo analizzare le feature importance. \ngbrt \n= \nGradientBoostingClassifier\n (\nrandom_state\n =\n0\n, \nmax_depth\n =\n1\n)\ngbrt\n.\nfit\n(\nX_train\n, \ny_train\n)\nplot_feature_importances_cancer\n (\ngbrt\n)\nNoti differenze rispetto ai RF?\n21\n",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#21": "Gradient boosted regression trees\nI generale otteniamo \n importance\n  simili, ma in questo caso alcune features \nhanno peso pari a 0, cioè sono completamente ignorate dal modello.\n22\n",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#22": "Gradient boosted regression trees: considerazioni (1)\nEntrambi gli approcci ensembles mostrano buoni risultati su dati simili. Si \npuò applicare prima l'approccio RF, piuttosto robusto.  \nI GBRT richiedono un tuning degli iperparametri più lungo rispetto ai RF. \nSe il tempo impiegato per la predizione non è soddisfacente, o è \nfondamentale raggiungere una accuracy massima, si può considerare il \nGBRT. \nCome per i RF, i GBRT funzionano bene senza rescaling, e su combinazioni \ndi feature binary o continous. Ma non sono efﬁcienti per dataset con molte \nfeatures. \nI due iperparametri fondamentali sono \n n_estimators\n  e \nlearning_rate\n . Sono \ndipendenti l'uno dall'altro. Un basso learning rate richiede più alberi per \nraggiungere la stessa complessità. Un valore elevato di \n n_estimators  \nmigliora il modello, ma fa tendere il modello all'overﬁtting. \nTipicamente si imposta \n n_estimators\n  in base alle risorse a disposizione, \ndopodiché si ottimizza il valore \n learning_rates\n .\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#23": "Gradient boosted regression trees: considerazioni (2)\nAltro iperparametro fondamentale è \n max_depth\n  (o alternativamente \nmax_leaf_nodes\n ) per ridurre la complessità per ogni albero. Tipicamente si \nimposta a un valore molto basso, es. < 5.  \nCon dataset di larghe dimensioni, si può considerare anche la libreria \nxgboost\n , che possiede una implementazione più ottimizzata.\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#24": "Esercizio su ensembles\nEsercizio\n : impiegare i due approcci ensembles sui restanti dataset introdotti \nnelle precedenti esercitazioni: \nForge dataset\n  (classiﬁcazione) \nwave dataset\n  (regressione) \nBoston housing dataset\n  (regressione) \nValutare la accuracy rispetto all'approccio basato sulla regressione lineare \ne al singolo decision tree.  \nOperare un tuning degli iperparametri per incrementare le performance. \n25",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#25": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n26",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#3": "Ensembles: Random forests\nI \nrandom forests\n  (\nRF\n) sono una collezione di DTs, ognuno costruito in \nmodo leggermo diverso dall'altro durante il training. \nI DTs tendono a mostrare overﬁtting. I RF tendono ad affrontare questa \nproblematica: ogni albero può mostrare overﬁtting su certi dati, ma se ne \ncostruiamo diversi in modo indipendente e mediamo i risultati complessivi, \nl'effetto dell'overﬁtting si riduce.  \nPer creare diversi DTs, introduciamo un elemento casuale durante il \nprocesso di training, ad esempio selezionando: \ndiversi set di training \ndiverse features in ogni split test\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#4": "Scikit-learn: Random forests\nIn scikit-learn esiste una implementazione dei RF per la classiﬁcazione e \nper la regressione: \n RandomForestClassiﬁer\n  e \nRandomForestRegressor\n . \nIl numero di DTs è un iperparametro del modello RF e si imposta col \nparametro \n n_estimators\n  del costruttore (es. 10). \nInizialmente si costruisce un \n bootstrap sample\n  dei dati.  \nDal training set estraiamo \n n_samples\n  istanze in modo casuale, con \nripetizione, e ripetiamo n_samples volte. \nIl dataset che si ottiene è grande come quello originale, ma alcune \nistanze si possono ripetere, altre sono mancanti (approssimativamente \n1/3)  \nEs.: se il dataset = ['a', 'b', 'c', 'd'], un possibile bootstrap è ['b', 'd', 'd', \n'c'], un altro ['d', 'a', 'd', 'a']. \nDopodiché si addestra un DT per ogni boostrap sample.\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#5": "Scikit-learn: Random forests\nUn ulteriore elemento casuale è introdotto in ogni nodo dell'albero. \nDurante la costruzione, invece di scegliere il test migliore, si selezionando \nun modo casuale un sottoinsieme di features e si seleziona la migliore \nconsiderando tale sottoinsieme.  \nIl numero di features è impostato col parametro del costruttore \nmax_features\n  (ulteriore iperparametro del modello). \nUn valore alto di \n max_features\n  riduce la casualità nel modello RF, ma \nmigliora il ﬁt sui dati. Un valore basso produce degli alberi molto \ncomplessi per raggiungere lo stesso livello di ﬁt. \nPer generare l'output, ogni DT è valutato sull'istanza in input e i risultati \nsono sottoposti a \n soft voting, \n cioè le probabilità per ogni \n label\n ottenute dai \nsingoli DT sono mediate e la classe con probabilità più alta è l'output del \nRF.\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#6": "Scikit-learn: Random forests e two_moons\nEsercizio\n : col dataset \n two_moons\n  crea un modello RF con 5 alberi. \nfrom \nsklearn.ensemble \n import \nRandomForestClassifier\nfrom \nsklearn.datasets \n import \nmake_moons\nX\n, \ny \n= \nmake_moons\n (\nn_samples\n =\n100\n, \nnoise\n=\n0.25\n, \nrandom_state\n =\n3\n)\n...\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#7": "Scikit-learn: Random forests e two_moons\nCol dataset two_moons creiamo un modello RF con 5 alberi: \nfrom \nsklearn.ensemble \n import \nRandomForestClassifier\nfrom \nsklearn.datasets \n import \nmake_moons\nX\n, \ny \n= \nmake_moons\n (\nn_samples\n =\n100\n, \nnoise\n=\n0.25\n, \nrandom_state\n =\n3\n)\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\nX\n, \ny\n, \nstratify\n =\ny\n,\nrandom_state\n =\n42\n)\nforest \n= \nRandomForestClassifier\n (\nn_estimators\n =\n5\n, \nrandom_state\n =\n2\n)\nforest\n.\nfit\n(\nX_train\n, \ny_train\n)\nI parametri sono salvati nella variabile \n estimator_\n  del modello. \nPossiamo rappresentare i decision boundary per ogni modello: \nfig\n, \naxes \n= \nplt\n.\nsubplots\n (\n2\n, \n3\n, \nfigsize\n=\n(\n20\n, \n10\n))\nfor \ni\n, (\nax\n, \ntree\n) in \nenumerate\n (\nzip\n(\naxes\n.\nravel\n(), \nforest\n.\nestimators_\n )):\nax\n.\nset_title\n (\n\"Tree {}\"\n .\nformat\n(\ni\n))\nmglearn\n.\nplots\n.\nplot_tree_partition\n (\nX_train\n, \ny_train\n, \ntree\n, \nax\n=\nax\n)\nmglearn\n.\nplots\n.\nplot_2d_separator\n (\nforest\n, \nX_train\n, \nfill\n=\nTrue\n, \nax\n=\naxes\n[\n-\n1\n, \n-\n1\n],\nalpha\n=.\n4\n)\naxes\n[\n-\n1\n, \n-\n1\n]\n.\nset_title\n (\n\"Random Forest\"\n )\nmglearn\n.\ndiscrete_scatter\n (\nX_train\n[:, \n0\n], \nX_train\n[:, \n1\n], \ny_train\n)\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#8": "Scikit-learn: Random forests e two_moons\nCosa puoi notare riguardo i modelli e i training set? \n9\n",
    "data_test\\rootfolder\\università\\MachineLearning\\20-Ex_06 Esercitazione su Ensembles di Decision Trees-sbloccato.pdf#9": "Scikit-learn: Random forests e two_moons\nOgni modello ha decision boundaries distinti, dove alcune istanze non sono \ncorrettamente classiﬁcati.  \nOgni modello ha un training set leggermente distinto: alcune istanze del \ntraining set complessivo non sono presenti. \nLe boundaries del modello ﬁnale sono più \"smooth\". \n10\n",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Voting e Stacking ensembles (Ex 07)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#1": "Sommario\nVoting \nStacking \nMutilayer Stacking \nDatasets MNIST e notMNIST \nAltri dataset di immagini \nEsercitazioni",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#10": "Multilayer Stacking ensemble\nÈ possibile considerare più blender, ognuno basato su un modello distinto \n(es. regressione lineare, random forest, etc), ottenendo un nuovo layer. \nIn questo caso si suddivide il training set in 3 parti. La prima usata nel \nprimo layer, come nel caso precedente. La seconda usata dai modelli che \ncombinano le predizioni del primo layer. E la restate parte che combina le \npredizioni del secondo layer. \nNota: scikit-learn non supporta lo stacking.  \nMa ci sono librerie open source, es.  \nhttps://github.com/viisar/brew   \nhttps://github.com/Menelau/DESlib  \n11\n",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#11": "MNIST\nE’ un dataset molto conosciuto (rielaborato da \n NIST\n ) di cifre per addestrare sistemi \ndi classiﬁcazione basati sulle immagini. \n\"If it doesn't work on MNIST, it won't work at all”; \"Well, if it does work on \nMNIST, it may still fail on others.\" \nContiene 60K immagini di addestramento e 10K di training. \n1998: un linear classiﬁer ha ottenuto 7.6% di errore rate. \n2012: per mezzo di una architettura DL (convolutional neural networks) si è \narrivati al 0.23%. \nOgni immagine è rappresentata in scala di grigi (256 livelli). Le cifre sono centrate \nin un box 28x28 pixel: abbiamo 784 valori in [0-255] per rappresentare una cifra. \nhttp://yann.lecun.com/exdb/mnist/  \nhttps://www.kaggle.com/c/digit-recognizer/data   \nImplementazione online JS (ott’17) \n http://myselph.de/neuralNet.html\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#12": "MNIST: train.csv e test.csv\nIl ﬁle train.csv contiene una matrice con 785 colonne. La prima \ncolonna è il \n label\n della cifra (es. 3) e le restanti colonne sono la \nrappresentazione sequenziale dell’immagine: \nIl ﬁle test.csv ha la stessa rappresentazione senza la prima colonna. \nEsempio di immagini:\n13\n",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#13": "MNIST: Considerazioni\nNon è impiegato per sistemi avanzati poiché è un task semplice. \nAlgoritmi classici di ML raggiungono i 97% di precisione, \napprocci Deep Learning il 99.7% \nTroppo utilizzato: si rischia di ideare nuovi approcci adatti solo per \nquesto dataset. \nMolto diverso dai task studiati oggi.\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#14": "MNIST dataset\nscikit-learn include il dataset che può essere facilmente usato: \nfrom\n sklearn.datasets \n import\n fetch_openml\nimport\n numpy \nas\n np\nmnist = fetch_openml(\n 'mnist_784'\n , version=\n 1\n, as_frame=\n False\n)\nmnist.target = mnist.target.astype(np.uint8)\nfrom\n sklearn.model_selection \n import\n train_test_split\n# 50K instanze per il training, 10K validation e 10K test\nX_train_val, X_test, y_train_val, y_test = train_test_split(\n    mnist.data, mnist.target, test_size=\n 10000\n, random_state=\n 42\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_val, y_train_val, test_size=\n 10000\n, random_state=\n 42\n)\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#15": "notMNIST\nSimile a MNIST, contiene 10 labels (lettere da A a J), ma ogni lettera \nnel dataset occorre con font diversi, es: \nhttp://yaroslavvb.blogspot.ﬁ/2011/09/notmnist-dataset.html   \nDownload \n http://yaroslavvb.com/upload/notMNIST/  \nnotMNIST_large.tar.gz -> training e validazione \nnotMNIST_small.tar.gz -> test \n16\n",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#16": "fashion-MNIST\nFornito da Zalando. 10 classi che fanno riferimento a generi di vestiario (es. \nsandali, t-shirt, borse, etc). \nContiene 60K immagini di addestramento e 10K di training.  \nOgni immagine è rappresentata in scala di grigi di 28x28 pixel  \nhttps://github.com/zalandoresearch/fashion-mnist   \nSide-by-side accuracy MNIST vs fashion MNIST: \nhttp://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#\n17\n",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#17": "Altri dataset popolari sulle immagini\nCIFAR-10 (e 100)\n : 60K 32x32 colour images in 10 classes. \nImageNet\n : 1,5 milioni di immagini organizzate etichettate su \nWordNet. In media 1K immagini per concetto. \nILSVRC2012 task 1\n : 10 milioni di immagini e +1K classi. \nOpen Image\n : 9 milioni di URLs di immagini annotate con bounding \nboxes e migliaia di classi. \nVisualQA\n : open-ended questions su 265K immagini. In media 5.4 \nquestions per immagini con 10 ground truth answers per question. \nThe Street View House Numbers\n : 600K immagini di numeri civici. \nRisultati sperimentali ottenuti per varie architetture avanzate: \nhttp://rodrigob.github.io/are_we_there_yet/build/#datasets  \n18",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#18": "Esercitazione: Voting Classiﬁer\nImpiega il dataset MNIST con uno split 50K/10K/10K. Scegli almeno tre \nclassiﬁcatori e addestrali singolarmente.  \nCrea un ensemble Voting, e valutalo sia con approccio soft che hard voting, \nsia sul validation sia sul test set.  \nConfronta i risultati con i classiﬁcatori singoli. \nProva a rimuovere il classiﬁcatore che si comporta meglio e valuta \nnuovamente le prestazioni.\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#19": "Esercitazione: Stacking Ensemble\nEsegui i singoli classiﬁcatori scelti in precedenza e colleziona gli output sul \nvalidation set. \nCrea un nuovo training set con tali predizioni. Ogni istanza del set è una \nvettore che contiene l'insieme di predizioni per una certa immagine, e il \ntarget e la classe associata all'immagine. Addestra un classiﬁcatore con tale \ntraining set. Valutalo sul test set. \nHai appena realizzato un Stacking ensemble.\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#2": "Ensembles: Voting\nL'approccio voting si ispira alla ﬁlosoﬁa \n wisdom of the crowd.\n  Supponiamo \ndi avere più classiﬁcatori (es. Logistic regression, SVM, Random forest, k-\nNN). Prendiamo la predizione di ognuno e scegliamo quella che riceve \n\"più voti\". Questa forma di aggregazione prende il nome di \n hard-voting\n . \nSe partiamo da weak classiﬁers con accuracy non soddisfacente, il \nclassiﬁcatore risultante può raggiungere accuracy elevate.\n3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#20": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#3": "Scikit-learn: Voting\nLa classe \n VotingClassiﬁer\n  di scikit-learn implementa l'approccio.  \nEsercizio\n : completa il seguente frammento di codice basandoti sulla \ndocumentazione online di VotingClassiﬁer. \nfrom \nsklearn.ensemble \n import \nVotingClassifier\n(... importa gli altri classificatori ...)\nfrom\n sklearn.model_selection \n import\n train_test_split\nfrom\n sklearn.datasets \n import\n make_moons\nX, y = make_moons(n_samples=\n 500\n, noise=\n 0.30\n, random_state=\n 42\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=\n 42\n)\n...\nvoting_clf \n = \nVotingClassifier\n (\n    \n...\n, \n    \nvoting\n=\n'hard'\n)\nvoting_clf\n .\nfit\n(\nX_train\n, \ny_train\n)\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#4": "Scikit-learn: Voting\nImpieghiamo SVM, RandomForest e LogisticRegression: \nfrom \nsklearn.ensemble \n import \nRandomForestClassifier\nfrom \nsklearn.ensemble \n import \nVotingClassifier\nfrom \nsklearn.linear_model \n import \nLogisticRegression\nfrom \nsklearn.svm \n import \nSVC\nfrom\n sklearn.model_selection \n import\n train_test_split\nfrom\n sklearn.datasets \n import\n make_moons\nX, y = make_moons(n_samples=\n 500\n, noise=\n 0.30\n, random_state=\n 42\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=\n 42\n)\nlog_clf \n = \nLogisticRegression\n ()\nrnd_clf \n = \nRandomForestClassifier\n ()\nsvm_clf \n = \nSVC\n()\nvoting_clf \n = \nVotingClassifier\n (\n    \nestimators\n =\n[(\n'lr'\n, \nlog_clf\n), (\n'rf'\n, \nrnd_clf\n), (\n'svc'\n, \nsvm_clf\n)], \n    \nvoting\n=\n'hard'\n)\nvoting_clf\n .\nfit\n(\nX_train\n, \ny_train\n)\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#5": "Scikit-learn: Voting\n(segue)\nfrom \nsklearn.metrics \n import \naccuracy_score\nfor \nclf \nin (\nlog_clf\n, \nrnd_clf\n, \nsvm_clf\n, \nvoting_clf\n ):\n    \nclf\n.\nfit\n(\nX_train\n, \ny_train\n)\n    \ny_pred \n= \nclf\n.\npredict\n(\nX_test\n)\n    \nprint\n(\nclf\n.\n__class__\n .\n__name__\n , \naccuracy_score\n (\ny_test\n, \ny_pred\n))\nLogisticRegression 0.864\nRandomForestClassifier 0.896\nSVC 0.888\nVotingClassifier 0.904\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#6": "Scikit-learn: Voting\nSe i classiﬁcatori impiegati sono in grado di stimare probabilità di \nappartenenza alle singole label, cioè implementano la funzione \npredict_proba(), il voting può valutare le medie delle probabilità prodotte \nda ogni classiﬁcatore.  \nL'approccio si chiama \n soft voting,\n  e si seleziona col parametro voting del \ncostruttore: \n    \nvoting\n=\n'soft'\nEsercizio\n : controlla che i classiﬁcatori impiegati in precedenza \nimplementino predict_proba() e, in caso affermativo, lancia nuovamente il \ncodice precedente e valuta la differenza di performance.\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#7": "Scikit-learn: Voting\nog_clf = LogisticRegression(solver=\n \"lbfgs\"\n, random_state=\n 42\n)\nrnd_clf = RandomForestClassifier(n_estimators=\n 100\n, random_state=\n 42\n)\nsvm_clf = SVC(gamma=\n \"scale\"\n, probability=\n True\n, random_state=\n 42\n)\nvoting_clf = VotingClassifier(\n    estimators=[(\n 'lr'\n, log_clf), (\n 'rf'\n, rnd_clf), (\n 'svc'\n, svm_clf)],\n    voting=\n 'soft'\n)\nvoting_clf.fit(X_train, y_train)\nVotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n                             ('rf', RandomForestClassifier(random_state=42)),\n                             ('svc', SVC(probability=True, random_state=42))],\n                 \n voting='soft'\n )\nfrom\n·\nsklearn.metrics\n ·\nimport\n·\naccuracy_score\nfor\n·\nclf\n·\nin\n·\n(log_clf,\n ·\nrnd_clf,\n ·\nsvm_clf,\n ·\nvoting_clf):\n    \nclf.fit(X_train,\n ·\ny_train)\n    \ny_pred\n·\n=\n·\nclf.predict(X_test)\n    \nprint\n(clf.__class__.__name__,\n ·\naccuracy_score(y_test,\n ·\ny_pred))\nfrom sklearn.metrics import accuracy_score\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\nLogisticRegression 0.864\nRandomForestClassifier 0.896\nSVC 0.896\nVotingClassifier 0.92\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#8": "Ensembles: Stacking\nUn ulteriore approcio ensembles è lo \n stacking\n , che sta per \n stacked \ngeneralization\n . \nInvece di aggregare il risultato con una tecnica di voting, addestriamo un \nulteriore modello per questo scopo, chiamato \n blender\n  o \nmeta learner\n . \n9\n",
    "data_test\\rootfolder\\università\\MachineLearning\\21-Ex_07 Esercitazione Voting e Stacking ensembles-sbloccato.pdf#9": "Stacking\nUn approccio che spesso si impiega per addestrare il blender è il \n hold-out \nset\n. Inizialmente il training set è suddiviso in 2. Il primo è usato per \naddestrare i modelli nel primo layer, mentre il secondo (held-out) è usato \nper creare le predizioni. Per ogni istanza ci sono 3 predizioni. Tali \npredizioni costituiscono le features di una istanza in un \n nuovo training set\n , \nil cui valore target è quello originale. Il blender è addestrato sul nuovo set.\n10\n",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nIntroduzione al \nClustering\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#1": "Sommario\nSupervised e Unsupervised Learning \nIntroduzione al Clustering \nAlgoritmo k-means  \nAlgoritmo k-means++\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#10": "k-means Clustering \n \n11\nVediamo un esempio di esecuzione dell’algoritmo nel caso in cui i \ndata points siano quelli riportati in ﬁgura. \nSupponiamo di scegliere come numero di cluster: k=3 ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#11": "k-means Clustering \n \n12\nµ1,µ2,...,µ k\nScelta del numero di cluster k e inizializzazione dei k centroidi:\nEsempio per k = 3\nμ1\nμ2\nμ3",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#12": "Voronoi Tesselation \n \n13\n",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#13": "k-means Clustering \n \n14\nzi argmin\njkµj\u0000xik2\nAssegnazione delle osservazioni al più vicino centroide:\nμ1\nμ2\nμ3",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#14": "k-Means Clustering \n \n15\nSi ricalcolano i centroidi come media delle osservazioni assegnate \nad ogni cluster:\nµj=1\nnjX\ni:zi=jxi\nμ1\nμ2μ3",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#15": "k-Means Clustering \n \n16\nzi argmin\njkµj\u0000xik2\nSi riassegnano le osservazioni al centroide più vicino:\n… e così via ﬁno al raggiungimento di una cond. di terminazione.μ1\nμ2μ3",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#16": "Algoritmo k-means \n \n17\nL’algoritmo può essere pertanto sintetizzato come segue:\nScegliamo il numero kdei cluster\nInizializziamo i centroidi µ1,µ2,...,µ k\nwhile not converged\nfor i=1,. . . ,N\nzi argmin\njkµj\u0000xik2; assegniamo i data points al cluster center pi` u vicino\nfor j=1,. . . ,k\nµj=1\nnjX\ni:zi=jxi; aggiorniamo ciascun cluster center come media dei suoi data points",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#17": "Algoritmo k-means \ncome Coordinate Descent \n \n18µj argmin\nµX\ni:zi=jkµ\u0000xik2\nSi noti che la formula per il calcolo delle medie: \nè equivalente alla seguente espressione: µj=1\nnjX\ni:zi=jxi",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#18": "Algoritmo k-means \ncome Coordinate Descent \n \n19\nAbbiamo dunque la seguente versione equivalente dell’algoritmo:\ndove si alternano le minimizzazioni (a): z dato \n μ\n e (b): \n μ\n dato z. Scegliamo il numero kdei cluster\nInizializziamo i centroidi µ1,µ2,...,µ k\nwhile not converged\nfor i=1,. . . ,N\nzi argmin\njkµj\u0000xik2; assegniamo i data points al cluster center pi` u vicino\nfor j=1,. . . ,k\nµj argmin\nµX\ni:zi=jkµ\u0000xik2; calcolo centroidi che minimizzano la somma del\n; quadrato delle norme per i loro data points",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#19": "In genere k-means converge ad un ottimo locale. \nL’algoritmo è molto sensibile all’inizializzazione dei centroidi. \nVediamo un esempio: \n \n20\nConvergenza di k-means ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#2": "Supervised vs. Unsupervised \nLearning\nCome sappiamo, molti problemi e metodi di Machine Learning \nrientrano in una delle due seguenti categorie: apprendimento \nsupervisionato\n  o \nnon supervisionato\n . \nGli esempi visti ﬁno ad ora rientrano nel dominio \ndell’apprendimento supervisionato: \n•\nIn quei casi (linear regression, logistic regression, ecc.) si \nhanno delle osservazioni che, a fronte di una certa \nconﬁgurazione delle features, ci dicono quale sia la \nsoluzione corretta.\n \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#20": " \n21\nConvergenza di k-means \nData la scelta dei centroidi iniziali mostrata nella ﬁgura a sinistra, \nsi ottiene il risultato mostrato a destra:",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#21": " \n22\nConvergenza di k-means \nAltra scelta dei centroidi iniziali:",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#22": " \n23\nConvergenza di k-means \nAltra scelta dei centroidi iniziali:",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#23": "k-means++ \n \n24Arthur, D. e Vassilvitskii, S. “k-means++: the advantages of careful seeding”, in Proc. of the \n18th ACM-SIAM Symp. on Discrete Algorithms , 2007, pp. 1027-1035.\nBahmani, B., Moseley, B., Vattani, A., Kumar, R. e Vassilvitskii, S. “Scalable k-means++”, in \nProc. of VLDB , 2012.\nCome abbiamo visto, l’inizializzazione di k-means è critica ai ﬁni \ndella qualità dell’ottimo locale trovato. \nOra vediamo k-means++, un metodo che consiste in una \nparticolare inizializzazione dei centroidi che in genere dà buoni \nrisultati. \nRiferimenti:",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#24": "k-means++ \n \n25\nSmart initialization\n : \n1.\n Scegliere il primo centroide in modo casuale tra tutti i data \npoints. \n2.\n Per ogni osservazione \n x\ni\n, calcolare la distanza d(\n x\ni\n) tra \nx\ni\n e il più \nvicino centroide. \n3.\n Scegliere il nuovo centroide tra i data point, con la probabilità \ndi \nx\ni\n di essere scelto proporzionale a d(\n x\ni\n) , ossia al quadrato \ndella distanza tra \n x\ni\n e il centroide più vicino già scelto. \n4.\n Ripeti gli step 2 e 3 ﬁno ad arrivare a scegliere k centroidi.\n2 ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#25": "k-means++: esempio \n \n26\nVediamo un esempio di inizializzazione con k=3, relativo alle \nosservazioni in ﬁgura:",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#26": " \n27\nScelta random del primo cluster center:\nk-means++: esempio ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#27": " \n28\nScelta del secondo cluster center. Si sceglie il punto con la \nprobabilità maggiore, dove la probabilità è proporzionale a d(\n x\n). \nIn ﬁgura sono mostrate le varie distanze. \n2 \nk-means++: esempio ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#28": " \n29\nSupponiamo che venga scelto il secondo cluster center in verde: \nk-means++: esempio ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#29": " \n30\nScelta dell’ultimo cluster center. Di nuovo, si sceglie il punto con \nla probabilità maggiore, dove la probabilità è proporzionale a \nd(\nx\ni\n), quadrato della distanza tra il punto i e il più vicino \ncentroide: \n2 \nk-means++: esempio ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#3": "Supervised vs. Unsupervised \nLearning\nNel caso non supervisionato ci troviamo in una situazione più \nimpegnativa, nella quale abbiamo le varie osservazioni \ncaratterizzate dai vari valori delle \n features\n , ma per le quali non \nabbiamo disponibili le soluzioni. \nIn questa situazione, in un certo senso dobbiamo lavorare alla \ncieca. \nLa situazione è deﬁnita \n unsupervised\n  proprio perché nei \n data \npoints\n  disponibili ci manca la risposta che può supervisionare la \nnostra analisi. \n \n4",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#30": " \n31\nSupponiamo che il cluster center scelto sia quello in blu. I tre \ncentroidi scelti sono quelli con cui inizializziamo l’algoritmo k-\nmeans. \nk-means++: esempio ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#31": "k-means++: pros & cons \n \n32\nEseguire k-means++ per individuare i centroidi iniziali è \ncertamente più oneroso computazionalmente rispetto alla scelta \nrandom dei suddetti centroidi. \nPer contro, l’esecuzione di k-means con l’inizializzazione di k-\nmeans++ è spesso più efﬁciente, nel senso che converge in genere \npiù rapidamente. \nIn generale possiamo dire che k-means++ tende a migliorare la \nqualità dell’ottimo locale trovato e diminuire il tempo di \nesecuzione. ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#32": "Cluster Heterogeneity \n \n33\nL’algoritmo k-means cerca di minimizzare la somma dei quadrati \ndelle distanze (\n distortion\n ): \nCome abbiamo visto, in genere l’algoritmo trova un minimo \nlocale. costo kmeans =kX\nj=1X\ni:zi=jkµj\u0000xik2",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#33": "Cluster Heterogeneity \n \n34\nConfrontiamo i seguenti due risultati: la ﬁgura a destra è \nsicuramente migliore. La ﬁgura a sinistra è più “eterogenea”. \n",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#34": "Cosa accade al crescere di k \n \n35\nConsideriamo il caso estremo k = N: \n•\n Signiﬁca che ogni cluster center è un data point. \n•\n Il costo (heterogeneity) è uguale a zero. \nIl costo (heterogeneity) decresce al crescere di k. ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#35": "Scelta del numero di cluster k \n \n36\n“Elbow Method”: Un’euristica usata è quella di scegliere un punto \nche si trova nel “gomito” della curva: \nk (# di cluster)(minimo della \ncluster heterogeneity)costo_k_means \nminimo\n123456",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#36": "Riferimenti\n \n37\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , Apogeo, 3a edizione, \n2015. \nMachine Learning: Clustering & retrieval\n , University of Washington - Coursera, \n2017. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. \nMurphy, K.P. \n Machine Learning - A Probabilistic Approach\n , The MIT Press, 2012.",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#4": "Clustering \nDobbiamo chiederci quale tipo di analisi sia possibile in tale \ncontesto. \nPossiamo ad esempio cercare di comprendere le relazioni tra le \nosservazioni. \nUn approccio che possiamo usare in tali situazioni è quello della \ncluster analysis\n , o \nclustering\n . \nL’obiettivo del \n clustering\n  è quello di veriﬁcare, date le features in \ninput, se le osservazioni disponibili ricadono all’interno di gruppi \nrelativamente distinti tra di loro. \n \n5",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#5": "Clustering \nIl \nclustering\n  è in effetti una delle tecniche più utilizzate per la \nexploratory data analysis\n . \nIn tante discipline, dalle scienze sociali alla biologia alla computer \nscience, gli studiosi cercano di avere delle prime “intuizioni” sui \ndati di cui dispongono identiﬁcando gruppi signiﬁcativi dei data \npoints: \n•\ni venditori cercano di identiﬁcare cluster di clienti, in base ai loro proﬁli, \nper migliorare l’attività di marketing (\n market segmentation\n ); \n•\ni medici cercano di raggruppare i pazienti in base alle loro condizioni \ncliniche; \n•\ngli astronomi identiﬁcano cluster di stelle in base alla loro prossimità \nspaziale; \n•\necc. ecc.\n \n6",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#6": "Clustering \nEsempio in due dimensioni: individuare la \n cluster structure\n  solo \ndagli input: \n \n7\nfeature 1feature 2",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#7": "Clustering \nOgni cluster è deﬁnito dal \n centroide\n  (\ncluster center\n ) e dalla forma \n(shape/spread): \n \n8\nfeature 1feature 2\n1 2\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#8": "Ciascuna osservazione \n x\ni\n è assegnata al cluster \n k\n se: \n•\n Il punteggio (\n score\n ) di \nx\ni\n sotto il cluster \n k\n è migliore rispetto agli \naltri cluster. \nPer semplicità, spesso si deﬁnisce lo \n score\n  come la distanza dal \ncentroide\n  del cluster (si ignora lo shape). \n \n9\nClustering ",
    "data_test\\rootfolder\\università\\MachineLearning\\22-Clustering-sbloccato.pdf#9": "k-means Clustering \nL’algoritmo \n k-means\n  assume come \n score\n  proprio la distanza di una \nosservazione dal \n centroide\n . Più bassa è la distanza, “migliore” è lo \nscore\n . \nDeﬁnizione dei simboli utilizzati nell’esempio che segue: \n \n10nj: numero di  elementi nel cluster jµj: centroide  del cluster j\nzi: label del cluster a cui appartiene xiN: numero delle osservazioni\nj: indice dei cluster \nk: numero dei clusterxi: osservazione i-esima (              ) xi2Rd",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Clustering (Ex 08)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#1": "Sommario\nPreprocessing: Scaling \nScaling in Scikit-learn \nScaling e classiﬁcazione \nScikit-learn e K-Means  \nEsempi di limiti dell'algoritmo K-Means ",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#10": "Scikit-learn: Scaling\nCosa succede se applicassimo due distinti rescaling sul training e sul test \nset? \nLe istanze nel test set sono state scalate in modo improprio rispetto ai valori \noriginali, e si trovano in posizioni relative diverse da quelle originali.\n11\n",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#11": "Scikit-learn: Scaling\nNota: In scikit-learn, gli scaler hanno spesso il metodo ﬁt_transform() che \ncombina le 2 operazioni: \nfrom \nsklearn.preprocessing \n import \nStandardScaler\nscaler \n= \nStandardScaler\n ()\nX_scaled \n = \nscaler\n.\nfit\n(\nX\n)\n.\ntransform\n (\nX\n)\n# stesso risultato ma più efficient\nX_scaled_d \n = \nscaler\n.\nfit_transform\n (\nX\n)\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#12": "Scikit-learn: Scaling e Classiﬁcazione\nEsercizio: Impiega il MinMaxScaler sul dataset breast cancer e impiega \nl'algoritmo di classiﬁcazione SVC(C=100). Confronta la performance senza \nscaling. \nfrom \nsklearn.svm \n import \nSVC\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nrandom_state\n =\n0\n)\n...\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#13": "Scikit-learn: Scaling e Classiﬁcazione\nEsercizio: Impiega il MinMaxScaler e StandardScaler sul dataset breast cancer \ne impiega l'algoritmo SVC(C=100). Confronta la performance senza scaling. \nfrom \nsklearn.svm \n import \nSVC\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n, \nrandom_state\n =\n0\n)\nsvm \n= \nSVC\n(\nC\n=\n100\n)\nsvm\n.\nfit\n(\nX_train\n, \ny_train\n)\nprint\n(\n\"Test set accuracy: {:.2f}\"\n .\nformat\n(\nsvm\n.\nscore\n(\nX_test\n, \ny_test\n)))\n>> Test set accuracy: 0.63\n# con scaling\nscaler \n= \nMinMaxScaler\n ()\nscaler\n.\nfit\n(\nX_train\n)\nX_train_scaled \n = \nscaler\n.\ntransform\n (\nX_train\n)\nX_test_scaled \n = \nscaler\n.\ntransform\n (\nX_test\n)\nsvm\n.\nfit\n(\nX_train_scaled\n , \ny_train\n)\nprint\n(\n\"Scaled test set accuracy: {:.2f}\"\n .\nformat\n(\nsvm\n.\nscore\n(\nX_test_scaled\n , \ny_test\n)))\n>> Scaled test set accuracy: 0.97 (con StandardScaler si ottiene 0.96)\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#14": "Scikit-learn: K-means\nScikit-learn implementa l'algoritmo con la classe \n KMeans\n . Il parametro \nn_clusters\n  è richiesto per speciﬁcare il numero di cluster.  \nSupponiamo di avere il seguente  \ndataset: \nL'output dell'algoritmo può essere  \nrappresentato col diagramma  \nVoronoi Tesselation. \nfrom \nsklearn.cluster \n import \nKMeans\nk \n= \n5\nkmeans \n= \nKMeans\n(\nn_clusters\n =\nk\n)\ny_pred \n= \nkmeans\n.\nfit_predict\n (\nX\n)\nprint (\ny_pred)\n>> array([4, 0, 1, ..., 2, 1, 0],  \ndtype=int32)\nprint (y_pred \n is \nkmeans\n.\nlabels_)\n>> True\n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#15": "Scikit-learn: Limiti K-means\nPossiamo ottenere le coordinate dei 5 centroidi: \nkmeans\n.\ncluster_centers_\n>> array([[-2.80389616, 1.80117999],\n[ 0.20876306, 2.25551336],\n[-2.79290307, 2.79641063],\n[-1.46679593, 2.28585348],\n[-2.80037642, 1.30082566]])\nE predire la classe di nuove istanze: \nX_new \n= \nnp\n.\narray\n([[\n0\n, \n2\n], [\n3\n, \n2\n], [\n-\n3\n, \n3\n], [\n-\n3\n, \n2.5\n]])\nkmeans\n.\npredict\n(\nX_new\n)\n>> array([1, 1, 2, 2], dtype=int32)\nNota\n : K-Means non si comporta molto bene con cluster che hanno \ndiametri molto distinti tra loro, poiché l'algoritmo valuta solo la distanza \ncol centroide.\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#16": "Scikit-learn: K-means\nInvece dell'\n hard clustering\n  visto ﬁnora, dove l'output è un singolo cluster, \npossiamo ottenere uno score (anche chiamato \n similarity score\n  o \nafﬁnity\n ) per \nogni cluster col \n soft clustering\n  mediante la funzione \n transform\n (): \nkmeans\n.\ntransform\n (\nX_new\n)\n>> array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\n[5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\n[1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\n[0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\nÈ possibile impostare i centroidi iniziali in modo manuale col parametro \ninit\n: \ngood_init \n = \nnp\n.\narray\n([[\n-\n3\n, \n3\n], [\n-\n3\n, \n2\n], [\n-\n3\n, \n1\n], [\n-\n1\n, \n2\n], [\n0\n, \n2\n]])\nkmeans \n= \nKMeans\n(\nn_clusters\n =\n5\n, \ninit\n=\ngood_init\n , \nn_init\n=\n1\n)\nL'iperparametro \n n_init\n  speciﬁca quante volte l'algoritmo deve essere \neseguito prima di selezionare la soluzione migliore ottenuta.\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#17": "Scikit-learn: K-means\nPer valutare la bontà della soluzione si misura il costo basato sulla cluster \nheterogeneity, chiamato anche \n inertia\n  del modello, cioè la distanza \nquadratica media con i centroidi. \nkmeans\n.\ninertia_\n>> 211.59853725816856\nkmeans\n.\nscore\n(\nX\n)\n>> -211.59853725816856\nNota\n : di default KMeans() usa l'inizializzazione dei centroidi proposta in K-\nMeans++. Se vuoi impiegare quella dell'algoritmo originale, imposta il \nparametro \n init='random'\n .\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#18": "Scikit-learn: K-means\nEsempio con un dataset toy: \nfrom \nsklearn.datasets \n import \nmake_blobs\nfrom \nsklearn.cluster \n import \nKMeans\n# generate synthetic two-dimensional data\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n1\n)\nkmeans \n= \nKMeans\n(\nn_clusters\n =\n3\n)\nkmeans\n.\nfit\n(\nX\n)\n19\nn_clusters=2 n_clusters=4 n_clusters=3",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#19": "Scikit-learn: Limiti K-means\nX_varied\n , \ny_varied \n = \nmake_blobs\n (\nn_samples\n =\n200\n,\ncluster_std\n =\n[\n1.0\n, \n2.5\n, \n0.5\n],\nrandom_state\n =\n170\n)\ny_pred \n= \nKMeans\n(\nn_clusters\n =\n3\n, \nrandom_state\n =\n0\n)\n.\nfit_predict\n (\nX_varied\n )\nmglearn\n.\ndiscrete_scatter\n (\nX_varied\n [:, \n0\n], \nX_varied\n [:, \n1\n], \ny_pred\n)\nplt\n.\nlegend\n([\n\"cluster 0\"\n , \n\"cluster 1\"\n , \n\"cluster 2\"\n ], \nloc\n=\n'best'\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\n20Secondo te è un output ideale?\n",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#2": "Clustering\nCi focalizziamo sugli algoritmi di \n clustering\n . Esistono anche \n trasformazioni \nunsupervised\n , utili per creare nuove rappresentazioni utili per analizzare \ndati o per darli in input a successivi algoritmi. Un approccio comune è la \nriduzione di dimensionalità\n , dove le N dimensioni corrispondenti alle \nfeatures vengono \"compresse\" in poche dimensione (es. 2 o 3). \nLa challenge del clustering è capire se l'algoritmo applicato su dati non \netichettati (cioè senza output) riesce comunque a trovare qualcosa di utile. \nEsempio: Classiﬁcation (sx) e Clustering senza label (dx) \n3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#20": "Scikit-learn: Limiti K-means\nX_varied\n , \ny_varied \n = \nmake_blobs\n (\nn_samples\n =\n200\n,\ncluster_std\n =\n[\n1.0\n, \n2.5\n, \n0.5\n],\nrandom_state\n =\n170\n)\ny_pred \n= \nKMeans\n(\nn_clusters\n =\n3\n, \nrandom_state\n =\n0\n)\n.\nfit_predict\n (\nX_varied\n )\nmglearn\n.\ndiscrete_scatter\n (\nX_varied\n [:, \n0\n], \nX_varied\n [:, \n1\n], \ny_pred\n)\nplt\n.\nlegend\n([\n\"cluster 0\"\n , \n\"cluster 1\"\n , \n\"cluster 2\"\n ], \nloc\n=\n'best'\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\n21K-means assume che ogni cluster abbia lo \nstesso diametro, e deﬁnisce la boundary tra i \ncluster esattamente a metà tra i due centroidi. \nAlcuni punti del graﬁco potevano essere \nclassiﬁcati in modo diverso.\n",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#21": "Scikit-learn: Limiti K-means\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n170\n, \nn_samples\n =\n600\n)\nrng \n= \nnp\n.\nrandom\n.\nRandomState\n (\n74\n)\n# trasforma i dati per mezzo di una distribuzione gaussiana\ntransformation \n = \nrng\n.\nnormal\n(\nsize\n=\n(\n2\n, \n2\n))\nX \n= \nnp\n.\ndot\n(\nX\n, \ntransformation\n )\nkmeans \n= \nKMeans\n(\nn_clusters\n =\n3\n)\nkmeans\n.\nfit\n(\nX\n)\ny_pred \n= \nkmeans\n.\npredict\n(\nX\n)\nplt\n.\nscatter\n(\nX\n[:, \n0\n], \nX\n[:, \n1\n], \nc\n=\ny_pred\n, \ncmap\n=\nmglearn\n.\ncm3\n)\nplt\n.\nscatter\n(\nkmeans\n.\ncluster_centers_\n [:, \n0\n], \nkmeans\n.\ncluster_centers_\n [:, \n1\n],\nmarker\n=\n'^'\n, \nc\n=\n[\n0\n, \n1\n, \n2\n], \ns\n=\n100\n, \nlinewidth\n =\n2\n, \ncmap\n=\nmglearn\n.\ncm3\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\n22\nSecondo te è un output ideale?",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#22": "Scikit-learn: Limiti K-means\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n170\n, \nn_samples\n =\n600\n)\nrng \n= \nnp\n.\nrandom\n.\nRandomState\n (\n74\n)\n# trasforma i dati per mezzo di una distribuzione gaussiana\ntransformation \n = \nrng\n.\nnormal\n(\nsize\n=\n(\n2\n, \n2\n))\nX \n= \nnp\n.\ndot\n(\nX\n, \ntransformation\n )\nkmeans \n= \nKMeans\n(\nn_clusters\n =\n3\n)\nkmeans\n.\nfit\n(\nX\n)\ny_pred \n= \nkmeans\n.\npredict\n(\nX\n)\nplt\n.\nscatter\n(\nX\n[:, \n0\n], \nX\n[:, \n1\n], \nc\n=\ny_pred\n, \ncmap\n=\nmglearn\n.\ncm3\n)\nplt\n.\nscatter\n(\nkmeans\n.\ncluster_centers_\n [:, \n0\n], \nkmeans\n.\ncluster_centers_\n [:, \n1\n],\nmarker\n=\n'^'\n, \nc\n=\n[\n0\n, \n1\n, \n2\n], \ns\n=\n100\n, \nlinewidth\n =\n2\n, \ncmap\n=\nmglearn\n.\ncm3\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\n23\nI dati sono distribuiti (\"allungati\") sulla diagonale, \nnon seguono una distribuzione sferica. \nL'algoritmo valuta solo la distanza dal centroide.",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#23": "Scikit-learn: Limiti K-means\nfrom \nsklearn.datasets \n import \nmake_moons\nX\n, \ny \n= \nmake_moons\n (\nn_samples\n =\n200\n, \nnoise\n=\n0.05\n, \nrandom_state\n =\n0\n)\nkmeans \n= \nKMeans\n(\nn_clusters\n =\n2\n)\nkmeans\n.\nfit\n(\nX\n)\ny_pred \n= \nkmeans\n.\npredict\n(\nX\n))\nplt\n.\nscatter\n(\nX\n[:, \n0\n], \nX\n[:, \n1\n], \nc\n=\ny_pred\n, \ncmap\n=\nmglearn\n.\ncm2\n, \ns\n=\n60\n)\nplt\n.\nscatter\n(\nkmeans\n.\ncluster_centers_\n [:, \n0\n], \nkmeans\n.\ncluster_centers_\n [:, \n1\n],\nmarker\n=\n'^'\n, \nc\n=\n[\nmglearn\n.\ncm2\n(\n0\n), \nmglearn\n.\ncm2\n(\n1\n)], \ns\n=\n100\n, \nlinewidth\n =\n2\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\n24\nShape complesse non sono \nvalutate correttamente.",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#24": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n25",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#3": "Preprocessing: Scaling\nAlcuni algoritmi di ML sono sensibili allo \n scaling\n  dei dati. Per tale motivo \nspesso si opera un rescaling e shifting. \nVediamo qualche esempio dalla libreria mglearn: \nmglearn\n.\nplots\n.\nplot_scaling\n ()\n4\n",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#4": "Preprocessing: Scaling\nIl diagramma mostra 4 scaler della libreria scikit-learn.  \nStandardScaler\n : garantisce media 0 e varianza 1  \nNon garantisce alcun intervallo max e min \nRobustScaler\n : approccio statistico simile,  \nusa mediana e quartili, è meno sensibile  \nagli \noutliers\n . \nMinMaxScaler\n : sposta i dati nell'intervallo [0,1] \nNormalizer\n : effettua un rescaling in modo che  \nla distanza euclidea sia pari a 1, cioè proietta  \ni punti su una circonferenza (o sfera) di raggio 1.  \nOgni punto è scalato per l'inverso della lunghezza.  \nUtile quando si ha interesse soprattutto riguardo la direzione, piuttosto \nche della lunghezza del feature vector.\n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#5": "Scikit-learn: Scaling\nUsiamo il breast cancer dataset per testare i vari scaling su un contesto \nsupervised con algoritmo SVM/SVC: \nfrom \nsklearn.datasets \n import \nload_breast_cancer\nfrom \nsklearn.model_selection \n import \ntrain_test_split\ncancer \n= \nload_breast_cancer\n ()\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\ncancer\n.\ndata\n, \ncancer\n.\ntarget\n,\nrandom_state\n =\n1\n)\nprint\n(\nX_train\n.\nshape\n)\nprint\n(\nX_test\n.\nshape\n)\n>> (426, 30)\n>> (143, 30)\nfrom \nsklearn.preprocessing \n import \nMinMaxScaler\nscaler \n= \nMinMaxScaler\n ()\n# consideriamo solo X_train, \n non il y_train\nscaler\n.\nfit\n(\nX_train\n)\n>> MinMaxScaler(copy=True, feature_range=(0, 1))\n# trasformiamo i dati\nX_train_scaled \n = \nscaler\n.\ntransform\n (\nX_train\n)\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#6": "Scikit-learn: Scaling\n# stampa i valori delle features prima e dopo il rescaling\nprint\n(\n\"transformed shape: {}\"\n .\nformat\n(\nX_train_scaled\n .\nshape\n))\nprint\n(\n\"per-feature minimum before scaling:\\n {}\"\n .\nformat\n(\nX_train\n.\nmin\n(\naxis\n=\n0\n)))\nprint\n(\n\"per-feature maximum before scaling:\\n {}\"\n .\nformat\n(\nX_train\n.\nmax\n(\naxis\n=\n0\n)))\nprint\n(\n\"per-feature minimum after scaling:\\n {}\"\n .\nformat\n(\nX_train_scaled\n .\nmin\n(\naxis\n=\n0\n)))\nprint\n(\n\"per-feature maximum after scaling:\\n {}\"\n .\nformat\n(\nX_train_scaled\n .\nmax\n(\naxis\n=\n0\n)))\n>> transformed shape: (426, 30)\nper-feature minimum before scaling:\n[ 6.98 9.71 43.79 143.50 0.05 0.02 0. 0. 0.11\n0.05 0.12 0.36 0.76 6.80 0. 0. 0. 0.\n0.01 0. 7.93 12.02 50.41 185.20 0.07 0.03 0.\n0. 0.16 0.06]\nper-feature maximum before scaling:\n[ 28.11 39.28 188.5 2501.0 0.16 0.29 0.43 0.2\n0.300 0.100 2.87 4.88 21.98 542.20 0.03 0.14\n0.400 0.050 0.06 0.03 36.04 49.54 251.20 4254.00\n0.220 0.940 1.17 0.29 0.58 0.15]\nper-feature minimum after scaling:\n[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\nper-feature maximum after scaling:\n[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#7": "Scikit-learn: Scaling\nApplichiamo lo scaling anche sul X_test \n# transform test data\nX_test_scaled \n = \nscaler\n.\ntransform\n (\nX_test\n)\n# print test data properties after scaling\nprint\n(\n\"per-feature minimum after scaling:\n\\n{}\"\n.\nformat\n(\nX_test_scaled\n .\nmin\n(\naxis\n=\n0\n)))\nprint\n(\n\"per-feature maximum after scaling:\n\\n{}\"\n.\nformat\n(\nX_test_scaled\n .\nmax\n(\naxis\n=\n0\n)))\n>> per-feature minimum after scaling:\n[ 0.034 0.023 0.031 0.011 0.141 0.044 0. 0. 0.154 -0.006\n-0.001 0.006 0.004 0.001 0.039 0.011 0. 0. -0.032 0.007\n0.027 0.058 0.02 0.009 0.109 0.026 0. 0. -0. -0.002]\nper-feature maximum after scaling:\n[ 0.958 0.815 0.956 0.894 0.811 1.22 0.88 0.933 0.932 1.037\n0.427 0.498 0.441 0.284 0.487 0.739 0.767 0.629 1.337 0.391\n0.896 0.793 0.849 0.745 0.915 1.132 1.07 0.924 1.205 1.631]\nNon sono nel range [0,1], è corretto?\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#8": "Scikit-learn: Scaling\nApplichiamo lo scaling anche sul X_test \n# transform test data\nX_test_scaled \n = \nscaler\n.\ntransform\n (\nX_test\n)\n# print test data properties after scaling\nprint\n(\n\"per-feature minimum after scaling:\n\\n{}\"\n.\nformat\n(\nX_test_scaled\n .\nmin\n(\naxis\n=\n0\n)))\nprint\n(\n\"per-feature maximum after scaling:\n\\n{}\"\n.\nformat\n(\nX_test_scaled\n .\nmax\n(\naxis\n=\n0\n)))\n>> per-feature minimum after scaling:\n[ 0.034 0.023 0.031 0.011 0.141 0.044 0. 0. 0.154 -0.006\n-0.001 0.006 0.004 0.001 0.039 0.011 0. 0. -0.032 0.007\n0.027 0.058 0.02 0.009 0.109 0.026 0. 0. -0. -0.002]\nper-feature maximum after scaling:\n[ 0.958 0.815 0.956 0.894 0.811 1.22 0.88 0.933 0.932 1.037\n0.427 0.498 0.441 0.284 0.487 0.739 0.767 0.629 1.337 0.391\n0.896 0.793 0.849 0.745 0.915 1.132 1.07 0.924 1.205 1.631]\nNon sono nel range [0,1], è corretto?  \nSì, perché il max e min sono stati ricavati dal training set, e possono \nessere distinti da quelli nel X_test.\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\23-Ex_08 Esercitazione Cluster-sbloccato.pdf#9": "Scikit-learn: Scaling\nCosa succede se applicassimo due distinti rescaling sul training e sul test \nset?\n10\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Clustering (Ex 09)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#1": "Sommario\nAgglomerative clustering \nHierarchical clustering \nDendograms \nDBSCAN \nAccelerated K-Means e Mini-batch K-Means \nSilhoutte score",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#10": "DBSCAN\nDBSCAN (density-based spatial clustering of applications with noise) è un \nalgoritmo che non richiede la scelta del numero di cluster a priori, inoltre può \ngestire conﬁgurazioni complesse dei dati (es. non sferiche). a differenza degli \napprocci visti ﬁnora. \nÈ generalmente più lento ma può scalare su dataset molto grandi. \nL'algoritmo identiﬁca i punti nel feature space che si trovano in regioni \n\"popolate\" o \n dense \n e costruisce i cluster in base ad esse. I punti in queste \nregioni si chiamano \n core samples\n .  \nCi sono 2 iperparametri: \n min_samples\n  e \neps\n. Se esistono almeno \n min_samples  \npunti con distanza inferiore a \n eps\n rispetto a un punto X, allora  X è un \n core \nsample\n . I core sample che sono vicini tra loro (distanza < eps) sono inseriti \nnello stesso cluster. Un cluster deve avere almeno min_samples punti. \nI punti che non sono assegnati a nessun cluster diventano i punti di partenza \nper una nuova iterazione. Quelli che non sono assegnati ad alcun cluster \nsono considerati rumore.\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#11": "DBSCAN\nNota\n : anche in DBSCAN la funzione predict() non è implementata. \nEsempio: \nfrom \nsklearn.cluster \n import \nDBSCAN\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n0\n, \nn_samples\n =\n12\n)\ndbscan \n= \nDBSCAN\n()\nclusters \n = \ndbscan\n.\nfit_predict\n (\nX\n)\nprint\n(\n\"Cluster memberships:\\n{}\"\n .\nformat\n(\nclusters\n ))\nCluster memberships:\n[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nPerché l'output è sempre -1?\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#12": "DBSCAN\nfrom \nsklearn.cluster \n import \nDBSCAN\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n0\n, \nn_samples\n =\n12\n)\ndbscan \n= \nDBSCAN\n()\nclusters \n = \ndbscan\n.\nfit_predict\n (\nX\n)\nprint\n(\n\"Cluster memberships:\\n{}\"\n .\nformat\n(\nclusters\n ))\nCluster memberships:\n[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nL'algoritmo usa il valore di default per eps che non è adatto per il piccolo dataset \nanalizzato. \nmglearn\n.\nplots\n.\nplot_dbscan\n ()\nmin_samples: 2 eps: 1.000000 cluster: [-1 0 0 -1 0 -1 1 1 0 1 -1 -1]\nmin_samples: 2 eps: 1.500000 cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\nmin_samples: 2 eps: 2.000000 cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\nmin_samples: 2 eps: 3.000000 cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 3 eps: 1.000000 cluster: [-1 0 0 -1 0 -1 1 1 0 1 -1 -1]\nmin_samples: 3 eps: 1.500000 cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\nmin_samples: 3 eps: 2.000000 cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\nmin_samples: 3 eps: 3.000000 cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 5 eps: 1.000000 cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nmin_samples: 5 eps: 1.500000 cluster: [-1 0 0 0 0 -1 -1 -1 0 -1 -1 -1]\nmin_samples: 5 eps: 2.000000 cluster: [-1 0 0 0 0 -1 -1 -1 0 -1 -1 -1]\nmin_samples: 5 eps: 3.000000 cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#13": "DBSCAN\nDiverse conﬁgurazioni variando gli iperparametri (i punti in bianco sono \nconsiderati rumore). Cosa noti? \n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#14": "DBSCAN\nIncrementando \n eps\n ci sono più punti che appartengono a clusters, e si \nriducono anche il numero di clusters. \nNota: è più facile impostare il valore di eps operando prima la normalizzazione delle features con \nStandardScaler\n  o \nMinMaxScaler.  \nIncrementando \n min_samples\n , meno punti saranno core points, e più punti \nsaranno etichettati come rumore. \n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#15": "DBSCAN - Esercizio\nEsercizio\n : impiega l'algoritmo DBSCAN sul moon dataset, con o senza la \nnormalizzazione. Valuta i cluster ottenuti.  \nX\n, \ny \n= \nmake_moons\n (\nn_samples\n =\n200\n, \nnoise\n=\n0.05\n, \nrandom_state\n =\n0\n)\n...\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#16": "DBSCAN - Esercizio\nEsercizio: impiega l'algoritmo DBSCAN sul moon dataset.  \nX\n, \ny \n= \nmake_moons\n (\nn_samples\n =\n200\n, \nnoise\n=\n0.05\n, \nrandom_state\n =\n0\n)\n# media 0 e varianza unitaria\nscaler \n= \nStandardScaler\n ()\nscaler\n.\nfit\n(\nX\n)\nX_scaled \n = \nscaler\n.\ntransform\n (\nX\n)\ndbscan \n= \nDBSCAN\n()\nclusters \n = \ndbscan\n.\nfit_predict\n (\nX_scaled\n )\nplt\n.\nscatter\n(\nX_scaled\n [:, \n0\n], \nX_scaled\n [:, \n1\n], \nc\n=\nclusters\n , \ncmap\n=\nmglearn\n.\ncm2\n, \ns\n=\n60\n)\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#17": "DBSCAN - Esercizio\nQuesta volta il clustering ottimale è identiﬁcato. \nEsercizio\n : Cosa succede se decrementiamo il valore di default di eps (0.5) a \n0.2, o lo incrementiamo a 0.7?\n18\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#18": "DBSCAN - Esercizio\nQuesta volta il clustering ottimale è identiﬁcato. \neps = 0.2 -> 8 clusters \neps = 0.7 -> 1 cluster\n19\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#19": "Accelerated K-Means e Mini-batch K-Means\nL'algoritmo \n accelerated\n  K-Means evita di calcolare distanze non \nnecessarie.  \nImpiega la \n triangle inequality \n  AC< AB+BC, dove A,B e C sono 3 punti, e \ntiene traccia del valore del upper e lower bounds delle distanze tra \ncentroidi e istanze. È l'approccio normalmente impiegato \nnell'implementazione KMeans di scikit-learn. \nL'approccio \n Mini-batch\n  seleziona un piccolo insieme di istanze su cui \nvalutare le distanze, creando una \n inerzia\n  nella modiﬁca dei clusters. \nIncrementa la velocità, ma se il numero di cluster è elevato si ottengono \nconﬁgurazioni meno ottimali. \nfrom \nsklearn.cluster \n import \nMiniBatchKMeans\nminibatch_kmeans \n = \nMiniBatchKMeans\n (\nn_clusters\n =\n5\n)\nminibatch_kmeans\n .\nfit\n(\nX\n)\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#2": "Agglomerative Clustering\nL'algoritmo segue i seguenti passi: \nDeﬁniamo una serie di cluster, ognuno con una serie di istanze al suo \ninterno.  \nAd ogni iterazione \n uniamo\n  i due cluster valutati maggiormente simili.  \nAl raggiungimento di un certo \n criterio di stop\n  ci fermiamo. Tipicamente \nil criterio è basato sul numero di cluster desiderato. \nQuali criteri di unione (merge) puoi immaginare?\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#20": "Accelerated K-Means e Mini-batch K-Means\nMini-batch vs K-Means tradizionale impiegando diversi numeri di clusters \nk. Con un numero elevato di clusters l'inerzia si riduce notevolmente, e si \nlimita il tempo di training. \nRicordiamo che l'\n inertia\n  del modello e' la distanza quadratica media con i \ncentroidi, cioè la\n  cluster heterogeneity \n (vedi lezione sul clustering).\n21\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#21": "Il numero ottimale di clusters\nAlcuni algoritmi richiedono di speciﬁcare il numero di clusters, che \npossono produrre risultati molto diversi anche con valori simili: \nPotremmo scegliere il modello con minore inertia, ma nell'esempio con \nk=3 otteniamo 653.2, mentre con k=8 si ha inertia=119.1. Più cluster \nabbiamo, più si riduce la distanza col rispettivo centroide e la rispettiva \ninertia del modello. \n22\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#22": "Richiami: elbow method\nSe graﬁchiamo il valore dell'inertia in funzione del numero di clusters \n k\n si \nvede chiaramente con dopo un \"drop\" elevato, il decremento si riduce \nnotevolmente. \n23\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#23": "Silhoutte score\nUn metodo più formale è il calcolo del valore della silhoutte su tutte le \nistanze. Si ricava con\n  (b-a)/max(a,b)\n  dove \n a\n è la distanza media rispetto \nalle altre istanze nel cluster, \n b\n è la \n mean nearest-cluster distance\n , cioè la \ndistanza media delle istanze rispetto al cluster più vicino. \nIl coefﬁciente varia in [-1,+1], dove un valore vicino:  \na +1 indica una istanza vicina al proprio cluster e lontana dagli altri,  \nallo 0, istanza vicina al boundary del cluster \na -1 l'istanza potrebbe essere stata assegnata al cluster sbagliato. \nfrom \nsklearn.metrics \n import \nsilhouette_score\nsilhouette_score\n (\nX\n, \nkmeans\n.\nlabels_\n)\n0.655517642572828\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#24": "Silhoutte score\nUn valore pari a 4 di cluster massimizza il valore della silhoutte\n25\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#25": "Silhoutte diagram\nGraﬁci del valore di silhoutte per ogni istanza, ordinati per il cluster di \nappartenenza. Per k=4 abbiamo che gran parte delle istanze sorpassano il \nvalore di silhoutte associato a quella conﬁgurazione (linea rossa)\n26\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#26": "Silhoutte: Esercizio\nEsercizio\n : riprendi il dataset blobs e l'approccio agglomerative e ricava il \nnumero ottimale di cluster col approccio \n Agglomerative\n . \nfrom \nsklearn.cluster \n import \nAgglomerativeClustering\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n1\n)\n...\n27",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#27": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n28",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#3": "Agglomerative Clustering\nL'algoritmo segue i seguenti passi: \nDeﬁniamo una serie di cluster, ognuno con una serie di istanze al suo \ninterno.  \nAd ogni iterazione \n uniamo\n  i due cluster valutati maggiormente simili.  \nAl raggiungimento di un certo \n criterio di stop\n  ci fermiamo. Tipicamente \nil criterio è basato sul numero di cluster desiderato. \nIn scikit-learn la fusione di cluster può seguire uno dei seguenti criteri: \nward\n  (default): si scelgono i cluster i cui merge riducono al massimo \nl'incremento di varianza tra tutti i cluster. Di solito porta ad avere \ncluster di dimensione confrontabile. \naverage\n : i due cluster che hanno distanza media tra tutti punti minore \ncomplete\n : i due cluster che hanno distanza massima tra due punti \nminore.\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#4": "Agglomerative Clustering\nEsempio libreria \n mglearn\n : \nmglearn\n.\nplots\n.\nplot_agglomerative_algorithm\n ()\n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#5": "Scikit-learn: Agglomerative Clustering\nEsempio scikit-learn: \nfrom \nsklearn.cluster \n import \nAgglomerativeClustering\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n1\n)\nagg \n= \nAgglomerativeClustering\n (\nn_clusters\n =\n3\n) # parametro obbligatorio\nassignment \n = \nagg\n.\nfit_predict\n (\nX\n)\nmglearn\n.\ndiscrete_scatter\n (\nX\n[:, \n0\n], \nX\n[:, \n1\n], \nassignment\n )\nplt\n.\nxlabel\n(\n\"Feature 0\"\n )\nplt\n.\nylabel\n(\n\"Feature 1\"\n )\nNota\n : l'agglomerative clustering non può predire un cluster per nuovi dati, \nperciò non è possibile usare la funzione predict().\n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#6": "Hierarchical clustering\nAd ogni passo dell'algoritmo agglomerative si creano diverse \nconﬁgurazioni che possono essere rilevanti per analizzare i dati, soprattutto \nse si hanno poche features. \nmglearn\n.\nplots\n.\nplot_agglomerative\n ()\n7\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#7": "Dendrograms\nPer dataset con molte features è comunque possibile rappresentare i dati \nsottoforma di dendogrammi. Scikit-learn non implementa tale funzionalità, \nusiamo la libreria \n scipy\n : \nfrom \nscipy.cluster.hierarchy \n import \ndendrogram\n , \nward\nX\n, \ny \n= \nmake_blobs\n (\nrandom_state\n =\n0\n, \nn_samples\n =\n12\n)\n# ward clustering sui dati\n# la funzione restituisce un array che contiene le distanze ricavate\n# durante il clustering agglomerative\nlinkage_array \n = \nward\n(\nX\n)\n# Visualizziamo il dendogramma con le distanze tra i cluster\ndendrogram\n (\nlinkage_array\n )\n# Nel plot aggiungiamo il numero di clsuter\nax \n= \nplt\n.\ngca\n()\nbounds \n= \nax\n.\nget_xbound\n ()\nax\n.\nplot\n(\nbounds\n, [\n7.25\n, \n7.25\n], \n'--'\n, \nc\n=\n'k'\n)\nax\n.\nplot\n(\nbounds\n, [\n4\n, \n4\n], \n'--'\n, \nc\n=\n'k'\n)\nax\n.\ntext\n(\nbounds\n[\n1\n], \n7.25\n, \n' two clusters'\n , \nva\n=\n'center'\n , \nfontdict\n =\n{\n'size'\n: \n15\n})\nax\n.\ntext\n(\nbounds\n[\n1\n], \n4\n, \n' three clusters'\n , \nva\n=\n'center'\n , \nfontdict\n =\n{\n'size'\n: \n15\n})\nplt\n.\nxlabel\n(\n\"Sample index\"\n )\nplt\n.\nylabel\n(\n\"Cluster distance\"\n )\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#8": "Dendrograms\nDal seguente diagramma come immagini che si sia comportato l'algoritmo \ndi clustering?\n9\n",
    "data_test\\rootfolder\\università\\MachineLearning\\24-Ex_09 Esercitazione Cluster-sbloccato.pdf#9": "Dendrograms\nSulle ascisse abbiamo le istanze numerate (da 0 a 11). Salendo si notano i \nnuovi cluster che uniscono le istanze, oppure cluster già presenti. \nEs. al principio si uniscono 1 e 4 in un cluster, poi 6 e 9 in un altro, etc. \nIn cima abbiamo 2 cluster, uno con 11,0,5,10,7,6 e 9; l'altro coi \nrestanti punti. \nLa lunghezza in verticale delle linee rappresentano le distanze tra i due \ncluster o punti che si fondono. \n10\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Clustering (Ex 10)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#1": "Sommario\nImage segmentation \nClustering per il preprocessing \nGrid search \nActive learning \nGaussian Mixtures",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#10": "Clustering per il semi-supervised learning\nSoluzione: \nk \n= \n50\nkmeans \n= \nKMeans\n(\nn_clusters\n =\nk\n)\nX_digits_dist \n = \nkmeans\n.\nfit_transform\n (\nX_train\n)\nrepresentative_digit_idx \n = \nnp\n.\nargmin\n(\nX_digits_dist\n , \naxis\n=\n0\n)\nX_representative_digits \n = \nX_train\n[\nrepresentative_digit_idx\n ]\n# facciamo un labeling manuale delle 50 cifre\ny_representative_digits \n = \nnp\n.\narray\n([\n4\n, \n8\n, \n0\n, \n6\n, \n8\n, \n3\n, \n...\n, \n7\n, \n6\n, \n2\n, \n3\n, \n1\n, \n1\n])\nl\nog_reg \n= \nLogisticRegression\n ()\nlog_reg\n.\nfit\n(\nX_representative_digits\n , \ny_representative_digits\n )\nlog_reg\n.\nscore\n(\nX_test\n, \ny_test\n)\n>> 0.9244444444444444\n11\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#11": "Clustering per il preprocessing\nEtichettiamo le restanti istanze nei cluster con le label che abbiamo creato \nin modo manuale (\n label propagation\n ), e proviamo nuovamente ad \naddestrare la logistic regression: \ny_train_propagated \n = \nnp\n.\nempty\n(\nlen\n(\nX_train\n), \ndtype\n=\nnp\n.\nint32\n)\nfor \ni \nin \nrange\n(\nk\n):\n  \ny_train_propagated\n [\nkmeans\n.\nlabels_\n==\ni\n] \n= \ny_representative_digits\n [\ni\n]\nlog_reg \n = \nLogisticRegression\n ()\nlog_reg\n.\nfit\n(\nX_train\n, \ny_train_propagated\n )\nlog_reg\n.\nscore\n(\nX_test\n, \ny_test\n)\n>> 0.9288888888888889\nUn leggero incremento. Non conviene propagare le label alle istanze \nlontano dal centroide e vicine al boundary.\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#12": "Clustering per il preprocessing\nProviamo a fare propagation solo al 20% delle istanze più vicine al centroide \npercentile_closest \n = \n20\nX_cluster_dist \n = \nX_digits_dist\n [\nnp\n.\narange\n(\nlen\n(\nX_train\n)), \nkmeans\n.\nlabels_\n]\nfor \ni \nin \nrange\n(\nk\n):\nin_cluster \n = \n(\nkmeans\n.\nlabels_ \n == \ni\n)\ncluster_dist \n = \nX_cluster_dist\n [\nin_cluster\n ]\ncutoff_distance \n = \nnp\n.\npercentile\n (\ncluster_dist\n , \npercentile_closest\n )\nabove_cutoff \n = \n(\nX_cluster_dist \n > \ncutoff_distance\n )\nX_cluster_dist\n [\nin_cluster \n & \nabove_cutoff\n ] \n= -\n1\npartially_propagated \n = \n(\nX_cluster_dist \n != -\n1\n)\nX_train_partially_propagated \n = \nX_train\n[\npartially_propagated\n ]\ny_train_partially_propagated \n = \ny_train_propagated\n [\npartially_propagated\n ]\nlog_reg \n = \nLogisticRegression\n ()\nlog_reg\n.\nfit\n(\nX_train_partially_propagated\n , \ny_train_partially_propagated\n )\nlog_reg\n.\nscore\n(\nX_test\n, \ny_test\n)\n>> 0.9422222222222222\n94.2% è molto vicino al risultato ottenuto con l'addestramento sull'interno \ndataset etichettato (96.7%). In effetti le istanze etichettate automatichemente \ncon \nlabel propagation\n  sono corrette al 99%.\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#13": "Active Learning\nÈ un approccio iterativo dove l'algoritmo propone alcune istanze per essere \netichettate manualmente. Ci sono diverse strategie per selezionare queste \nistanze: \nquelle su cui l'algoritmo mostra maggiore incertezza,  \nquelle che potenzialmente riducono maggiormente il tasso di errore, \nquelle su cui diversi modelli (es. SVM, Random forest, etc) trovano \nmaggiore disaccordo. \nIl procedimento continua ﬁnché non si hanno miglioramenti di \nperformance tangibili. \n14",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#14": "Gaussian Mixtures\nIl \nGaussian mixture model (GMM)\n  suppone che le istanze siano generate \nda un mix di diverse distribuzioni gaussiano i cui parametri sono incogniti. \nLe istanze generate da una singola distribuzione formano un cluster a \nforma di ellissoide, con diverse forme, dimensioni, densità e orientamenti.  \nAl principio non sappiamo quali distribuzioni generino una speciﬁca \nistanza. Occorre stimarle durante la fase di training. \n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#15": "Gaussian Mixtures\nLa classe \n GaussianMixture\n  suppone di conoscere in anticipo il numero \n k\n di \ndistribuzioni. \nPer ogni istanza, prendiamo casualmente un cluster dei \n k\n. La probabilità di \nscegliere il \n j\n-mo cluster e deﬁnita dal peso del cluster \n ϕ\n(j)\n. L'indice del \ncluster selezionato per l'istanza \n i\n-ma è \n z\n(i)\n. \nSe \nz\n(i)\n=\nj\n, la posizione della istanza \n x\n(i)\n è campionata in modo casuale da \nuna distribuzione gaussiana con media \n μ\n(j)\n e matrice di covarianza \n Σ\n(j)\n, e la \nindichiamo con:\n16\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#16": "Gaussian Mixtures\nRappresentiamo il modello graﬁcamente dove si notano le dipendenze tra le \nvariabili casuali. Le circonferenze sono le variabili casuali, i quadrati i parametri. \nI rettangoli sono \n plates\n , e indicano che i loro contenuti sono ripetuti diverse volte \n(es. \nm\n volte corrispondenti al numero di variabili casuali, o \n k\n volte, cioè il \nnumero di medie e covarianze, ed un solo array di parametri \n ϕ\n). \nOgni variabile \n z\n(i) \nè ricavata da una distribuzione \n categorical\n  con pesi \n ϕ\n. \nOgni \nvariabile \n x\n(i)\n è ricavata da una distribuzione gaussiana con media e matrice di \ncovarianza deﬁnita dal suo cluster \n z\n(i)\n.\n17\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#17": "Gaussian Mixtures\n(cont...) Le frecce rappresentano dipendenze tra le variabili. Ad esempio, \nz\n(i)\n dipende dal vettore dei pesi \n ϕ\n, per ogni \n i\n. \nA seconda del valore di \n z\n(i)\n, l'istanza \n x\n(i)\n è campionata da una diversa \ndistribuzione (freccia ondulata).  \nI nodi colorati rappresentano dati noti, gli altri contengono parametri da \nstimare.\n18\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#18": "Scikit-learn: Gaussian Mixtures\nUna volta addestrato il modello impiegando la classe \n GaussianMixture\n () \npossiamo ottenere facilmente i parametri \n ϕ\n, \nμ\n e \nΣ\n: \nfrom \nsklearn.mixture \n import \nGaussianMixture\ngm \n= \nGaussianMixture\n (\nn_components\n =\n3\n, \nn_init\n=\n10\n)\ngm\n.\nfit\n(\nX\n)\n# mostriamo i parametri stimati\ngm\n.\nweights_\n>>> array([0.20965228, 0.4000662 , 0.39028152])\ngm\n.\nmeans_\n>>> array([[ 3.39909717, 1.05933727],\n[-1.40763984, 1.42710194],\n[ 0.05135313, 0.07524095]])\ngm\n.\ncovariances_\n>>> array([[[ 1.14807234, -0.03270354],\n[-0.03270354, 0.95496237]],\n[[ 0.63478101, 0.72969804],\n[ 0.72969804, 1.1609872 ]],\n[[ 0.68809572, 0.79608475],\n[ 0.79608475, 1.21234145]]])\nE ora?\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#19": "Scikit-learn: Gaussian Mixtures\nImpieghiamo l'algoritmo di Expectation Maximization (EM), simile come \nidea al K-Means. Inizializza i parametri dei cluster in modo casuale, e \niterativamente raggiunge lo stato di convergenza. Assegna le istanze ai \ncluter (\n expectation step\n ) e poi aggiorna i cluster (\n maximization step\n ). Ricava \ni valori del centro dei cluster (medie), la loro dimensione, forma e \norientazione (matrice di covarianze) e il relativo peso (\n Φ\n). \nEM usa un soft clustering, stimando la probabilità di appartenenza. Durante \nil \nmaximization step\n  ogni cluster è aggiornato con tutte le istanze nel \ndataset, dove ogni istanza è pesata con la relativa probabilità di \nappartenenza (chiamata anche \n responsability\n  del cluster per l'istanza). \nPerciò ogni cluster viene aggiornato maggiormente dalle istanze che più \nverosimilmente appartengono ad esso. \nL'algoritmo richiede diversi run (es. n_init=10), poiché può facilmente \nprodurre cattive conﬁgurazioni.\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#2": "Image segmentation e Instance segmentation\nNella \n Image segmentation\n  si suddivide una immagine in porzioni, dove \nogni porzione contiene pixel che rappresentano un oggetto associato alla \nporzione (es. pedone, portiera di un auto, etc). Una porzione può \ncontenere istanze multiple di un oggetto. \nNella Instance segmentation ogni porzione contiene una singola istanza \n(es. ogni pedone ha una segmentation distinta). \nAffrontiamo il problema con un approccio basato sulla \n color segmentation.  \nNon è il più efﬁcace, ma per alcuni domini è sufﬁciente (es. analizzare la \npercentuale di zone verdi da immagini satellitari). \nAssociamo un pixel ad un segmento se ha colore simile. \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#20": "Scikit-learn: Gaussian Mixtures\nPossiamo valutare la convergenza e il numero di iterazioni: \ngm\n.\nconverged_\n>> True\ngm\n.\nn_iter_\n>> 3\nUna volta ricavati i parametri possiamo predire il cluster (hard clustering) o \ni clusters (soft clustering) più adatti per una certa istanza: \ngm\n.\npredict\n(\nX\n)\n>> array([2, 2, 1, ..., 0, 0, 0])\ngm\n.\npredict_proba\n (\nX\n)\n>> array([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01],\n[1.64685609e-02, 6.75361303e-04, 9.82856078e-01],\n[2.01535333e-06, 9.99923053e-01, 7.49319577e-05],\n...,\n[9.99999571e-01, 2.13946075e-26, 4.28788333e-07],\n[1.00000000e+00, 1.46454409e-41, 5.12459171e-16],\n[1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#21": "Scikit-learn: Gaussian Mixtures\nEssendo un modello generativo, puoi anche generare nuove istanze dal \nmodello: \nX_new\n, \ny_new \n= \ngm\n.\nsample\n(\n6\n)\nX_new\n>> array([[ 2.95400315, 2.63680992],\n[-1.16654575, 1.62792705],\n[-1.39477712, -1.48511338],\n[ 0.27221525, 0.690366 ],\n[ 0.54095936, 0.48591934],\n[ 0.38064009, -0.56240465]])\ny_new\n>>array([0, 1, 2, 2, 2, 2])\noppure stimare la densità del modello per un certo punto. Col metodo \nscore_samples\n () si ricava la log della \n probability density function \n (PDF): \ngm\n.\nscore_samples\n (\nX\n)\n>> array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,\n-4.39802535, -3.80743859])\nPer stimare la prob che una istanza cada in una certa regione occorre \nintegrare la funzione sull'intervallo.\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#22": "Scikit-learn: Gaussian Mixtures\nIl graﬁco precedente rappresenta la densità per mezzo dei colori: \nÈ stato facile rappresentare i dati perché abbiamo usato una gaussiana 2D. \nPer altri dataset occorrono più dimensioni (e molte più istanze). Se \nl'algoritmo non riesce a convergere si possono impostare vincoli sulla \nforma e orientazione delle distribuzioni (es. parametro \n covariance_type\n )\n23\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#23": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#3": "Scikit-learn: Image segmentation\nCarichiamo una immagine, che sarà memorizzata in un array 3d, dove la \nprofondità (numero di canali) rappresenta l'intensità RGB in [0,1], o \n[0,255] se si impiega imageio.imread() \nfrom \nmatplotlib.image \n import \nimread\nimport\n urllib2\nf = \nurllib2.urlopen\n (\n'\nhttp://.../image.png\n '\n)\nf = \nos\n.\npath\n.\njoin\n(\n\"images\"\n ,\n\"image.png\"\n )       # in alternativa\nimage \n= \nimread\n(f)\nimage\n.\nshape\n>> (533, 800, 3)\nNota: alcune immagini hanno meno canali (es. scala di grigio), o più canali \n(es. alpha channel, segnale infrared).\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#4": "Scikit-learn: Image segmentation\nIl seguente codice ridimensiona l'array come un array 1D, dove ogni \nelemento è una tripla. Dopodiché fa clustering raggruppando pixel con \ncolori simili. Inﬁne ricava il colore \"medio\" per mezzo del centroide e \nriordina il risultato come le dimensioni dell'immagine iniziale: \nX \n= \nimage\n.\nreshape\n(\n-\n1\n, \n3\n)\nkmeans \n= \nKMeans\n(\nn_clusters\n =\n8\n)\n.\nfit\n(\nX\n)\nsegmented_img \n = \nkmeans\n.\ncluster_centers_\n [\nkmeans\n.\nlabels_\n]\nsegmented_img \n = \nsegmented_img\n .\nreshape\n(\nimage\n.\nshape\n)\n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#5": "Clustering per il preprocessing\nIl clustering può essere usato anche come tecnica di \n dimensionality \nreduction\n , ad esempio per rendere più adatto un dataset per un approccio \nsupervised, riducendo il numero di features e la dimensione totale.  \nEs. prendiamo il MNIST dataset (1797 immagini 8x8 in scala di grigio) e \nimpieghiamo la logistic regression per la classiﬁcazione: \nfrom \nsklearn.datasets \n import \nload_digits\nX_digits\n , \ny_digits \n = \nload_digits\n (\nreturn_X_y\n =\nTrue\n)\nfrom \nsklearn.model_selection \n import \ntrain_test_split\nX_train\n, \nX_test\n, \ny_train\n, \ny_test \n= \ntrain_test_split\n (\nX_digits\n , \ny_digits\n )\nfrom \nsklearn.linear_model \n import \nLogisticRegression\nlog_reg \n = \nLogisticRegression\n (\nrandom_state\n =\n42\n)\nlog_reg\n.\nfit\n(\nX_train\n, \ny_train\n)\nlog_reg\n.\nscore\n(\nX_test\n, \ny_test\n)\n0.9666666666666667\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#6": "Clustering per il preprocessing\nUsiamo inizialmente il cluster per raggruppare le immagini simili usando \n50 clusters (usarne solo 10 non è ottimale poiché esistono molti modi per \nrappresentare la stessa cifra), e usiamo la distanza da questi cluster come \ninput al posto dell'immagine originale: \nfrom \nsklearn.pipeline \n import \nPipeline\npipeline \n = \nPipeline\n ([\n    (\n\"kmeans\"\n , \nKMeans\n(\nn_clusters\n =\n50\n)),\n    (\n\"log_reg\"\n , \nLogisticRegression\n ()),\n])\npipeline\n .\nfit\n(\nX_train\n, \ny_train\n)\npipeline\n .\nscore\n(\nX_test\n, \ny_test\n)\n0.9822222222222222\nAbbiamo dimezzato il tasso d'errore! \nNota: Pipeline combina più operazioni di \n trasformazione\n  sui dati, cioè \ndevono comparire classi che implementano \n ﬁt\n() e \ntransform\n (). Per ultimo \nc'è l'estimator che deve includere l'implementazione di \n ﬁt\n().\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#7": "Tuning degli iperparametri: grid search\nPer il tuning degli iperparametri (numero di cluster) possiamo impiegare lo \nscore della fase supervised, senza il bisogno di calcolare la silhoutte. \nLa classe GridSearchCV ottimizza il valore degli iperparametri in modo \nesaustivo iterando su intervalli (approccio grid-search con cross-\nvalidazione). \nfrom \nsklearn.model_selection \n import \nGridSearchCV\n# dizionario chiave->valore, dove la chiave è il nome del iperparametro,\n# il valore è il range di valori da valutare\nparam_grid \n = \ndict\n(\nkmeans__n_clusters\n =\nrange\n(\n2\n, \n100\n))\ngrid_clf \n = \nGridSearchCV\n (\npipeline\n , \nparam_grid\n , \ncv\n=\n3\n, \nverbose\n=\n2\n)\ngrid_clf\n .\nfit\n(\nX_train\n, \ny_train\n)\ngrid_clf\n .\nbest_params_\n>> {'kmeans__n_clusters': 90}\ngrid_clf\n .\nscore\n(\nX_test\n, \ny_test\n)\n>> 0.9844444444444445\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#8": "Clustering per il semi-supervised learning\nPotremmo avere dataset poche istanze con label (etichettate), e molte \nistanze senza label. Non è sufﬁciente per l'addestramento supervised. \nAd esempio, impiegando solo 50 istanze dal dataset delle cifre otteniamo \nuna accuracy piuttosto bassa: \nn_labeled \n = \n50\nlog_reg \n = \nLogisticRegression\n ()\nlog_reg\n.\nfit\n(\nX_train\n[:\nn_labeled\n ], \ny_train\n[:\nn_labeled\n ])\nlog_reg\n.\nscore\n(\nX_test\n, \ny_test\n)\n>> 0.826666666666666\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\25-Ex_10 Esercitazione Cluster-sbloccato.pdf#9": "Clustering per il semi-supervised learning\nEsercizio: prova a fare il clustering del dataset delle cifre (split X_train) \nusando 50 clusters impiegando KMeans. Per ogni cluster trova la cifra con \ndistanza minima dal centroide dal cluster. Usa queste cifre come il nuovo \ndataset di 50 immagini per addestrare la logistic regressione e valuta la \ndifferena nelle performance. \nk \n= \n50\nkmeans \n= \nKMeans\n(\nn_clusters\n =\nk\n)\n...\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nIntroduzione alle  \nReti Neurali Artiﬁciali\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#1": "Sommario\nIntroduzione alle Reti Neurali Artiﬁciali \nUnità di Calcolo nelle Reti Neurali  \nReti Neurali a uno strato alimentate in avanti (percettroni) \nReti Neurali multistrato alimentate in avanti \nAlgoritmo di Back-propagation \nEsempio di esecuzione dell’algoritmo\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#10": "ReLU \n(Rectiﬁed Linear Unit)\ngg(in i) = max(in i,0)\n<latexit sha1_base64=\"fdZuD83MuMKqvRY6gR2pKYYhfhM=\">AAACH3icbVDLTgJBEJzFF+IL9ehlIjGBaMgumujFhOjFIybySICQ3qHBibOPzPQaCOEj/AS/wquevBmvHPwXl8cBwTrVVHWnp8oNlTRk2yMrsbK6tr6R3Extbe/s7qX3DyomiLTAsghUoGsuGFTSxzJJUlgLNYLnKqy6T7djv/qM2sjAf6B+iE0Pur7sSAEUS630aTfbIOzRQPrDlszxaz59etAbzjtn3M610hk7b0/Al4kzIxk2Q6mV/mm0AxF56JNQYEzdsUNqDkCTFAqHqUZkMATxBF2sx9QHD01zMAk15CeRAQp4iJpLxScizm8MwDOm77nxpAf0aBa9sfifV4+oc9WMU4URoS/Gh0gqnBwyQsu4LeRtqZEIxj9HLn0uQAMRaslBiFiM4vpScR/OYvplUinknfN84f4iU7yZNZNkR+yYZZnDLlmR3bESKzPBXtgbe2cf1qv1aX1Z39PRhDXbOWR/YI1+AVy+orM=</latexit>\nini 0\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#11": "Funzione Gradino\nLa motivazione biologica è che un 1 rappresenta \nl’emissione di un impulso lungo l’assone, mentre uno 0 \nrappresenta l’assenza di una tale emissione.\nLa soglia individua l’ingresso pesato minimo che fa in \nmodo che il neurone invii l’impulso.\nLa funzione a gradino ha una soglia t tale che il \nrisultato è 1 quando l’ingresso supera questa soglia.\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#12": "Funzione Gradino\nIn molti casi risulterà dal punto di vista matematico \nconveniente sostituire la soglia con un peso d’ingresso \nextra.\nQuesto consentirà di avere un elemento di \napprendimento più semplice in quanto si dovrà \npreoccupare solo di modiﬁcare dei pesi anziché \nmodiﬁcare sia dei pesi che delle soglie.\nQuindi, invece di avere una soglia t, considereremo per \nciascuna unità un ingresso aggiuntivo, la cui attivazione \na\n0\n è ﬁssata a -1.\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#13": "Da altri\nneuroni\nUnità di Calcolo nelle Reti Neurali\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#14": "Unità di Calcolo nelle Reti Neurali \nIl peso extra W\n 0,i\n associato ad a\n 0\n ricopre il ruolo della \nsoglia t, dove W\n 0,i\n = t e a\n 0 \n= -1. \nIn questo modo tutte le unità possono avere una soglia \nﬁssata a 0.ai=gradinot0\n@nX\nj=1Wj,iaj1\nA=gradino00\n@nX\nj=0Wj,iaj1\nA\n<latexit sha1_base64=\"bnb4YwLA9yDvbdcHkbSf+9auugw=\">AAACfXicfVFNb9NAEF2brxI+moI4IcGIqFKRosgulcqlUgUXjkUiTaU4WOPNJJ12vbZ2x4jK8g/gJ3LgN/ATwA45QFrxTk9v5r3ZnclKw16i6HsQ3rp95+69rfu9Bw8fPd7u7zw59UXlNI11YQp3lqEnw5bGwmLorHSEeWZokl2+7+qTL+Q8F/aTXJU0y3FpecEapZXS/jdMGY4gEfrauuulwznbokkFEkML2YPEV3laXxzFzefaNjBp+ZAbSIaA6QUkjpfn8vqmhGgzIfpvQi/tD6JRtAJcJ/GaDNQaJ2n/RzIvdJWTFW3Q+2kclTKr0QlrQ00vqTyVqC9xSdOWWszJz+rVyhrYrTxKASU5YAMrkf521Jh7f5VnbWeOcu43a514U21ayeLtrGZbVkJWd4OEDa0Gee24vQXBnB2JYPdyArag0aEIOQbUuhWr9jjdPuLN318np/uj+M1o/+PB4PjdejNb6rl6pfZUrA7VsfqgTtRYafUzeBa8CF4Gv8LdcBiO/rSGwdrzVP2D8PA3NkzAIQ==</latexit>\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#15": "Funzione Gradino \n(con soglia zero)\ng\n0g(in i)=⇢1i f i n i\u00000\n0 otherwise\n<latexit sha1_base64=\"xUm3gk/S64XZW4FVFriL5T2ehIM=\">AAACfnicbVHLbtNAFB2bR4t5BSqxYcEVKVVZkNoFqd0gVbBhWSTSVspE0Xhy41x1PDYz19DI8g/whyz4Bz4BO7VQaTmro3PuY+bctDTkOY5/BuGt23fubmzei+4/ePjo8eDJ0xNfVE7jWBemcGep8mjI4piJDZ6VDlWeGjxNzz92/uk3dJ4K+4VXJU5zlVlakFbcSrPBj2xXMl5wTbaZ0WuA9yANLljWADLFjGytnFOrpjamgQhaJLADMk+Li5oWsH2lG2SGXyHebkDKy9L4b2nBS3TfyWNnRhLtvJ8L0lG25BFEs8EwHsVrwE2S9GQoehzPBr/kvNBVjpa1Ud5PkrjkaTuXSRtsIll5LJU+VxlOWmpVjn5arzNr4FXlFRdQogMysBbxaketcu9XedpW5oqX/rrXif/zJhUvDqdtIGXFaHW3iMngepHXjtpjIMzJIbPqXo5AFrRyihkdgdK6Fav2Ol0eyfXf3yQn+6Pk7Wj/87vh0Yc+mU3xXLwUuyIRB+JIfBLHYiy0+B08C14EEIpwJ3wT7l2WhkHfsyX+QXj4B8OsvNA=</latexit>\nini\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#16": " Vediamo adesso alcuni semplici esempi di \nreti neurali per la realizzazione di\nPorte Logiche\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#17": "Porte logiche\nLavorando in modo  adeguato sui pesi si possono realizzare \nporte logiche con una rete neurale formata da un solo \nneurone:\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#18": "Porta AND\n•\n(Soglia t =1.5) \n•\n W\n0\n=1.5 \n•\nW\n1\n=1 \n•\nW\n2\n=1 \n•\na\n0\n= -1 \n•\nPer a\n1\n=1 e a\n2\n= 1si ha: in=0.5 => \n g(in)=1\n  (funzione g a gradino) \n•\nPer a\n1\n=1 e a\n2\n= 0 si ha: in=-0,5 => \n g(in)=0   \n19",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#19": "Porta OR\n•\n(Soglia t = 0.5)\n•\n W\n0\n=0.5\n•\nW\n1\n=1\n•\nW\n2\n=1\n•\na\n0\n= -1\n•\nPer a\n1\n=1 e a\n2\n= 1si ha: in=1.5 => \n g(in)=1\n  (funzione g a gradino)\n•\nPer a\n1\n=1 e a\n2\n= 0 si ha: in=0.5 => \n g(in)=1\n•\n Per a\n1\n=0 e a\n2\n= 0 si ha: in=-0.5 => \n g(in)=0\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#2": "Il cervello\n•Costituito da circa 1011 neuroni\n•1014 sinapsi\n•Segnali basati su potenziale elettrochimico\n Quando il potenziale \nsinaptico supera una certa \nsoglia la cellula emette un \nimpulso\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#20": "Porta NOT\n•\n(Soglia t = - 0.5)\n•\n W\n0\n= - 0.5\n•\nW\n1\n= - 1\n•\na\n0\n= -1\n•\nPer a\n1\n=1 => \n g(in)=0\n  (funzione g a gradino)\n•\nPer a\n1\n=0 si ha: in=0.5 => \n g(in)=1\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#21": "Strutture di Rete\n•\nCi sono due categorie principali di strutture di reti \nneurali:\n•\nFeed-forward \n (o acicliche o alimentate in avanti)\n•\nRicorrenti\n  (o cicliche)\n•\nNoi ci occuperemo solo di reti feed-forward.\n•\nEsse sono una tipologia di reti neurali caratterizzate \ndall’avere un verso delle sinapsi, dallo strato di input allo \nstrato di output.\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#22": "Esempio di Rete Feed-Forward\n•\nUna rete alimentata in avanti rappresenta una funzione dei \nsuoi input:\na5\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#23": "•\nL’output dell’intera rete \n a\n5\n è funzione dei suoi input \n a \n•\nI pesi \n W\n agiscono da parametri della funzione. \n•\nLa rete calcola una funzione \n f\nW\n(x) \n•\nLa funzione \n f\nW \nrappresenta una funzione dello \n spazio \ndelle ipotesi \n H \nche può essere booleana o continua. \n•\nSe i pesi vengono modificati, cambia la funzione \nrappresentata dalla rete. \n•\nLe reti feed-forward sono in genere organizzate a strati, \nin modo tale che ogni unità riceva gli input solo dalle \nunità dello strato immediatamente precedente.\nEsempio di Rete Feed-Forward\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#24": "•\nSi tratta di una rete feed-forward in cui \n tutti\n gli input sono \ncollegati direttamente a \n tutti\n gli output.\n•\nEsempio:\n• 3 unità di output\n• 5 unità di input\n• 1 unità di output\n• 2 unità di input\nReti Feed-Forward a Strato Singolo \n(percettroni)\n25",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#25": "•\nEsaminiamo lo spazio delle ipotesi che un percettrone \npuò rappresentare.\n•\nSe ha una funzione di attivazione a soglia, si può \npensare che il percettrone rappresenti una funzione \nbooleana. \n•\nOltre alle funzioni elementari AND, OR e NOT viste \nprima, un percettrone può rappresentare funzioni \nbooleane “complesse” in modo molto compatto. \n•\nVedi, ad esempio, la \n funzione di maggioranza\n .\nReti Feed-Forward a Strato Singolo \n(percettroni)\n26",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#26": "La Funzione di Maggioranza\n•\nPercettrone a soglia \n•\nRestituisce 1 se e solo se più della metà dei suoi \n n\n input binari \nvale 1 \n•\nBasta porre: W\nj\n=1 per ogni input e W\n0\n=n/2 \n•\nUn albero di decisione necessiterebbe di \n O(2\nn\n)\n nodi per \nrappresentare la stessa funzione. \n27",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#27": "•Un percettrone a soglia non può rappresentare tutte le \nfunzioni booleane. \n•Infatti restituisce 1 se e solo se la somma pesata dei suoi \ninput è positiva: \n•L’equazione                 deﬁnisce un iperpiano  nello spazio \ndegli input. \n•Il percettrone restituisce 1 se e solo se l’input si trova da \nuna parte speciﬁca rispetto a tale iperpiano. \n•Per questo il percettore a soglia è chiamato anche \nseparatore lineare .nX\nj=0Wjxj>0 oppure:\nSeparabilità Lineare di un Percettrone a Soglia\nWT·x>0\n<latexit sha1_base64=\"d9P2mD7Jg+O+gyHjQQ5KI5NUFrA=\">AAACFnicbVC7TsNAEDyHVwgvAyXNKRESVWQHJKhQBA1lkPKSkhCdL5twyvmhuzVKZKXnE/gKWqjoEC0tBf+CbYwECVONZna1O+MEUmi0rA8jt7S8srqWXy9sbG5t75i7e03th4pDg/vSV22HaZDCgwYKlNAOFDDXkdByxpeJ37oDpYXv1XEaQM9lI08MBWcYS32z2EWYoDOMWrObOu3ygY/0R5rM6Dm1Cn2zZJWtFHSR2BkpkQy1vvnZHfg8dMFDLpnWHdsKsBcxhYJLmBW6oYaA8TEbQSemHnNB96I0y4wehpqhTwNQVEiaivB7I2Ku1lPXiSddhrd63kvE/7xOiMOzXiS8IETweHIIhYT0kOZKxCUBHQgFiCz5HKjwKGeKIYISlHEei2HcWtKHPZ9+kTQrZfu4XLk+KVUvsmby5IAUyRGxySmpkitSIw3CyT15JE/k2XgwXoxX4+17NGdkO/vkD4z3LwRxnsk=</latexit>\nWT·x=0\n<latexit sha1_base64=\"ChE2zA/JsnaGrMeC8O8z0h7ASAE=\">AAACFnicbVC7SgNBFJ2Nrxhfq5Y2Q4JgFXajoI0QtLGMkBckMcxObuKQ2QczdyVhSe8n+BW2WtmJra2F/+LuuoImnupwzr3ce44TSKHRsj6M3NLyyupafr2wsbm1vWPu7jW1HyoODe5LX7UdpkEKDxooUEI7UMBcR0LLGV8mfusOlBa+V8dpAD2XjTwxFJxhLPXNYhdhgs4was1u6rTLBz7SH2kyo+fUKvTNklW2UtBFYmekRDLU+uZnd+Dz0AUPuWRad2wrwF7EFAouYVbohhoCxsdsBJ2YeswF3YvSLDN6GGqGPg1AUSFpKsLvjYi5Wk9dJ550Gd7qeS8R//M6IQ7PepHwghDB48khFBLSQ5orEZcEdCAUILLkc6DCo5wphghKUMZ5LIZxa0kf9nz6RdKslO3jcuX6pFS9yJrJkwNSJEfEJqekSq5IjTQIJ/fkkTyRZ+PBeDFejbfv0ZyR7eyTPzDevwAC357I</latexit>\n28",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#28": "• Percettrone elementare (senza strati nascosti): non può classificare \npattern che non siano linearmente separabili.\n• Questi casi però sono frequenti: ad esempio problema dello XOR .\n• Caso particolare della classificazione di punti nell’ipercubo unitario: \nogni punto è in classe 0 o in classe 1.\n• Per lo XOR si considerano gli angoli del quadrato unitario (i punti \n(0,0), (0,1), (1,0) e (1,1))\nSeparabilità Lineare di un Percettrone a Soglia\n29",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#29": "Marvin Minsky\nLimiti del Percettrone \n30",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#3": " Warren Sturgis McCulloch  (1899 – 1969) Neurofisiologo e cibernetico \namericano.  \n Walter Pitts  (1923 – 1969) fu un logico che lavorò nel campo della \npsicologia conoscitiva.  \nPrimo modello matematico di una cellula nervosa descritto in un famoso \narticolo: A Logical Calculus of the Ideas Immanent in Nervous Activity \n(1943).  \n Nello scritto del 1943 tentarono di dimostrare che il programma della \nmacchina di Turing poteva essere effettuato anche in una rete finita di \nneuroni  e che il neurone fosse l’unità logica di base del cervello. I pionieri\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#30": "Marvin Minsky\n1969 : Minsky e Papert, Perceptrons\nLimiti del Percettrone \n31",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#31": "Apprendimento nel Percettrone\n•\nNonostante il loro potere espressivo limitato, esiste un semplice \nalgoritmo di apprendimento capace di adattare un percettrone a \nsoglia a qualsiasi insieme di addestramento linearmente \nseparabile (noi vedremo una versione dell’algoritmo per \nl’apprendimento nei percettroni a sigmoide). \n•\nL’idea base dell’algoritmo è quella di calcolare i pesi della rete in \nmodo tale da minimizzare una determinata funzione di costo \nsull’insieme di training. \n•\nIn tal modo il processo di apprendimento è formulato come una \nricerca di ottimizzazione nello spazio dei pesi.\n32",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#32": "•\nLa funzione di costo sull’insieme di training che viene usata \ntradizionalmente è la \n somma dei quadrati degli errori\n , dove \nil singolo errore è la differenza tra l’output desiderato y e \nl’output della rete f\nW\n(\nx\n)\n. \nIl quadrato dell’errore per un singolo \nesempio di training è il seguente: \nessendo \n x\n il vettore relativo ai dati di input dell’esempio,            \ny il valore corretto della funzione di output e f\n w\n(\nx\n) il valore di \noutput ottenuto dalla rete avente in input \n x\n.\nE=1\n2Err2=1\n2(y\u0000fw(x))2\nApprendimento nel Percettrone\n33",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#33": "Possiamo usare tale metodo per ridurre il quadrato dell’errore \n(ricerca del minimo globale) calcolando la derivata parziale di E \nrispetto ad ogni peso:\ndove g’ è la derivata della funzione di attivazione.\nPer la sigmoide:\nMetodo della Discesa del Gradiente\n34",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#34": " α = step size  o tasso di apprendimento .Il peso deve essere aggiornato in questo modo:\nL’idea è quella di modificare il peso proporzionalmente \nal negativo della derivata dell’errore E vista in precedenza:\nL’aggiornamento del peso è pertanto il seguente:\nMetodo della Discesa del Gradiente\n35",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#35": "Algoritmo completo di apprendimento \na discesa di gradiente per percettroni\nMetodo della discesa del gradiente\n36",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#36": " Gli esempi di addestramento vengono fatti passare attraverso la \nrete uno per volta, modificando leggermente i pesi a ogni \niterazione per ridurre l’errore.  \n Ogni ciclo attraverso tutti gli esempi prende il nome di epoca . \n Le epoche sono ripetute secondo un ben preciso criterio di \nterminazione (e.g., quando le modifiche dei pesi sono piccole). \nAltri metodi calcolano il gradiente per l’intero training set, \nsommando tutti i contributi dati dall’equazione precedente prima \ndi aggiornare i pesi. \nMetodo della Discesa del Gradiente\n37",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#37": "Percettrone a soglia\n Per percettroni a soglia  la g’(in)  è indefinita.\n In questo caso la regola di apprendimento del percettrone \noriginale sviluppata da Rosenblatt (1957) è la seguente:\nEssa è simile a quella vista, tranne per il fatto che la g’(in) è \nomessa. \nPoiché g’(in) è la stessa per tutti i pesi, la sua omissione \ncambia solo la dimensione e non la direzione \ndell’aggiornamento globale dei pesi.\n38",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#38": "Il Percettrone di Rosemblatt\n39",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#39": "Reti Feed-Forward Multistrato\n Si tratta di reti con unità nascoste, in cui esiste un \nverso di propagazione del segnale dall’input \nall’output. \n Ciascun nodo dello strato i-mo è collegato con tutti \ni nodi dello strato i+1-mo. \n Percettrone multistrato . \n40",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#4": "John McCarthy ha indicato il lavoro di Nicolas Rashevsky  (1936, \n1938) come il primo modello matematico di apprendimento neurale. \n Alan Turing  (1948) scrisse un rapporto di ricerca intitolato Intelligent \nMachinery  che inizia con la frase \" I propose to investigate the \nquestion as to whether it is possible for machinery to show intelligent \nbehaviour \" e prosegue descrivendo un'architettura di rete neurale \nricorrente che ha definito \" B-type unorganized machines  \"e un \napproccio per addestrarla. \nSfortunatamente, tale rapporto non è stato pubblicato fino al 1969 ed \nè stato quasi ignorato fino a poco tempo fa.I pionieri\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#40": "Esempio di Rete Neurale Feed-Forward Multistrato \n41",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#41": "Spazio delle ipotesi H per il  \npercettrone multistrato\n•\nIl vantaggio di aggiungere strati nascosti è quello di \nampliare lo spazio delle ipotesi rappresentabili dalla rete. \n•\nPossiamo infatti considerare ogni unità nascosta come un \npercettrone che rappresenta una funzione a soglia \nmorbida nello spazio di input (vedi figura seguente). \n•\nOgni unità di output può dunque rappresentare una \ncombinazione lineare (a soglia morbida) di molte \nfunzioni simili.\n42",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#42": "Output di un Percettrone  \na due Input (con sigmoide)\n43",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#43": "• Fig. (a): cresta  prodotta da due funzioni a \nsoglia morbida rivolte in direzioni opposte \ne limitando il risultato con un’altra soglia.\n• Fig. (b): protuberanza  prodotta dalla \ncombinazione di due creste ad angolo retto \n(cioè, combinando le uscite di quattro unità \nnascoste).\n(a) (b)⇓\nCombinazione di Funzioni  \na Soglia Morbida\nfw(x 1,x2) fw(x 1,x2)\n44",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#44": "Con un solo strato nascosto sufficientemente grande \npossiamo rappresentare qualsiasi funzione continua degli \ninput con accuratezza arbitraria. \nCon due strati nascosti possono essere rappresentate anche \nfunzioni discontinue (il numero delle unità nascoste cresce \nesponenzialmente con il numero degli input).\nPurtroppo, data una qualsiasi struttura di rete prefissata , è \ndifficile stabilire esattamente quali funzioni possano essere \nrappresentate e quali non possano esserlo.\nReti Feed-Forward Multistrato\n45",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#45": "Apprendimento nelle Reti  \nMultistrato Feed-Forward \nGli algoritmi per l’apprendimento per le reti multistrato sono \nsimili all’algoritmo di apprendimento per i percettroni visto in \nprecedenza.\nUna differenza è costituita dal fatto che nelle reti multistrato \navremo in generale più unità di output.\nCiò comporta che avremo un vettore di output fw(x) calcolato \ndalla rete anziché un valore singolo e, per ogni esempio, un \nvettore di output y.\n46",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#46": "•L’idea base rimane la stessa, che è quella di calcolare i pesi della \nrete in modo da minimizzare la somma dei quadrati degli errori che, \nper un singolo esempio, è definita come segue:\n•Dato un certo esempio, il vettore di errore in output è il seguente:\n•Indichiamo come segue l’i-esimo componente del suddetto vettore:\n•E’ inoltre utile definire come segue un errore modificato:\n \nApprendimento nelle Reti  \nMultistrato Feed-Forward \ny\u0000fw(x)\n47",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#47": "Back-propagation\n : strato di output\nPer lo strato di output, il peso deve essere aggiornato\n in questo modo:\nL’idea è quella di modificare il peso proporzionalmente \nal negativo della derivata dell’errore E \n(vedi lucido n. 52 per i dettagli della derivazione ):\nL’aggiornamento del peso è pertanto il seguente:\n48",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#48": "Back-propagation\n : strato nascosto  \n(versione intuitiva)\nAnche per lo strato nascosto il generico peso deve essere aggiornato in \nquesto modo:\nDobbiamo però definire una quantità analoga all’errore per i nodi di \noutput.\nE’ a questo punto che entra in gioco la retropropagazione :\nL’idea  è  che  il  nodo  nascosto  j  sia  “responsabile”  per  una  parte  \ndell’errore ∆i in ognuno dei nodi di output ai quali è collegato.\nIn tal modo i valori ∆ sono suddivisi in base alla forza delle connessioni \ntra nodo nascosto e nodo di output e passati all’indietro per fornire i \nvalori ∆j allo strato nascosto.\n49",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#49": "La regola di propagazione per i valori       è dunque la seguente:\nL’aggiornamento del peso è pertanto il seguente, identica a quella \nche riguarda lo strato di output:\nBack-propagation\n : strato nascosto  \n(versione intuitiva)\n50",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#5": "Da altri\nneuroni\nUnità di Calcolo nelle Reti Neurali\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#50": "Anche per lo strato nascosto il generico peso deve essere aggiornato in \nquesto modo:\nL’idea è, di nuovo, quella di modificare il peso proporzionalmente \nal negativo della derivata dell’errore E \n(vedi lucido n. 53 per i dettagli della derivazione ):\nL’aggiornamento del peso è pertanto il seguente:\nBack-propagation\n : strato nascosto  \n(versione formale)\n51",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#51": "Calcolo del gradiente  \n(strato di output)\n52",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#52": "Calcolo del gradiente  \n(strato nascosto)\n53",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#53": "Processo di Retropropagazione\nIn sintesi, il processo di retropropagazione può essere descritto \ncome segue: \n•\nSi calcolano i valori \n ∆ \nper le unità di output usando l’errore \nosservato. \n•\nCominciando dallo strato di output, si ripete quanto segue per \nogni strato della rete fino a raggiungere l’ultimo strato \nnascosto: \no\nsi propagano all’indietro i valori ∆ verso lo strato \nprecedente; \no\nsi aggiornano i pesi tra i due strati.\n54",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#54": "Back-propagation\n1. Presentazione pattern d’ingresso \nUnità di input akUnità di output ai\nUnità nascoste aj\nWk,jWj,i\n55",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#55": "Unità di input akUnità di output ai\nUnità nascoste aj2. Propagazione dell’input in avanti sullo strato nascosto \nWk,jWj,i\nBack-propagation\n56",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#56": "3. Propagazione dallo strato nascosto allo strato di output\nUnità di input akUnità di output ai\nUnità nascoste aj\nWk,jWj,i\nBack-propagation\n57",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#57": "4. Calcolo dei valori DELTA per lo strato di output\nUnità di input akUnità di output ai\nUnità nascoste aj\nWk,jWj,i\nBack-propagation\n58",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#58": "5. Retropropagazione dell’errore\nUnità di input akUnità di output ai\nUnità nascoste aj\nWk,jWj,i\nL’errore si retropropaga  su \nciascun nodo proporzionalmente  \nalla forza  di connessione tra il \nnodo nascosto e il nodo di output\nBack-propagation\n59",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#59": "6. Aggiornamento dei pesi\nUnità di input akUnità di output ai\nUnità nascoste aj\nWk,jWj,i\nBack-propagation\n60",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#6": " Ogni unità i calcola per prima cosa una somma pesata \ndei propri input:\n Successivamente si applica una funzione di attivazione  \ng alla somma per derivare l’output:\nUnità di Calcolo nelle Reti Neurali\nini=nX\nj=1Wj,iaj\n<latexit sha1_base64=\"eWDCyQ6ONiLa3NNG6gCYSqPL4Ko=\">AAACI3icbVC7TsNAEDzzDOEVoKQ5ESFRoGADEjRIETSUIBGCFAdrfSxwcD5bd2sEsvwZfAJfQQsVHaKh4F+4hBS8phrN7Gp3Js6UtOT7797Q8Mjo2Hhlojo5NT0zW5ubP7ZpbgS2RKpScxKDRSU1tkiSwpPMICSxwnZ8vdfz2zdorEz1Ed1l2E3gQstzKYCcFNXWQsJbt1dIXUaS73Ae2jyJiqudoDwtdMnbjq/KkoerHKIrHtXqfsPvg/8lwYDU2QAHUe0jPEtFnqAmocDaTuBn1C3AkBQKy2qYW8xAXMMFdhzVkKDtFv1gJV/OLVDKMzRcKt4X8ftGAYm1d0nsJhOgS/vb64n/eZ2czre7LnOWE2rRO0RSYf+QFUa6xpCfSYNE0PscudRcgAEiNJKDEE7MXYVV10fwO/1fcrzeCDYa64eb9ebuoJkKW2RLbIUFbIs12T47YC0m2D17ZE/s2XvwXrxX7+1rdMgb7CywH/A+PgGXqaO/</latexit>\nai=g(ini)=g0\n@nX\nj=1Wj,iaj1\nA\n<latexit sha1_base64=\"L8MWGsH2A4rKpI1HGBXATIbImqQ=\">AAACOnicbVDBShxBEO3RxOjG6MYcvTRZAivIMqOB5CKIguRoIOsKO+tQ09aOpT09Q3dNUIb5o3xCvsKbJF68idd8QHo3e0g07/R4rx5V9dJSk+MwvAnm5p89X3ixuNR6ufxqZbX9eu3IFZVV2FeFLuxxCg41Gewzscbj0iLkqcZBerE/8Qdf0ToqzBe+KnGUQ2ZoTArYS0n7ABKSOzLrxoyXPl+TaRLa8JLMYo1j7srYVXlSn+9EzUltGjnwfJMaGW9KSM5jS9kZbyTtTtgLp5BPSTQjHTHDYdK+jU8LVeVoWGlwbhiFJY9qsExKY9OKK4clqAvIcOipgRzdqJ7+28h3lQMuZIlWkpZTEf9O1JA7d5WnfjIHPnOPvYn4P29Y8fjjyFdQVoxGTRYxaZwucsqSLxLlKVlkhsnlKMlIBRaY0ZIEpbxY+WZbvo/o8fdPydFWL9rubX1+39ndmzWzKNbFW9EVkfggdsUncSj6Qolv4lr8ED+D78FdcB88/BmdC2aZN+IfBL9+A3R4rDw=</latexit>\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#60": "7. Aggiornamento dei pesi\nUnità di input akUnità di output ai\nUnità nascoste aj\nWk,jWj,i\nBack-propagation\n61",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#61": "Esempio di Esecuzione\n• Vediamo un esempio di esecuzione dell’algoritmo di Back-\npropagation applicato sulla seguente rete:\na1 a2\n0 1U1 U2\n1 1U3 U4U5\n1a5\nW3,5=1.5 W4,5=-1.0\na3 a4\nW1,3=1 W2,4=2W1,4=-1 W2,3=0.51 11Target = 1\nOutput Layer\nHidden Layer\nInput Layer\n62",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#62": "Esempio di Esecuzione\n•Eseguiamo l’algoritmo su un solo esempio di addestramento e, per il \nquale dunque conosciamo l’output corretto ( Target = 1 ) a fronte di un \ncerto input  X.\n• Supponiamo che i valori dell’input per l’esempio e in questione \nsiano i seguenti:\n•  Input U1: \n•  Input U2:  \n63",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#63": "Esempio di Esecuzione\n1. Presentazione del pattern in ingresso :\n64",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#64": "Esempio di Esecuzione\n2. Passo Feed-Forward ( hidden layer ):\n65",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#65": "Esempio di Esecuzione\n3. Passo Feed-Forward ( output layer ):\n66",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#66": "Esempio di Esecuzione\n• A fronte di questo risultato in uscita possiamo calcolare il quadrato \ndell’errore:  \n• L’errore non è molto alto, ma applicando l’algoritmo alla rete \npossiamo cercare di ridurlo.  \n67",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#67": "Esempio di Esecuzione\n4. Calcolo del valore ∆ in uscita ( output layer ):\n68",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#68": "Esempio di Esecuzione\n  5. Passo di Backward Propagation dell’errore ( hidden layer ):\n69",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#69": "Esempio di Esecuzione\n6. Passo di aggiornamento dei pesi ( link in ingresso all’output layer ):\n70",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#7": "Unità di Calcolo nelle Reti Neurali\nUsando funzioni diverse come \n g\n si possono ottenere \nmodelli differenti. Ad esempio:\ng(in i)=⇢1i f i n i\u0000t\n0i f i n i<t\n<latexit sha1_base64=\"3dhiPmh/49rcUH3TuNXPStviJU0=\">AAACfHicdVHLbtNAFB2bVzGPhscOga5IQEWokV0QdAFSBRuWRSJtpUwUjSc37lXHYzNzjRpZ3vOLXfALfALCDl6UFu7q6Jxz32lpyHMcnwXhlavXrt/YuBndun3n7ubg3v0DX1RO40QXpnBHqfJoyOKEiQ0elQ5Vnho8TE8+dvrhN3SeCvuFVyXOcpVZWpJW3FLzwfdsSzKeck22mdMLeA/S4JJlDTLFjGytnFOrpjamiRKA5yDztDitaQmjc3kgM/wKPGpAyij+v+1d54kk2kVfN5KOsmMeR9F8MIzH8TrgMkh6MBR97M8HP+Si0FWOlrVR3k+TuORZW5dJG2wrVx5LpU9UhtMWWpWjn9XrizXwrPKKCyjRARlYk3g+o1a596s8bZ254mN/UevIf2nTipe7s3bbsmK0umvEZHDdyGtH7SsQFuSQWXWTI5AFrZxiRkegtG7Jqv1Nd4/k4vaXwcHOOHk13vn8erj3ob/MhngknootkYi3Yk98EvtiIrT4GTwMHgdPgl/hKHwZbv+xhkGf80D8FeGb33pBvsA=</latexit>\nFunzione a gradino:\ng(in i) = tanh(in i)\n<latexit sha1_base64=\"Cms3QN8iP8ejsRthjNVvwV/Syf0=\">AAACH3icbVDLSgNBEJz1GeMr6tHLYBAUIexGQS9C0ItHBRMDSQi9Y5sMmZ1dZnrFEPIRfoJf4VVP3sSrB//F3c0ejFqnmqpueqr8SElLrvvpzMzOzS8sFpaKyyura+uljc2GDWMjsC5CFZqmDxaV1FgnSQqbkUEIfIU3/uA89W/u0VgZ6msaRtgJoKflnRRAidQtHfT22oQPNJJ63JX7nJ/yyZtA98dTXrFbKrsVNwP/S7yclFmOy27pq30bijhATUKBtS3PjagzAkNSKBwX27HFCMQAethKqIYAbWeUhRrz3dgChTxCw6XimYg/N0YQWDsM/GQyAOrb314q/ue1Yro76SSZophQi/QQSYXZISuMTNpCfisNEkH6c+RScwEGiNBIDkIkYpzUl/bh/U7/lzSqFe+wUr06KtfO8mYKbJvtsD3msWNWYxfsktWZYI/smb2wV+fJeXPenY/J6IyT72yxKTif33PtosY=</latexit>\n Tangente iperbolica:\nSigmoide: g(in i)=1\n1+e\u0000ini\n<latexit sha1_base64=\"of82qDQd5scNysrZocadTZaCmmw=\">AAACI3icbVDLSgNBEJyN7/iKevQyGISIGHdV0IsgevGoYBIhG0PvpBMHZx/M9Iqy7Gf4CX6FVz15Ey8e8i9u4h6isU5FVTfdVV6kpCHb/rIKE5NT0zOzc8X5hcWl5dLKat2EsRZYE6EK9bUHBpUMsEaSFF5HGsH3FDa8u7OB37hHbWQYXNFjhC0feoHsSgGUSe3Sbq/iEj5QIoO0Lbf4MXe7GkTipInDtzneJDsjfpq2S2W7ag/Bx4mTkzLLcdEu9d1OKGIfAxIKjGk6dkStBDRJoTAturHBCMQd9LCZ0QB8NK1kGCzlm7EBCnmEmkvFhyKObiTgG/Poe9mkD3Rr/noD8T+vGVP3qJVlimLCQAwOkVQ4PGSEllljyDtSIxEMPkcuAy5AAxFqyUGITIyzCotZH87f9OOkvld19qt7lwflk9O8mVm2zjZYhTnskJ2wc3bBakywJ/bCXtmb9Wy9Wx/W589owcp31tgvWP1vXhWkTA==</latexit>\nReLU:g(in i) = max(in i,0)\n<latexit sha1_base64=\"fdZuD83MuMKqvRY6gR2pKYYhfhM=\">AAACH3icbVDLTgJBEJzFF+IL9ehlIjGBaMgumujFhOjFIybySICQ3qHBibOPzPQaCOEj/AS/wquevBmvHPwXl8cBwTrVVHWnp8oNlTRk2yMrsbK6tr6R3Extbe/s7qX3DyomiLTAsghUoGsuGFTSxzJJUlgLNYLnKqy6T7djv/qM2sjAf6B+iE0Pur7sSAEUS630aTfbIOzRQPrDlszxaz59etAbzjtn3M610hk7b0/Al4kzIxk2Q6mV/mm0AxF56JNQYEzdsUNqDkCTFAqHqUZkMATxBF2sx9QHD01zMAk15CeRAQp4iJpLxScizm8MwDOm77nxpAf0aBa9sfifV4+oc9WMU4URoS/Gh0gqnBwyQsu4LeRtqZEIxj9HLn0uQAMRaslBiFiM4vpScR/OYvplUinknfN84f4iU7yZNZNkR+yYZZnDLlmR3bESKzPBXtgbe2cf1qv1aX1Z39PRhDXbOWR/YI1+AVy+orM=</latexit>\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#70": "Esempio di Esecuzione\n7. Passo di aggiornamento dei pesi ( link in ingresso all’hidden layer ):\n71",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#71": "Esempio di Esecuzione\n7. Passo di aggiornamento dei pesi ( link in ingresso all’hidden layer ):\n72",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#72": "Esempio di Esecuzione\n• Ciò completa l’aggiornamento dei pesi per il training example \ncorrente.\n• Per verificare che l’algoritmo abbia effettivamente ridotto \nl’errore in output, eseguiamo la parte feed-forward ancora una \nvolta per confrontare l’uscita attuale con la precedente.\n73",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#73": "Esempio di Esecuzione\n• Nuovo  Passo Feed-Forward ( hidden layer ):\n74",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#74": "Esempio di Esecuzione\n• Nuovo Passo Feed-Forward ( output layer ):\n75",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#75": "Esempio di Esecuzione\n• Il nuovo quadrato dell’errore è il seguente:\n• La differenza con il vecchio valore è:\n76",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#76": "Esempio di Esecuzione\n• L’esecuzione dell’algoritmo di Backpropagation che abbiamo \nvisto è relativo ad un solo passaggio per un solo esempio di \naddestramento.  \n• Si ricorda che l’algoritmo completo fa passare gli esempi di \naddestramento attraverso la rete uno per volta, modificando \nleggermente i pesi a ogni iterazione per ridurre l’errore.  \n  \n• Ogni ciclo attraverso tutti gli esempi prende il nome di epoca .  \n• Le epoche sono ripetute fino a quando non viene soddisfatto un \ncriterio di terminazione (in genere, quando le modifiche ai pesi \nsono diventate molto piccole).  \n77",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#77": "Sintesi degli Argomenti \nTrattati nella Lezione \nUna Rete Neurale è un modello computazionale che presenta alcune proprietà del cervello: consiste di molte \nunità semplici che lavorano in parallelo senza alcun controllo centralizzato.  Le connessioni tra le unità hanno \npesi numerici che possono essere modiﬁcati dall’elemento di apprendimento.  \nIl comportamento di una rete neurale è determinato dalla topologia delle connessioni e dalla natura delle \nsingole unità. Le reti alimentate in avanti  in cui le connessioni formano un grafo diretto aciclico, sono le più \nsemplici da analizzare. Le reti alimentate in avanti implementano funzioni senza stato. \nUn percettrone è una rete alimentata in avanti con un singolo strato di unità e può rappresentare solo funzioni \nlinearmente separabili . Se i dati sono linearmente separabili si può utilizzare la regola di apprendimento del \npercettrone  per modiﬁcare i pesi della rete in modo da farli corrispondere esattamente ai dati. \nLe reti alimentate in avanti multistrato possono rappresentare qualsiasi funzione, dato un sufﬁciente numero di \nunità. \nL’algoritmo di apprendimento backpropagation (propagazione all’indietro ) funziona su reti multistrato \nalimentate in avanti effettuando una discesa del gradiente nello spazio dei pesi per minimizzare l’errore in \nuscita. Esso converge a una soluzione localmente ottima ed è stato usato con successo in un’ampia varietà di \napplicazioni. La sua convergenza è spesso molto lenta.\n78",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#8": "Funzione Gradino\ng\nt0g(in i)=⇢1i f i n i\u0000t\n0i f i n i<t\n<latexit sha1_base64=\"3dhiPmh/49rcUH3TuNXPStviJU0=\">AAACfHicdVHLbtNAFB2bVzGPhscOga5IQEWokV0QdAFSBRuWRSJtpUwUjSc37lXHYzNzjRpZ3vOLXfALfALCDl6UFu7q6Jxz32lpyHMcnwXhlavXrt/YuBndun3n7ubg3v0DX1RO40QXpnBHqfJoyOKEiQ0elQ5Vnho8TE8+dvrhN3SeCvuFVyXOcpVZWpJW3FLzwfdsSzKeck22mdMLeA/S4JJlDTLFjGytnFOrpjamiRKA5yDztDitaQmjc3kgM/wKPGpAyij+v+1d54kk2kVfN5KOsmMeR9F8MIzH8TrgMkh6MBR97M8HP+Si0FWOlrVR3k+TuORZW5dJG2wrVx5LpU9UhtMWWpWjn9XrizXwrPKKCyjRARlYk3g+o1a596s8bZ254mN/UevIf2nTipe7s3bbsmK0umvEZHDdyGtH7SsQFuSQWXWTI5AFrZxiRkegtG7Jqv1Nd4/k4vaXwcHOOHk13vn8erj3ob/MhngknootkYi3Yk98EvtiIrT4GTwMHgdPgl/hKHwZbv+xhkGf80D8FeGb33pBvsA=</latexit>\nini\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\26-Reti-Neurali-sbloccato.pdf#9": "Sigmoide\ng\n01g(in i)=1\n1+e\u0000ini\n<latexit sha1_base64=\"of82qDQd5scNysrZocadTZaCmmw=\">AAACI3icbVDLSgNBEJyN7/iKevQyGISIGHdV0IsgevGoYBIhG0PvpBMHZx/M9Iqy7Gf4CX6FVz15Ey8e8i9u4h6isU5FVTfdVV6kpCHb/rIKE5NT0zOzc8X5hcWl5dLKat2EsRZYE6EK9bUHBpUMsEaSFF5HGsH3FDa8u7OB37hHbWQYXNFjhC0feoHsSgGUSe3Sbq/iEj5QIoO0Lbf4MXe7GkTipInDtzneJDsjfpq2S2W7ag/Bx4mTkzLLcdEu9d1OKGIfAxIKjGk6dkStBDRJoTAturHBCMQd9LCZ0QB8NK1kGCzlm7EBCnmEmkvFhyKObiTgG/Poe9mkD3Rr/noD8T+vGVP3qJVlimLCQAwOkVQ4PGSEllljyDtSIxEMPkcuAy5AAxFqyUGITIyzCotZH87f9OOkvld19qt7lwflk9O8mVm2zjZYhTnskJ2wc3bBakywJ/bCXtmb9Wy9Wx/W589owcp31tgvWP1vXhWkTA==</latexit>\nini\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Reti Neurali (Ex 11)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#1": "Sommario\nRichiami percettrone e MLP \nSci-kit learn e percettrone \nMLP e regressione \nMLP e classiﬁcazione \nKeras \nEsempio fashion_mnist \nKeras: Sequential models, parametri, metriche, training, predizione",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#10": "Richiami: Multi-Layer Perceptron (MLP)\nPerché è fondamentale inserire una funzione di attivazione? \nSe combiniamo diversi layer e unità otteniamo semplicemente una \nsequenza di combinazioni lineari, perciò una trasformazione lineare \ninput-output. È come ottenere un singolo layer. Non possiamo \nrappresentare funzioni complesse non lineari.\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#11": "Richiami: Multi-Layer Perceptron (MLP)\nDomanda: Perché nella MLP si preferisce la funzione logistica (o sigmoide) \nalla funzione gradino (o step function)?\n12g(in i)=⇢1i f i n i\u0000t\n0i f i n i<t\n<latexit sha1_base64=\"3dhiPmh/49rcUH3TuNXPStviJU0=\">AAACfHicdVHLbtNAFB2bVzGPhscOga5IQEWokV0QdAFSBRuWRSJtpUwUjSc37lXHYzNzjRpZ3vOLXfALfALCDl6UFu7q6Jxz32lpyHMcnwXhlavXrt/YuBndun3n7ubg3v0DX1RO40QXpnBHqfJoyOKEiQ0elQ5Vnho8TE8+dvrhN3SeCvuFVyXOcpVZWpJW3FLzwfdsSzKeck22mdMLeA/S4JJlDTLFjGytnFOrpjamiRKA5yDztDitaQmjc3kgM/wKPGpAyij+v+1d54kk2kVfN5KOsmMeR9F8MIzH8TrgMkh6MBR97M8HP+Si0FWOlrVR3k+TuORZW5dJG2wrVx5LpU9UhtMWWpWjn9XrizXwrPKKCyjRARlYk3g+o1a596s8bZ254mN/UevIf2nTipe7s3bbsmK0umvEZHDdyGtH7SsQFuSQWXWTI5AFrZxiRkegtG7Jqv1Nd4/k4vaXwcHOOHk13vn8erj3ob/MhngknootkYi3Yk98EvtiIrT4GTwMHgdPgl/hKHwZbv+xhkGf80D8FeGb33pBvsA=</latexit>\nFunzione a gradino:\nSigmoide: g(in i)=1\n1+e\u0000ini\n<latexit sha1_base64=\"of82qDQd5scNysrZocadTZaCmmw=\">AAACI3icbVDLSgNBEJyN7/iKevQyGISIGHdV0IsgevGoYBIhG0PvpBMHZx/M9Iqy7Gf4CX6FVz15Ey8e8i9u4h6isU5FVTfdVV6kpCHb/rIKE5NT0zOzc8X5hcWl5dLKat2EsRZYE6EK9bUHBpUMsEaSFF5HGsH3FDa8u7OB37hHbWQYXNFjhC0feoHsSgGUSe3Sbq/iEj5QIoO0Lbf4MXe7GkTipInDtzneJDsjfpq2S2W7ag/Bx4mTkzLLcdEu9d1OKGIfAxIKjGk6dkStBDRJoTAturHBCMQd9LCZ0QB8NK1kGCzlm7EBCnmEmkvFhyKObiTgG/Poe9mkD3Rr/noD8T+vGVP3qJVlimLCQAwOkVQ4PGSEllljyDtSIxEMPkcuAy5AAxFqyUGITIyzCotZH87f9OOkvld19qt7lwflk9O8mVm2zjZYhTnskJ2wc3bBakywJ/bCXtmb9Wy9Wx/W589owcp31tgvWP1vXhWkTA==</latexit>",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#12": "Richiami: Multi-Layer Perceptron (MLP)\nPerché nella MLP si preferisce la funzione logistica (o sigmoide) alla \nfunzione gradino (o step function)? \nCon la funzione gradino i gradienti genererebbero una superﬁcie piatta, \nche non permetterebbe di adattare i parametri. \nLa funzione logistica è deﬁnita ed ha derivata ovunque.  \nLa ReLU non è differenziabile per \n in=0\n, e ha derivata 0 per \n in<0\n. Ma \nempiricamente mostra buone performance ed è rapido il calcolo della \nderivata. Inoltre non avendo un valore max in output riduce alcune \nproblematiche nelle architetture più complesse.\n13\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#13": "MLP e regressione\nUna MLP può essere usata per produrre un singolo valore, es. creando un \nlayer di output con una singola unità. Nel caso di \n multivariate regression\n , il \nlayer può contenere più unità. \nSolitamente non si inserisce la funzione di attivazione in output in modo da \nnon imporre intervalli. Se c'è bisogno di valori positivi si può inserire una \nReLU\n  o una \n softplus activation function\n  (una versione smooth della ReLU). \nLa loss function usata durante il training è la \n mean squared error\n . Nel caso \ndi molti outlier nel training set è possibile considerare anche la \n mean \nabsolute error\n . La Huber loss è una combinazione di entrambe.\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#14": "MLP e regressione: architettura tipica\nConﬁgurazione tipica degli iperparametri di una MLP usata per la \nregressione:\n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#15": "MLP e classiﬁcazione\nInserendo un layer con una singola unità e con funzione di attivazione \nlogistica possiamo stimare la probabilità di appartenenza dell'input a una \ncerta classe (binary classiﬁcation). Nel caso \n multilabel binary classiﬁcation\n , \n(es. email spam/no_spam, urgent/no_urgent) si avranno più unità di output. \nSe una istanza può appartenere ad una di n possibili classi (es. una cifra da \n0 a 9), l'output layer conterrà n unità con una funzione \n softmax\n  che \ngarantisce che ogni unità produca una probabilità la cui somma sia 1. In \nquesto caso si impiega la \n cross-entropy\n  come funzione di loss. \n16\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#16": "Keras\nKeras (\n https://keras.io\n ) \nsono API per il Deep Learning ad alto livello per \ncostruire ed addestrare architetture di reti neurali. \nSi basa a sua volta su librerie che permettono di eseguire le reti su varie \npiattaforme, es. TensorFlow, Microsoft Cognitive Toolkit (CNTK), Theano; \nApache MXNet, Apple’s Core ML, Javascript o Typescript (Keras code in \nweb browsers), or PlaidML (on GPUs). \nTensorFlow integra Keras e lo arricchisce di altre funzionalità (es. \nTensorFlow’s Data API) \nInstallazione (via PIP):  \npython3 -m pip install --upgrade tensorﬂow\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#17": "Keras: esempio \n fashion_mnist\nEntrambe le versioni corrispondono alla 2.8.0 \nimport\n tensorflow \n as\n tf\nfrom\n tensorflow \n import\n keras\nprint\n(tf.__version__)\nprint\n(keras.__version__)\nImpieghiamo il dataset fashion_mnist: \nfashion_mnist = keras.datasets.fashion_mnist\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\nprint\n(X_train_full.shape)\nprint\n(X_train_full.dtype)\n>> (60000, 28, 28)\n>> uint8\nX_valid, X_train = X_train_full[:\n 5000\n] / \n255.0\n, X_train_full[\n 5000\n:] / \n255.0\ny_valid, y_train = y_train_full[:\n 5000\n], y_train_full[\n 5000\n:]\nclass_names = [\n \"T-shirt/top\"\n , \n\"Trouser\"\n , \n\"Pullover\"\n , \n\"Dress\"\n, \n\"Coat\"\n,\n\"Sandal\"\n , \n\"Shirt\"\n, \n\"Sneaker\"\n , \n\"Bag\"\n, \n\"Ankle boot\"\n ]\nprint\n(class_names[y_train[\n 0\n]])\n>> \n'Coat'\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#18": "Keras: sequential models\nIl modello \n Sequential\n  è il più semplice e consiste in una singolo stack di \nlayers connessi sequenzialmente: \nmodel = keras.models.Sequential()\n# converto le istanze in input in array 1D\n# equivale a una operazione: X.reshape(-1,1)\n# è obbligatorio specificare il input_shape\nmodel.add(keras.layers.Flatten(input_shape=[\n 28\n, \n28\n]))\n# layer denso con 300 unità e ReLU come activation function\n# ogni layer contiene i propri parametri (pesi e bias) riferiti alle \nconnessioni\n# con il layer precedente\nmodel.add(keras.layers.Dense(\n 300\n, activation=\n \"relu\"\n))\n# layer denso di 100 unità\nmodel.add(keras.layers.Dense(\n 100\n, activation=\n \"relu\"\n))\n# layer di output con 10 unità (una per classe) e softmax activation function\nmodel.add(keras.layers.Dense(\n 10\n, activation=\n \"softmax\"\n ))\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#19": "Keras: sequential models\nIn alternativa, invece di creare un layer alla volta, possiamo passare una \nlista al costruttore:  \nmodel = keras.models.Sequential([\nkeras.layers.Flatten(input_shape=[\n 28\n, \n28\n]),\nkeras.layers.Dense(\n 300\n, activation=\n \"relu\"\n),\nkeras.layers.Dense(\n 100\n, activation=\n \"relu\"\n),\nkeras.layers.Dense(\n 10\n, activation=\n \"softmax\"\n )\n])\nSono possibile varie forme di import, tutte equivalenti: \nfrom \nkeras.layers \n import \nDense\noutput_layer \n = \nDense\n(\n10\n)\nfrom \ntensorflow.keras.layers \n import \nDense\noutput_layer \n = \nDense\n(\n10\n)\nfrom \ntensorflow \n import \nkeras\noutput_layer \n = \nkeras\n.\nlayers\n.\nDense\n(\n10\n)\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#2": "Richiami: Percettrone\nUna delle architetture più semplici, dove un singolo layer è connesso con \ntutti gli input dello strato precedente (\n fully connected \n o\n dense layer\n ), cioè \nl'\ninput layer\n : \nNota\n : nei precedenti lucidi si è usata la notazione dove gli input \n x\n sono \nanche indicati con la lettera \n a\n. La step function \n step()\n  corrisponde alla \nfunzione di attivazione \n g(in).\n3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#20": "Keras: parametri\nPer monitorare l'architettura creata usiamo la funzione summary():  \nmodel\n.\nsummary()\nNota: i layer densi contengono molti parametri (es. 235.500!)\n21\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#21": "Keras: parametri\nPer accedere ai singoli layers usiamo il parametro \n layers\n , e le funzioni \nget_weights\n () e \nset_weights\n (): \n>>> \nmodel\n.\nlayers\n[<tensorflow.python.keras.layers.core.Flatten at 0x132414e48>,\n<tensorflow.python.keras.layers.core.Dense at 0x1324149b0>,\n<tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0>,\n<tensorflow.python.keras.layers.core.Dense at 0x13240d240>]\n>>> \nmodel\n.\nlayers\n[\n1\n]\n.\nname\n'dense_3'\n>>> \nmodel\n.\nget_layer\n (\n'dense_3'\n )\n.\nname\n'dense_3'\n>>> \nweights\n, \nbiases \n= \nhidden1\n.\nget_weights\n ()\n>>> \nweights\narray([[ 0.03854964, -0.04054524, 0.00599282, ..., 0.02566582,\n0.01032123, 0.06914985],\n...,\n[ 0.02632413, -0.05105981, -0.00332005, ..., 0.04175945,\n0.0443138 , -0.05558084]], dtype=float32)\n>>> \nweights\n.\nshape\n(784, 300)\n>>> \nbiases\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0.], dtype=float32)\n>>> \nbiases\n.\nshape\n(300,)\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#22": "Keras: parametri\nA cosa può servire una funzione set_weights()?\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#23": "Keras: parametri\nA cosa può servire una funzione set_weights()? \nPossiamo operare regolarizzazioni manuali, oppure sovrascrivere i \nvalori iniziali random con valori ottenuti da precedenti fasi di training. \nPer impiegare altri criteri di inizializzazione dei kernel (cioè delle matrici \ndei parametri della rete) consultare \n https://keras.io/initializers/\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#24": "Keras: metriche\nLa funzione compile() prende in input la \n loss \nfunction\n  e il \noptimizer\n , cioè \nl'algoritmo per stimare i parametri, ed eventuali altri parametri, come la \nmetrica per stimare l'errore:  \nmodel\n.\ncompile\n(\nloss\n=\n\"sparse_categorical_crossentropy\"\n ,\noptimizer\n =\n\"sgd\"\n,\nmetrics\n=\n[\n\"accuracy\"\n ])\nDove:  \nloss=\"sparse_categorical_crossentropy\" è equivalente a  \nloss=keras.losses.sparse_categorical_crossentropy.  \noptimizer=\"sgd\" è equivalente a optimizer=keras.optimizers.SGD()  \nmetrics=[\"accuracy\"] è equivalente a \nmetrics=[keras.metrics.sparse_categorical_accuracy]  \nPer una lista completa consultare \n https://keras.io/losses/  \nhttps://keras.io/\noptimizers/\n   e  \nhttps://keras.io/metrics/  \n25",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#25": "Keras: metriche\nNell'esempio impieghiamo \n sparse_categorical_crossentropy\n  loss perché \nabbiamo label sparse, cioè per ogni istanza abbiamo solo una target class \nda 0 a 9, e ogni classe è esclusiva.  \nSe avessimo avuto un target on vettore di 10 reali, es [0,0,...,1.0,...,0] \navremmo dovuto impiegare la \n categorical_crossentropy \n loss. Per convertire \nlabel sparse in vettori impiegare \n keras.utils.to_categorical()\n . \nPer la binary classiﬁcation avremmo usato la \n sigmoid\n  activation invece \ndella softmax, e la \n binary_crossentropy\n  loss.\n26",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#26": "Keras: training\nInﬁne non ci resta che addestrare il modello: \n# il validation set è opzionale\n>>> \nhistory \n = \nmodel\n.\nfit\n(\nX_train\n, \ny_train\n, \nepochs\n=\n30\n,\n... \nvalidation_data\n =\n(\nX_valid\n, \ny_valid\n))\n...\nTrain on 55000 samples, validate on 5000 samples\nEpoch 1/30\n55000/55000 [==========] - 3s 55us/sample - loss: 1.4948 - acc: 0.5757\n- val_loss: 1.0042 - val_acc: 0.7166\nEpoch 2/30\n55000/55000 [==========] - 3s 55us/sample - loss: 0.8690 - acc: 0.7318\n- val_loss: 0.7549 - val_acc: 0.7616\n[...]\nEpoch 50/50\n55000/55000 [==========] - 4s 72us/sample - loss: 0.3607 - acc: 0.8752\n- val_loss: 0.3706 - val_acc: 0.8728\nOtteniamo una accuracy del 87% sul validation set dopo 50 epoche, simile \nall'accuracy del training set, perciò non dovrebbe esserci overﬁtting. \n27",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#27": "Keras: training\nSel nel dataset ci sono classi meno frequenti di altre, si può impiegare il \nparametro \n class_weight\n  nella funzione \n ﬁt\n() in modo da diminuire l'effetto \ndelle classi più rappresentate.  \nSi può fare lo stesso ma per le singole istanze col parametro \n sample_weight \nIl parametro \n history\n  è creato dopo il ﬁt, e contiene un oggetto \n History\n  con \ndati utili relativi all'addestramento: \nimport \npandas \nas \npd\npd\n.\nDataFrame\n (\nhistory\n.\nhistory\n)\n.\n     \nplot\n(\nfigsize\n=\n(\n8\n, \n5\n))\nplt\n.\ngrid\n(\nTrue\n)\n# set the vertical range to [0-1]\nplt\n.\ngca\n()\n.\nset_ylim\n (\n0\n, \n1\n) \nplt\n.\nshow\n()\nCosa puoi dire dal graﬁco?\n28\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#28": "Keras: training\nConferma il probabile scarso overﬁtting.  \nAl principio il modello si comporta meglio col validation set, ma spesso è \ndovuto al caso. \nIl ﬁtting termina con l'accuracy sul training leggermente migliori rispetto al \nvalidation, fenomeno che capita spesso per training lunghi. \nIl validation error è ancora in discesa quando termina il training. Conviene \naumentare le epoche.\n29\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#29": "Keras: training e test\nUna volta terminato il training è possibile validare il modello sul test set: \n>>> \nmodel\n.\nevaluate\n (\nX_test\n, \ny_test\n)\n8832/10000 [==========================] - ETA: 0s - loss: 0.4074 - acc: \n0.8540\n[0.40738476498126985, 0.854]\nLe performance sono leggermente minori poiché gli iperparametri li \nabbiamo scelti in base al training e validation set. \nRicordati di non modiﬁcarli in base al test set.\n30",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#3": "Richiami: Percettrone\nUn singolo percettrone, indicato anche con Threshold logic unit (TLU) o \nLinear threshold unit (LTU), può essere usato come classiﬁcatore.  \nSe la combinazione lineare degli input è oltre una certa soglia l'output \nassumerà la classe \"positiva\", altrimenti \"negativa\". \nIl training consiste nel trovare i pesi \n w\n (parametri). \nUna rappresentazione alternativa indica esplicitamente un layer \npassthtough\n  per i valori in input, e una unità \n bias\n che restituisce sempre 1. \nNell'esempio ci sono 3 outputs, perciò 3 distinte classi binarie in output: \n4\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#30": "Keras: predizione\nUna volta addestrato possiamo fare predizione: \n>>> \nX_new \n= \nX_test\n[:\n3\n]\n>>> \ny_proba \n = \nmodel\n.\npredict\n(\nX_new\n)\n>>> \ny_proba\n.\nround\n(\n2\n)\narray([[0. , 0. , 0. , 0. , 0. , 0.09, 0. , 0.12, 0. , 0.79],\n[0. , 0. , 0.94, 0. , 0.02, 0. , 0.04, 0. , 0. , 0. ],\n[0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]],\ndtype=float32)\nDall'esempio: class 9 (ankle boot) prob=79%, class 7 (sneaker) prob=12%, \nclass 5 (sandal) prob=9% \nSe ci interessa solo la classe con probabilità più alta: \n>>> \ny_pred \n= \nmodel\n.\npredict_classes\n (\nX_new\n)\n>>> \ny_pred\narray([9, 2, 1])\n>>> \nnp\n.\narray\n(\nclass_names\n )[\ny_pred\n]\narray(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\n31",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#31": "Esercizio\nImpiegare il dataset MNIST (cifre numeriche) e una architettura simile \nall'esempio precedente.  \nValutare l'accuracy dopo 50 epoche. \n# import dataset\nfrom\n keras.datasets \n import\n mnist\n# load dataset\n(x_train, y_train),(x_test, y_test) \n =\n mnist\n.\nload_data()\n32\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#32": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n33",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#4": "Sci-kit learn: Perceptron\nLa classe Perceptron implementa un singolo TLU: \nimport \nnumpy \nas \nnp\nfrom \nsklearn.datasets \n import \nload_iris\nfrom \nsklearn.linear_model \n import \nPerceptron\niris \n= \nload_iris\n ()\nX \n= \niris\n.\ndata\n[:, (\n2\n, \n3\n)] \n# petal length, petal width\ny \n= \n(\niris\n.\ntarget \n== \n0\n)\n.\nastype\n(\nnp\n.\nint\n) \n# Iris Setosa?\nper_clf \n = \nPerceptron\n ()\nper_clf\n.\nfit\n(\nX\n, \ny\n)\ny_pred \n= \nper_clf\n.\npredict\n([[\n2\n, \n0.5\n]])\nLa classe Perceptron implementa un singolo TLU.  \nL'apprendimento è basato sull'algoritmo Stochastic Gradient Descent, cioè \nsulla classe SGDClassiﬁer con i seguenti parametri: \n loss\n=\"perceptron\", \nlearning_rate\n =\"constant\", \n eta0\n=1 (\nlearning rate\n ), and \n penalty\n =None \n(\nnessuna regolarizzazione\n ). Per ogni istanza in input i pesi sono aggiornati \nin base all'errore prodotto.\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#5": "Richiami: Multi-Layer Perceptron (MLP)\nAl contrario della classiﬁcazione basata sulla logistic regression, il \npercettrone non produce probabilità, ma effettua predizioni in base ad una \nsoglia preﬁssata. Per tale motivo si preferisce la logistic regression. \nPer stimare funzioni anche non lineare, si possono \"impilare\" più TLU \nraggruppati in singoli layer creando architetture \n deep\n . Il ﬂusso dei segnali è \nmonodirezionale (\n feedforward\n ).\n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#6": "Richiami: Multi-Layer Perceptron (MLP)\nL'algoritmo per stimare i pesi è \n backpropagation\n  training algorithm ed è \nbasato sul calcolo dei gradienti degli errori rispetto ad ogni singolo \nparametro (\n automatic differentation\n  o \nautodiff\n ).  \nIn particolare viene impiegato il \n reverse-mode autodiff\n , adatto quando ci \nsono molte connessioni (pesi) e pochi output. \nIn sintesi si analizzano \n mini-batch\n  di istanze estratte dal training set (es. 32). \nAlla ﬁne di una \n epoca\n  si è analizzato l'intero dataset. Il processo itera ﬁno \nalla convergenza. \nIl mini-batch viene dato in input alla rete e per ogni istanza viene ricavato \nl'output (\n forward pass\n ).  \nPer mezzo della loss function è ricavato l'errore commesso dalla rete. \nLa \nchain rule\n  determina quanto ogni output contribuisce all'errore. Il \nprocesso è ripetuto anche per i layer precedenti, ﬁno all'input (\n reverse pass\n ). \nInﬁne il \n gradient descent\n  impiega tali error gradients per aggiornare i pesi.\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#7": "Richiami: Multi-Layer Perceptron (MLP)\nDomanda: L'inizializzazione dei pesi deve essere random. Se tutti i pesi e \nbias fossero impostati a 0 cosa accadrebbe?\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#8": "Richiami: Multi-Layer Perceptron (MLP)\nL'inizializzazione dei pesi deve essere random. Se tutti i pesi e bias fossero \nimpostati a 0 cosa accadrebbe? \nTutte le unità di un layer si comporterebbero nello stesso modo.  \nIl backpropagation inﬂuenzerebbe tutte le unità allo stesso modo. \nPotremmo avere 100ia di unità per layer, ma è come se ne avessimo \nuna sola. \nL'assegnazione casuale dei pesi evita la simmetria e, il backpropagation \n\"addestra\" gruppi di unità in modo diverso.\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\27-Ex_11 Esercitazione Reti Neurali-sbloccato.pdf#9": "Richiami: Multi-Layer Perceptron (MLP)\nDomanda: Perché è fondamentale inserire una funzione di attivazione?\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Reti Neurali (Ex 12)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#1": "Sommario\nArchitetture non sequenziali e Keras \nOutput multipli \nKeras: Modelli statici e dinamici  \nSave & Restore \nCallbacks \nEarly stopping \nTensorBoard \nFine tuning degli iperparametri \nNumero hidden layers, numero nodi per layers \nTensorFlow playground",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#10": "Keras: Modelli dinamici\nSi crea una subclass di \n Model\n , nel costruttore si deﬁnisce il modello (cioè i \nlayers) e nella funzione \n call()\n  si deﬁnisce come saranno elaborati i dati, e \npuò comprendere loop, istruzioni if-else, etc. \nPer esempio, per il Wide & deep model: \nclass \nWideAndDeepModel\n (\nkeras\n.\nmodels\n.\nModel\n):\ndef \n__init__\n (\nself\n, \nunits\n=\n30\n, \nactivation\n =\n\"relu\"\n, \n**\nkwargs\n):\nsuper\n()\n.\n__init__\n (\n**\nkwargs\n) \n# standard args (e.g., name)\nself\n.\nhidden1 \n = \nkeras\n.\nlayers\n.\nDense\n(\nunits\n, \nactivation\n =\nactivation\n )\nself\n.\nhidden2 \n = \nkeras\n.\nlayers\n.\nDense\n(\nunits\n, \nactivation\n =\nactivation\n )\nself\n.\nmain_output \n = \nkeras\n.\nlayers\n.\nDense\n(\n1\n)\nself\n.\naux_output \n = \nkeras\n.\nlayers\n.\nDense\n(\n1\n)\ndef \ncall\n(\nself\n, \ninputs\n):\ninput_A\n, \ninput_B \n = \ninputs\nhidden1 \n = \nself\n.\nhidden1\n(\ninput_B\n)\nhidden2 \n = \nself\n.\nhidden2\n(\nhidden1\n)\nconcat \n= \nkeras\n.\nlayers\n.\nconcatenate\n ([\ninput_A\n, \nhidden2\n])\nmain_output \n = \nself\n.\nmain_output\n (\nconcat\n)\naux_output \n = \nself\n.\naux_output\n (\nhidden2\n)\nreturn \nmain_output\n , \naux_output\nmodel \n= \nWideAndDeepModel\n ()\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#11": "Keras: Modelli dinamici\nI modelli dinamici hanno lo svantaggio che \n non\n possono essere facilmente \nispezionati da Keras, tantomeno essere salvati o clonati. \nIl metodo summary() restituisce una lista di layer ma non come sono \nconnessi. \n12",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#12": "Keras: Save & Restore\nAddestrare i modelli può richiedere molto tempo. È fondamentale poter \nsalvare i parametri durante (\n checkpoints\n ) o alla ﬁne dell'addestramento. \nmodel\n.\nsave\n(\n\"my_keras_model.h5\"\n )\nmodel \n= \nkeras\n.\nmodels\n.\nload_model\n (\n\"my_keras_model.h5\"\n )\nIl salvataggio interessa i parametri, l'architettura, e gli iperparametri. \nPer il Model subclassing si usano le funzioni save_weights() e \nload_weights(), che interessano però solo i pesi.\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#13": "Keras: callbacks\nÈ possibile deﬁnire una funzione \n callback\n  che verrà invocata al principio e \nalla ﬁne di ogni epoca, o batch. Nell'esempio la funzione \nModelCheckpoint salva il modello a intervalli regolari (default: alla ﬁne di \nogni epoca): \n[\n...\n] \n# dopo la compilazione del modello\ncheckpoint_cb \n = \nkeras\n.\ncallbacks\n .\nModelCheckpoint\n (\n\"my_keras_model.h5\"\n )\nhistory \n = \nmodel\n.\nfit\n(\nX_train\n, \ny_train\n, \nepochs\n=\n10\n, \ncallbacks\n =\n[\ncheckpoint_cb\n ])\nSe impiego un validation set, posso usare il parametro save_best_only=True \nin ModelCheckpoint per salvare il modello quando le prestazioni sono le \nmigliori. Se interrompo e incomincio di nuovo l'addestramento, riparto \ndall'ultimo modello potenzialmente privo di overﬁtting. \nSi può deﬁnire la propria callback agganciandola agli eventi \non_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_begin(), \non_batch_end(), on_batch_end()\n , es.: \nclass \nPrintValTrainRatioCallback\n (\nkeras\n.\ncallbacks\n .\nCallback\n ):\n  \ndef \non_epoch_end\n (\nself\n, \nepoch\n, \nlogs\n):\n      \nprint\n(\n\"\\nval/train: {:.2f}\"\n .\nformat\n(\nlogs\n[\n\"val_loss\"\n ] \n/ \nlogs\n[\n\"loss\"\n]))\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#14": "Keras: early stopping\nCon la stessa tecnica possiamo interrompere il training se, dopo un certo \nnumero di epoche (parametro \n patience\n ), non ci sono incrementi di \nprestazioni tangibili: \ncheckpoint_cb \n = \nkeras\n.\ncallbacks\n .\nModelCheckpoint\n                                    \n (\n\"my_keras_model.h5\"\n ,\nsave_best_only\n =\nTrue\n)\nearly_stopping_cb \n = \nkeras\n.\ncallbacks\n .\nEarlyStopping\n                                    \n (\npatience\n =\n10\n, \nrestore_best_weights\n =\nTrue\n)\nhistory \n = \nmodel\n.\nfit\n(\nX_train\n, \ny_train\n, \nepochs\n=\n100\n, \n                    \n validation_data\n =\n(\nX_valid\n, \ny_valid\n),\n                    \n callbacks\n =\n[\ncheckpoint_cb\n , \nearly_stopping_cb\n ]) \n# rollback al best model \nmodel \n= \nkeras\n.\nmodels\n.\nload_model\n (\n\"my_keras_model.h5\"\n )\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#15": "TensorBoard\nUn tool utile per visualizzare l'andamento dell'addestramento. Aggiorna la \nvisualizzazione in base a un ﬁle binario chiamato event ﬁle.  \nSi possono salvare i dati di ogni training in una directory distinta, così è \npossibile caricarli e confrontarli. Di seguito TensorBoard si occupa di \ncreare la directory e salvarci i dati: \nroot_logdir \n = \nos\n.\npath\n.\njoin\n(\nos\n.\ncurdir\n, \n\"my_logs\"\n )\ndef \nget_run_logdir\n ():\nimport \ntime\nrun_id \n= \ntime\n.\nstrftime\n (\n\"run_\n%Y\n_\n%m\n_\n%d\n-\n%H\n_\n%M\n_\n%S\n\"\n)\nreturn \nos\n.\npath\n.\njoin\n(\nroot_logdir\n , \nrun_id\n)\nrun_logdir \n = \nget_run_logdir\n () \n# es. './my_logs/run_2019_01_16-11_28_43'\n[\n...\n] \n# Build and compile your model\ntensorboard_cb \n = \nkeras\n.\ncallbacks\n .\nTensorBoard\n (\nrun_logdir\n )\nhistory \n = \nmodel\n.\nfit\n(\nX_train\n, \ny_train\n, \nepochs\n=\n30\n,\n                    \n validation_data\n =\n(\nX_valid\n, \ny_valid\n), \n                    \n callbacks\n =\n[\ntensorboard_cb\n ])\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#16": "TensorBoard\nTensorBoard può funzionare come server in locale: \n$ \ntensorboard --logdir\n =\n./my_logs --port\n =\n6006\nTensorBoard 2.0.0 at http://mycomputer.local:6006 \n (\nPress CTRL+C to quit\n )\nPer l'interfacciamento con Colab consultare: \nhttps://colab.research.google.com/github/tensorflow/tensorboard/blob/master/\ndocs/get_started.ipynb\n17\n",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#17": "Fine tuning degli iperparametri\nRispetto ad altri modelli le reti neurali hanno numerosi iperparametri da \ndeﬁnire. Un approccio spesso usato è quello di esplorare lo spazio delle \nconﬁgurazioni con le classi \n GridSearchCV\n  o \nRandomizedSearchCV.  \nDeﬁniamo una funzione che prende in input gli iperparametri da \nottimizzare:  \ndef \nbuild_model\n (\nn_hidden\n =\n1\n, \nn_neurons\n =\n30\n, \nlearning_rate\n =\n3e-3\n, \ninput_shape\n =\n[\n8\n]):\nmodel \n= \nkeras\n.\nmodels\n.\nSequential\n ()\n# necessario per far si che il primo layer sia inizializzato correttamente\noptions \n = \n{\n\"input_shape\"\n : \ninput_shape\n }\nfor \nlayer \nin \nrange\n(\nn_hidden\n ):\nmodel\n.\nadd\n(\nkeras\n.\nlayers\n.\nDense\n(\nn_neurons\n , \nactivation\n =\n\"relu\"\n, \n**\noptions\n))\noptions \n = \n{}\nmodel\n.\nadd\n(\nkeras\n.\nlayers\n.\nDense\n(\n1\n, \n**\noptions\n))\noptimizer \n = \nkeras\n.\noptimizers\n .\nSGD\n(\nlearning_rate\n )\nmodel\n.\ncompile\n(\nloss\n=\n\"mse\"\n, \noptimizer\n =\noptimizer\n )\nreturn \nmodel\n...\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#18": "Fine tuning degli iperparametri\nDopodiché istanziamo una regressione per Keras; \nkeras_reg \n = \nkeras\n.\nwrappers\n .\nscikit_learn\n .\nKerasRegressor\n (\nbuild_model\n )\nNon speciﬁcando altri parametri, build_model() userà quelli di default.  \nAbbiamo appena creato un modello, e possiamo seguire i soliti step: \nkeras_reg\n .\nfit\n(\nX_train\n, \ny_train\n, \nepochs\n=\n100\n, \n              \n validation_data\n =\n(\nX_valid\n, \ny_valid\n),\n              \n callbacks\n =\n[\nkeras\n.\ncallbacks\n .\nEarlyStopping\n (\npatience\n =\n10\n)])\nmse_test \n = \nkeras_reg\n .\nscore\n(\nX_test\n, \ny_test\n)\ny_pred \n= \nkeras_reg\n .\npredict\n(\nX_new\n)\nQualsiasi parametro aggiuntivo passato a ﬁt() sarà inoltrato al modello \nKeras. \n19",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#19": "Fine tuning degli iperparametri\nMiglioriamo l'esplorazione con un comportamento random, e deﬁnendo \ndegli intervallo per i parametri impiegati: \nkeras_reg \n = \nkeras\n.\nwrappers\n .\nscikit_learn\n .\nKerasRegressor\n (\nbuild_model\n )\nNon speciﬁcando altri parametri, build_model() userà quelli di default. \nPossiamo deﬁnire intervalli da cui campionare casualmente i valori degli \niperparametri che abbiamo deﬁnito in build_model(): \nfrom \nscipy.stats \n import \nreciprocal\nfrom \nsklearn.model_selection \n import \nRandomizedSearchCV\nparam_distribs \n = \n{\n\"n_hidden\"\n : [\n0\n, \n1\n, \n2\n, \n3\n],\n\"n_neurons\"\n : \nnp\n.\narange\n(\n1\n, \n100\n),\n\"learning_rate\"\n : \nreciprocal\n (\n3e-4\n, \n3e-2\n),\n}\n# RandomizedSearchCV usa la K-fold cross-validation, ignora X/y_valid\nrnd_search_cv \n = \nRandomizedSearchCV\n (\nkeras_reg\n , \nparam_distribs\n , \nn_iter\n=\n10\n, \ncv\n=\n3\n)\nrnd_search_cv\n .\nfit\n(\nX_train\n, \ny_train\n, \nepochs\n=\n100\n,\nvalidation_data\n =\n(\nX_valid\n, \ny_valid\n),\ncallbacks\n =\n[\nkeras\n.\ncallbacks\n .\nEarlyStopping\n (\npatience\n =\n10\n)]) \n20",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#2": "Architetture non sequenziali: wide & deep\nSi possono impiegare architetture più complesse di quelle viste ﬁnora, ad \nesempio quelle non sequenziali. \nNella \n wide & deep \n l'input è connesso direttamente con l'output. Questo \npermette di apprendere sia patterns \n deep\n  (con la pipeline MLP \ntradizionale), sia regole semplici, per mezzo del percorso breve.\n3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#20": "Fine tuning degli iperparametri\nI valori degli iperparametri si ottengono alle variabili: \n>>> \nrnd_search_cv\n .\nbest_params_\n{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}\n>>> \nrnd_search_cv\n .\nbest_score_\n-0.3189529188278931\n>>> \nmodel \n= \nrnd_search_cv\n .\nbest_estimator_\n .\nmodel\nSi possono impiegare per validare il modello sul test set. \nSe lo spazio degli iperparametri è molto grande, si parte con una \nesplorazione grossolana degli intervalli, e successivamente si rafﬁna lo \nspazio limitandolo agli intervalli potenzialmente più promettenti. \n21",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#21": "Fine tuning degli iperparametri\nAltre librerie per il tuning degli iperparametri: \n• \nHyperopt\n : a popular Python library for optimizing over all sorts of complex \nsearch spaces (including real values such as the learning rate, or discrete values \nsuch as the number of layers).\n• \nHyperas\n , \nkopt \n or \nTalos\n : optimizing hyperparameters for Keras model (the ﬁrst \ntwo are based on Hyperopt).\n• \nScikit-Optimize \n (skopt): a general-purpose optimization library. The \nBayesSearchCV \n class performs Bayesian optimization using an interface \nsimilar to \nGrid\n SearchCV .\n• \nSpearmint\n : a Bayesian optimization library.\n• \nSklearn-Deap\n : a hyperparameter optimization library based on evolutionary \nalgorithms, also with a \n GridSearchCV\n -like interface.\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#22": "Numero di hidden layers\nUna MLP con 1 hidden layer e un numero sufﬁciente di nodi può \nmodellare qualsiasi funzione complessa. Ma le deep networks usano i nodi \nin modo più efﬁcienti, perciò richiedono meno potenza computazionale. \nGli strati più vicini all'input possono rappresentare forme semplici e relative \ncaratteristiche (es. segmenti, orientazioni), i layer intermedi combinano questi \nelementi per forme più complesse (es. quadrati, cerchi), mentre i layer ﬁnali si \nfocalizzano sulle forme ad alto livello (es. viso delle persone). \nInoltre le architetture deep riescono più facilmente a generalizzare a nuovi \ndatasets.  \nUna parte dei layers di una rete addestrata a riconoscere facce possono essere \nimpiegati in una nuova rete per riconoscere tagli di capelli, evitando una scelta \nrandom dei parametri iniziali (\n transfer learning\n ). \nIn generale si parte con pochi hidden layer (1 o 2) per task semplici, \nincrementandoli per task complessi, ﬁnché si raggiunge l'overﬁtting. Per i \ntask molto complessi si cercano modelli pre-addestrati da cui partire con \nnuovi addestramenti. \n23",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#23": "Numero di nodi per layers\nIl numero di nodi per l'input layer è determinato dalle istanze in entrata. \nI restanti layer tipicamente formano una piramide, dove i nodi si riducono \nall'avvicinarsi del layer di output. L'idea è che gli ultimi layer \nrappresentano poche e salienti features ad alto livello. \nMa sperimentazioni più recenti suggeriscono di mantenere costante il \nnumero di nodi per layer, ottenendo un singolo iperparametro da \nottimizzare. \nAnche per il numero di nodi si può partire da un numero basso e \nincrementarlo ﬁno a quando può comparire l'overﬁtting.\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#24": "Altri iperparametri\nLearning rate: il valore ottimale è solitamente la metà di quello massimo, \ncioè quello che genera divergenza nell'algoritmo di training.  \nSi parte da un valore alto, dove si ha sicura divergenza, e poi si divide \nper 3 e si ripete ﬁno a quando la divergenza scompare. \nBatch size: inﬂuisce sia sulle performance che su tempo di addestramento. \nSolitamente inferiore a 32. Un valore basso garantisce una iterazione di \ntraining veloce. Un valore alto più precisione nella stima dei gradienti. \nPer altre raccomandazioni:  \nPractical recommendations for gradient-based training of deep \narchitectures   \n https://arxiv.org/abs/1206.5533  \n25",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#25": "TensorFlow Playground\nTool interattivo per sperimentare reti neurali \nhttps://playground.tensorﬂow.org/   \n26\n",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#26": "Esercitazione\nAddestra la rete di default. Analizza i patterns riconosciuti dai vari layers, \ncosa puoi constatare?  \nRimpiazza la Tanh con la ReLU. Cosa cambia?  \nModiﬁca l'architettura e rendila con 1 solo hidden layer e 3 neuroni. \nAddestrala varie volte, cosa noti? \nRimuovi un nodo (ne rimagono 2). Riprova, cosa noti? \nAumenta i nodi a 8. Riprova. \nUsa il dataset a spirale e una architettura con 4 hidden layers, ognuno con \n8 nodi. Cosa noti?\n27",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#27": "Esercitazione - soluzione\nAddestra la rete di default. Analizza i patterns riconosciuti dai vari layers, cosa puoi constatare?  \nGli strati più vicini all'output sono più complessi. \nRimpiazza la Tanh con la ReLU. Cosa cambia?  \nSi accelera il training, ma ora i boundaries sono lineari. \nModiﬁca l'architettura e rendila con 1 solo hidden layer e 3 neuroni. Addestrala varie volte, cosa noti? \nI tempi di apprendimento variano molto e spesso ci si blocca in minimi locali. \nRimuovi un nodo (ne rimagono 2). Riprova, cosa noti? \nLa rete non trova soluzioni buone. Troppi pochi parametri generano underﬁtting. \nAumenta i nodi a 8. Riprova. \nPiù veloce, e non ferma più come nel caso precedente. Reti più complesse hanno più chance di \ntrovare soluzioni ottime o tendenti all'ottimo, anche se possono comunque rimanere \"bloccate\" su \nplateaus. \nUsa il dataset a spirale e una architettura con 4 hidden layers, ognuno con 8 nodi. Cosa noti? \nTraining time più lungo, spesso rallentanti da plateaus. I nodi nei layer verso l'output si aggiornano \npiù velocemente degli altri. È il \n vanishing gradinets\n  problem. Si può risolvere con una \ninizializzazione più accurata dei pesi, altri ottimizzatori (es. AdaGrad e Adam) e con la Batch \nnormalization.\n28",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#28": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n29",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#3": "Architetture non sequenziali e Keras\nImpieghiamo le \n functional API\n  di Keras.  \nQuando creiamo un layer possiamo passargli un parametro aggiuntivo che \ncorrisponde all'input del layer, es: \nhidden1 \n = \nkeras\n.\nlayers\n.\nDense\n(\n30\n, \nactivation\n =\n\"relu\"\n)(\ninput\n)\nIl layer \n Concatenate\n  permette di concatenare e creare un input composito \nper un certo layer. \ninput \n= \nkeras\n.\nlayers\n.\nInput\n(\nshape\n=\nX_train\n.\nshape\n[\n1\n:])\nhidden1 \n = \nkeras\n.\nlayers\n.\nDense\n(\n30\n, \nactivation\n =\n\"relu\"\n)(\ninput\n)\nhidden2 \n = \nkeras\n.\nlayers\n.\nDense\n(\n30\n, \nactivation\n =\n\"relu\"\n)(\nhidden1\n)\nconcat \n= \nkeras\n.\nlayers\n.\nConcatenate\n ()[\ninput\n, \nhidden2\n])\noutput \n= \nkeras\n.\nlayers\n.\nDense\n(\n1\n)(\nconcat\n)\nmodel \n= \nkeras\n.\nmodels\n.\nModel\n(\ninputs\n=\n[\ninput\n], \noutputs\n=\n[\noutput\n])\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#4": "Architetture non sequenziali e Keras\nSe volessimo suddividere l'input in 2 parti, eventualmente in \nsovrapposizione, e mandare su 2 strati distinti, allora dobbiamo creare 2 \ninput layers: \ninput_A \n = \nkeras\n.\nlayers\n.\nInput\n(\nshape\n=\n[\n5\n])\ninput_B \n = \nkeras\n.\nlayers\n.\nInput\n(\nshape\n=\n[\n6\n])\nhidden1 \n = \nkeras\n.\nlayers\n.\nDense\n(\n30\n, \nactivation\n =\n\"relu\"\n)(\ninput_B\n)\nhidden2 \n = \nkeras\n.\nlayers\n.\nDense\n(\n30\n, \nactivation\n =\n\"relu\"\n)(\nhidden1\n)\nconcat \n= \nkeras\n.\nlayers\n.\nconcatenate\n ([\ninput_A\n, \nhidden2\n])\noutput \n= \nkeras\n.\nlayers\n.\nDense\n(\n1\n)(\nconcat\n)\nmodel \n= \nkeras\n.\nmodels\n.\nModel\n(\ninputs\n=\n[\ninput_A\n, \ninput_B\n]\n, \noutputs\n=\n[\noutput\n])\n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#5": "Architetture non sequenziali e Keras\nAvendo creato due input, dobbiamo speciﬁcarli esplicitamente nella \nfunzione ﬁt(), dopo aver compilato il modello: \nmodel\n.\ncompile\n(\nloss\n=\n\"mse\"\n, \noptimizer\n =\n\"sgd\"\n)\nX_train_A\n , \nX_train_B \n = \nX_train\n[:, :\n5\n], \nX_train\n[:, \n2\n:]\nX_valid_A\n , \nX_valid_B \n = \nX_valid\n[:, :\n5\n], \nX_valid\n[:, \n2\n:]\nX_test_A\n , \nX_test_B \n = \nX_test\n[:, :\n5\n], \nX_test\n[:, \n2\n:]\nX_new_A\n, \nX_new_B \n = \nX_test_A\n [:\n3\n], \nX_test_B\n [:\n3\n]\nhistory \n = \nmodel\n.\nfit\n(\n(\nX_train_A\n , \nX_train_B\n )\n, \ny_train\n, \nepochs\n=\n20\n,\nvalidation_data\n =\n((\nX_valid_A\n , \nX_valid_B\n ), \ny_valid\n))\nmse_test \n = \nmodel\n.\nevaluate\n ((\nX_test_A\n , \nX_test_B\n ), \ny_test\n)\ny_pred \n= \nmodel\n.\npredict\n((\nX_new_A\n, \nX_new_B\n))\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#6": "Output multipli\nPerché costruire architetture con output multipli? \nIl task lo potrebbe richiedere, es. localizzare e classiﬁcare un oggetto in \nuna foto, cioè un problema di regressione e classiﬁcazione.  \nLo stesso vale per task più distinti. Sebbene si possano addestrare reti \ndistinte, conviene condividere i parametri che in qualche modo \nrappresentano potenziali features che sono di interesse per entrambi i \ntask, in modo da dover addestrare una sola volta la rete. \nImplementare una forma di regolarizzazione dei parametri per ridurre \nl'overﬁtting. Se per esempio aggiungiamo un secondo output in una \ncerta parte della rete, imponiamo che  \nla sottorete si addestri in modo autonomo,  \nsenza dipendere dalla restante parte della rete.\n7\n",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#7": "Output multipli e Keras\nPer aggiungere un secondo output (aux) è sufﬁciente collegarlo al layer \ngiusto e aggiungerlo alla lista degli output: \n[\n...\n] # Stesso codice visto fino al layer di output\noutput \n= \nkeras\n.\nlayers\n.\nDense\n(\n1\n)(\nconcat\n)\naux_output \n = \nkeras\n.\nlayers\n.\nDense\n(\n1\n)(\nhidden2\n)\nmodel \n= \nkeras\n.\nmodels\n.\nModel\n(\ninputs\n=\n[\ninput_A\n, \ninput_B\n],\n                             \n outputs\n=\n[\noutput\n, \naux_output\n ])\nOgni output deve possedere la propria \n loss function\n , da indicare quando \ncompiliamo. Solitamente si da più peso alla loss dell'output ﬁnale: \nmodel\n.\ncompile\n(\nloss\n=\n[\n\"mse\"\n, \n\"mse\"\n], \nloss_weights\n =\n[\n0.9\n, \n0.1\n], \noptimizer\n =\n\"sgd\"\n)\nNella architettura vogliamo che entrambi gli output producano lo stesso \nrisultato (y_train): \nhistory \n = \nmodel\n.\nfit\n([\nX_train_A\n , \nX_train_B\n ], [\ny_train\n, \ny_train\n], \nepochs\n=\n20\n,\n                    \n validation_data\n =\n([\nX_valid_A\n , \nX_valid_B\n ], [\ny_valid\n, \ny_valid\n])) \n8",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#8": "Output multipli e Keras\nDurante l'addestramento, oltre alla loss totale, sarà prodotta anche la loss \ndel layer di output principale e aux: \ntotal_loss\n , \nmain_loss\n , \naux_loss \n = \nmodel\n.\nevaluate\n (\n                             [\n X_test_A\n , \nX_test_B\n ], [\ny_test\n, \ny_test\n])\nAnche la funzione predict() produrrà un doppio output: \ny_pred_main\n , \ny_pred_aux \n = \nmodel\n.\npredict\n([\nX_new_A\n, \nX_new_B\n])\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\28-Ex_12 Esercitazione Reti Neurali-sbloccato.pdf#9": "Keras: Modelli statici e dinamici\nEntrambe le API, sequential e functional, seguono un approccio \ndichiarativo\n , dove prima si deﬁniscono i layer, come sono connessi, e \nsuccessivamente viene avviato il ﬂusso dei dati. Si hanno i seguenti \nvantaggi: \nil modello può facilmente essere salvato, clonato e condiviso \nla struttura può essere visualizzata \nil framework può inferire il tipo di dati e controllare i tipi (favorisce il \ndebug) \nMa non si possono prevedere loop, architetture dinamiche, conditional \nbranching e altri comportamenti dinamici. \nPer tale motivo si impiega il Subclassing API.\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#0": "Machine Learning\nUniversità Roma Tre  \nDipartimento di Ingegneria \nAnno Accademico 2021 -2022\nClassiﬁcatore di Bayes",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#1": "Sommario\n!Approccio parametrico (distribuzione MultiNormale)\n!Approccio non parametrico (Parzen Window) ",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#10": "Classiﬁcatore di Bayes\n3prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore di Bayes\nDato unpattern𝐱daclassificare inunadelle𝑠classi𝑤1,𝑤2…𝑤𝑠di\ncuisono note:\nle\nprobabilità apriori𝑃𝑤1,𝑃𝑤2…𝑃𝑤𝑠\nledensità diprobabilità condizionali 𝑝𝐱𝑤1,𝑝𝐱𝑤2…𝑝𝐱𝑤𝑠\nlaregola diclassificazione diBayes assegna 𝐱allaclasse𝑏percui\nèmassima laprobabilità aposteriori :\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑃𝑤𝑖𝐱\nMassimizzare laprobabilità aposteriori significa massimizzare la\ndensità diprobabilità condizionale tenendo comunque conto della\nprobabilità apriori delle classi .\nLa\nregola sidimostra ottima inquanto minimizza l’errore di\nclassificazione .Adesempio nelcaso di2classi e𝑑=1:\n𝑃𝑒𝑟𝑟𝑜𝑟=න\n1𝑝𝑥𝑤2𝑃𝑤2𝑑𝑥+න\n2𝑝𝑥𝑤1𝑃𝑤1𝑑𝑥\n",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#11": "Classiﬁcatore di Bayes\n3prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore di Bayes\nDato unpattern𝐱daclassificare inunadelle𝑠classi𝑤1,𝑤2…𝑤𝑠di\ncuisono note:\nle\nprobabilità apriori𝑃𝑤1,𝑃𝑤2…𝑃𝑤𝑠\nledensità diprobabilità condizionali 𝑝𝐱𝑤1,𝑝𝐱𝑤2…𝑝𝐱𝑤𝑠\nlaregola diclassificazione diBayes assegna 𝐱allaclasse𝑏percui\nèmassima laprobabilità aposteriori :\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑃𝑤𝑖𝐱\nMassimizzare laprobabilità aposteriori significa massimizzare la\ndensità diprobabilità condizionale tenendo comunque conto della\nprobabilità apriori delle classi .\nLa\nregola sidimostra ottima inquanto minimizza l’errore di\nclassificazione .Adesempio nelcaso di2classi e𝑑=1:\n𝑃𝑒𝑟𝑟𝑜𝑟=න\n1𝑝𝑥𝑤2𝑃𝑤2𝑑𝑥+න\n2𝑝𝑥𝑤1𝑃𝑤1𝑑𝑥\n",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#12": "Classiﬁcatore di Bayes\n3prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore di Bayes\nDato unpattern𝐱daclassificare inunadelle𝑠classi𝑤1,𝑤2…𝑤𝑠di\ncuisono note:\nle\nprobabilità apriori𝑃𝑤1,𝑃𝑤2…𝑃𝑤𝑠\nledensità diprobabilità condizionali 𝑝𝐱𝑤1,𝑝𝐱𝑤2…𝑝𝐱𝑤𝑠\nlaregola diclassificazione diBayes assegna 𝐱allaclasse𝑏percui\nèmassima laprobabilità aposteriori :\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑃𝑤𝑖𝐱\nMassimizzare laprobabilità aposteriori significa massimizzare la\ndensità diprobabilità condizionale tenendo comunque conto della\nprobabilità apriori delle classi .\nLa\nregola sidimostra ottima inquanto minimizza l’errore di\nclassificazione .Adesempio nelcaso di2classi e𝑑=1:\n𝑃𝑒𝑟𝑟𝑜𝑟=න\n1𝑝𝑥𝑤2𝑃𝑤2𝑑𝑥+න\n2𝑝𝑥𝑤1𝑃𝑤1𝑑𝑥\n3prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore di Bayes\nDato unpattern𝐱daclassificare inunadelle𝑠classi𝑤1,𝑤2…𝑤𝑠di\ncuisono note:\nle\nprobabilità apriori𝑃𝑤1,𝑃𝑤2…𝑃𝑤𝑠\nledensità diprobabilità condizionali 𝑝𝐱𝑤1,𝑝𝐱𝑤2…𝑝𝐱𝑤𝑠\nlaregola diclassificazione diBayes assegna 𝐱allaclasse𝑏percui\nèmassima laprobabilità aposteriori :\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑃𝑤𝑖𝐱\nMassimizzare laprobabilità aposteriori significa massimizzare la\ndensità diprobabilità condizionale tenendo comunque conto della\nprobabilità apriori delle classi .\nLa\nregola sidimostra ottima inquanto minimizza l’errore di\nclassificazione .Adesempio nelcaso di2classi e𝑑=1:\n𝑃𝑒𝑟𝑟𝑜𝑟=න\n1𝑝𝑥𝑤2𝑃𝑤2𝑑𝑥+න\n2𝑝𝑥𝑤1𝑃𝑤1𝑑𝑥\nℜ1eℜ2rappresentano due regioni disgiunte\ndell’insieme deinumeri reali .\nSipuò dimostrare che esiste unpunto x*per il\nquale l’errore èminimo .\nInfatti per x*=x B(dove Bstaper Bayes) l’area\nindicata come reducible error èpari a0.\nCiascuno dei due integrali esprime laparte\ndella distribuzione diprobabilità diuna classe\nche cade nell’area dell’altra classe (errata) .\nInquesto caso, lasuperficie decisionale (vedi\ndopo) èunpunto sull’asse deinumeri reali .",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#13": "Classificatore di Bayes30 CHAPTER 2. BAYESIAN DECISION THEORY\n2.7 Error Probabilities and Integrals\nWe can obtain additional insight into the operation of a general classiﬁer — Bayes or\notherwise — if we consider the sources of its error. Consider ﬁrst the two-category\ncase, and suppose the dichotomizer has divided the space into two regions R1andR2\nin a possibly non-optimal way. There are two ways in which a classiﬁcation error can\noccur; either an observation xfalls in R2and the true state of nature is ω1, orxfalls\ninR1and the true state of nature is ω2. Since these events are mutually exclusive\nand exhaustive, the probability of error is\nP(error )= P(x∈R2,ω1)+P(x∈R1,ω2)\n=P(x∈R2|ω1)P(ω1)+P(x∈R1|ω2)P(ω2)\n=/integraldisplay\nR2p(x|ω1)P(ω1)dx+/integraldisplay\nR1p(x|ω2)P(ω2)dx. (68)\nThis result is illustrated in the one-dimensional case in Fig. 2.17. The two in-\ntegrals in Eq. 68 represent the pink and the gray areas in the tails of the functions\np(x|ωi)P(ωi). Because the decision point x∗(and hence the regions R1andR2) were\nchosen arbitrarily for that ﬁgure, the probability of error is not as small as it might\nbe. In particular, the triangular area marked “reducible error” can be eliminated if\nthe decision boundary is moved to xB. This is the Bayes optimal decision boundary\nand gives the lowest probability of error. In general, if p(x|ω1)P(ω1)>p(x|ω2)P(ω2),\nit is advantageous to classify xas in R1so that the smaller quantity will contribute\nto the error integral; this is exactly what the Bayes decision rule achieves.\nω2 ω1\nx\nx* R2 R1p(x|ωi)P(ωi)\nreducible\nerror\n∫p(x|ω1)P(ω1)dx\nR2∫p(x|ω2)P(ω2)dx\nR1xB\nFigure 2.17: Components of the probability of error for equal priors and (non-optimal)\ndecision point x∗. The pink area corresponds to the probability of errors for deciding\nω1when the state of nature is in fact ω2; the gray area represents the converse, as\ngiven in Eq. 68. If the decision boundary is instead at the point of equal posterior\nprobabilities, xB, then this reducible error is eliminated and the total shaded area is\nthe minimum possible — this is the Bayes decision and gives the Bayes error rate.\nIn the multicategory case, there are more ways to be wrong than to be right, and\nit is simpler to compute the probability of being correct. Clearly\nP(correct )=c/summationdisplay\ni=1P(x∈Ri,ωi)",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#14": "Esempio\n4prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempio\nUna stima (grossolana) delle probabilità a priori e delle densità può \nessere effettuata a partire dal training set come segue (si vedranno in \nseguito tecniche più rigorose per effettuare tale stima):\nProbabilità a priori\n : si considera semplicemente l’occorrenza dei \npattern nel training set: 𝑃𝑤1=8/18, 𝑃𝑤2=10/18\nDensità di probabilità condizionali\n per un nuovo pattern 𝐱da \nclassificare: si contano le occorrenze dei pattern del training set \ndelle due classi in un intorno di 𝐱:\n𝑝\n𝐱𝑤1=1/8\n𝑝\n𝐱𝑤2=Τ210=1/5\n𝑝\n𝐱=1\n8×8\n18+1\n5×10\n18=1\n18+2\n18=1\n6\nSi ottiene quindi :\n𝑃𝑤1𝐱=𝑝𝐱𝑤1∙𝑃𝑤1\n𝑝𝐱=𝟏/𝟏𝟖\n𝟏/𝟔=𝟏\n𝟑\n𝑃𝑤2𝐱=𝑝𝐱𝑤2∙𝑃𝑤2\n𝑝𝐱=𝟐/𝟏𝟖\n𝟏/𝟔=𝟐\n𝟑\nL’approccio Bayesiano assegna il pattern 𝐱alla classe 𝑤2(femmine).\nx\nPesoAltezzaClassificare le persone in\nmaschi/femmine in base a \npeso e all’altezza , a partire \ndal training set in figura.\n𝐕\nè uno spazio a 2 \ndimensioni ( 𝑑=2)\nW=𝑤1,𝑤2\n𝑤1= maschi ( blu),\n𝑤2= femmine ( rosso )\n!Vogliamo eseguire la stima (grossolana) delle probabilità a priori \ndelle classi wie delle densità di probabilità condizionali per un nuovo \npattern xdata la classe wi  a partire dal training set (per la seconda \nstima consideriamo l’intorno del pattern xcerchiato in figura)",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#15": "Esempio\n4prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempio\nUna stima (grossolana) delle probabilità a priori e delle densità può \nessere effettuata a partire dal training set come segue (si vedranno in \nseguito tecniche più rigorose per effettuare tale stima):\nProbabilità a priori\n : si considera semplicemente l’occorrenza dei \npattern nel training set: 𝑃𝑤1=8/18, 𝑃𝑤2=10/18\nDensità di probabilità condizionali\n per un nuovo pattern 𝐱da \nclassificare: si contano le occorrenze dei pattern del training set \ndelle due classi in un intorno di 𝐱:\n𝑝\n𝐱𝑤1=1/8\n𝑝\n𝐱𝑤2=Τ210=1/5\n𝑝\n𝐱=1\n8×8\n18+1\n5×10\n18=1\n18+2\n18=1\n6\nSi ottiene quindi :\n𝑃𝑤1𝐱=𝑝𝐱𝑤1∙𝑃𝑤1\n𝑝𝐱=𝟏/𝟏𝟖\n𝟏/𝟔=𝟏\n𝟑\n𝑃𝑤2𝐱=𝑝𝐱𝑤2∙𝑃𝑤2\n𝑝𝐱=𝟐/𝟏𝟖\n𝟏/𝟔=𝟐\n𝟑\nL’approccio Bayesiano assegna il pattern 𝐱alla classe 𝑤2(femmine).\nx\nPesoAltezzaClassificare le persone in\nmaschi/femmine in base a \npeso e all’altezza , a partire \ndal training set in figura.\n𝐕\nè uno spazio a 2 \ndimensioni ( 𝑑=2)\nW=𝑤1,𝑤2\n𝑤1= maschi ( blu),\n𝑤2= femmine ( rosso )",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#16": "Esempio\n4prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempio\nUna stima (grossolana) delle probabilità a priori e delle densità può \nessere effettuata a partire dal training set come segue (si vedranno in \nseguito tecniche più rigorose per effettuare tale stima):\nProbabilità a priori\n : si considera semplicemente l’occorrenza dei \npattern nel training set: 𝑃𝑤1=8/18, 𝑃𝑤2=10/18\nDensità di probabilità condizionali\n per un nuovo pattern 𝐱da \nclassificare: si contano le occorrenze dei pattern del training set \ndelle due classi in un intorno di 𝐱:\n𝑝\n𝐱𝑤1=1/8\n𝑝\n𝐱𝑤2=Τ210=1/5\n𝑝\n𝐱=1\n8×8\n18+1\n5×10\n18=1\n18+2\n18=1\n6\nSi ottiene quindi :\n𝑃𝑤1𝐱=𝑝𝐱𝑤1∙𝑃𝑤1\n𝑝𝐱=𝟏/𝟏𝟖\n𝟏/𝟔=𝟏\n𝟑\n𝑃𝑤2𝐱=𝑝𝐱𝑤2∙𝑃𝑤2\n𝑝𝐱=𝟐/𝟏𝟖\n𝟏/𝟔=𝟐\n𝟑\nL’approccio Bayesiano assegna il pattern 𝐱alla classe 𝑤2(femmine).\nx\nPesoAltezzaClassificare le persone in\nmaschi/femmine in base a \npeso e all’altezza , a partire \ndal training set in figura.\n𝐕\nè uno spazio a 2 \ndimensioni ( 𝑑=2)\nW=𝑤1,𝑤2\n𝑤1= maschi ( blu),\n𝑤2= femmine ( rosso )",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#17": "Approccio Bayesiano\n5prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes : approccio parametrico e\nnon-parametrico\nMentre\n lastima delle probabilità apriori èabbastanza semplice\n(senon sihanno elementi sipossono ipotizzare leclassi\nequiprobabili) ,laconoscenza delle densità condizionali è\npossibile “solo inteoria”;nella pratica duesoluzioni :\nApproccio\n parametrico :sifanno ipotesi sulla forma delle\ndistribuzioni (es.distribuzione multinormale )esiapprendono i\nparametri fondamentali (vettore medio ,matrice dicovarianza )\ndaltraining set.\nApproccio\n nonparametrico :siapprendono ledistribuzioni dal\ntraining set(es.attraverso ilmetodo Parzen Window ).\nGeneralmente l’approccio parametrico siutilizza quando, oltre ad\navere unaragionevole certezza (osperanza )chelaforma della\ndistruzione siaadeguata, ladimensione deltraining setnon è\nsufficiente perunabuona stima della densità .\nL’approccio\n parametrico èinfatti generalmente caratterizzato\ndaunminor numero digradi dilibertà eilrischio dioverfitting\ndeidati, quando iltraining setèpiccolo, èminore .",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#18": "Sommario\n!Approccio parametrico (distribuzione MultiNormale)\n!Approccio non parametrico (Parzen Window) ",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#19": "Approccio Bayesiano\nGeneralmente l’approccio parametrico siutilizza quando, oltre ad\navere una ragionevole certezza (osperanza )che laforma della\ndistribuzione siaadeguata, ladimensione deltraining setnon è\nsufficiente perunabuona stima della densità .\n!L’approccio parametrico èinfatti generalmente caratterizzato\ndaunminor numero digradi dilibertà eilrischio dioverfitting\ndeidati, quando iltraining setèpiccolo, èminore .",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#2": "Probabilità\nQuando un agente conosce l’ambiente circostante , l’approccio\nlogico gliconsente di derivare piani efficaci . Ma gliagenti non \nhanno quasi maiaccesso a tutta l’informazione necessaria : \ndevono quindi agire in condizioni di incertezza .\nLa teoria della probabilità èilmodo migliore di ragionare in \ncondizioni di incertezza .",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#20": "Distribuzione Normale (d=1)\n6prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistribuzione Normale (d=1) \nLa\ndensità diprobabilità della distribuzione normale (𝑑=1)è:\n𝑝𝑥=1\n𝜎2𝜋𝑒−𝑥−𝜇2\n2𝜎2\ndove𝜇èilvalor medio è𝜎ladeviazione standard (oscarto\nquadratico medio )eilsuoquadrato 𝜎2lavarianza .\nSolo il5%circa del“volume” èesternoall’intervallo [𝜇−2𝜎,𝜇+\n2𝜎].\nSolitamente siassume che ladistribuzione valga 0adistanze\nmaggiori di3𝜎dalvalore medio .\n6prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistribuzione Normale (d=1) \nLa\ndensità diprobabilità della distribuzione normale (𝑑=1)è:\n𝑝𝑥=1\n𝜎2𝜋𝑒−𝑥−𝜇2\n2𝜎2\ndove𝜇èilvalor medio è𝜎ladeviazione standard (oscarto\nquadratico medio )eilsuoquadrato 𝜎2lavarianza .\nSolo il5%circa del“volume” èesternoall’intervallo [𝜇−2𝜎,𝜇+\n2𝜎].\nSolitamente siassume che ladistribuzione valga 0adistanze\nmaggiori di3𝜎dalvalore medio .\n6prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistribuzione Normale (d=1) \nLa\ndensità diprobabilità della distribuzione normale (𝑑=1)è:\n𝑝𝑥=1\n𝜎2𝜋𝑒−𝑥−𝜇2\n2𝜎2\ndove𝜇èilvalor medio è𝜎ladeviazione standard (oscarto\nquadratico medio )eilsuoquadrato 𝜎2lavarianza .\nSolo il5%circa del“volume” èesternoall’intervallo [𝜇−2𝜎,𝜇+\n2𝜎].\nSolitamente siassume che ladistribuzione valga 0adistanze\nmaggiori di3𝜎dalvalore medio .\nN.B. La funzione p(x) sopra nonvaconfusa con la densità di probabilità assoluta \nchecompare a denominatore del Teorema di Bayes. Sono due grandezze diverse !",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#21": "Esempio Stima di µes(d=1)\n7prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempio stima di 𝜇e 𝜎(d=1)\nDato\n untraining setdipattern mono -dimensionali composto da\n𝑛=10elementi :\n3,7,9,−2,15,54,−11,0,23,−8\nLa\nstima deiparametri permassima verosimiglianza (maximum\nlikelihood )sidimostra [1]essere :\nStima\n per𝜇:media campionaria deivalori .\nStima\n per𝜎2:varianza campionaria deivalori .\n\u000b\f \u000b\f \u000b\f91090\n108 230 11 54 152 973 1\n1  \u0010\u000e\u000e\u000e\u0010\u000e\u000e\u000e\u0010\u000e\u000e\u000e  ¦\n n\niixnP\n\u000b\f¦\n  \u0010  n\niixn 12 21P V\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f8.3181098 9 23 90 911 9 54 9 15 92 99 97 9322 2 2 2 2 2 2 2 2\n \u0010\u0010\u000e\u0010\u000e\u0010\u000e\u0010\u0010\u000e\u0010\u000e\u0010\u000e\u0010\u0010\u000e\u0010\u000e\u0010\u000e\u0010 \n[1] https://it.wikipedia.org/wiki/Metodo_della_massima_verosimiglianza!Vogliamo eseguire la stima dei parametri tramite il Metodo \ndella Massima Verosimiglianza ( Maximum Likelihood )\nIl metodo consiste nel massimizzare la funzione di verosimiglianza, definita in \nbase alla probabilità di osservare una data realizzazione campionaria , \ncondizionatamente ai valori assunti dai parametri statistici oggetto di stima",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#22": "Esempio Stima di µes(d=1)\n7prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempio stima di 𝜇e 𝜎(d=1)\nDato\n untraining setdipattern mono -dimensionali composto da\n𝑛=10elementi :\n3,7,9,−2,15,54,−11,0,23,−8\nLa\nstima deiparametri permassima verosimiglianza (maximum\nlikelihood )sidimostra [1]essere :\nStima\n per𝜇:media campionaria deivalori .\nStima\n per𝜎2:varianza campionaria deivalori .\n\u000b\f \u000b\f \u000b\f91090\n108 230 11 54 152 973 1\n1  \u0010\u000e\u000e\u000e\u0010\u000e\u000e\u000e\u0010\u000e\u000e\u000e  ¦\n n\niixnP\n\u000b\f¦\n  \u0010  n\niixn 12 21P V\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f8.3181098 9 23 90 911 9 54 9 15 92 99 97 9322 2 2 2 2 2 2 2 2\n \u0010\u0010\u000e\u0010\u000e\u0010\u000e\u0010\u0010\u000e\u0010\u000e\u0010\u000e\u0010\u0010\u000e\u0010\u000e\u0010\u000e\u0010 \n[1] https://it.wikipedia.org/wiki/Metodo_della_massima_verosimiglianza",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#23": "Esempio Stima di µes(d=1)\n8prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…in forma grafica\n\u000b\f\n01495.07559.441\n1416.32 855.171)25(4015.0 8.31829252\n   \n \u0010 \u0010\u0010e e p\nATTENZIONE SIAMO NEL \nCONTINUO :\n𝑝è unadensità di \nprobabilità : 𝑝(25) non è la \nprobabilità del valore 25 \n(questa vale 0!) ma la \ndensità di probabilità nel\npunto 25. Solo considerando\nun intervallo di valori (anche\npiccolo) sulla base possiamo\nparlare di probabilità . \nIn altre parole l’intervallo\n𝑥,𝑥+𝑑𝑥ha probabilità\n𝑝𝑥𝑑𝑥.N.B. Siamo nelcontinuo: p(25) è una densità di probabilità , non una probabilità !  ",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#24": "Esempio Stima di µes(d=1)\n",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#3": "Spazio di Probabilità\nUno spazio di probabilità èuna terna (Ω, !, P) dove\n!Ωèun insieme qualunque (in genere pensato come l’insieme\ndeirisultati possibili di un esperimento casuale );!!èdetta σ-algebra , ovvero un insieme di insiemi (glieventi ) \nper iquali sipuòcalcolare una probabilit à;\n!P()èappunto una misura di probabilit àsuΩ(P:Ω → [0, 1]).\nPer la precisione , una σ-algebra èuna famiglia di insiemi taliche\n!∅∈!;!se A ∈!allora anche ilsuocomplementare Āèin!;\n!unioni numerabili di elementi di !appartengono ancora ad !.",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#4": "Spazio di Probabilità\nAd esempio : nell’esperimento “lancio di un dado”,\nΩ= {1, 2, 3, 4, 5, 6}, !èla σ-algebra generata dagli\neventi elementari di Ω, cioè di fatto, quelli per iquali è\npossibile calcolare una probabilit à.\nAd esempio E= “numero pari” = {2, 4, 6}, F= “numero\nmaggiore di 4” = {5, 6}. \nG = “ numero 7” appartiene a !?",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#5": "Esempio di Misura di Probabilità\nSe ildado non ètruccato , cioèglieventi elementari sono\nequiprobabili , allora\n\"#=#&'() *'+,-.+,/) '#\n#&'() 0,(()1)/) 2)Ω\ncioè, negli esempi precedenti\n\"#=#2,4,6\n#1,2,3,4,5,6=3\n6=1\n2\n\";=#5,6\n#1,2,3,4,5,6=2\n6=1\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#6": "Probabilità\nUna probabilità (grado di credenza) èuna misura su un insieme di \neventi che soddisfa tre assiomi (assiomi di Kolmogorov [1]):\n!La misura di ogni evento è compresa fra 0 e 1;\n!La misura dell’ intero insieme di eventi è 1;\n!La probabilità dell’ unione di eventi disgiunti (o mutuamente\nesclusivi )è pari alla somma delle probabilità dei singoli eventi .\nDato unospazio di probabilità (Ω, !, P), due eventi Ae Bsidicono\ndisgiunti quando la lorointersezione èvuota .\nDato unospazio di probabilità (Ω, !, P), due eventi Ae Bsidicono\nindipendenti se P(A∩B) = P(A) ⋅P(B). \n[1] S. Russell & P. Norvig, Artificial Intelligence: A Modern Approach (4 ed.) , Pearson, 2020.",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#7": "Probabilità\nUn modello probabilistico consiste in uno spazio di possibili esiti\n(cioè descrizioni complete di stati) mutuamente esclusivi insieme \nalla misura di probabilità associata ad ogni esito. \nProbabilità condizionata P(A/B) , dove Ae Bsono proposizioni (cioè \nenunciati che affermano che qualcosa è verificato): “la probabilità \ndi A, posto che tutto quello che sappiamo èB”.\nIn altritermini, la probabilità condizionata P(A/B) esprime una \n“correzione ” delle aspettative per A, dettata dall’osservazione di B.\nEsempio: P(carie/maldidenti)=0.8 indica che se un paziente ha il mal di \ndenti e non è disponibile nessun’altra informazione, la probabilità che \nabbia una carie sarà 0.8.\nN.B. La probabilità condizionata P(A/B) ha senso solo se Bha \nprobabilità non nulla di verificarsi .",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#8": "Approccio Bayesiano\n2prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneApproccio Bayesiano\nIlproblema èposto intermini probabilistici .Setutte ledistribuzioni\ningioco sono notel’approccio Bayesiano costituisce lamigliore\nregola diclassificazione possibile :soluzione OTTIMA !\nSia\n𝐕unospazio dipattern𝑑-dimensionali eW=𝑤1,𝑤2…𝑤𝑠\nuninsieme di𝑠classi disgiunte costituite daelementi di𝐕\nPer\nogni𝐱∈𝐕eperogni𝑤𝑖∈W,indichiamo con𝑝𝐱𝑤𝑖la\ndensità diprobabilità condizionale (ocondizionata) di𝐱data𝑤𝑖,\novvero ladensità diprobabilità che ilprossimo pattern sia𝐱\nsottol’ipotesi chelasuaclasse diappartenenza sia𝑤𝑖\nPer\nogni𝑤𝑖∈W,indichiamo con𝑃𝑤𝑖laprobabilità apriori di\n𝑤𝑖ovvero laprobabilità, indipendentemente dall’osservazione,\ncheilprossimo pattern daclassificare siadiclasse𝑤𝑖\nPer\n ogni𝐱∈𝐕indichiamo con𝑝𝐱ladensità diprobabilità\nassoluta di𝐱,ovvero ladensità diprobabilità cheilprossimo\npattern daclassificare sia𝐱\nPer\nogni𝑤𝑖∈Weperogni𝐱∈𝐕indichiamo con𝑃𝑤𝑖𝐱la\nprobabilità aposteriori di𝑤𝑖dato𝐱,ovvero laprobabilità che\navendo osservato ilpattern𝐱,laclasse diappartenenza sia𝑤𝑖.\nPerilteorema diBayes :\n𝑃𝑤𝑖𝐱=𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n𝑝𝐱𝑝𝐱=෍\n𝑖=1𝑠\n𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖 dove ෍\n𝒊=𝟏𝒔\n𝑃𝑤𝑖=1",
    "data_test\\rootfolder\\università\\MachineLearning\\29-CB(1)-sbloccato.pdf#9": "Approccio Bayesiano\n2prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneApproccio Bayesiano\nIlproblema èposto intermini probabilistici .Setutte ledistribuzioni\ningioco sono notel’approccio Bayesiano costituisce lamigliore\nregola diclassificazione possibile :soluzione OTTIMA !\nSia\n𝐕unospazio dipattern𝑑-dimensionali eW=𝑤1,𝑤2…𝑤𝑠\nuninsieme di𝑠classi disgiunte costituite daelementi di𝐕\nPer\nogni𝐱∈𝐕eperogni𝑤𝑖∈W,indichiamo con𝑝𝐱𝑤𝑖la\ndensità diprobabilità condizionale (ocondizionata) di𝐱data𝑤𝑖,\novvero ladensità diprobabilità che ilprossimo pattern sia𝐱\nsottol’ipotesi chelasuaclasse diappartenenza sia𝑤𝑖\nPer\nogni𝑤𝑖∈W,indichiamo con𝑃𝑤𝑖laprobabilità apriori di\n𝑤𝑖ovvero laprobabilità, indipendentemente dall’osservazione,\ncheilprossimo pattern daclassificare siadiclasse𝑤𝑖\nPer\n ogni𝐱∈𝐕indichiamo con𝑝𝐱ladensità diprobabilità\nassoluta di𝐱,ovvero ladensità diprobabilità cheilprossimo\npattern daclassificare sia𝐱\nPer\nogni𝑤𝑖∈Weperogni𝐱∈𝐕indichiamo con𝑃𝑤𝑖𝐱la\nprobabilità aposteriori di𝑤𝑖dato𝐱,ovvero laprobabilità che\navendo osservato ilpattern𝐱,laclasse diappartenenza sia𝑤𝑖.\nPerilteorema diBayes :\n𝑃𝑤𝑖𝐱=𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n𝑝𝐱𝑝𝐱=෍\n𝑖=1𝑠\n𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖 dove ෍\n𝒊=𝟏𝒔\n𝑃𝑤𝑖=1",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nIntroduzione alla  \nRegressione\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#1": "Sommario\nIntroduzione alla Regressione \nSimple Linear Regression \n•\n Fase di Training (minimizzazione della funzione di \ncosto) \nMultiple Regression \n•\n Fase di Training (minimizzazione della funzione di \ncosto)\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#10": "Il Processo di Training\nDati di \nTraining\nEstrazione \ndelle \nFeatures  \nModello \ndi ML\nComparazione\ntra valori \nosservati e \nprevisti ∀ iyi osservatoxi ŷi previsto\n 11xi\nFunzione  \ndi Costopesi ŵ 0 e ŵ 1  \ncalcolati(N esempi)\nAlgoritmo di \nApprendimento\nCalcolo vettore \ndei pesi ŵ",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#11": " 12Il Processo di Training\n•Dati di training : insieme di esempi ( xi, yi) relativi a casi conosciuti \n(i punti nel piano x-y), da utilizzare per calcolare la funzione di \ncosto RSS. \n•Estrazione di features : in questo caso tale funzione è inattiva, nel \nsenso che riproduce in uscita il suo ingresso xi. \n•Modello di ML : ipotesi f scelta, istanziata con i valori dei pesi più \nopportuni. \n•Comparazione tra dati osservati e dati previsti : per ogni esempio \nabbiamo il valore vero yi e il valore previsto ŷi, da utilizzare per il \ncalcolo della funzione RSS. \n•Algoritmo di Apprendimento : algoritmo che calcola i pesi che \nminimizzano la funzione di costo RSS, da utilizzare per deﬁnire il \nmodello di ML.",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#12": "Minimizzazione  della funzione RSS\nIn sintesi, il processo di apprendimento è formulato come una \nricerca di ottimizzazione (ricerca del minimo) nello \n spazio dei \npesi\n. \nA tal ﬁne possiamo calcolare e avvalerci del \n gradiente\n  della \n“misura d’errore” \n RSS\n deﬁnita in precedenza.  \nSi può dimostrare che la \n RSS\n è una funzione convessa.  \nRicordiamoci che, per funzioni convesse, quando il gradiente è \nuguale a zero si ha un minimo globale. \n \n13",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#13": "Minimo  di una funzione convessa\n \n14\nw0w1g\nŵ\nw\nŵ0ŵ1\ngradiente:ij\nrg(w)=@g(w)\n@w0i+@g(w)\n@w1j\n<latexit sha1_base64=\"UbdTBP63qiqYyxQKEMwdHSMgLt8=\">AAACkniclVFNT9tAEF27UGigEKC3XlaNkIBKkQ1IICQkKJceOIDUAFIcRePNJCys19buGIpW+1f6v3rgv9QOkfgIl77T03sz83Zn0kJJS1H0Nwg/zMx+nJv/1FhY/Ly03FxZvbB5aQR2RK5yc5WCRSU1dkiSwqvCIGSpwsv09qT2L+/QWJnrX/RQYC+DkZZDKYAqqd/8k2hIFSSEv8mN/EaS5mrg7v0mP+TJ0IBwSQGGJCg3XeP9s3vfd5H3nj+Z0jd4he//PSN+nnHjeaPfbEXtaAw+TeIJabEJzvrNx2SQizJDTUKBtd04Kqjn6gCh0DeS0mIB4hZG2K2ohgxtz4336Pl6aYFyXqDhUvGxiC87HGTWPmRpVZkBXdu3Xi2+53VLGu73nNRFSahFHURS4TjICiOrAyEfSINEUL8cudRcgAEiNJKDEJVYVher9xG//f00udhuxzvt7fPd1tGPyWbm2Vf2jW2wmO2xI/aTnbEOE8FMsBXsBLvhl/AgPA5PnkrDYNKzxl4hPP0HqF7LxA==</latexit>\n",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#14": "Gradiente della funzione RSS\nIl gradiente della funzione RSS: \n    \n   è deﬁnito come segue:\n \n15rRSS( w0,w1)=2\n4@RSS\n@w0\n@RSS\n@w13\n5RSS( w0,w1)=NX\ni=1[yi\u0000(w0+w1xi)]2",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#15": "Calcolo del Gradiente  \ndella funzione RSS\n \n16rRSS( w0,w1)=2\n4\u00002PN\ni=1[yi\u0000(w0+w1xi)]\n\u00002PN\ni=1[yi\u0000(w0+w1xi)]xi3\n5@RSS\n@w1=NX\ni=12[yi\u0000(w0+w1xi)]1·(\u0000xi)=\u00002NX\ni=1[yi\u0000(w0+w1xi)]xi@RSS\n@w0=NX\ni=12[yi\u0000(w0+w1xi)]1·(\u00001) =\u00002NX\ni=1[yi\u0000(w0+w1xi)]",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#16": "Algoritmi per adattare il modello \nUna volta calcolato il gradiente della funzione \n RSS\n, ci sono \ndue possibili approcci per minimizzare la funzione di \ncosto: \n“\nForma chiusa\n ”: Si uguaglia il gradiente a zero (ossia al vettore nullo) e si \nrisolvono le equazioni (non sempre è possibile o conveniente dal punto di \nvista computazionale) \nAlgoritmo di Discesa del Gradiente (\n Gradient Descent\n ) (richiede la \ndeﬁnizione del criterio di convergenza e dello “step size”)\n \n17",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#17": "Forma Chiusa (1/4)\nPoniamo il gradiente uguale al vettore nullo: \nossia:\n \n18\u00002NX\ni=1[yi\u0000(w0+w1xi)] = 0\n\u00002NX\ni=1[yi\u0000(w0+w1xi)]xi=0rRSS( w0,w1)=2\n4\u00002PN\ni=1[yi\u0000(w0+w1xi)]\n\u00002PN\ni=1[yi\u0000(w0+w1xi)]xi3\n5=0",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#18": "Forma Chiusa (2/4)\nDalla prima equazione otteniamo:\n \n19PN\ni=1yi\u0000ˆw0PN\ni=11\u0000ˆw1PN\ni=1xi=0\nˆw0=PN\ni=1yi\nN\u0000ˆw1PN\ni=1xi\nN\nda cui si ha:",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#19": "Forma Chiusa (3/4)\nDalla seconda equazione otteniamo: \nda cui si ha:\n \n20PN\ni=1xiyi\u0000ˆw0PN\ni=1xi\u0000ˆw1PN\ni=1x2\ni=0\nˆw1=PN\ni=1xiyi\u0000ˆw0PN\ni=1xiPN\ni=1x2\ni",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#2": "Introduzione alla Regressione\n \n3I modelli a regressione vengono utilizzati per prevedere \nvariabili target su scala continua , il che li rende \ninteressanti per risolvere molte questioni in ambito \nscientiﬁco e anche industriale, come ad esempio:\n• trovare relazioni fra variabili \n• valutare tendenze \n• effettuare previsioni ( e.g., vendite di una azienda nei prossimi mesi )",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#20": "Forma Chiusa (4/4)\ne con facili passaggi otteniamo: \nche, insieme a: \nvista in precedenza, ci consente di calcolare i valori dei due \npesi che minimizzano la funzione RSS.\n \n21ˆw1=PN\ni=1xiyi\u0000PN\ni=1xiPN\ni=1yi\nNPN\ni=1x2\ni\u0000(PN\ni=1xi)2\nN\nˆw0=PN\ni=1yi\nN\u0000ˆw1PN\ni=1xi\nN",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#21": "Gradient Descent (1/3)\nCome sappiamo, con questo approccio dobbiamo \naggiornare i pesi in modo tale da spostarci nella direzione \nopposta al gradiente:\n \n22w(t+1) w(t)\u0000↵·rRSS(w(t))\nw=w0\nw1\u0000\ndove:\nossia:\nw(t+1)\n0 w(t)\n0\u0000↵·@RSS(w(t))\n@w0\nw(t+1)\n1 w(t)\n1\u0000↵·@RSS(w(t))\n@w1",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#22": "Gradient Descent (2/3)\nRivediamo l’espressione del gradiente di RSS: \nL’aggiornamento dei due pesi può dunque essere effettuato \ncome segue, scegliendo un opportuno \n step size\n :\n \n23rRSS( w0,w1)=2\n4\u00002PN\ni=1[yi\u0000(w0+w1xi)]\n\u00002PN\ni=1[yi\u0000(w0+w1xi)]xi3\n5=2\n4\u00002PN\ni=1[yi\u0000ˆyi(w0,w1)]\n\u00002PN\ni=1[yi\u0000ˆyi(w0,w1)]xi3\n5\nw(t+1)\n0 w(t)\n0+2↵·NX\ni=1[yi\u0000ˆyi(w(t))]\nw(t+1)\n1 w(t)\n1+2↵·NX\ni=1[yi\u0000ˆyi(w(t))]xi",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#23": "Gradient Descent (3/3)\nDobbiamo inﬁne scegliere un \n criterio di convergenza\n . \nCome già detto, per funzioni convesse si ha un minimo \nglobale quando il gradiente è uguale a zero. \nIn pratica, possiamo terminare l’elaborazione quando:\n \n24krRSS(w(t))k2✏",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#24": "Algoritmo di Gradient Descent \n \n25w(1)= 0 (oppure lo inizializziamo in modo casuale)\nt=1\nwhilekrRSS( w(t))k2>✏\nw(t+1)\n0 w(t)\n0+2↵·NX\ni=1[yi\u0000ˆyi(w(t))]\nw(t+1)\n1 w(t)\n1+2↵·NX\ni=1[yi\u0000ˆyi(w(t))]xi\nt t+1",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#25": "Multiple Regression \n[caso di Linear Regression con Multiple Features]\nFino ad ora abbiamo ipotizzato, per la funzione \n f(\nx\n)\n, un andamento \nlineare per il nostro caso di studio relativo ai prezzi degli appartamenti. \nTuttavia l’esperienza comune ci induce a pensare che la relazione tra \nle due variabili (area e prezzo di un appartamento) non sia proprio \nlineare. In genere, all’aumentare della metratura il prezzo aumenta ma \nnon in modo esattamente proporzionale. Potremmo ipotizzare ad \nesempio una funzione quadratica o addirittura polinomiale di grado p: \n \n26f(x)=w0+w1x+w2x2\nf(x)=w0+w1x+w2x2+···+wpxpy\nAreax\ny\nAreax\n",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#26": "Multiple Regression \n[caso di Linear Regression con Multiple Features]\nIn quest’ultimo caso avremmo una \n Polinomial Regression\n , \nil cui modello è il seguente: \n \n27yi=w0+w1xi+w2x2\ni+···+wpxp\ni+✏i\nIn genere, le potenze della x sono trattate come differenti \nfeatures\n : \nfeature 1 = 1\nfeature 2 = x\nfeature 3 = x2\n······ ···\nfeature p+1 = xp",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#27": "Multiple Regression \n[caso di Linear Regression con Multiple Features]\nIl caso generale, con un solo input x\n i\n, è il seguente: \n \n28yi=w0\u00000(xi)+w1\u00001(xi)+ ···+wD\u0000D(xi)+✏i=\n=DX\nj=0wj\u0000j(xi)+✏i\ndove le features che compaiono possono assumere forme \ndiverse (non necessariamente solo potenze della x). ",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#28": "Multiple Regression \n[caso di Linear Regression con Multiple Features]\nInoltre, è importante considerare anche il caso in cui ci \nsiano più input. \nPer l’esempio degli appartamenti potremmo voler \nconsiderare non solo l’area ma anche altre caratteristiche \n(#bagni, #camere da letto, anno di costruzione, ecc.). \nIn tal caso avremmo in input un vettore \n x\ni\n per ogni \nesempio noto, le cui componenti sono appunto l’area, il \n#bagni, ecc. \n    \n \n29",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#29": "Multiple Regression \n[caso di Linear Regression con Multiple Features]\nIl caso generale, che ha in input un vettore \n x\ni\n, è pertanto il \nseguente: \n \n30\ndove le features che compaiono, ciascuna delle quali è \nfunzione del vettore \n x\ni\n, possono assumere forme diverse. yi=w0\u00000(xi)+w1\u00001(xi)+ ···+wD\u0000D(xi)+✏i=\n=DX\nj=0wj\u0000j(xi)+✏i",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#3": "Modello a \nRegressione Lineare Semplice\n \n4L’obiettivo di un modello a Regressione Lineare \nSemplice ( univariata ) consiste nell’individuare le \nrelazioni esistenti tra un’unica caratteristica (la variabile \ndescrittiva x) e una risposta continua (variabile target y).",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#30": " \n31Anche in questo caso possiamo utilizzare, come funzione di \ncosto da minimizzare, la RSS deﬁnita come segue, a partire \nda N osservazioni disponibili:\nIl problema di addestrare il nostro modello è dunque quello \ndi trovare i valori dei pesi ŵ0 ,ŵ1 ,…, ŵD che minimizzano la \nfunzione RSS (convessa anche in questo caso).\nMultiple Regression \n[caso di Linear Regression con Multiple Features]\nRSS(w)=NX\ni=1(yi\u0000ˆyi)2=NX\ni=1[yi\u0000(w0\u00000(xi)+ ···+wD\u0000D(xi))]2",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#31": "Il Processo di Training \n[caso di Linear Regression con Multiple Features]\nDati di \nTraining\nEstrazione \ndelle \nFeatures  \nModello \ndi ML\nComparazione \ntra valori \nosservati e \nprevisti ∀ iyi osservato(xi) ŷi previsto\n 32xi\nFunzione  \ndi Costovettore di pesi \n ŵ calcolatoɸ\n(N esempi)\nAlgoritmo di \nApprendimento\nCalcolo vettore \ndei pesi ŵ",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#32": "Regression Model \n in notazione matriciale \n[caso di Linear Regression con Multiple Features]\nIn molti casi può essere conveniente usare una \nnotazione matriciale. \nL’espressione: \n \n33\nrelativa all’i-esimo valore per y, può essere scritta \ncome segue: yi=DX\nj=0wj\u0000j(xi)+✏i",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#33": "Regression Model \n in notazione matriciale \n[caso di Linear Regression con Multiple Features]\n \n34\noppure: yi=[w0w1···wD]·2\n664\u00000(xi)\n\u00001(xi)\n···\n\u0000D(xi)3\n775+✏i\nyi=[\u00000(xi)\u00001(xi)···\u0000D(xi)]·2\n664w0\nw1\n···\nwD3\n775+✏i",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#34": "Regression Model \n in notazione matriciale \n[caso di Linear Regression con Multiple Features]\nIn sintesi: \n \n35yi=DX\nj=0wj\u0000j(xi)+✏i=wT·\u0000(xi)+✏i=\u0000T(xi)·w+✏i\nw=2\n664w0\nw1\n···\nwD3\n775\u0000(xi)=2\n664\u00000(xi)\n\u00001(xi)\n···\n\u0000D(xi)3\n775\ndove: \nxi=2\n664xi,1\nxi,2\n···\nxi,d3\n775",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#35": "Regression Model \n in notazione matriciale \n[caso di Linear Regression con Multiple Features]\nPossiamo inﬁne rappresentare tutte le osservazioni y in \nmodo compatto come segue: \n \n36\n    ossia: \ny=\u0000 ·w+✏2\n664y1\ny2\n···\nyN3\n775=2\n664\u00000(x1)\u00001(x1)... \u0000D(x1)\n\u00000(x2)\u00001(x2)... \u0000D(x2)\n... ... ... ...\n\u00000(xN)\u00001(xN)... \u0000D(xN)3\n775·2\n664w0\nw1\n···\nwD3\n775+2\n664✏1\n✏2\n···\n✏N3\n775",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#36": "Regression Model \n in notazione matriciale \n[caso di Linear Regression con Multiple Features]\ndove: \n    \n \n37✏=2\n664✏1\n✏2\n···\n✏N3\n775 y=2\n664y1\ny2\n···\nyN3\n775\n\u0000=2\n664\u00000(x1)\u00001(x1) ... \u0000D(x1)\n\u00000(x2)\u00001(x2) ... \u0000D(x2)\n... ... ... ...\n\u00000(xN)\u00001(xN) ... \u0000D(xN)3\n775",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#37": "Regression Model \n in notazione matriciale \n[caso di Linear Regression con Multiple Features]\nCalcoliamo ora la funzione RSS: \n \n38RSS(w)=NX\ni=1(yi\u0000ˆyi)2=NX\ni=1✏2\ni=✏T·✏\nche possiamo scrivere anche così: RSS(w)=NX\ni=1(yi\u0000ˆyi)2=NX\ni=1[yi\u0000(\u00000(xi)w0+...+\u0000D(xi)wD)]2=NX\ni=1[yi\u0000\u0000T(xi)·w]2",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#38": "Regression Model \n in notazione matriciale \n[caso di Linear Regression con Multiple Features]\nDa una precedente espressione per \n y\n ricaviamo il vettore \n ε\n: \n \n39✏=y\u0000\u0000w y=\u0000w+✏)\nLa funzione RSS assume pertanto la seguente forma in notazione \nmatriciale: \nRSS(w)=(y\u0000\u0000w)T(y\u0000\u0000w)",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#39": "Gradiente della funzione RSS \n[caso di Linear Regression con Multiple Features]\nCalcoliamo ora il gradiente della funzione RSS, partendo dalla \nprecedente espressione matriciale. \nApplicando una nota regola di calcolo differenziale matriciale \nsi ottiene: \n \n40rRSS(w)=r[(y\u0000\u0000w)T(y\u0000\u0000w)] =\u00002\u0000T(y\u0000\u0000w)",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#4": "Esempio \n \n5A titolo di esempio possiamo considerare il caso della \nprevisione del prezzo di un appartamento (variabile target y) \ndata la sua metratura (variabile descrittiva x).\nTipicamente, in casi come questo abbiamo a disposizione un \ncerto numero di esempi (osservazioni), costituiti da \nappartamenti già venduti per ciascuno dei quali abbiamo a \ndisposizione l’area in mq o in sq.ft. ( x) e il prezzo pagato per \nl’acquisto ( y).\nCiascuna delle suddette osservazioni può essere rappresentata \nda un punto in un piano cartesiano x-y, come illustrato nella \nﬁgura che segue.",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#40": "Algoritmi per adattare il modello \n[caso di Linear Regression con Multiple Features]\nAnche in questo caso, una volta calcolato il gradiente della \nfunzione \n RSS\n, ci sono due possibili approcci per \nminimizzare la funzione di costo: \n“Forma chiusa”: Si uguaglia il gradiente a zero e si risolvono le equazioni \n(non sempre è possibile o conveniente dal punto di vista computazionale) \nAlgoritmo di Discesa del Gradiente (\n Gradient Descent\n ) (richiede la \ndeﬁnizione del criterio di convergenza e dello “step size”)\n \n41",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#41": "Forma Chiusa \n[caso di Linear Regression con Multiple Features]\nPoniamo il gradiente uguale al vettore nullo: \n \n42ˆw=(\u0000T\u0000)\u00001\u0000Ty\nda cui si ha: \u00002\u0000Ty+2\u0000T\u0000ˆw=0\n\u0000T\u0000ˆw=\u0000Ty\n(\u0000T\u0000)\u00001(\u0000T\u0000)ˆw=(\u0000T\u0000)\u00001\u0000Ty\nIˆ w =(\u0000T\u0000)\u00001\u0000TyrRSS(w)=\u00002\u0000T(y\u0000\u0000w)=0",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#42": "Gradient Descent (1/4) \n[caso di Linear Regression con Multiple Features]\nDobbiamo aggiornare il vettore dei pesi in modo tale da \nspostarci nella direzione opposta al gradiente:\n \n43w(t+1) w(t)\u0000↵·rRSS(w(t))\ndove:\nrRSS(w(t))=2\n66664@RSS\n@w0\n@RSS\n@w1\n···\n@RSS\n@wD3\n77775w(t)=2\n6664w(t)\n0\nw(t)\n1\n···\nw(t)\nD3\n7775",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#43": "Gradient Descent (2/4) \n[caso di Linear Regression con Multiple Features]\nI singoli pesi devono dunque essere aggiornati come segue:\n \n44w(t+1)\n0 w(t)\n0\u0000↵·@RSS(w(t))\n@w0\nw(t+1)\n1 w(t)\n1\u0000↵·@RSS(w(t))\n@w1\n······ ·····················\nw(t+1)\nj w(t)\nj\u0000↵·@RSS(w(t))\n@wj\n······ ·····················\nw(t+1)\nD w(t)\nD\u0000↵·@RSS(w(t))\n@wD",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#44": "Gradient Descent (3/4) \n[caso di Linear Regression con Multiple Features]\nPer comprendere gli aggiornamenti da fare per i singoli pesi, \ncalcoliamo la derivata parziale di RSS, espressa in questa \nforma:\n \n45\nrispetto al generico peso j-esimo:\n@RSS(w(t))\n@wj=NX\ni=12[yi\u0000ˆyi(w(t))]·[\u0000@ˆyi(w(t))\n@wj]=\n=2NX\ni=1[yi\u0000ˆyi(w(t))]·[\u0000\u0000j(xi)] =\u00002NX\ni=1\u0000j(xi)[yi\u0000ˆyi(w(t))]RSS(w(t))=NX\ni=1[yi\u0000ˆyi(w(t))]2=NX\ni=1{yi\u0000[\u00000(xi)w(t)\n0+···+\u0000j(xi)w(t)\nj+···+\u0000D(xi)w(t)\nD]}2",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#45": "Gradient Descent (4/4) \n[caso di Linear Regression con Multiple Features]\nAnche in questo caso dobbiamo inﬁne scegliere un \n criterio \ndi convergenza\n . \nSappiamo che per funzioni convesse si ha un minimo \nglobale quando il gradiente è uguale a zero. \nIn pratica, possiamo terminare l’elaborazione quando:\n \n46krRSS(w(t))k2✏",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#46": "Algoritmo di Gradient Descent \n[caso di Linear Regression con Multiple Features]\n \n47w(1)= 0 (oppure lo inizializziamo in modo casuale)\nt=1\nwhilekrRSS( w(t))k2>✏\nfor j=0,1,. . . ,D\nderivata parziale[ j]=\u00002NX\ni=1\u0000j(xi)[yi\u0000ˆyi(w(t))]\nw(t+1)\nj w(t)\nj\u0000↵⇤derivata parziale[ j]\nt t+1",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#47": "Riferimenti\n \n48\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , 3a edizione, \nApogeo, 2015. \nMachine Learning: Regression\n , University of Washington - Coursera, 2015. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. ",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#5": "Esempio\n \n6Il problema da risolvere è il seguente: scegliere lo spazio delle \nipotesi  H (e.g., insieme di polinomi di grado massimo k) e la \nfunzione f(x) (ipotesi) che approssima meglio le osservazioni \ndisponibili di una funzione sconosciuta, da utilizzare per \nprevedere i prezzi di altri appartamenti (diversi dagli esempi).\ny\nArea xy\nArea x\ny\nArea x\n",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#6": "Apprendimento induttivo \n(Inductive Learning method)\nLa difﬁcoltà che si incontra in tale attività è dovuta al \nfatto che non è facile stabilire se una particolare f sia una \nbuona approssimazione della funzione sconosciuta. \nUna buona ipotesi si potrà generalizzare  bene, ossia \npotrà predire correttamente esempi che non ha ancora \nincontrato.Il problema dell’induzione\n 7",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#7": "Simple Linear Regression Model\n \n8In ﬁgura è rappresentato un modello lineare per f(x), dove il \npeso wo rappresenta l’intercetta e il peso w1 rappresenta la \npendenza della retta. Si noti l’offset verticale che costituisce \nl’errore che in genere esiste tra la previsione e il valore effettivo. \nAbbiamo dunque, per il valore vero e quello previsto per un \ncerto valore dell’ascissa:\nyi=w0+w1xi+✏i\nˆyi=f(xi)=w0+w1xiy\nArea x\nPrezzo",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#8": "Simple Linear Regression Model\n \n9Supponiamo di scegliere il modello lineare. Una volta \ndeﬁnito tale modello, ossia la forma della funzione f(x), \noccorre determinare i due pesi incogniti, ossia l’intercetta e la \npendenza, che deﬁniscano la f(x) “migliore” secondo un certo \ncriterio.\nUn criterio possibile è quello di minimizzare gli errori che si \nhanno sulle osservazioni. ",
    "data_test\\rootfolder\\università\\MachineLearning\\3-Regression-Introduzione-sbloccato.pdf#9": "Simple Linear Regression Model\n \n10Una delle funzioni utilizzate a tal ﬁne, che deve essere per \nl’appunto minimizzata, è la Residual Sum of Squares (RSS) , \ndeﬁnita come segue, a partire da N osservazioni disponibili:\nRSS( w0,w1)=NX\ni=1(yi\u0000ˆyi)2=NX\ni=1[yi\u0000(w0+w1xi)]2\nIl problema di addestrare il nostro modello è dunque quello \ndi trovare i valori dei due pesi ŵ0 e ŵ1 che minimizzano la \nfunzione RSS.",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#0": "Distribuzione Normale Multivariata (Multinormale)\n9prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistribuzione Normale Multivariata \n(Multinormale ) \nNotazione\n :perevitare confusione utilizziamo apedicel’indice del\npattern e(ove necessario) adapice lacomponente (scalare) :\n•𝐱𝑖pattern i-esimo (vettore)\n•𝑥𝑖𝑗componente j-esima delpattern i-esimo (scalare)\nLa\ndensità diprobabilità nella distribuzione multinormale (𝑑>1):\n𝑝𝐱=1\n2𝜋𝑑/2Σ1/2𝑒−1\n2𝐱−𝛍𝑡Σ−1𝐱−𝛍\ndove𝛍=𝜇1,𝜇2…𝜇𝑑èilvettore medio èΣ=𝜎𝑖𝑗lamatrice di\ncovarianza (𝑑×𝑑).\nSi\nassume che ivettori siano ditipo «colonna» .L’apice𝑡\n(trasposto) litrasforma inrighe .\n|6|e6-1sono rispettivamente ildeterminante el’inversa di6.\nLa\nmatrice dicovarianza èsempre simmetrica edefinita\npositiva, pertanto ammette inversa .Essendo simmetrica il\nnumero diparametri cheladefinisce è𝑑∙𝑑+1/2\nGli\nelementi diagonali 𝜎𝑖𝑖sono levarianze deirispettivi 𝑥𝑖\n(ovvero 𝜎𝑖2);glielementi non diagonali 𝜎𝑖𝑗sono le\ncovarianze tra𝑥𝑖e𝑥𝑗:\n•se𝑥𝑖e𝑥𝑗sono statisticamente indipendenti 𝜎𝑖𝑗=0\n•se𝑥𝑖e𝑥𝑗sono correlati positivamente 𝜎𝑖𝑗>0\n•se𝑥𝑖e𝑥𝑗sono correlati negativamente 𝜎𝑖𝑗<0",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#1": "Richiami\n!La Matrice di Covarianza si indica di solito con Σed è una generalizzazione del \nconcetto di varianza al caso di dimensione maggiore di uno\n!E’ una matrice che rappresenta la variazione di ogni variabile rispetto alle altre \n(inclusa se stessa)\n!E’ sempre simmetrica e definita positiva (i.e., ha tutti gli autovalori strettamente \npositivi) ---> ammette sempre Matrice Inversa\n!La Matrice Simmetrica è una matrice quadrata che ha la proprietà di essere la \ntrasposta (vedi sotto) di se stessa \n!La Matrice Inversa di una matrice A è pari alla sua Matrice Aggiunta (i.e., Matrice \nTrasposta Coniugata) diviso il det(A)\n!La Matrice Trasposta di una matrice è la matrice ottenuta scambiando le righe con \nle colonne\n!La Matrice Trasposta Coniugata di una matrice a valori complessi è la matrice \nottenuta effettuando la trasposta e scambiando ogni valore con il suo complesso \nconiugato",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#10": "Esempio Stima di µes(d=2)\n13prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…prosegue\n\u000b\f\u000b\f ¦\n \u0010\u0010  \n»»»»»\n¼º\n«««««\n¬ª\n \nn kj j\nki i\nkij\ndd dd\nx xn ...1\n122 211 12 11\n1       ,  \n... ...... ... ... ...... ......\nP P V\nV VVVV VV\nΣ\n»¼º\n«¬ª »\n¼º\n«\n¬ª 456.1732.2532.25 44.66\n22 2112 11\nVVVVΣ\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f44.6659.823 9.81 9.813 9.85.4 9.832 2 2 2 2\n21 11 \u0010\u000e\u0010\u000e\u0010\u000e\u0010\u000e\u0010  VV\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f32.25\n52.98.159.8232.949.812.92.79.8132.9129.85.42.979.8321 12 \u0010\u0010\u000e\u0010\u0010\u000e\u0010\u0010\u000e\u0010\u0010\u000e\u0010\u0010  VV\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f456.17\n52.98.15 2.94 2.92.7 2.912 2.972 2 2 2 2\n22 22 \u0010\u000e\u0010\u000e\u0010\u000e\u0010\u000e\u0010  VV\n»¼º\n«¬ª\n\u0010\u0010 \u0010\n1281.0 0488.00488.0 0337.01Σ\u000b \f\u000b \f 674.518 32.2532.25 456.1744.66  \u0010 Σo,innotazione vettoriale :\n𝚺=1\n𝑛෍\n𝑖=1…𝑛𝐱𝑖−𝛍𝐱𝑖−𝛍𝑡calcolata come somma dimatrici,\nciascuna ottenuta come vettore\ncolonna pervettore riga.",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#11": "Esempio Stima di µes(d=2)\n13prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…prosegue\n\u000b\f\u000b\f ¦\n \u0010\u0010  \n»»»»»\n¼º\n«««««\n¬ª\n \nn kj j\nki i\nkij\ndd dd\nx xn ...1\n122 211 12 11\n1       ,  \n... ...... ... ... ...... ......\nP P V\nV VVVV VV\nΣ\n»¼º\n«¬ª »\n¼º\n«\n¬ª 456.1732.2532.25 44.66\n22 2112 11\nVVVVΣ\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f44.6659.823 9.81 9.813 9.85.4 9.832 2 2 2 2\n21 11 \u0010\u000e\u0010\u000e\u0010\u000e\u0010\u000e\u0010  VV\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f32.25\n52.98.159.8232.949.812.92.79.8132.9129.85.42.979.8321 12 \u0010\u0010\u000e\u0010\u0010\u000e\u0010\u0010\u000e\u0010\u0010\u000e\u0010\u0010  VV\n\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f\u000b\f456.17\n52.98.15 2.94 2.92.7 2.912 2.972 2 2 2 2\n22 22 \u0010\u000e\u0010\u000e\u0010\u000e\u0010\u000e\u0010  VV\n»¼º\n«¬ª\n\u0010\u0010 \u0010\n1281.0 0488.00488.0 0337.01Σ\u000b \f\u000b \f 674.518 32.2532.25 456.1744.66  \u0010 Σo,innotazione vettoriale :\n𝚺=1\n𝑛෍\n𝑖=1…𝑛𝐱𝑖−𝛍𝐱𝑖−𝛍𝑡calcolata come somma dimatrici,\nciascuna ottenuta come vettore\ncolonna pervettore riga.",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#12": "Esempio Stima di µes(d=2)\n14prof. Davide Maltoni –Università di Bologna\nML\nClassificazionein forma grafica\nvista \ndall’alto\nvista \nlaterale",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#13": "Classiﬁcatore di Bayes con Distribuzioni Multinormali\n15prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore di Bayes con \ndistribuzioni Multinormali\nNell’esempio\n sono visualizzate ledensità condizionali di2classi\ndipattern (distribuiti condistribuzione normale 2-dimensionale)\ncorrette sulla base delle rispettive probabilità apriori .\nLa\nclassificazione èeseguita utilizzando laregola Bayesiana .Lo\nspazio èsuddiviso inregioni nonconnesse .Nelcaso specifico\n2ècostituita daduecomponenti disgiunte .\nUn\ndecision boundary odecision surface (superficie decisionale)\nèunazona diconfine traregioni cheilclassificatore associa a\nclassi diverse .Sulboundary laclassificazione èambigua .\nLe\nsuperfici decisionali possono assumere forme diverse .Nel\ncaso specifico sitratta didueiperboli .Ingenerale :\nSe\nle2matrici dicovarianza sono uguali traloro:lasuperficie\ndecisionale èuniper-piano .\nSe\nle2matrici dicovarianza sono arbitrarie :lasuperficie\ndecisionale èuniper-quadratica .\n",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#14": "15prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore di Bayes con \ndistribuzioni Multinormali\nNell’esempio\n sono visualizzate ledensità condizionali di2classi\ndipattern (distribuiti condistribuzione normale 2-dimensionale)\ncorrette sulla base delle rispettive probabilità apriori .\nLa\nclassificazione èeseguita utilizzando laregola Bayesiana .Lo\nspazio èsuddiviso inregioni nonconnesse .Nelcaso specifico\n2ècostituita daduecomponenti disgiunte .\nUn\ndecision boundary odecision surface (superficie decisionale)\nèunazona diconfine traregioni cheilclassificatore associa a\nclassi diverse .Sulboundary laclassificazione èambigua .\nLe\nsuperfici decisionali possono assumere forme diverse .Nel\ncaso specifico sitratta didueiperboli .Ingenerale :\nSe\nle2matrici dicovarianza sono uguali traloro:lasuperficie\ndecisionale èuniper-piano .\nSe\nle2matrici dicovarianza sono arbitrarie :lasuperficie\ndecisionale èuniper-quadratica .\nClassiﬁcatore di Bayes con Distribuzioni Multinormali",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#15": "Altri Esempi di Superfici Decisionali\n16prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…altri esempi di superfici decisionali\nStessa \nmatrice\ndi \ncovarianza:\niper-piani\nDifferenti \nmatrici\ndi covarianza:\niper-\nquadraticheStessa Matrice di Covarianza: Iper-piani\nIper-piano: sottospazio di dimensione inferiore di uno rispetto allo spazio in cui è \ncontenuto",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#16": "Altri Esempi di Superﬁci Decisionali\n16prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…altri esempi di superfici decisionali\nStessa \nmatrice\ndi \ncovarianza:\niper-piani\nDifferenti \nmatrici\ndi covarianza:\niper-\nquadratiche\nDifferenti Matrici di Covarianza: Iper-quadratiche\nIper-quadratica: (iper -)superficie di uno spazio d -dimensionale sui complessi o sui \nreali rappresentata da un'equazione polinomiale del secondo ordine nelle variabili \nspaziali (coordinate)",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#17": "Esempio con Bayes Parametrico (Multinormali)\n17prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes parametrico ( multinormali )\n>@T9.170,4.631 μ >@T3.162,5.512 μ »¼º\n«¬ª ¦9.343.233.232.35\n1 »¼º\n«¬ª ¦0.139.89.81.10\n2\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n11\n1 1 2/1\n12/121exp\nπ21| μxμx xt\ndw p\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n21\n2 2 2/1\n22/221exp\nπ21| μxμx xt\ndw p\nx\nPesoAltezza\n\u000b\f1|w px\n \u000b\f2|w pxStima dei parametri dal training set:\n17prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes parametrico ( multinormali )\n>@T9.170,4.631 μ >@T3.162,5.512 μ »¼º\n«¬ª ¦9.343.233.232.35\n1 »¼º\n«¬ª ¦0.139.89.81.10\n2\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n11\n1 1 2/1\n12/121exp\nπ21| μxμx xt\ndw p\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n21\n2 2 2/1\n22/221exp\nπ21| μxμx xt\ndw p\nx\nPesoAltezza\n\u000b\f1|w px\n \u000b\f2|w pxStima dei parametri dal training set:\nObiettivo: stimare la classe di appartenenza del pattern x (57,168)",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#18": "17prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes parametrico ( multinormali )\n>@T9.170,4.631 μ >@T3.162,5.512 μ »¼º\n«¬ª ¦9.343.233.232.35\n1 »¼º\n«¬ª ¦0.139.89.81.10\n2\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n11\n1 1 2/1\n12/121exp\nπ21| μxμx xt\ndw p\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n21\n2 2 2/1\n22/221exp\nπ21| μxμx xt\ndw p\nx\nPesoAltezza\n\u000b\f1|w px\n \u000b\f2|w pxStima dei parametri dal training set:Esempio con Bayes Parametrico (Multinormali)\n17prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes parametrico ( multinormali )\n>@T9.170,4.631 μ >@T3.162,5.512 μ »¼º\n«¬ª ¦9.343.233.232.35\n1 »¼º\n«¬ª ¦0.139.89.81.10\n2\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n11\n1 1 2/1\n12/121exp\nπ21| μxμx xt\ndw p\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n21\n2 2 2/1\n22/221exp\nπ21| μxμx xt\ndw p\nx\nPesoAltezza\n\u000b\f1|w px\n \u000b\f2|w pxStima dei parametri dal training set:\n17prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes parametrico ( multinormali )\n>@T9.170,4.631 μ >@T3.162,5.512 μ »¼º\n«¬ª ¦9.343.233.232.35\n1 »¼º\n«¬ª ¦0.139.89.81.10\n2\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n11\n1 1 2/1\n12/121exp\nπ21| μxμx xt\ndw p\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n21\n2 2 2/1\n22/221exp\nπ21| μxμx xt\ndw p\nx\nPesoAltezza\n\u000b\f1|w px\n \u000b\f2|w pxStima dei parametri dal training set:",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#19": "18prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…continua\nSupponendo di non avere altre informazioni, si possono stimare le \nprobabilità a priori come: 𝑃𝑤1=8/18, 𝑃𝑤2=10/18\n\u000b\f\n\u000b\f\u000b\f\u000b\f0.003321exp\nπ21|11\n1 1 2/1\n12/ 1  »¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010μxμx xt\ndwp\n\u000b\f\n\u000b\f\u000b\f\u000b\f 0045.021exp\nπ21|21\n2 2 2/1\n22/2  »¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010μxμx xt\ndw p\nPesoAltezza\n>@T168 ,57 x\n\u000b\f\u000b\f\u000b\f 0040.0 w w|is\n1ii   ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f36.0w w||w1 1\n1 # xxxpP pP\n\u000b\f\u000b\f\u000b\f\n\u000b\f64.0w w||w2 2\n2 # xxxpP pPEsempio con Bayes Parametrico (Multinormali)\n17prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes parametrico ( multinormali )\n>@T9.170,4.631 μ >@T3.162,5.512 μ »¼º\n«¬ª ¦9.343.233.232.35\n1 »¼º\n«¬ª ¦0.139.89.81.10\n2\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n11\n1 1 2/1\n12/121exp\nπ21| μxμx xt\ndw p\n\u000b\f\n\u000b\f\u000b\f\u000b\f»¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010\n21\n2 2 2/1\n22/221exp\nπ21| μxμx xt\ndw p\nx\nPesoAltezza\n\u000b\f1|w px\n \u000b\f2|w pxStima dei parametri dal training set:",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#2": "Richiami\n!Premessa :lanozione diautovalore siriferisce alle sole matrici\nquadrate ,ossia alle matrici aventi lostesso numero dirighe edi\ncolonne .\n!Chiarito ciò,siaAuna matrice quadrata diordine nacoefficienti in\nuncampo !(dove !potrebbe essere ilcampo ℝdeinumeri reali oil\ncampo ℂdeinumeri complessi ).\n!Sidice cheloscalare λ0∈!èunautovalore della matrice quadrata\nAseesiste unvettore colonna non nullo v∈!ntale che\nAv=λ0v\n!Ilvettore vèdetto autovettore relativo all’autovalore λ0",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#20": "18prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…continua\nSupponendo di non avere altre informazioni, si possono stimare le \nprobabilità a priori come: 𝑃𝑤1=8/18, 𝑃𝑤2=10/18\n\u000b\f\n\u000b\f\u000b\f\u000b\f0.003321exp\nπ21|11\n1 1 2/1\n12/ 1  »¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010μxμx xt\ndwp\n\u000b\f\n\u000b\f\u000b\f\u000b\f 0045.021exp\nπ21|21\n2 2 2/1\n22/2  »¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010μxμx xt\ndw p\nPesoAltezza\n>@T168 ,57 x\n\u000b\f\u000b\f\u000b\f 0040.0 w w|is\n1ii   ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f36.0w w||w1 1\n1 # xxxpP pP\n\u000b\f\u000b\f\u000b\f\n\u000b\f64.0w w||w2 2\n2 # xxxpP pPEsempio con Bayes Parametrico (Multinormali)",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#21": "Bayes e Conﬁdenza di Classiﬁcazione\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\nIn figura un \nesempio con\ns=5 classi",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#22": "Bayes e Confidenza di Classificazione\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#23": "Bayes Parametrico In Pratica\n20prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes parametrico in pratica\nMolto\n spesso sifanno ipotesi azzardate sulla normalità delle\ndensità diprobabilità delle classi delproblema senza aver\nsperimentalmente eseguito nessuna verifica ;ciòporta ad\nottenere cattivi risultati diclassificazione .\nPertanto, dato unproblema con𝑠classi edato untraining set\n(significativo), deve essere innanzitutto valutata larispondenza\nalla“normalità” delle𝑠distribuzioni ;questo puòessere fatto:\nin\nmodo formale (es:teststatistico diMalkovich -Afifi [1]\nbasatosull’indice diKolmogorov -Smirnov )\nin\nmodo empirico ,visualizzando invarimodi lenuvole dei\ndati (esistono deitool giàpredisposti perquesto tipo di\nanalisi finoa3D)ogliistogrammi sulle diverse componenti\neconfrontandoli conlecurve teoriche .\nUna\n volta provata una (seppur vaga) normalità delle\ndistribuzioni, sistimano apartire daidati, vettore medioPe\nmatrice dicovarianza 6(maximum likelihood ).\nPer\n quanto riguarda leprobabilità apriori queste possono\nessere estratte dalle percentuale dicampioni cheneltraining\nsetappartengono allediverse classi, oincaso diassenza di\ninformazioni possono essere poste tutte uguali traloro.\nOgni\n nuovo pattern daclassificare ,èassegnato auna delle\npossibili classi inaccordo conlaregola diBayes nella quale\nmedia ecovarianza sono oranote.\n[1] K. Fukunaga , Statistical Pattern Recognition , Academic Press, 1990.\n20prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes parametrico in pratica\nMolto\n spesso sifanno ipotesi azzardate sulla normalità delle\ndensità diprobabilità delle classi delproblema senza aver\nsperimentalmente eseguito nessuna verifica ;ciòporta ad\nottenere cattivi risultati diclassificazione .\nPertanto, dato unproblema con𝑠classi edato untraining set\n(significativo), deve essere innanzitutto valutata larispondenza\nalla“normalità” delle𝑠distribuzioni ;questo puòessere fatto:\nin\nmodo formale (es:teststatistico diMalkovich -Afifi [1]\nbasatosull’indice diKolmogorov -Smirnov )\nin\nmodo empirico ,visualizzando invarimodi lenuvole dei\ndati (esistono deitool giàpredisposti perquesto tipo di\nanalisi finoa3D)ogliistogrammi sulle diverse componenti\neconfrontandoli conlecurve teoriche .\nUna\n volta provata una (seppur vaga) normalità delle\ndistribuzioni, sistimano apartire daidati, vettore medioPe\nmatrice dicovarianza 6(maximum likelihood ).\nPer\n quanto riguarda leprobabilità apriori queste possono\nessere estratte dalle percentuale dicampioni cheneltraining\nsetappartengono allediverse classi, oincaso diassenza di\ninformazioni possono essere poste tutte uguali traloro.\nOgni\n nuovo pattern daclassificare ,èassegnato auna delle\npossibili classi inaccordo conlaregola diBayes nella quale\nmedia ecovarianza sono oranote.\n[1] K. Fukunaga , Statistical Pattern Recognition , Academic Press, 1990.",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#24": "Bayes Parametrico In Pratica\n",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#25": "Problemi Closed e Open Set\n",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#26": "Problemi Closed e Open Set\n",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#3": "Richiami\n!E’utile osservare chesevèunautovettore relativo all’autovalore λ0,\nallora anche %v,con%∈!e%≠0,èunautovettore relativo aλ0.\n!Infatti moltiplicando ambo imembri della relazione\nAv=λ0v\nperloscalare %≠0,siottiene\n%(Av)=%(λ0v)⟺A(%v)=λ0(%v)\n!Ciòdimostra cheanche %vèunautovettore associato aλ0.",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#4": "9prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistribuzione Normale Multivariata \n(Multinormale ) \nNotazione\n :perevitare confusione utilizziamo apedicel’indice del\npattern e(ove necessario) adapice lacomponente (scalare) :\n•𝐱𝑖pattern i-esimo (vettore)\n•𝑥𝑖𝑗componente j-esima delpattern i-esimo (scalare)\nLa\ndensità diprobabilità nella distribuzione multinormale (𝑑>1):\n𝑝𝐱=1\n2𝜋𝑑/2Σ1/2𝑒−1\n2𝐱−𝛍𝑡Σ−1𝐱−𝛍\ndove𝛍=𝜇1,𝜇2…𝜇𝑑èilvettore medio èΣ=𝜎𝑖𝑗lamatrice di\ncovarianza (𝑑×𝑑).\nSi\nassume che ivettori siano ditipo «colonna» .L’apice𝑡\n(trasposto) litrasforma inrighe .\n|6|e6-1sono rispettivamente ildeterminante el’inversa di6.\nLa\nmatrice dicovarianza èsempre simmetrica edefinita\npositiva, pertanto ammette inversa .Essendo simmetrica il\nnumero diparametri cheladefinisce è𝑑∙𝑑+1/2\nGli\nelementi diagonali 𝜎𝑖𝑖sono levarianze deirispettivi 𝑥𝑖\n(ovvero 𝜎𝑖2);glielementi non diagonali 𝜎𝑖𝑗sono le\ncovarianze tra𝑥𝑖e𝑥𝑗:\n•se𝑥𝑖e𝑥𝑗sono statisticamente indipendenti 𝜎𝑖𝑗=0\n•se𝑥𝑖e𝑥𝑗sono correlati positivamente 𝜎𝑖𝑗>0\n•se𝑥𝑖e𝑥𝑗sono correlati negativamente 𝜎𝑖𝑗<0Distribuzione Normale Multivariata (Multinormale)",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#5": "10prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneRappresentazione grafica\nNormale Multivariata\nPer\n𝑑=2laforma della distribuzione èquella diun’ellisse .\n𝛍\n=𝜇1,𝜇2controlla laposizione delcentro .\n𝜎11e𝜎22determinano l’allungamento suidueassidell’ellisse .\n𝜎12=𝜎21controlla larotazione dell’ellisse rispetto agli assi\ncartesiani .\n•se=0(matrice dicovarianza diagonale ),ladistribuzione\nmultinormale èdefinita come prodotto di𝑑normali\nmonodimensionali .Intalcaso gliassidell’ellisse sono\nparalleli agliassicartesiani (es.Naive Bayes Classifier ).\n•Se >0(come nel caso della figura )𝑥1e𝑥2sono\npositivamente correlate (quando aumenta 𝑥1aumenta\nanche𝑥2).\n•Se<0𝑥1e𝑥2sono negativamente correlate (quando\naumenta 𝑥1cala𝑥2).\nGli\nassidell’ellisse sono paralleli agliautovettori diΣ.Le diverse ellissi \nindividuano \nluoghi di punti a \ndensità costante\n𝑥1𝑥2Distribuzione Normale Multivariata (Multinormale)",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#6": "10prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneRappresentazione grafica\nNormale Multivariata\nPer\n𝑑=2laforma della distribuzione èquella diun’ellisse .\n𝛍\n=𝜇1,𝜇2controlla laposizione delcentro .\n𝜎11e𝜎22determinano l’allungamento suidueassidell’ellisse .\n𝜎12=𝜎21controlla larotazione dell’ellisse rispetto agli assi\ncartesiani .\n•se=0(matrice dicovarianza diagonale ),ladistribuzione\nmultinormale èdefinita come prodotto di𝑑normali\nmonodimensionali .Intalcaso gliassidell’ellisse sono\nparalleli agliassicartesiani (es.Naive Bayes Classifier ).\n•Se >0(come nel caso della figura )𝑥1e𝑥2sono\npositivamente correlate (quando aumenta 𝑥1aumenta\nanche𝑥2).\n•Se<0𝑥1e𝑥2sono negativamente correlate (quando\naumenta 𝑥1cala𝑥2).\nGli\nassidell’ellisse sono paralleli agliautovettori diΣ.Le diverse ellissi \nindividuano \nluoghi di punti a \ndensità costante\n𝑥1𝑥2\nDistribuzione Normale Multivariata (Multinormale)\nN.B. Per quanto l’assunzione chele variabili siano statisticamente indipendenti (!12=0) \nnon siavera in generale , iClassificatori Naïve Bayes sidimostrano lavorare bene su\nmolti dataset. Per tale motivo , taliclassificatori sono fraipiùpopolari",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#7": "Distanza Mahalanobis\n11prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistanza Mahalanobis\nLa\ndistanza diMahalanobis 𝑟tra𝐱e𝛍,definitadall’equazione :\n𝑟2=𝐱−𝛍𝑡Σ−1𝐱−𝛍\ndefinisce ibordi adensità costante inuna distribuzione\nmultinormale .Tale distanza viene spesso utilizzata in\nsostituzione della distanza euclidea ,essendo ingrado di\n“pesare”lediverse componenti tenendo conto deirelativi spazi\ndivariazione edella lorocorrelazione .\n𝑟=1\n𝑟=2\n𝑟=3\n𝑟=4\n𝑥1𝑥2\n11prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistanza Mahalanobis\nLa\ndistanza diMahalanobis 𝑟tra𝐱e𝛍,definitadall’equazione :\n𝑟2=𝐱−𝛍𝑡Σ−1𝐱−𝛍\ndefinisce ibordi adensità costante inuna distribuzione\nmultinormale .Tale distanza viene spesso utilizzata in\nsostituzione della distanza euclidea ,essendo ingrado di\n“pesare”lediverse componenti tenendo conto deirelativi spazi\ndivariazione edella lorocorrelazione .\n𝑟=1\n𝑟=2\n𝑟=3\n𝑟=4\n𝑥1𝑥2",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#8": "Esempio Stima di µes(d=2)\n12prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempio stima di 𝜇e 𝜎(d=2)\nDato\n untraining setdipattern bi-dimensionali composto da𝑛=\n5elementi :\nLa\nstima deiparametri permassima verosimiglianza (maximum\nlikelihood )è:\no,innotazione vettoriale :\n𝛍=1\n𝑛෍\n𝑖=1…𝑛𝐱𝑖>@>@>@>@>@ ^ ` 8.15,23,1,4 ,2.7,13,12,5.4,3,7 t t t t t\n¦\n  \n»»»»\n¼º\n««««\n¬ª\n \nn ki\nki\ndxn ...121\n1    ,    \n...P\nPPP\nμ »¼º\n«¬ª \n»»»\n¼º\n«««\n¬ª\n\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\n 2.99.8\n58.1542.71275231135.43\nμ!Vogliano eseguire la stima dei parametri tramite il Metodo della \nMassima Verosimiglianza ( Maximum Likelihood )campioni in blu\nvettore medio da \nstimare in rosso",
    "data_test\\rootfolder\\università\\MachineLearning\\30-CB(2)-sbloccato.pdf#9": "Esempio Stima di µes(d=2)\n12prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempio stima di 𝜇e 𝜎(d=2)\nDato\n untraining setdipattern bi-dimensionali composto da𝑛=\n5elementi :\nLa\nstima deiparametri permassima verosimiglianza (maximum\nlikelihood )è:\no,innotazione vettoriale :\n𝛍=1\n𝑛෍\n𝑖=1…𝑛𝐱𝑖>@>@>@>@>@ ^ ` 8.15,23,1,4 ,2.7,13,12,5.4,3,7 t t t t t\n¦\n  \n»»»»\n¼º\n««««\n¬ª\n \nn ki\nki\ndxn ...121\n1    ,    \n...P\nPPP\nμ »¼º\n«¬ª \n»»»\n¼º\n«««\n¬ª\n\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\n 2.99.8\n58.1542.71275231135.43\nμ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#0": "Sommario\n!Approccio parametrico (distribuzione MultiNormale)\n!Approccio non parametrico (Parzen Window) ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#1": "Appr occi Non Parametrici e Stima della Densità\n21prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneApprocci non parametrici e\nStima della Densità\nNon vengono fatte ipotesi sulle distribuzioni deipattern eledensità\ndiprobabilità sono stimate direttamente daltraining set.\nIlproblema della stima accurata della densità èritenuto damolti un\nproblema piùcomplesso della classificazione .Pertanto perché\nrisolvere come sotto -problema unproblema che èpiùcomplesso\ndell’intero compito diclassificazione ?\nIngenerale lastima della densità èaffrontabile inspazi a\ndimensionalità ridotta (es.𝑑=3)ediventa critica alcrescere della\ndimensionalità (curse ofdimensionality ):ilvolume dello spazio\naumenta così tanto cheipattern diventato troppo sparsi .\nStima Densità\nLaprobabilità cheunpattern𝐱cadaall’interno diè:\n𝑃1=න\n𝑝𝐱′𝑑𝐱′\nDati𝑛pattern indipendenti, laprobabilità che𝑘diquesti cadano\nnella regioneècalcolabile attraverso ladistribuzione binomiale :\n𝑃𝑘=𝑛\n𝑘𝑃1𝑘1−𝑃1𝑛−𝑘\nilcuivalor medio è𝑘=𝑛𝑃1(equindi𝑃1=𝑘/𝑛)\nAssumendo che laregione (divolume𝑉)siapiccola eche\nquindi𝑝∙nonvarisignificativamente all’interno diessa :\n𝑃1=න\n𝑝𝐱′𝑑𝐱′≈𝑝𝐱∙𝑉\n𝑝𝐱=𝑃1\n𝑉=𝑘\n𝑛∙𝑉",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#10": "Parzen Window con Soft Kernel\n23prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneParzen Window con Soft kernel\nNella pratica, invece difunzioni finestra ipercubo siutilizzano kernel\nfunction piùsoftgrazie allequali ogni pattern𝐱𝑖contribuisce alla\nstima didensità inunintorno di𝐱inaccordo conladistanza da𝐱.In\nquesto modo lesuperfici decisionali risultano molto piùregolari\n(smoothed ).\nLekernel function devono essere funzioni densità (sempre ≥0e\nconintegrale sututto lospazio uguale a1).Utilizzando lafunzione\nmultinormale (con𝛍=[0…0]eΣ=I):\n𝜑𝐮=1\n2𝜋𝑑/2𝑒−𝐮𝑡𝐮\n2\nn=15\nn=40\nn=120h=3 h=8\n h=15\nRicordiamo che x è il pattern da classificare, xisono i pattern del \nTraining Set",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#11": "Kernel Trick\nSVM Classification\nOvviamente le SVM possono essere\nusate per separare classi che non\npotrebbero essere separate con un\nclassificatore lineare, altrimenti la loro\napplicazione a casi di reale interesse\nnon sarebbe possibile. In questi casi le\ncoordinate degli oggetti sono mappate\nin uno spazio detto “feature space”\nutilizzando funzioni non lineare,\nchiamate “feature function” ϕ.Ilfeature\n chiamate “feature function” ϕ.Ilfeature\nspace è uno spazio fortemente\nmultidimensionale in cui le due classi\npossono essere separate con un\nclassificatore lineare.\nQuindi lo spazio iniziale viene rimappato\nnel nuovo spazio, a questo punto viene\nidentificato il classificatore che poi viene\nriportato nello spazio iniziale, come\nillustrato in figura.Fonte: Stefano Cavuoti\nSVM Classification\nLa funzione ϕcombina quindi lo spazio iniziale (le \ncaratteristiche originali degli oggetti) nello spaz io \ndelle features che potrebbe in linea di principio \navere anche dimensione infinita. A causa del fatto \nche questo spazio ha molte dimensioni non \nsarebbe pratico utilizzare una funzione generica \nper trovare l’iperpiano di separazione, quindi \nvengono usate delle funzioni dette “kernel” e si \nidentifica la funzione ϕtramite una combinazione \ndi funzioni di kernel.\nFonte: http://www.ivanciuc.org/\ndi funzioni di kernel.\nL’implementazione più famosa delle SVM (libSVM) \nusa quattro possibili kernel:\nFonte: http://www.imtech.res.in/raghava/rbpred/svm. jpg\nKernel trick–(1)\n•Possiamo trasformare i dati nell' input space in un nuovo \nspazio, detto feature space , a più alta dimensionalità\n•I vettori che prima non erano linearmente separabili hanno più \nprobabilità di esserlo in uno spazio a più dimensioni\n25\nIdea:trasformare idati nell’Input Space inunnuovo spazio, detto\nFeature Space ,apiùaltadimensionalità .\nIpattern che prima non erano linearmente separabili hanno più\nprobabilità diesserlo inunospazio apiùdimensioni .\nQualsiasi modello lineare può essere trasformato inunmodello non\nlineare applicando ilkernel trick (stratagemma del kernel) almodello :\nsostituendo lesuefeature (predittori) conunafunzione kernel .",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#12": "Kernel Trick\n!Le funzioni kernel sono usate per operare nello spazio delle \nfeature senza calcolare le coordinate dei dati nello spazio di \ninput, ma piuttosto calcolando il prodotto scalare fra le immagini \ndi tutte le copie di dati nello spazio funzione!Tale operazione è spesso computazionalmente più economica \nche l’esplicito calcolo delle coordinate, in quanto il prodotto \nscalare gode di alcune proprietà speciali!Infatti spesso si può calcolare φ(xi)\"φ(xj) senza prima calcolare il \nvalore di φ in ogni punto [dove x è il pattern nell’ input space (con \nddimensioni) e φ(x) è il corrispondente pattern nel feature space \n(con m>d dimensioni)!Le funzioni kernel sono state introdotte per sequenze di dati, \ngrafi, testi, immagini e vettori",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#13": "Parzen Window con Soft Kernel\n23prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneParzen Window con Soft kernel\nNella pratica, invece difunzioni finestra ipercubo siutilizzano kernel\nfunction piùsoftgrazie allequali ogni pattern𝐱𝑖contribuisce alla\nstima didensità inunintorno di𝐱inaccordo conladistanza da𝐱.In\nquesto modo lesuperfici decisionali risultano molto piùregolari\n(smoothed ).\nLekernel function devono essere funzioni densità (sempre ≥0e\nconintegrale sututto lospazio uguale a1).Utilizzando lafunzione\nmultinormale (con𝛍=[0…0]eΣ=I):\n𝜑𝐮=1\n2𝜋𝑑/2𝑒−𝐮𝑡𝐮\n2\nn=15\nn=40\nn=120h=3 h=8\n h=15\nIn questo caso il valore \ndell’iperparametro h\nnon è legato alla \nlunghezza di uno \nspigolo dell’ipercubo, \nma all’ ampiezza della \nfunzione multinormale ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#14": "Esempio con Parzen Window\n24prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes + Parzen Window\nStima non -parametrica della densità attraverso Parzen Window\n(nell’ipotesi che 𝑃𝑤1=8/18, 𝑃𝑤2=10/18)\nPesoAltezza\n>@T168 ,57 x\nPesoAltezza\n>@T168 ,57 x\n¾Funzione Kernel ipercubo con ℎ=10 (grafico a sinistra)\n¾Funzione Kernel normale con ℎ=3 (grafico a destra)\u000b\f0.0038 |1 wpx \u000b\f 0040.0 |2 wpx \u000b\f\u000b\f\u000b\f 0039.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f43.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f57.0w w||w2 2\n2 # xxxpP pP\n\u000b\f0.0024 |1 wpx \u000b\f 0041.0 |2 wpx \u000b\f\u000b\f\u000b\f 0033.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f32.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f68.0w w||w2 2\n2 # xxxpP pP(N.B. In questo caso il valore dell’iperparametro hè legato alla lunghezza \ndello spigolo dell’ ipercubo )\n(N.B. In questo caso il valore dell’iperparametro hè legato all’ampiezza della \nfunzione multinormale )",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#15": "Esempio con Parzen Window\n24prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes + Parzen Window\nStima non -parametrica della densità attraverso Parzen Window\n(nell’ipotesi che 𝑃𝑤1=8/18, 𝑃𝑤2=10/18)\nPesoAltezza\n>@T168 ,57 x\nPesoAltezza\n>@T168 ,57 x\n¾Funzione Kernel ipercubo con ℎ=10 (grafico a sinistra)\n¾Funzione Kernel normale con ℎ=3 (grafico a destra)\u000b\f0.0038 |1 wpx \u000b\f 0040.0 |2 wpx \u000b\f\u000b\f\u000b\f 0039.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f43.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f57.0w w||w2 2\n2 # xxxpP pP\n\u000b\f0.0024 |1 wpx \u000b\f 0041.0 |2 wpx \u000b\f\u000b\f\u000b\f 0033.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f32.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f68.0w w||w2 2\n2 # xxxpP pP\n24prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes + Parzen Window\nStima non -parametrica della densità attraverso Parzen Window\n(nell’ipotesi che 𝑃𝑤1=8/18, 𝑃𝑤2=10/18)\nPesoAltezza\n>@T168 ,57 x\nPesoAltezza\n>@T168 ,57 x\n¾Funzione Kernel ipercubo con ℎ=10 (grafico a sinistra)\n¾Funzione Kernel normale con ℎ=3 (grafico a destra)\u000b\f0.0038 |1 wpx \u000b\f 0040.0 |2 wpx \u000b\f\u000b\f\u000b\f 0039.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f43.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f57.0w w||w2 2\n2 # xxxpP pP\n\u000b\f0.0024 |1 wpx \u000b\f 0041.0 |2 wpx \u000b\f\u000b\f\u000b\f 0033.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f32.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f68.0w w||w2 2\n2 # xxxpP pP\n24prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMaschi/Femmine\ncon Bayes + Parzen Window\nStima non -parametrica della densità attraverso Parzen Window\n(nell’ipotesi che 𝑃𝑤1=8/18, 𝑃𝑤2=10/18)\nPesoAltezza\n>@T168 ,57 x\nPesoAltezza\n>@T168 ,57 x\n¾Funzione Kernel ipercubo con ℎ=10 (grafico a sinistra)\n¾Funzione Kernel normale con ℎ=3 (grafico a destra)\u000b\f0.0038 |1 wpx \u000b\f 0040.0 |2 wpx \u000b\f\u000b\f\u000b\f 0039.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f43.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f57.0w w||w2 2\n2 # xxxpP pP\n\u000b\f0.0024 |1 wpx \u000b\f 0041.0 |2 wpx \u000b\f\u000b\f\u000b\f 0033.0 w w|is\n1ii    ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f32.0w w||w1 1\n1 # xxxpP pP \u000b\f\u000b\f\u000b\f\n\u000b\f68.0w w||w2 2\n2 # xxxpP pP(con hlegato alla lunghezza dello spigolo dell’ ipercubo )\n(con hlegato all’ampiezza della funzione normale )\nSipuò notare come nelcaso della Funzione Kernel normale ,lesuperfici\ndecisionali risultino molto piùregolari (smoothed)",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#16": "Riferimenti\n!S.J. Russell, and P. Norvig, Artificial Intelligence: A Modern \nApproach (4 ed.) , Pearson, 2020.\n!K. Fukunaga, Statistical Pattern Recognition , Academic Press, \n1990.\n!R. O. Duda, P. Hart, and D. G. Stork Pattern Classification , \nWiley -Interscience, 2000.\n!D. Maltoni, Machine Learning , Università di Bologna, 2021.\n!C.M. Bishop, Pattern Recognition and Machine Learning , \nSpringer, 2006.\n!K.P. Murphy, Machine Learning: A Probabilistic Perspective , The \nMIT Press, 2012.",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#17": "Matlab\nQuesto esempio mostra come eseguire la classificazione in Matlab tramite \nNaÏve Bayes Classifier\nMATLAB > Help > Examples > Statistics and Machine Learning Toolbox > \nClassification\nDataset: Fisher’s Iris Data\nFisher's iris data consists of measurements on the sepal length, sepal width, \npetal length, and petal width for 150 iris specimens. There are 50 specimens \nfrom each of three species. Load the data and see how the sepal \nmeasurements differ between species. You can use the two columns \ncontaining sepal measurements.\nload fisheriris\ngscatter(meas(:,1), meas(:,2), species,'rgb','osd');\nxlabel('Sepal length');\nylabel('Sepal width');\nN = size(meas,1);\n",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#18": "Matlab\n",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#19": "Matlab\nApproccio Parametrico: modelliamo ciascuna variabile in ciascuna classe tramite \nuna distribuzione Gaussiana. Ci calcoliamo il resubstitution error (errore sul \ntraining set, di solito stima ottimistica dell’errore reale sul test set) e il cross -\nvalidation error (in cui si suddivide il training set in gruppi di eguale numerosità, \nsi esclude iterativamente un gruppo alla volta e lo si cerca di predire con i gruppi \nnon esclusi)\nnbGau = fitcnb(meas(:,1:2), species);\nnbGauResubErr = resubLoss(nbGau)\n[x,y] = meshgrid(4:.1:8,2:.1:4.5);\nx = x(:);\ny = y(:);\ncp = cvpartition(species,'KFold',10)\nnbGauCV = crossval(nbGau, 'CVPartition',cp);\nnbGauCVErr = kfoldLoss(nbGauCV)\nlabels = predict(nbGau, [x y]);\ngscatter(x,y,labels,'grb','sod')\nnbGauResubErr = 0.2200\nnbGauCVErr = 0.2200\n",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#2": "Appr occi Non Parametrici e Stima della Densità\nL'ipercubo (o n-cubo) è una forma geometrica regolare immersa in \nuno spazio di quattro o piùdimensioni\n",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#20": "Matlab\nApproccio Non Parametrico: in questo caso modelliamo ciascuna variabile in \nciascuna classe tramite una stima della densità di probabilità mediante funzione \nkernel (settata a ‘box’)\nnbKD = fitcnb(meas(:,1:2), species, 'DistributionNames','kernel', 'Kernel','box');\nnbKDResubErr = resubLoss(nbKD)\nnbKDCV = crossval(nbKD, 'CVPartition',cp);\nnbKDCVErr = kfoldLoss(nbKDCV)\nlabels = predict(nbKD, [x y]);\ngscatter(x,y,labels,'rgb','osd')\nlabels = predict(nbGau, [x y]);\ngscatter(x,y,labels,'grb','sod')\nnbKDResubErr = 0.2067\nnbKDCVErr = 0.2133\n",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#21": "Esercizio 1\n5) Date due distribuzioni multinormali identificate dai seguenti parametri:  \n𝝁0=[1,03\n1,03] \n𝚺0−1=[52,56−10,05\n−10,0536,88] \n|𝚺0|=0,000544  \n𝑃(𝑤0)=0,6 𝝁1=[2,02\n1,53] \n𝚺1−1=[100,58−22,34\n−22,3437,85] \n|𝚺1|=0,000302  \n𝑃(𝑤1)=0,4 \nNell’ipotesi dell’impiego di un classificatore di Bayes multinormale, calcolare per il punto 𝒙=[1,60\n1,25]: \nx le densità di probabilità condizionali;  \nx le probabilità a posteriori ; \nx l’indice della classe restituita in output.  \n \nSi ricorda che  la densità di probabilità , nel caso della  distribuzione multinormale è:  \n𝑝(𝒙)=1\n(2𝜋)𝑑\n2⋅|𝚺|1\n2⋅𝑒−1\n2⋅(𝒙−𝝁)t⋅𝚺−1⋅(𝒙−𝝁) \n \nSvolgimento  \n \n𝒙−𝝁0=[1,60\n1,25]−[1,03\n1,03]=[0,57\n0,22] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√0,000544⋅𝑒−1\n2⋅[0,570,22]⋅[52,56−10,05\n−10,0536,99]⋅[0,57\n0,22] \n \n[52,56−10,05\n−10,0536,88]⋅[0,57\n0,22]=[52,56⋅0,57+(−10,05)⋅0,22\n(−10,05)⋅0,57+36,88⋅0,22]=[27,7482\n2,3851] \n \n[0,570,22]⋅[27,7482\n2,3851]=0,57⋅27,7482+0,22⋅2,3851=16,3412  \n \n𝑝(𝒙|𝑤0)=6,8237⋅𝑒−16,3412\n2=0,00193  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[1,60\n1,25]−[2,02\n1,53]=[−0,42\n−0,28] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√0,000302⋅𝑒−1\n2⋅[−0,42−0,28]⋅[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28] \n \n[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28]=[100,58⋅(−0,42)+(−22,34)⋅(−0,28)\n(−22,34)⋅(−0,42)+37,85⋅(−0,28)]=[−35,9884\n−1,2152] \n \n[−0,42−0,28]⋅[−35,9884\n−1,2152]=(−0,42)⋅(−35,9884)+(−0,28)⋅(−1,2152)=15,4554  \n \n𝑝(𝒙|𝑤1)=9,1583⋅𝑒−15,4554\n2=0,00403  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,00193⋅0,6+0,00403⋅0,4=0,00277  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,00193⋅0,6\n0,00277=0,418 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,00403⋅0,4\n0,00277=0,582 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 1 5) Date due distribuzioni multinormali identificate dai seguenti parametri:  \n𝝁0=[1,03\n1,03] \n𝚺0−1=[52,56−10,05\n−10,0536,88] \n|𝚺0|=0,000544  \n𝑃(𝑤0)=0,6 𝝁1=[2,02\n1,53] \n𝚺1−1=[100,58−22,34\n−22,3437,85] \n|𝚺1|=0,000302  \n𝑃(𝑤1)=0,4 \nNell’ipotesi dell’impiego di un classificatore di Bayes multinormale, calcolare per il punto 𝒙=[1,60\n1,25]: \nx le densità di probabilità condizionali;  \nx le probabilità a posteriori ; \nx l’indice della classe restituita in output.  \n \nSi ricorda che  la densità di probabilità , nel caso della  distribuzione multinormale è:  \n𝑝(𝒙)=1\n(2𝜋)𝑑\n2⋅|𝚺|1\n2⋅𝑒−1\n2⋅(𝒙−𝝁)t⋅𝚺−1⋅(𝒙−𝝁) \n \nSvolgimento  \n \n𝒙−𝝁0=[1,60\n1,25]−[1,03\n1,03]=[0,57\n0,22] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√0,000544⋅𝑒−1\n2⋅[0,570,22]⋅[52,56−10,05\n−10,0536,99]⋅[0,57\n0,22] \n \n[52,56−10,05\n−10,0536,88]⋅[0,57\n0,22]=[52,56⋅0,57+(−10,05)⋅0,22\n(−10,05)⋅0,57+36,88⋅0,22]=[27,7482\n2,3851] \n \n[0,570,22]⋅[27,7482\n2,3851]=0,57⋅27,7482+0,22⋅2,3851=16,3412  \n \n𝑝(𝒙|𝑤0)=6,8237⋅𝑒−16,3412\n2=0,00193  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[1,60\n1,25]−[2,02\n1,53]=[−0,42\n−0,28] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√0,000302⋅𝑒−1\n2⋅[−0,42−0,28]⋅[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28] \n \n[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28]=[100,58⋅(−0,42)+(−22,34)⋅(−0,28)\n(−22,34)⋅(−0,42)+37,85⋅(−0,28)]=[−35,9884\n−1,2152] \n \n[−0,42−0,28]⋅[−35,9884\n−1,2152]=(−0,42)⋅(−35,9884)+(−0,28)⋅(−1,2152)=15,4554  \n \n𝑝(𝒙|𝑤1)=9,1583⋅𝑒−15,4554\n2=0,00403  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,00193⋅0,6+0,00403⋅0,4=0,00277  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,00193⋅0,6\n0,00277=0,418 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,00403⋅0,4\n0,00277=0,582 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 1 ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#22": "Esercizio 15) Date due distribuzioni multinormali identificate dai seguenti parametri:  \n𝝁0=[1,03\n1,03] \n𝚺0−1=[52,56−10,05\n−10,0536,88] \n|𝚺0|=0,000544  \n𝑃(𝑤0)=0,6 𝝁1=[2,02\n1,53] \n𝚺1−1=[100,58−22,34\n−22,3437,85] \n|𝚺1|=0,000302  \n𝑃(𝑤1)=0,4 \nNell’ipotesi dell’impiego di un classificatore di Bayes multinormale, calcolare per il punto 𝒙=[1,60\n1,25]: \nx le densità di probabilità condizionali;  \nx le probabilità a posteriori ; \nx l’indice della classe restituita in output.  \n \nSi ricorda che  la densità di probabilità , nel caso della  distribuzione multinormale è:  \n𝑝(𝒙)=1\n(2𝜋)𝑑\n2⋅|𝚺|1\n2⋅𝑒−1\n2⋅(𝒙−𝝁)t⋅𝚺−1⋅(𝒙−𝝁) \n \nSvolgimento  \n \n𝒙−𝝁0=[1,60\n1,25]−[1,03\n1,03]=[0,57\n0,22] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√0,000544⋅𝑒−1\n2⋅[0,570,22]⋅[52,56−10,05\n−10,0536,99]⋅[0,57\n0,22] \n \n[52,56−10,05\n−10,0536,88]⋅[0,57\n0,22]=[52,56⋅0,57+(−10,05)⋅0,22\n(−10,05)⋅0,57+36,88⋅0,22]=[27,7482\n2,3851] \n \n[0,570,22]⋅[27,7482\n2,3851]=0,57⋅27,7482+0,22⋅2,3851=16,3412  \n \n𝑝(𝒙|𝑤0)=6,8237⋅𝑒−16,3412\n2=0,00193  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[1,60\n1,25]−[2,02\n1,53]=[−0,42\n−0,28] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√0,000302⋅𝑒−1\n2⋅[−0,42−0,28]⋅[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28] \n \n[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28]=[100,58⋅(−0,42)+(−22,34)⋅(−0,28)\n(−22,34)⋅(−0,42)+37,85⋅(−0,28)]=[−35,9884\n−1,2152] \n \n[−0,42−0,28]⋅[−35,9884\n−1,2152]=(−0,42)⋅(−35,9884)+(−0,28)⋅(−1,2152)=15,4554  \n \n𝑝(𝒙|𝑤1)=9,1583⋅𝑒−15,4554\n2=0,00403  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,00193⋅0,6+0,00403⋅0,4=0,00277  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,00193⋅0,6\n0,00277=0,418 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,00403⋅0,4\n0,00277=0,582 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 1 ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#23": "Esercizio 15) Date due distribuzioni multinormali identificate dai seguenti parametri:  \n𝝁0=[1,03\n1,03] \n𝚺0−1=[52,56−10,05\n−10,0536,88] \n|𝚺0|=0,000544  \n𝑃(𝑤0)=0,6 𝝁1=[2,02\n1,53] \n𝚺1−1=[100,58−22,34\n−22,3437,85] \n|𝚺1|=0,000302  \n𝑃(𝑤1)=0,4 \nNell’ipotesi dell’impiego di un classificatore di Bayes multinormale, calcolare per il punto 𝒙=[1,60\n1,25]: \nx le densità di probabilità condizionali;  \nx le probabilità a posteriori ; \nx l’indice della classe restituita in output.  \n \nSi ricorda che  la densità di probabilità , nel caso della  distribuzione multinormale è:  \n𝑝(𝒙)=1\n(2𝜋)𝑑\n2⋅|𝚺|1\n2⋅𝑒−1\n2⋅(𝒙−𝝁)t⋅𝚺−1⋅(𝒙−𝝁) \n \nSvolgimento  \n \n𝒙−𝝁0=[1,60\n1,25]−[1,03\n1,03]=[0,57\n0,22] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√0,000544⋅𝑒−1\n2⋅[0,570,22]⋅[52,56−10,05\n−10,0536,99]⋅[0,57\n0,22] \n \n[52,56−10,05\n−10,0536,88]⋅[0,57\n0,22]=[52,56⋅0,57+(−10,05)⋅0,22\n(−10,05)⋅0,57+36,88⋅0,22]=[27,7482\n2,3851] \n \n[0,570,22]⋅[27,7482\n2,3851]=0,57⋅27,7482+0,22⋅2,3851=16,3412  \n \n𝑝(𝒙|𝑤0)=6,8237⋅𝑒−16,3412\n2=0,00193  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[1,60\n1,25]−[2,02\n1,53]=[−0,42\n−0,28] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√0,000302⋅𝑒−1\n2⋅[−0,42−0,28]⋅[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28] \n \n[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28]=[100,58⋅(−0,42)+(−22,34)⋅(−0,28)\n(−22,34)⋅(−0,42)+37,85⋅(−0,28)]=[−35,9884\n−1,2152] \n \n[−0,42−0,28]⋅[−35,9884\n−1,2152]=(−0,42)⋅(−35,9884)+(−0,28)⋅(−1,2152)=15,4554  \n \n𝑝(𝒙|𝑤1)=9,1583⋅𝑒−15,4554\n2=0,00403  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,00193⋅0,6+0,00403⋅0,4=0,00277  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,00193⋅0,6\n0,00277=0,418 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,00403⋅0,4\n0,00277=0,582 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 1 ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#24": "Esercizio 15) Date due distribuzioni multinormali identificate dai seguenti parametri:  \n𝝁0=[1,03\n1,03] \n𝚺0−1=[52,56−10,05\n−10,0536,88] \n|𝚺0|=0,000544  \n𝑃(𝑤0)=0,6 𝝁1=[2,02\n1,53] \n𝚺1−1=[100,58−22,34\n−22,3437,85] \n|𝚺1|=0,000302  \n𝑃(𝑤1)=0,4 \nNell’ipotesi dell’impiego di un classificatore di Bayes multinormale, calcolare per il punto 𝒙=[1,60\n1,25]: \nx le densità di probabilità condizionali;  \nx le probabilità a posteriori ; \nx l’indice della classe restituita in output.  \n \nSi ricorda che  la densità di probabilità , nel caso della  distribuzione multinormale è:  \n𝑝(𝒙)=1\n(2𝜋)𝑑\n2⋅|𝚺|1\n2⋅𝑒−1\n2⋅(𝒙−𝝁)t⋅𝚺−1⋅(𝒙−𝝁) \n \nSvolgimento  \n \n𝒙−𝝁0=[1,60\n1,25]−[1,03\n1,03]=[0,57\n0,22] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√0,000544⋅𝑒−1\n2⋅[0,570,22]⋅[52,56−10,05\n−10,0536,99]⋅[0,57\n0,22] \n \n[52,56−10,05\n−10,0536,88]⋅[0,57\n0,22]=[52,56⋅0,57+(−10,05)⋅0,22\n(−10,05)⋅0,57+36,88⋅0,22]=[27,7482\n2,3851] \n \n[0,570,22]⋅[27,7482\n2,3851]=0,57⋅27,7482+0,22⋅2,3851=16,3412  \n \n𝑝(𝒙|𝑤0)=6,8237⋅𝑒−16,3412\n2=0,00193  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[1,60\n1,25]−[2,02\n1,53]=[−0,42\n−0,28] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√0,000302⋅𝑒−1\n2⋅[−0,42−0,28]⋅[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28] \n \n[100,58−22,34\n−22,3437,85]⋅[−0,42\n−0,28]=[100,58⋅(−0,42)+(−22,34)⋅(−0,28)\n(−22,34)⋅(−0,42)+37,85⋅(−0,28)]=[−35,9884\n−1,2152] \n \n[−0,42−0,28]⋅[−35,9884\n−1,2152]=(−0,42)⋅(−35,9884)+(−0,28)⋅(−1,2152)=15,4554  \n \n𝑝(𝒙|𝑤1)=9,1583⋅𝑒−15,4554\n2=0,00403  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,00193⋅0,6+0,00403⋅0,4=0,00277  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,00193⋅0,6\n0,00277=0,418 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,00403⋅0,4\n0,00277=0,582 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 1 ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#25": "Esercizio 25) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  5) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  5) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#26": "Esercizio 25) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  5) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  5) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  5) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#27": "Esercizio 25) Data una rete neurale MLP a 3 livelli con bias composta da:  \n \nx 6 neuroni per l’input layer  \nx 8 neuroni per l’hidden layer  \nx 5 neuroni di output  \n \nQuante somme e moltiplicazioni sono necessarie per il passo forward di un generico pattern trascurando le \noperazioni effettuate dalla funzione di attivazione? Motivare la rispost a riportando anche il numero di \noperazioni per livello.  \n \nSvolgimento  \n \nPer ogni neurone del livello corrente si deve calcolare la seguente formula:  \n𝑛𝑒𝑡𝑖=∑𝑤𝑗𝑖⋅𝑖𝑛𝑗\n𝑗=1..𝑑+𝑤0𝑖 \n \nche comprende una moltiplicazione e una somma per ogni neurone del livello precedente più la somma finale \ndel bias. Pertanto:  \n \n \n \n \n \nNumero operazioni l ivello hidden: 8⋅(6+6+1)=104 \n \nNumero operazioni livello di output: 5⋅(8+8+1)=85 \n \nTotale: 189 \n \n \n6) Dato un insieme  di pattern bi -dimensionali composto da 5 elementi:  \n \n{[6,1\n8,1],[6,5\n1,9],[8,8\n4,2],[5,2\n9,7],[0,8\n5,4]} \n \nCalcolare il vettore medio ( 𝛍) e la matrice di covarianza ( 𝚺=[𝜎𝑖𝑗]).  \n \nSi ricorda che ogni elemento della matrice di covarianza può essere calcolato come  \n𝜎𝑖𝑗=1\n𝑛∑(x𝑘𝑖−μ𝑖)⋅(x𝑘𝑗−μ𝑗)𝑛\n𝑘=1   \ndove x𝑘𝑚 è l’m-esimo elemento del k-esimo pattern, e n il numero di pattern.  \n \nSvolgimento  \n \n𝛍=[6,1+6,5+8,8+5,2+0,8\n5\n8,1+1,9+4,2+9,7+5,4\n5]=[5,5\n5,9] \n \n𝚺=[6,9−1,4\n−1,47,7] \n \n Machine Learning                        Matricola: ___________________  \n20-Gen-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n \nNeuroni livello \ncorrente  Somma Bias  Una somma  per ogni \nneurone livello \nprecedente  Una moltiplicazione \nper ogni neurone \nlivello precedente  \nNOTA:  Il calcolo a lato è eseguito t rascurando il fatto che \nun’implementazione ottimizzata della sommatoria potrebbe \nevitare una somma per il primo elemento della sommatoria \n(somma con 0).  ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#28": "Esercizio 3\n7) Date tre distribuzioni  multinormali identificate dai seguenti parametri:  \n𝝁1=[0,75\n4,69\n9,57] \n 𝝁2=[1,74\n0,80\n9,59] \n 𝝁3=[6,32\n7,94\n1,82] \n \n𝚺1=𝚺2=𝚺3=I (matrice identità) e 𝑃(𝑤1)=𝑃(𝑤2)=𝑃(𝑤3). \nIndicare la classe assegnata ai seguenti pattern da un classificatore di Bayes multinormale (motivandone la \nrisposta):  \n \n𝐩1=[0,85\n5,12\n9,52],𝐩2=[9,73\n4,30\n5,41] \n \nSvolgimento  \n \nLa regola di classificazione di Bayes assegna un pattern 𝐱 alla classe 𝑤𝑖 per cui è massima la probabilità a \nposteriori 𝑃(𝑤𝑖|𝐱)=𝑝(𝐱|𝑤𝑖)⋅𝑃(𝑤𝑖)\n𝑝(𝐱). \nDato  che la probabilità a priori delle tre distribuzioni è la stessa,  determinare la classe con  probabilità a \nposteriori  massima  equivale a individuare  la classe con  densità di probabilità condizionale 𝑝(𝐱|𝑤𝑖) massima . \nPertanto, sapendo che la densità di probabilità condizionale nella distribuzione multinormale è:  \n𝑝(𝐱|𝑤𝑖)=1\n(2𝜋)𝑑\n2⋅|𝚺𝑖|1\n2⋅𝑒−1\n2⋅(𝐱−𝝁𝑖)𝑡⋅𝚺𝑖−1⋅(𝐱−𝝁𝑖) \ne che le tre matrici di covarianza sono uguali  alla matrice  identità , la classe restituita dal classificatore di \nBayes sarà quella con il valore (𝐱−𝝁𝑖)𝑡⋅(𝐱−𝝁𝑖) minimo (che corrisponde alla distanza euclidea al \nquadrato  𝐷2 tra il pattern  𝐱 e il vettore medio  𝝁𝑖). \n \n𝐷2(𝐩1,𝝁1)=𝟎,𝟐𝟎  𝐷2(𝐩1,𝝁2)=19,46  𝐷2(𝐩1,𝝁3)=97,16 \n \n𝐷2(𝐩2,𝝁1)=98,10  𝐷2(𝐩2,𝝁2)=93,56  𝐷2(𝐩2,𝝁3)=𝟑𝟕,𝟕𝟕 \n \n𝐩1 viene assegnato alla classe 1 mentre 𝐩2 viene assegnato alla classe 3.  7) Date tre distribuzioni  multinormali identificate dai seguenti parametri:  \n𝝁1=[0,75\n4,69\n9,57] \n 𝝁2=[1,74\n0,80\n9,59] \n 𝝁3=[6,32\n7,94\n1,82] \n \n𝚺1=𝚺2=𝚺3=I (matrice identità) e 𝑃(𝑤1)=𝑃(𝑤2)=𝑃(𝑤3). \nIndicare la classe assegnata ai seguenti pattern da un classificatore di Bayes multinormale (motivandone la \nrisposta):  \n \n𝐩1=[0,85\n5,12\n9,52],𝐩2=[9,73\n4,30\n5,41] \n \nSvolgimento  \n \nLa regola di classificazione di Bayes assegna un pattern 𝐱 alla classe 𝑤𝑖 per cui è massima la probabilità a \nposteriori 𝑃(𝑤𝑖|𝐱)=𝑝(𝐱|𝑤𝑖)⋅𝑃(𝑤𝑖)\n𝑝(𝐱). \nDato  che la probabilità a priori delle tre distribuzioni è la stessa,  determinare la classe con  probabilità a \nposteriori  massima  equivale a individuare  la classe con  densità di probabilità condizionale 𝑝(𝐱|𝑤𝑖) massima . \nPertanto, sapendo che la densità di probabilità condizionale nella distribuzione multinormale è:  \n𝑝(𝐱|𝑤𝑖)=1\n(2𝜋)𝑑\n2⋅|𝚺𝑖|1\n2⋅𝑒−1\n2⋅(𝐱−𝝁𝑖)𝑡⋅𝚺𝑖−1⋅(𝐱−𝝁𝑖) \ne che le tre matrici di covarianza sono uguali  alla matrice  identità , la classe restituita dal classificatore di \nBayes sarà quella con il valore (𝐱−𝝁𝑖)𝑡⋅(𝐱−𝝁𝑖) minimo (che corrisponde alla distanza euclidea al \nquadrato  𝐷2 tra il pattern  𝐱 e il vettore medio  𝝁𝑖). \n \n𝐷2(𝐩1,𝝁1)=𝟎,𝟐𝟎  𝐷2(𝐩1,𝝁2)=19,46  𝐷2(𝐩1,𝝁3)=97,16 \n \n𝐷2(𝐩2,𝝁1)=98,10  𝐷2(𝐩2,𝝁2)=93,56  𝐷2(𝐩2,𝝁3)=𝟑𝟕,𝟕𝟕 \n \n𝐩1 viene assegnato alla classe 1 mentre 𝐩2 viene assegnato alla classe 3.  ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#29": "Esercizio 37) Date tre distribuzioni  multinormali identificate dai seguenti parametri:  \n𝝁1=[0,75\n4,69\n9,57] \n 𝝁2=[1,74\n0,80\n9,59] \n 𝝁3=[6,32\n7,94\n1,82] \n \n𝚺1=𝚺2=𝚺3=I (matrice identità) e 𝑃(𝑤1)=𝑃(𝑤2)=𝑃(𝑤3). \nIndicare la classe assegnata ai seguenti pattern da un classificatore di Bayes multinormale (motivandone la \nrisposta):  \n \n𝐩1=[0,85\n5,12\n9,52],𝐩2=[9,73\n4,30\n5,41] \n \nSvolgimento  \n \nLa regola di classificazione di Bayes assegna un pattern 𝐱 alla classe 𝑤𝑖 per cui è massima la probabilità a \nposteriori 𝑃(𝑤𝑖|𝐱)=𝑝(𝐱|𝑤𝑖)⋅𝑃(𝑤𝑖)\n𝑝(𝐱). \nDato  che la probabilità a priori delle tre distribuzioni è la stessa,  determinare la classe con  probabilità a \nposteriori  massima  equivale a individuare  la classe con  densità di probabilità condizionale 𝑝(𝐱|𝑤𝑖) massima . \nPertanto, sapendo che la densità di probabilità condizionale nella distribuzione multinormale è:  \n𝑝(𝐱|𝑤𝑖)=1\n(2𝜋)𝑑\n2⋅|𝚺𝑖|1\n2⋅𝑒−1\n2⋅(𝐱−𝝁𝑖)𝑡⋅𝚺𝑖−1⋅(𝐱−𝝁𝑖) \ne che le tre matrici di covarianza sono uguali  alla matrice  identità , la classe restituita dal classificatore di \nBayes sarà quella con il valore (𝐱−𝝁𝑖)𝑡⋅(𝐱−𝝁𝑖) minimo (che corrisponde alla distanza euclidea al \nquadrato  𝐷2 tra il pattern  𝐱 e il vettore medio  𝝁𝑖). \n \n𝐷2(𝐩1,𝝁1)=𝟎,𝟐𝟎  𝐷2(𝐩1,𝝁2)=19,46  𝐷2(𝐩1,𝝁3)=97,16 \n \n𝐷2(𝐩2,𝝁1)=98,10  𝐷2(𝐩2,𝝁2)=93,56  𝐷2(𝐩2,𝝁3)=𝟑𝟕,𝟕𝟕 \n \n𝐩1 viene assegnato alla classe 1 mentre 𝐩2 viene assegnato alla classe 3.  ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#3": "Appr occi Non Parametrici e Stima della Densità\n!In Teoria della Probabilità, la distribuzione binomiale è una \ndistribuzione di probabilità discreta che descrive il numero di \nsuccessi in un processo di Bernoulli!Tale processo vale nel caso di esperimenti di prove ripetute (i.e., \nesperimenti in cui si vuole misurare quante volte si verifichi un \ncerto esito su tutte le prove effettuate)!E’ necessario che il risultato di una prova non influenzi le \nsuccessive, ossia che le singole prove siano fra loro indipendenti!La formula da utilizzare in questi casi è la Formula di Bernoulli : \nse l’evento da noi indagato ha una probabilità p di verificarsi per \nciascuna prova ed effettuiamo n prove indipendenti, la probabilità \nche l’evento si verifichi kvolte (con k ≤ n) è data da\nP(k successi su n prove)=n\nk⎛\n⎝⎜⎜⎞\n⎠⎟⎟pk⋅(1−p)n−k",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#30": "Esercizio 4 \n5) Un multiclassificatore, composto da 3 classificatori combinati a livello di decisione utilizzando Borda \ncount come tecnica di fusione, viene utilizzato per riconoscere pattern appartenenti a 4 classi. Nella tabella \nseguente sono riportati i ranking restituit i dai singoli classificatori ( 𝐶𝑖) dati in input 3 diversi pattern ( 𝒑𝑗). \nCompletare la tabella nell’ipotesi che alla prima classe siano assegnati 10 punti, alla seconda 7, alla terza 5 e \nalla quarta 2.  \n \n 𝐶1 𝐶2 𝐶3 \n 𝒑1 4 2 1 3 2 4 1 3 1 4 2 3 \n𝒑2 1 2 3 4 1 3 2 4 2 1 3 4 \n𝒑3 3 2 4 1 2 4 3 1 3 2 1 4 \n \n Punteggi Classi  Classe \nscelta  1 2 3 4 \n 𝒑1 20 22 6 24 4 \n𝒑2 27 22 17 6 1 \n𝒑3 9 24 25 14 3 \n \n \n6) Data un rete neurale MLP a 3 livelli  con bias  composta da : \n \n• 6 neuroni di Input  \n• 8 neuroni Intermedi  \n• 5 neuroni di Output  \n \nCalcolare, motivandone la risposta, il numero di pesi totale.  \n \nSvolgimento  \n \nNel caso di una rete  neurale  MLP il numero di pesi è pari al numero di connessioni  presenti.  Il numero di \nconnessioni (e quindi di pesi) presenti t ra due livelli consecutivi ( 𝑖 e 𝑖+1) si può calcolare come il prodotto  \ndel numero di neuroni  del livello 𝑖 per il numero di neuroni del livello 𝑖+1. Nel caso dell’utilizzo del bias, il \nnumero di neuroni di ogni livello  𝑖 dovrà essere incrementato di u no. \n \nPertanto il numero totale di pesi sarà pari a: (6+1)⋅8+ (8+1)⋅5=101. \n \n \n7)  Date due distribuzioni multinormali identificate dai seguenti parametri:  \n𝝁0=[10,90\n−0,43] \n𝚺0−1=[1,531,27\n1,271,61] \n|𝚺0|=1,170996  \n𝑃(𝑤0)=0,55 𝝁1=[2,87\n2,90] \n𝚺1−1=[0,41−0,14\n−0,140,35] \n|𝚺1|=8,005816  \n𝑃(𝑤1)=0,45 \nNell’ipotesi dell’impiego di un classificatore di Bayes multinormale, calcolare per il punto 𝒙=[7,05\n0,96]: \nle densità di probabilità condizionali;  \nx le probabilità a posteriori;  \nx l’indice della  classe restituita in output.  \n \nSi ricorda che la densità di probabilità, nel caso della distribuzione multinormale è:  \n𝑝(𝒙)=1\n(2𝜋)𝑑\n2⋅|𝚺|1\n2⋅𝑒−1\n2⋅(𝒙−𝝁)t⋅𝚺−1⋅(𝒙−𝝁) Machine Learning                        Matricola: ___________________  \n17-Lug-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n  \n5) Un multiclassificatore, composto da 3 classificatori combinati a livello di decisione utilizzando Borda \ncount come tecnica di fusione, viene utilizzato per riconoscere pattern appartenenti a 4 classi. Nella tabella \nseguente sono riportati i ranking restituit i dai singoli classificatori ( 𝐶𝑖) dati in input 3 diversi pattern ( 𝒑𝑗). \nCompletare la tabella nell’ipotesi che alla prima classe siano assegnati 10 punti, alla seconda 7, alla terza 5 e \nalla quarta 2.  \n \n 𝐶1 𝐶2 𝐶3 \n 𝒑1 4 2 1 3 2 4 1 3 1 4 2 3 \n𝒑2 1 2 3 4 1 3 2 4 2 1 3 4 \n𝒑3 3 2 4 1 2 4 3 1 3 2 1 4 \n \n Punteggi Classi  Classe \nscelta  1 2 3 4 \n 𝒑1 20 22 6 24 4 \n𝒑2 27 22 17 6 1 \n𝒑3 9 24 25 14 3 \n \n \n6) Data un rete neurale MLP a 3 livelli  con bias  composta da : \n \n• 6 neuroni di Input  \n• 8 neuroni Intermedi  \n• 5 neuroni di Output  \n \nCalcolare, motivandone la risposta, il numero di pesi totale.  \n \nSvolgimento  \n \nNel caso di una rete  neurale  MLP il numero di pesi è pari al numero di connessioni  presenti.  Il numero di \nconnessioni (e quindi di pesi) presenti t ra due livelli consecutivi ( 𝑖 e 𝑖+1) si può calcolare come il prodotto  \ndel numero di neuroni  del livello 𝑖 per il numero di neuroni del livello 𝑖+1. Nel caso dell’utilizzo del bias, il \nnumero di neuroni di ogni livello  𝑖 dovrà essere incrementato di u no. \n \nPertanto il numero totale di pesi sarà pari a: (6+1)⋅8+ (8+1)⋅5=101. \n \n \n7)  Date due distribuzioni multinormali identificate dai seguenti parametri:  \n𝝁0=[10,90\n−0,43] \n𝚺0−1=[1,531,27\n1,271,61] \n|𝚺0|=1,170996  \n𝑃(𝑤0)=0,55 𝝁1=[2,87\n2,90] \n𝚺1−1=[0,41−0,14\n−0,140,35] \n|𝚺1|=8,005816  \n𝑃(𝑤1)=0,45 \nNell’ipotesi dell’impiego di un classificatore di Bayes multinormale, calcolare per il punto 𝒙=[7,05\n0,96]: \nle densità di probabilità condizionali;  \nx le probabilità a posteriori;  \nx l’indice della  classe restituita in output.  \n \nSi ricorda che la densità di probabilità, nel caso della distribuzione multinormale è:  \n𝑝(𝒙)=1\n(2𝜋)𝑑\n2⋅|𝚺|1\n2⋅𝑒−1\n2⋅(𝒙−𝝁)t⋅𝚺−1⋅(𝒙−𝝁) Machine Learning                        Matricola: ___________________  \n17-Lug-2017 ( 90 minuti )     Cognome: __________ Nome: ____________  \n \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#31": "Esercizio 4\nSvolgimento  \n \n𝒙−𝝁0=[7,05\n0,96]−[10,90\n−0,43]=[−3,85\n1,39] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√1,170996⋅𝑒−1\n2⋅[−3,851,39]⋅[1,531,27\n1,271,61]⋅[−3,85\n1,39] \n \n[1,531,27\n1,271,61]⋅[−3,85\n1,39]=[1,53⋅(−3,85)+1,27⋅1,39\n1,27⋅(−3,85)+1,61⋅1,39]=[−4,1252\n−2,6516] \n \n[−3,851,39]⋅[−4,1252\n−2,6516]=(−3,85)⋅(−4,1252)+1,39⋅(−2,6516)=12,196296  \n \n𝑝(𝒙|𝑤0)=0,147076⋅𝑒−12,196296\n2=0,000330  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[7,05\n0,96]−[2,87\n2,90]=[4,18\n−1,94] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√8,005816⋅𝑒−1\n2⋅[4,18−1,94]⋅[0,41−0,14\n−0,140,35]⋅[4,18\n−1,94] \n \n[0,41−0,14\n−0,140,35]⋅[4,18\n−1,94]=[0,41⋅4,18+(−0,14)⋅(−1,94)\n(−0,14)⋅4,18+0,35⋅(−1,94)]=[1.9854\n−1,2642] \n \n[4,18−1,94]⋅[1.9854\n−1,2642]=4,18⋅1.9854+(−1,94)⋅(−1,2642)=10,75152  \n \n𝑝(𝒙|𝑤1)=0,056249⋅𝑒−10,75152\n2=0,000260  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,000330⋅0,55+0,000260⋅0,45=0,000299  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,000330⋅0,55\n0,000299=0,608 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,000260⋅0,45\n0,000299=0,392 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 0 \n \n \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#32": "Esercizio 4Svolgimento  \n \n𝒙−𝝁0=[7,05\n0,96]−[10,90\n−0,43]=[−3,85\n1,39] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√1,170996⋅𝑒−1\n2⋅[−3,851,39]⋅[1,531,27\n1,271,61]⋅[−3,85\n1,39] \n \n[1,531,27\n1,271,61]⋅[−3,85\n1,39]=[1,53⋅(−3,85)+1,27⋅1,39\n1,27⋅(−3,85)+1,61⋅1,39]=[−4,1252\n−2,6516] \n \n[−3,851,39]⋅[−4,1252\n−2,6516]=(−3,85)⋅(−4,1252)+1,39⋅(−2,6516)=12,196296  \n \n𝑝(𝒙|𝑤0)=0,147076⋅𝑒−12,196296\n2=0,000330  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[7,05\n0,96]−[2,87\n2,90]=[4,18\n−1,94] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√8,005816⋅𝑒−1\n2⋅[4,18−1,94]⋅[0,41−0,14\n−0,140,35]⋅[4,18\n−1,94] \n \n[0,41−0,14\n−0,140,35]⋅[4,18\n−1,94]=[0,41⋅4,18+(−0,14)⋅(−1,94)\n(−0,14)⋅4,18+0,35⋅(−1,94)]=[1.9854\n−1,2642] \n \n[4,18−1,94]⋅[1.9854\n−1,2642]=4,18⋅1.9854+(−1,94)⋅(−1,2642)=10,75152  \n \n𝑝(𝒙|𝑤1)=0,056249⋅𝑒−10,75152\n2=0,000260  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,000330⋅0,55+0,000260⋅0,45=0,000299  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,000330⋅0,55\n0,000299=0,608 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,000260⋅0,45\n0,000299=0,392 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 0 \n \n \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#33": "Esercizio 4Svolgimento  \n \n𝒙−𝝁0=[7,05\n0,96]−[10,90\n−0,43]=[−3,85\n1,39] \n \n𝑝(𝒙|𝑤0)=1\n2𝜋⋅√1,170996⋅𝑒−1\n2⋅[−3,851,39]⋅[1,531,27\n1,271,61]⋅[−3,85\n1,39] \n \n[1,531,27\n1,271,61]⋅[−3,85\n1,39]=[1,53⋅(−3,85)+1,27⋅1,39\n1,27⋅(−3,85)+1,61⋅1,39]=[−4,1252\n−2,6516] \n \n[−3,851,39]⋅[−4,1252\n−2,6516]=(−3,85)⋅(−4,1252)+1,39⋅(−2,6516)=12,196296  \n \n𝑝(𝒙|𝑤0)=0,147076⋅𝑒−12,196296\n2=0,000330  (Densità di probabilità condizionale di 𝒙 data 𝑤0) \n \n𝒙−𝝁1=[7,05\n0,96]−[2,87\n2,90]=[4,18\n−1,94] \n \n𝑝(𝒙|𝑤1)=1\n2𝜋⋅√8,005816⋅𝑒−1\n2⋅[4,18−1,94]⋅[0,41−0,14\n−0,140,35]⋅[4,18\n−1,94] \n \n[0,41−0,14\n−0,140,35]⋅[4,18\n−1,94]=[0,41⋅4,18+(−0,14)⋅(−1,94)\n(−0,14)⋅4,18+0,35⋅(−1,94)]=[1.9854\n−1,2642] \n \n[4,18−1,94]⋅[1.9854\n−1,2642]=4,18⋅1.9854+(−1,94)⋅(−1,2642)=10,75152  \n \n𝑝(𝒙|𝑤1)=0,056249⋅𝑒−10,75152\n2=0,000260  (Densità di probabilità condizionale di 𝒙 data 𝑤1) \n \n𝑝(𝒙)=0,000330⋅0,55+0,000260⋅0,45=0,000299  (Densità di probabilità assoluta dato 𝒙) \n \n𝑝(𝑤0|𝒙)=0,000330⋅0,55\n0,000299=0,608 (Probabilità a posteriori di 𝑤0 dato 𝒙) \n \n𝑝(𝑤1|𝒙)=0,000260⋅0,45\n0,000299=0,392 (Probabilità a posteriori di 𝑤1 dato 𝒙) \n \nIndice della classe restituita: 0 \n \n \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#34": "Si supponga di partecipare a un gioco \na premi, in cui si può scegliere fra tre \nporte: dietro una di esse c’è \nun’automobile, dietro le altre, due \ncapre. \nSi sceglie una porta, diciamo la numero \n1. A quel punto, il conduttore del gioco \na premi, che sa cosa si nasconde \ndietro ciascuna porta, ne apre un’altra, \ndiciamo la 3, rivelando una capra. \nQuindi domanda: “vorresti scegliere la \nnumero 2 o conservare la tua scelta \niniziale?”\nTi conviene cambiare la tua scelta \noriginale?\nProblema",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#35": "Dal quiz televisivo americano Let’s Make a Deal , \ncondotto dal presentatore Maurice Halprin , noto\ncon lo pseudonimo di Monty Hall. \n4500 puntate dal 1963 al 1991.\nIl concorrente deve scegliere una delle tre\nporte chiuse cheha davanti a sé: dietro a due \ndi esse c’èuna capra , dietro l’altra c’èuna \nautomobile . \nOvviamente , né luiné ilpubblico sanno dietro\na quale porta sitrova l’auto .\nIl concorrente fa la suascelta .\nProblema di Monty Hall",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#36": "A questo punto, ilpresentatore apre\nuna delle altre due porte , rivelando\nuna capra .\nQuindi chiede al concorrente se vuole\nmantenere la porta scelta , o se vuole\ncambiarla .\nDomanda : al concorrente conviene\ncambiare ? \nLa risposta sembra ovvia : sono rimaste\ndue porte , e dietro una di esse c’è\nl’auto . Cambiare porta non dovrebbe\ninfluenzare le probabilità di vincita chea \nquesto punto èlogico ritenere pari a 1/2, \nchesidecida di cambiare o meno .Problema di Monty Hall",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#37": "Problema di Monty Hall",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#38": "Problema di Monty Hall",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#39": "Problema di Monty Hall",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#4": "Appr occi Non Parametrici e Stima della Densità\n!Esempi di casi di distribuzione binomiale sono i risultati di una \nserie di lanci di una stessa moneta o di una serie di estrazioni \nda un'urna (con reintroduzione o reimbussolamento), ognuna \ndelle quali può fornire due soli risultati : il successo con \nprobabilità pe il fallimento con probabilità q=1−p!Reimbussolamento: dovendo estrarre un certo numero di \ncarte/palline/numeri da un mazzo/urna/bussolo, ogni oggetto \nestratto è immesso nuovamente prima di estrarre \ncarte/palline/numeri successivi",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#40": "h\"ps ://www.youtube.com /watch?v =nYX8DMG8_ywProblema di Monty Hall",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#5": "Appr occi Non Parametrici e Stima della Densità\n21prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneApprocci non parametrici e\nStima della Densità\nNon vengono fatte ipotesi sulle distribuzioni deipattern eledensità\ndiprobabilità sono stimate direttamente daltraining set.\nIlproblema della stima accurata della densità èritenuto damolti un\nproblema piùcomplesso della classificazione .Pertanto perché\nrisolvere come sotto -problema unproblema che èpiùcomplesso\ndell’intero compito diclassificazione ?\nIngenerale lastima della densità èaffrontabile inspazi a\ndimensionalità ridotta (es.𝑑=3)ediventa critica alcrescere della\ndimensionalità (curse ofdimensionality ):ilvolume dello spazio\naumenta così tanto cheipattern diventato troppo sparsi .\nStima Densità\nLaprobabilità cheunpattern𝐱cadaall’interno diè:\n𝑃1=න\n𝑝𝐱′𝑑𝐱′\nDati𝑛pattern indipendenti, laprobabilità che𝑘diquesti cadano\nnella regioneècalcolabile attraverso ladistribuzione binomiale :\n𝑃𝑘=𝑛\n𝑘𝑃1𝑘1−𝑃1𝑛−𝑘\nilcuivalor medio è𝑘=𝑛𝑃1(equindi𝑃1=𝑘/𝑛)\nAssumendo che laregione (divolume𝑉)siapiccola eche\nquindi𝑝∙nonvarisignificativamente all’interno diessa :\n𝑃1=න\n𝑝𝐱′𝑑𝐱′≈𝑝𝐱∙𝑉\n𝑝𝐱=𝑃1\n𝑉=𝑘\n𝑛∙𝑉",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#6": "Parzen Window\n22prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneParzen Window\nLaregione ,denominata finestra (Window ),ècostituita daun\nipercubo 𝑑-dimensionale, definito dalla funzione 𝜑:\n𝜑𝐮=ቐ1𝑢𝑗≤1\n2,𝑗=1…𝑑\n0𝑎𝑙𝑡𝑟𝑖𝑚𝑒𝑛𝑡𝑖\nDato ungenerico ipercubo centrato in𝐱eavente latoℎ𝑛(equindi\nvolume𝑉𝑛=ℎ𝑛𝑑)ilnumero dipattern deltraining setchecadono\nall’interno dell’iper-cubo èdato da:\n𝑘𝑛=෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛\nsostituendo 𝑘𝑛(vedi lucido precedente) siottiene :\n𝑝𝑛𝐱=1\n𝑛∙𝑉𝑛෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛,dove𝑉𝑛=ℎ𝑛𝑑\nOvviamente, specie nelcaso incuiilnumero dipattern non sia\nelevato, ladimensione della finestra𝑉𝑛(equindi illatoℎ𝑛)haun\nforte impatto sulrisultato, infatti :\nSe\nlafinestra èpiccola ,lastima risulta piuttosto “rumorosa ”,\nmolto attratta daicampioni estatisticamente instabile .\nSe\nlafinestra ègrande lastima èpiùstabile mapiuttosto vaga\nesfuocata .\nSidimostra che perottenere convergenza ,ladimensione della\nfinestra deve essere calcolata tenendo conto del numero di\ncampioni deltraining set:\n𝑉𝑛=𝑉1\n𝑛,dove𝑉1oℎ1èuniperparametro",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#7": "Parzen Window\n22prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneParzen Window\nLaregione ,denominata finestra (Window ),ècostituita daun\nipercubo 𝑑-dimensionale, definito dalla funzione 𝜑:\n𝜑𝐮=ቐ1𝑢𝑗≤1\n2,𝑗=1…𝑑\n0𝑎𝑙𝑡𝑟𝑖𝑚𝑒𝑛𝑡𝑖\nDato ungenerico ipercubo centrato in𝐱eavente latoℎ𝑛(equindi\nvolume𝑉𝑛=ℎ𝑛𝑑)ilnumero dipattern deltraining setchecadono\nall’interno dell’iper-cubo èdato da:\n𝑘𝑛=෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛\nsostituendo 𝑘𝑛(vedi lucido precedente) siottiene :\n𝑝𝑛𝐱=1\n𝑛∙𝑉𝑛෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛,dove𝑉𝑛=ℎ𝑛𝑑\nOvviamente, specie nelcaso incuiilnumero dipattern non sia\nelevato, ladimensione della finestra𝑉𝑛(equindi illatoℎ𝑛)haun\nforte impatto sulrisultato, infatti :\nSe\nlafinestra èpiccola ,lastima risulta piuttosto “rumorosa ”,\nmolto attratta daicampioni estatisticamente instabile .\nSe\nlafinestra ègrande lastima èpiùstabile mapiuttosto vaga\nesfuocata .\nSidimostra che perottenere convergenza ,ladimensione della\nfinestra deve essere calcolata tenendo conto del numero di\ncampioni deltraining set:\n𝑉𝑛=𝑉1\n𝑛,dove𝑉1oℎ1èuniperparametro\n22prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneParzen Window\nLaregione ,denominata finestra (Window ),ècostituita daun\nipercubo 𝑑-dimensionale, definito dalla funzione 𝜑:\n𝜑𝐮=ቐ1𝑢𝑗≤1\n2,𝑗=1…𝑑\n0𝑎𝑙𝑡𝑟𝑖𝑚𝑒𝑛𝑡𝑖\nDato ungenerico ipercubo centrato in𝐱eavente latoℎ𝑛(equindi\nvolume𝑉𝑛=ℎ𝑛𝑑)ilnumero dipattern deltraining setchecadono\nall’interno dell’iper-cubo èdato da:\n𝑘𝑛=෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛\nsostituendo 𝑘𝑛(vedi lucido precedente) siottiene :\n𝑝𝑛𝐱=1\n𝑛∙𝑉𝑛෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛,dove𝑉𝑛=ℎ𝑛𝑑\nOvviamente, specie nelcaso incuiilnumero dipattern non sia\nelevato, ladimensione della finestra𝑉𝑛(equindi illatoℎ𝑛)haun\nforte impatto sulrisultato, infatti :\nSe\nlafinestra èpiccola ,lastima risulta piuttosto “rumorosa ”,\nmolto attratta daicampioni estatisticamente instabile .\nSe\nlafinestra ègrande lastima èpiùstabile mapiuttosto vaga\nesfuocata .\nSidimostra che perottenere convergenza ,ladimensione della\nfinestra deve essere calcolata tenendo conto del numero di\ncampioni deltraining set:\n𝑉𝑛=𝑉1\n𝑛,dove𝑉1oℎ1èuniperparametro\n22prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneParzen Window\nLaregione ,denominata finestra (Window ),ècostituita daun\nipercubo 𝑑-dimensionale, definito dalla funzione 𝜑:\n𝜑𝐮=ቐ1𝑢𝑗≤1\n2,𝑗=1…𝑑\n0𝑎𝑙𝑡𝑟𝑖𝑚𝑒𝑛𝑡𝑖\nDato ungenerico ipercubo centrato in𝐱eavente latoℎ𝑛(equindi\nvolume𝑉𝑛=ℎ𝑛𝑑)ilnumero dipattern deltraining setchecadono\nall’interno dell’iper-cubo èdato da:\n𝑘𝑛=෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛\nsostituendo 𝑘𝑛(vedi lucido precedente) siottiene :\n𝑝𝑛𝐱=1\n𝑛∙𝑉𝑛෍\n𝑖=1𝑛\n𝜑𝐱𝑖−𝐱\nℎ𝑛,dove𝑉𝑛=ℎ𝑛𝑑\nOvviamente, specie nelcaso incuiilnumero dipattern non sia\nelevato, ladimensione della finestra𝑉𝑛(equindi illatoℎ𝑛)haun\nforte impatto sulrisultato, infatti :\nSe\nlafinestra èpiccola ,lastima risulta piuttosto “rumorosa ”,\nmolto attratta daicampioni estatisticamente instabile .\nSe\nlafinestra ègrande lastima èpiùstabile mapiuttosto vaga\nesfuocata .\nSidimostra che perottenere convergenza ,ladimensione della\nfinestra deve essere calcolata tenendo conto del numero di\ncampioni deltraining set:\n𝑉𝑛=𝑉1\n𝑛,dove𝑉1oℎ1èuniperparametro",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#8": "Iperparametri\n14prof. Davide Maltoni –Università di Bologna\nML\nFondamentiIperparametri\nMolto\n algoritmi richiedono didefinire, primadell’apprendimento\nvero eproprio, ilvalore deicosiddetti iperparametri H.\nEsempi\n diiperparametri :\nIl\nnumero dineuroni inunareteneurale .\nIl\nnumero divicini kinunclassificatore k-NN.\nIl\ngrado diunpolinomio utilizzato inunaregressione .\nIl\ntipodilossfunction .\nSi\nprocede con unapproccio adue livelli nelquale perogni\nvalore «ragionevole » degli iperparametri si esegue\nl’apprendimento, ealtermine della procedura siscelgono gli\niperparametri chehanno fornito prestazioni migliori .\nMa\ncome sivalutano leprestazioni ,esuquali dati?",
    "data_test\\rootfolder\\università\\MachineLearning\\31-CB(3)-sbloccato.pdf#9": "Iperparametri\n!Ricordiamo che\n!Il Training Set è l’insieme di pattern su cui addestrare il \nsistema, trovando il valore ottimo per i parametri (e.g., i pesi \ndelle connessioni in una rete neurale)!Il Validation Set è l’insieme di pattern su cui tarare gli \niperparametri (ciclo esterno)!Il Test Set è l’insieme di pattern su cui valutare le prestazioni \nfinali del sistema\n!N.B. Sempre forte è la tentazione di tarare gli iperparametri \ndirettamente sul Test Set, ma questo dovrebbe essere evitato, \npena sovrastima delle prestazioni! ",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#0": "Machine Learning\nUniversità Roma Tre  \nDipartimento di Ingegneria \nAnno Accademico 2019 -2020\nBayes & Nearest Neighbor",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#1": "Classiﬁcatore Nearest Neighbor (NN)\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#10": "Da NN a k-NN\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#11": "Da NN a k-NN\n28prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDa NN a k-NN\nLa\nregola nearest neighbor produce unpartizionamento dello\ndello spazio, noto come tassellazione diVoronoi :\nOgni elemento 𝐱𝑖∈TSdetermina untassello, all’interno delquale i\npattern saranno assegnati allastessa classe di𝐱𝑖.\nLa\nregola diclassificazione nearest neighbor èpiuttosto radicale ;\ninfatti basta che unelemento deltraining setnon siamolto\n“affidabile” (outlier )affinché tuttiipattern nelle suevicinanze siano\ninseguito etichettati noncorrettamente .\nChe\n errore commette ilclassificatore NNsultraining set?\nUn\nmodo generalmente piùrobusto ,chepuòessere visto come\nestensione della regola nearest -neighbor (inquesto caso detta\n1-nearest neighbor )èilcosiddetto classificatore k-nearest\nneighbor (k-NN).\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#12": "k-Nearest Neighbor ( k-NN)\n29prof. Davide Maltoni –Università di Bologna\nML\nClassificazionek-Nearest -Neighbor (k-NN)\nLa\nregola k-Nearest Neighbor (k-NN)determina ikelementi più\nvicini alpattern𝐱daclassificare (kèuniperparametro );ogni\npattern traikvicini vota perlaclasse cuiesso stesso appartiene ;\nilpattern𝐱viene assegnato allaclasse chehaottenuto ilmaggior\nnumero divoti.\nPer\nTSinfiniti laregola diclassificazione k-NNsidimostra migliore\ndi1-NN, eall’aumentare dik,l’errore Pconverge all’errore\nBayesiano .\nNella\n pratica (TSlimitati), aumentare ksignifica estendere l’iper-\nsfera diricerca andando asondare laprobabilità aposteriori\nlontano dalpunto diinteresse ;ilvalore ottimale dik(solitamente\n<10)deve essere determinato suunvalidation setseparato .\nnella figura il classificatore 5-NN,\nassegna 𝐱alla classe “nera”\nin quanto quest’ultima ha ricevuto 3 \nvoti su 5.\nNel caso di 2 classi è bene \nscegliere k dispari per evitare \npareggi.",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#13": "k-Nearest Neighbor ( k-NN)4.5. THE NEAREST-NEIGHBOR RULE 27\nby examining the labels on the knearest neighbors and taking a vote (Fig. 4.15). We\nshall not go into a thorough analysis of the k-nearest-neighbor rule. However, by\nconsidering the two-class case with kodd (to avoid ties), we can gain some additional\ninsight into these procedures.\nx\nx1x2\nFigure 4.15: The k-nearest-neighbor query starts at the test point and grows a spher-\nical region until it encloses ktraining samples, and labels the test point by a majority\nvote of these samples. In this k= 5 case, the test point xwould be labelled the\ncategory of the black points.\nThe basic motivation for considering the k-nearest-neighbor rule rests on our ear-\nlier observation about matching probabilities with nature. We notice ﬁrst that if\nkis ﬁxed and the number nof samples is allowed to approach inﬁnity, then all of\ntheknearest neighbors will converge to x. Hence, as in the single-nearest-neighbor\ncases, the labels on each of the k-nearest-neighbors are random variables, which in-\ndependently assume the values ωiwith probabilities P(ωi|x),i=1,2. If P(ωm|x)\nis the larger a posteriori probability, then the Bayes decision rule always selects ωm.\nThe single-nearest-neighbor rule selects ωmwith probability P(ωm|x). The k-nearest-\nneighbor rule selects ωmif a majority of the knearest neighbors are labeled ωm, an\nevent of probability\nk/summationdisplay\ni=(k+1)/2/parenleftbiggk\ni/parenrightbigg\nP(ωm|x)i[1−P(ωm|x)]k−i. (54)\nIn general, the larger the value of k, the greater the probability that ωmwill be\nselected.\nWe could analyze the k-nearest-neighbor rule in much the same way that we\nanalyzed the single-nearest-neighbor rule. However, since the arguments become more\ninvolved and supply little additional insight, we shall content ourselves with stating\nthe results. It can be shown that if kis odd, the large-sample two-class error rate for\nthek-nearest-neighbor rule is bounded above by the function Ck(P∗), where Ck(P∗)\nis deﬁned to be the smallest concave function of P∗greater than\n(k−1)/2/summationdisplay\ni=0/parenleftbiggk\ni/parenrightbigg/bracketleftbig\n(P∗)i+1(1−P∗)k−i+(P∗)k−i(1−P∗)i+1/bracketrightbig\n. (55)",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#14": "k-Nearest Neighbor ( k-NN)\n29prof. Davide Maltoni –Università di Bologna\nML\nClassificazionek-Nearest -Neighbor (k-NN)\nLa\nregola k-Nearest Neighbor (k-NN)determina ikelementi più\nvicini alpattern𝐱daclassificare (kèuniperparametro );ogni\npattern traikvicini vota perlaclasse cuiesso stesso appartiene ;\nilpattern𝐱viene assegnato allaclasse chehaottenuto ilmaggior\nnumero divoti.\nPer\nTSinfiniti laregola diclassificazione k-NNsidimostra migliore\ndi1-NN, eall’aumentare dik,l’errore Pconverge all’errore\nBayesiano .\nNella\n pratica (TSlimitati), aumentare ksignifica estendere l’iper-\nsfera diricerca andando asondare laprobabilità aposteriori\nlontano dalpunto diinteresse ;ilvalore ottimale dik(solitamente\n<10)deve essere determinato suunvalidation setseparato .\nnella figura il classificatore 5-NN,\nassegna 𝐱alla classe “nera”\nin quanto quest’ultima ha ricevuto 3 \nvoti su 5.\nNel caso di 2 classi è bene \nscegliere k dispari per evitare \npareggi.",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#15": "k-Nearest Neighbor ( k-NN)28 CHAPTER 4. NONPARAMETRIC TECHNIQUES\nHere the summation over the ﬁrst bracketed term represents the probability of error\ndue to ipoints coming from the category having the minimum probability and k−i>i\npoints from the other category. The summation over the second term in the brackets\nis the probability that k−ipoints are from the minimum-probability category and\ni+1<k−ifrom the higher probability category. Both of these cases constitute\nerrors under the k-nearest-neighbor decision rule, and thus we must add them to ﬁnd\nthe full probability of error (Problem 18).\nFigure 4.16 shows the bounds on the k-nearest-neighbor error rates for several\nvalues of k. As kincreases, the upper bounds get progressively closer to the lower\nbound — the Bayes rate. In the limit as kgoes to inﬁnity, the two bounds meet and\nthek-nearest-neighbor rule becomes optimal.\n0 0.1 0.2 0.3 0.40.10.20.30.4\nBayes Rate\nP*P\n1\n3\n5\n9\n150.5\nFigure 4.16: The error-rate for the k-nearest-neighbor rule for a two-category problem\nis bounded by Ck(P∗) in Eq. 55. Each curve is labelled by k; when k=∞, the\nestimated probabilities match the true probabilities and thus the error rate is equal\nto the Bayes rate, i.e., P=P∗.\nAt the risk of sounding repetitive, we conclude by commenting once again on the\nﬁnite-sample situation encountered in practice. The k-nearest-neighbor rule can be\nviewed as another attempt to estimate the a posteriori probabilities P(ωi|x) from\nsamples. We want to use a large value of kto obtain a reliable estimate. On the\nother hand, we want all of the knearest neighbors x′to be very near xto be sure\nthatP(ωi|x′) is approximately the same as P(ωi|x). This forces us to choose a\ncompromise kthat is a small fraction of the number of samples. It is only in the limit\nasngoes to inﬁnity that we can be assured of the nearly optimal behavior of the\nk-nearest-neighbor rule.\n4.5.5 Computational Complexity of the k–Nearest-Neighbor\nRule\nThe computational complexity of the nearest-neighbor algorithm — both in space\n(storage of prototypes) and time (search) — has received a great deal of analy-\nsis. There are a number of elegant theorems from computational geometry on the\nconstruction of Voronoi tesselations and nearest-neighbor searches in one- and two-\ndimensional spaces. However, because the greatest use of nearest-neighbor techniques\nis for problems with many features, we concentrate on the more general d-dimensional\ncase.\nSuppose we have nlabelled training samples in ddimensions, and seek to ﬁnd\nthe closest to a test point x(k= 1). In the most naive approach we inspect each\nstored point in turn, calculate its Euclidean distance to x, retaining the identity only\nof the current closest one. Each distance calculation is O(d), and thus this search4.5. THE NEAREST-NEIGHBOR RULE 27\nby examining the labels on the knearest neighbors and taking a vote (Fig. 4.15). We\nshall not go into a thorough analysis of the k-nearest-neighbor rule. However, by\nconsidering the two-class case with kodd (to avoid ties), we can gain some additional\ninsight into these procedures.\nx\nx1x2\nFigure 4.15: The k-nearest-neighbor query starts at the test point and grows a spher-\nical region until it encloses ktraining samples, and labels the test point by a majority\nvote of these samples. In this k= 5 case, the test point xwould be labelled the\ncategory of the black points.\nThe basic motivation for considering the k-nearest-neighbor rule rests on our ear-\nlier observation about matching probabilities with nature. We notice ﬁrst that if\nkis ﬁxed and the number nof samples is allowed to approach inﬁnity, then all of\ntheknearest neighbors will converge to x. Hence, as in the single-nearest-neighbor\ncases, the labels on each of the k-nearest-neighbors are random variables, which in-\ndependently assume the values ωiwith probabilities P(ωi|x),i=1,2. If P(ωm|x)\nis the larger a posteriori probability, then the Bayes decision rule always selects ωm.\nThe single-nearest-neighbor rule selects ωmwith probability P(ωm|x). The k-nearest-\nneighbor rule selects ωmif a majority of the knearest neighbors are labeled ωm, an\nevent of probability\nk/summationdisplay\ni=(k+1)/2/parenleftbiggk\ni/parenrightbigg\nP(ωm|x)i[1−P(ωm|x)]k−i. (54)\nIn general, the larger the value of k, the greater the probability that ωmwill be\nselected.\nWe could analyze the k-nearest-neighbor rule in much the same way that we\nanalyzed the single-nearest-neighbor rule. However, since the arguments become more\ninvolved and supply little additional insight, we shall content ourselves with stating\nthe results. It can be shown that if kis odd, the large-sample two-class error rate for\nthek-nearest-neighbor rule is bounded above by the function Ck(P∗), where Ck(P∗)\nis deﬁned to be the smallest concave function of P∗greater than\n(k−1)/2/summationdisplay\ni=0/parenleftbiggk\ni/parenrightbigg/bracketleftbig\n(P∗)i+1(1−P∗)k−i+(P∗)k−i(1−P∗)i+1/bracketrightbig\n. (55)Inparticolare ,sipuò dimostrare (vedi Duda etal.,2000 )che Ck(P*) èdefinita come\nlapiùpiccola funzione concava diP*maggiore di",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#16": "Esempi k-NN\n30prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempi k-NN\nNell’esempio visto in precedenza, \n la regola k -NN con k=3 \nassegna il pattern 𝐱alla classe 𝑤2(femmine -rossi )\nL’animazione (scomposta nel lucido successivo) mostra il \npartizionamento dello spazio operato dalla regola k-NN sul \ntraining set con 5 classi visto in precedenza al variare di k\nPesoAltezza\n>@T168 ,57 x\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#17": "Esempi k-NN\n30prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempi k-NN\nNell’esempio visto in precedenza, \n la regola k -NN con k=3 \nassegna il pattern 𝐱alla classe 𝑤2(femmine -rossi )\nL’animazione (scomposta nel lucido successivo) mostra il \npartizionamento dello spazio operato dalla regola k-NN sul \ntraining set con 5 classi visto in precedenza al variare di k\nPesoAltezza\n>@T168 ,57 x\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#18": "Espansione Lucido Precedente (k=1,3)\n31prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEspansione dell’animazione\nlucido precedente (k=1,3,5,7,9,11)\nk=1 k=3\nk=5 k=7\nk=9 k=11",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#19": "Espansione Lucido Precedente (k=5,7)\n31prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEspansione dell’animazione\nlucido precedente (k=1,3,5,7,9,11)\nk=1 k=3\nk=5 k=7\nk=9 k=11",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#2": "Classiﬁcatore Nearest Neighbor (NN)\n26prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore Nearest Neighbor (NN)\nData una metrica 𝑑𝑖𝑠𝑡(∙)nello spazio multidimensionale (es.\ndistanza euclidea )ilclassificarore nearest neighbor (letteralmente “il\npiùvicino traivicini”),classifica unpattern𝐱conlastessa classe\ndell’elemento 𝐱′adesso piùvicino neltraining setTS:\n𝑑𝑖𝑠𝑡𝐱,𝐱′=𝑚𝑖𝑛\n𝐱𝑖∈TS𝑑𝑖𝑠𝑡𝐱,𝐱𝑖\nInvece\n diderivare daidatiledistribuzioni condizionali delle classi\nperpoifaruso della regola diBayes perlaclassificazione,\nquesto classificatore cerca inmodo piuttosto pragmatico di\nmassimizzare direttamente laprobabilità aposteriori ;infatti se𝐱′\nèmolto vicino a𝐱èlecito supporre che:\n𝑃𝑤𝑖𝐱≈𝑃𝑤𝑖𝐱′\nIn\neffetti, sipuòdimostrare (solo però nelcaso diTSpopolato da\ninfiniti campioni) chelaprobabilità dierrore P(nella figura sotto)\ndella regola nearest neighbor nonèmaipeggiore deldoppio del\nminimo errore possibile P*(quello Bayesiano ).\nNella\n pratica ,questo non significa però chel’approccio\nBayesiano fornisca sempre risultati migliori dinearest neighbor ,\ninfatti selastima delle densità condizionali èpoco accurata i\nrisultati delclassificatore Bayesiano possono essere peggiori .\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#20": "Espansione Lucido Precedente (k=9,11)\n31prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEspansione dell’animazione\nlucido precedente (k=1,3,5,7,9,11)\nk=1 k=3\nk=5 k=7\nk=9 k=11",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#21": "Esempi Bayes\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\nIn figura un \nesempio con\ns=5 classi",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#22": "Bayes e Conﬁdenza di Classiﬁcazione\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\nIn figura un \nesempio con\ns=5 classi",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#23": "Bayes e Conﬁdenza di Classiﬁcazione\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#24": "k-NN e Conﬁdenza di Classiﬁcazione\n32prof. Davide Maltoni –Università di Bologna\nML\nClassificazionek-NN e Confidenza di Classificazione\nDaunclassificatore k-NNrisulta piuttosto semplice estrarre una\nconfidenza (probabilistica) circa laclassificazione eseguita ;siano\n𝑣1,𝑣2…𝑣𝑠,෍\n𝑖=1𝑠\n𝑣𝑖=𝑘\nivotiottenuti dalpattern𝐱,allora leconfidenze (vedi sfumature in\nfigura sotto )possono essere semplicemente ottenute dividendo\nper𝑘ivotiottenuti :\n𝑣1\n𝑘,𝑣2\n𝑘…𝑣𝑠\n𝑘\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#25": "k-NN e Confidenza di Classificazione\n32prof. Davide Maltoni –Università di Bologna\nML\nClassificazionek-NN e Confidenza di Classificazione\nDaunclassificatore k-NNrisulta piuttosto semplice estrarre una\nconfidenza (probabilistica) circa laclassificazione eseguita ;siano\n𝑣1,𝑣2…𝑣𝑠,෍\n𝑖=1𝑠\n𝑣𝑖=𝑘\nivotiottenuti dalpattern𝐱,allora leconfidenze (vedi sfumature in\nfigura sotto )possono essere semplicemente ottenute dividendo\nper𝑘ivotiottenuti :\n𝑣1\n𝑘,𝑣2\n𝑘…𝑣𝑠\n𝑘\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#26": "NN e Complessità Computazionale\n33prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneNN e complessità computazionale\nL’utilizzo diunclassificatore NNok-NNnelcaso ditraining setdi\nelevate dimensioni puòdiventare problematico :\nNecessario\n memorizzare tuttiipattern delTraining Set\nPer\nogni classificazione ènecessario calcolare ladistanza del\npattern daclassificare datutti ipattern deltraining sete\nordinare (parzialmente ledistanze) perottenere lepiùpiccole\nTecniche diediting/ condensing (lucido successivo) possono\nalleviare questo problema, maquando l’efficienza èimportante è\nconsigliabile indicizzare idati attraverso strutture spaziali (es.\nkd-tree)checonsentono diindividuare ivicini senza effettuare una\nscansione esaustiva .\nLalibreria FLANN (C++) consente dieffettuare ricerche nearest\nneighbor approssimate molto efficientemente .\nhttp://www .cs.ubc.ca/research/flann/",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#27": "NN e Complessità Computazionale\n33prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneNN e complessità computazionale\nL’utilizzo diunclassificatore NNok-NNnelcaso ditraining setdi\nelevate dimensioni puòdiventare problematico :\nNecessario\n memorizzare tuttiipattern delTraining Set\nPer\nogni classificazione ènecessario calcolare ladistanza del\npattern daclassificare datutti ipattern deltraining sete\nordinare (parzialmente ledistanze) perottenere lepiùpiccole\nTecniche diediting/ condensing (lucido successivo) possono\nalleviare questo problema, maquando l’efficienza èimportante è\nconsigliabile indicizzare idati attraverso strutture spaziali (es.\nkd-tree)checonsentono diindividuare ivicini senza effettuare una\nscansione esaustiva .\nLalibreria FLANN (C++) consente dieffettuare ricerche nearest\nneighbor approssimate molto efficientemente .\nhttp://www .cs.ubc.ca/research/flann/\nhttps ://github.com /mariusmuja /flann",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#28": "NN e Complessità ComputazionaleFAST APPROXIMATE NEAREST NEIGHBORS\nWITH AUTOMATIC ALGORITHM CONFIGURATION\nMarius Muja, David G. Lowe\nComputer Science Department, University of British Columbia, Vancouver, B.C., Canada\nmariusm@cs.ubc.ca, lowe@cs.ubc.ca\nKeywords: nearest-neighbors search, randomized kd-trees, hierarchical k-means tree, clustering.\nAbstract: For many computer vision problems, the most time consuming component consists of nearest neighbor match-\ning in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional\nproblems that are faster than linear search. Approximate algorithms are known to provide large speedups with\nonly minor loss in accuracy, but many such algorithms have been published with only minimal guidance on\nselecting an algorithm and its parameters for any given problem. In this paper, we describe a system that\nanswers the question, “What is the fastest approximate nearest-neighbor algorithm for my data?” Our system\nwill take any given dataset and desired degree of precision and use these to automatically determine the best\nalgorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical\nk-means trees, which we have found to provide the best known performance on many datasets. After testing a\nrange of alternatives, we have found that multiple randomized k-d trees provide the best performance for other\ndatasets. We are releasing public domain code that implements these approaches. This library provides about\none order of magnitude improvement in query time over the best previously available software and provides\nfully automated parameter selection.\n1 INTRODUCTION\nThe most computationally expensive part of many\ncomputer vision algorithms consists of searching for\nthe closest matches to high-dimensional vectors. Ex-\namples of such problems include ﬁnding the best\nmatches for local image features in large datasets\n(Lowe, 2004; Philbin et al., 2007), clustering local\nfeatures into visual words using the k-means or sim-\nilar algorithms (Sivic and Zisserman, 2003), or per-\nforming normalized cross-correlation to compare im-\nage patches in large datasets (Torralba et al., 2008).\nThe nearest neighbor search problem is also of major\nimportance in many other applications, including ma-\nchine learning, document retrieval, data compression,\nbioinformatics, and data analysis.\nWe can deﬁne the nearest neighbor search prob-\nlem as follows: given a set of points P={p1,..., pn}\nin a vector space X, these points must be preprocessed\nin such a way that given a new query point q∈X,\nﬁnding the points in Pthat are nearest to qcan be per-formed efﬁciently. In this paper, we will assume that\nXis an Euclidean vector space, which is appropriate\nfor most problems in computer vision. We will de-\nscribe potential extensions of our approach to general\nmetric spaces, although this would come at some cost\nin efﬁciency.\nFor high-dimensional spaces, there are often no\nknown algorithms for nearest neighbor search that\nare more efﬁcient than simple linear search. As lin-\near search is too costly for many applications, this\nhas generated an interest in algorithms that perform\napproximate nearest neighbor search, in which non-\noptimal neighbors are sometimes returned. Such ap-\nproximate algorithms can be orders of magnitude\nfaster than exact search, while still providing near-\noptimal accuracy.\nThere have been hundreds of papers published on\nalgorithms for approximate nearest neighbor search,\nbut there has been little systematic comparison to\nguide the choice among algorithms and set their inter-\nnal parameters. One reason for this is that the relativeFAST APPROXIMATE NEAREST NEIGHBORS\nWITH AUTOMATIC ALGORITHM CONFIGURATION\nMarius Muja, David G. Lowe\nComputer Science Department, University of British Columbia, Vancouver, B.C., Canada\nmariusm@cs.ubc.ca, lowe@cs.ubc.ca\nKeywords: nearest-neighbors search, randomized kd-trees, hierarchical k-means tree, clustering.\nAbstract: For many computer vision problems, the most time consuming component consists of nearest neighbor match-\ning in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional\nproblems that are faster than linear search. Approximate algorithms are known to provide large speedups with\nonly minor loss in accuracy, but many such algorithms have been published with only minimal guidance on\nselecting an algorithm and its parameters for any given problem. In this paper, we describe a system that\nanswers the question, “What is the fastest approximate nearest-neighbor algorithm for my data?” Our system\nwill take any given dataset and desired degree of precision and use these to automatically determine the best\nalgorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical\nk-means trees, which we have found to provide the best known performance on many datasets. After testing a\nrange of alternatives, we have found that multiple randomized k-d trees provide the best performance for other\ndatasets. We are releasing public domain code that implements these approaches. This library provides about\none order of magnitude improvement in query time over the best previously available software and provides\nfully automated parameter selection.\n1 INTRODUCTION\nThe most computationally expensive part of many\ncomputer vision algorithms consists of searching for\nthe closest matches to high-dimensional vectors. Ex-\namples of such problems include ﬁnding the best\nmatches for local image features in large datasets\n(Lowe, 2004; Philbin et al., 2007), clustering local\nfeatures into visual words using the k-means or sim-\nilar algorithms (Sivic and Zisserman, 2003), or per-\nforming normalized cross-correlation to compare im-\nage patches in large datasets (Torralba et al., 2008).\nThe nearest neighbor search problem is also of major\nimportance in many other applications, including ma-\nchine learning, document retrieval, data compression,\nbioinformatics, and data analysis.\nWe can deﬁne the nearest neighbor search prob-\nlem as follows: given a set of points P={p1,..., pn}\nin a vector space X, these points must be preprocessed\nin such a way that given a new query point q∈X,\nﬁnding the points in Pthat are nearest to qcan be per-formed efﬁciently. In this paper, we will assume that\nXis an Euclidean vector space, which is appropriate\nfor most problems in computer vision. We will de-\nscribe potential extensions of our approach to general\nmetric spaces, although this would come at some cost\nin efﬁciency.\nFor high-dimensional spaces, there are often no\nknown algorithms for nearest neighbor search that\nare more efﬁcient than simple linear search. As lin-\near search is too costly for many applications, this\nhas generated an interest in algorithms that perform\napproximate nearest neighbor search, in which non-\noptimal neighbors are sometimes returned. Such ap-\nproximate algorithms can be orders of magnitude\nfaster than exact search, while still providing near-\noptimal accuracy.\nThere have been hundreds of papers published on\nalgorithms for approximate nearest neighbor search,\nbut there has been little systematic comparison to\nguide the choice among algorithms and set their inter-\nnal parameters. One reason for this is that the relativeFAST APPROXIMATE NEAREST NEIGHBORS\nWITH AUTOMATIC ALGORITHM CONFIGURATION\nMarius Muja, David G. Lowe\nComputer Science Department, University of British Columbia, Vancouver, B.C., Canada\nmariusm@cs.ubc.ca, lowe@cs.ubc.ca\nKeywords: nearest-neighbors search, randomized kd-trees, hierarchical k-means tree, clustering.\nAbstract: For many computer vision problems, the most time consuming component consists of nearest neighbor match-\ning in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional\nproblems that are faster than linear search. Approximate algorithms are known to provide large speedups with\nonly minor loss in accuracy, but many such algorithms have been published with only minimal guidance on\nselecting an algorithm and its parameters for any given problem. In this paper, we describe a system that\nanswers the question, “What is the fastest approximate nearest-neighbor algorithm for my data?” Our system\nwill take any given dataset and desired degree of precision and use these to automatically determine the best\nalgorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical\nk-means trees, which we have found to provide the best known performance on many datasets. After testing a\nrange of alternatives, we have found that multiple randomized k-d trees provide the best performance for other\ndatasets. We are releasing public domain code that implements these approaches. This library provides about\none order of magnitude improvement in query time over the best previously available software and provides\nfully automated parameter selection.\n1 INTRODUCTION\nThe most computationally expensive part of many\ncomputer vision algorithms consists of searching for\nthe closest matches to high-dimensional vectors. Ex-\namples of such problems include ﬁnding the best\nmatches for local image features in large datasets\n(Lowe, 2004; Philbin et al., 2007), clustering local\nfeatures into visual words using the k-means or sim-\nilar algorithms (Sivic and Zisserman, 2003), or per-\nforming normalized cross-correlation to compare im-\nage patches in large datasets (Torralba et al., 2008).\nThe nearest neighbor search problem is also of major\nimportance in many other applications, including ma-\nchine learning, document retrieval, data compression,\nbioinformatics, and data analysis.\nWe can deﬁne the nearest neighbor search prob-\nlem as follows: given a set of points P={p1,..., pn}\nin a vector space X, these points must be preprocessed\nin such a way that given a new query point q∈X,\nﬁnding the points in Pthat are nearest to qcan be per-formed efﬁciently. In this paper, we will assume that\nXis an Euclidean vector space, which is appropriate\nfor most problems in computer vision. We will de-\nscribe potential extensions of our approach to general\nmetric spaces, although this would come at some cost\nin efﬁciency.\nFor high-dimensional spaces, there are often no\nknown algorithms for nearest neighbor search that\nare more efﬁcient than simple linear search. As lin-\near search is too costly for many applications, this\nhas generated an interest in algorithms that perform\napproximate nearest neighbor search, in which non-\noptimal neighbors are sometimes returned. Such ap-\nproximate algorithms can be orders of magnitude\nfaster than exact search, while still providing near-\noptimal accuracy.\nThere have been hundreds of papers published on\nalgorithms for approximate nearest neighbor search,\nbut there has been little systematic comparison to\nguide the choice among algorithms and set their inter-\nnal parameters. One reason for this is that the relative",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#29": "NN e Prototipi di Classi\n34prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneNN e Prototipi di Classi\nTalvolta nella classificazione nearest neighbor invece dimantenere\ntuttiipattern delTSecalcolare ladistanza daciascuno diessi, si\npreferisce selezionare/derivare daessi uno (opiù) prototipi per\nciascuna classe eutilizzare questi ultimi perlaclassificazione come\nsefossero isolielementi diTS:queste tecniche prendono ilnome di:\nediting\n :quando sicancellano solo pattern daltraining set,senza\nderivare nuovi pattern\ncondensing\n :seiprototipi non appartenevano alTSesono stati\nderivati\nCiòcomporta solitamente iseguenti vantaggi :\nnon\nènecessario calcolare unelevato numero didistanze .\niprototipi sono spesso piùaffidabili erobusti disingoli pattern (si\nriduce ilrischio diaffidarsi adoutlier ).\nUnsingolo prototipo diclasse può essere derivato come vettore\nmedio deivettori diquella classe nelTS.Processi divector\nquantization oclustering consentono diottenere piùprototipi perogni\nclasse .sebbene il pattern di \nTS più vicino a 𝐱sia \n“blu”, 𝐱viene \nclassificato come \nrosso, in quanto il \nprototipo più vicino è \nquello rosso.x",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#3": "Classiﬁcatore di Bayes\n3prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneClassificatore di Bayes\nDato unpattern𝐱daclassificare inunadelle𝑠classi𝑤1,𝑤2…𝑤𝑠di\ncuisono note:\nle\nprobabilità apriori𝑃𝑤1,𝑃𝑤2…𝑃𝑤𝑠\nledensità diprobabilità condizionali 𝑝𝐱𝑤1,𝑝𝐱𝑤2…𝑝𝐱𝑤𝑠\nlaregola diclassificazione diBayes assegna 𝐱allaclasse𝑏percui\nèmassima laprobabilità aposteriori :\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑃𝑤𝑖𝐱\nMassimizzare laprobabilità aposteriori significa massimizzare la\ndensità diprobabilità condizionale tenendo comunque conto della\nprobabilità apriori delle classi .\nLa\nregola sidimostra ottima inquanto minimizza l’errore di\nclassificazione .Adesempio nelcaso di2classi e𝑑=1:\n𝑃𝑒𝑟𝑟𝑜𝑟=න\n1𝑝𝑥𝑤2𝑃𝑤2𝑑𝑥+න\n2𝑝𝑥𝑤1𝑃𝑤1𝑑𝑥\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#30": "NN e Prototipi di Classi\n34prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneNN e Prototipi di Classi\nTalvolta nella classificazione nearest neighbor invece dimantenere\ntuttiipattern delTSecalcolare ladistanza daciascuno diessi, si\npreferisce selezionare/derivare daessi uno (opiù) prototipi per\nciascuna classe eutilizzare questi ultimi perlaclassificazione come\nsefossero isolielementi diTS:queste tecniche prendono ilnome di:\nediting\n :quando sicancellano solo pattern daltraining set,senza\nderivare nuovi pattern\ncondensing\n :seiprototipi nonappartenevano alTSesono stati\nderivati\nCiòcomporta solitamente iseguenti vantaggi :\nnon\nènecessario calcolare unelevato numero didistanze .\niprototipi sono spesso piùaffidabili erobusti disingoli pattern (si\nriduce ilrischio diaffidarsi adoutlier ).\nUnsingolo prototipo diclasse può essere derivato come vettore\nmedio deivettori diquella classe nelTS.Processi divector\nquantization oclustering consentono diottenere piùprototipi perogni\nclasse .sebbene il pattern di \nTS più vicino a 𝐱sia \n“blu”, 𝐱viene \nclassificato come \nrosso, in quanto il \nprototipo più vicino è \nquello rosso.x",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#31": "NN e Prototipi di Classi\n34prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneNN e Prototipi di Classi\nTalvolta nella classificazione nearest neighbor invece dimantenere\ntuttiipattern delTSecalcolare ladistanza daciascuno diessi, si\npreferisce selezionare/derivare daessi uno (opiù) prototipi per\nciascuna classe eutilizzare questi ultimi perlaclassificazione come\nsefossero isolielementi diTS:queste tecniche prendono ilnome di:\nediting\n :quando sicancellano solo pattern daltraining set,senza\nderivare nuovi pattern\ncondensing\n :seiprototipi nonappartenevano alTSesono stati\nderivati\nCiòcomporta solitamente iseguenti vantaggi :\nnon\nènecessario calcolare unelevato numero didistanze .\niprototipi sono spesso piùaffidabili erobusti disingoli pattern (si\nriduce ilrischio diaffidarsi adoutlier ).\nUnsingolo prototipo diclasse può essere derivato come vettore\nmedio deivettori diquella classe nelTS.Processi divector\nquantization oclustering consentono diottenere piùprototipi perogni\nclasse .sebbene il pattern di \nTS più vicino a 𝐱sia \n“blu”, 𝐱viene \nclassificato come \nrosso, in quanto il \nprototipo più vicino è \nquello rosso.x",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#32": "NN e Metriche\n35prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneNN e Metriche\nIl\ncomportamento della regola k-NNèstrettamente legato alla\nmetrica (funzione distanza )adottata .\nLa\ndistanza euclidea ,cherappresenta ilcaso L2nella definizione\ndimetriche diMinkowski ,èsicuramente lametrica piùspesso\nutilizzata .\n𝐿𝑘𝐚,𝐛=෍\n𝑖=1𝑑\n𝑎𝑖−𝑏𝑖𝑘1/𝑘\nNella\n pratica ,prima diadottare semplicemente ladistanza\neuclidea èbene valutare lospazio divariazione delle componenti\n(ofeature )elapresenza dieventuali forti correlazioni trale\nstesse .\nSupponiamo\n adesempio divoler classificare lepersone sulla\nbasedell’altezza edella lunghezza delpiede .Ogni pattern𝐱\n(bidimensionale) risulta costituito dadue feature (𝑥1=altezza,\n𝑥2=lunghezza delpiede) .\nLospazio divariazione dell’altezza (210-140 =70cm) risulta\nmaggiore diquello della lunghezza delpiede (40-20=20cm).\nPertanto selasimilarità trapattern venisse misurata consemplice\ndistanza euclidea lacomponente altezza“peserebbe ”piùdella\ncomponente lunghezza delpiede .𝑥2140 cm 210 cm𝑥1\n20 cm40 cm",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#33": "NN e Metriche\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#34": "Normalizzazione\n36prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneNormalizzazione\nPerevitare iproblemi legati adiversi spazi divariazioni delle feature ,\nparticolarmente fastidiosi peralcune tecniche (es.retineurali), si\nconsiglia dinormalizzare ipattern .\nLenormalizzazioni piùcomuni sono :\nMin\n-Max scaling :per ogni feature 𝑖−𝑒𝑠𝑖𝑚𝑎 sicalcolano il\nmassimo 𝑚𝑎𝑥𝑖eilminimo𝑚𝑖𝑛𝑖esiapplica una trasformazione\nlineare (scaling )che«tipicamente» mappa𝑚𝑖𝑛𝑖a0e𝑚𝑎𝑥𝑖a1.\n𝑥′=𝑥−𝑚𝑖𝑛𝑖/𝑚𝑎𝑥𝑖−𝑚𝑖𝑛𝑖\nStandardization\n :perogni feature𝑖−𝑒𝑠𝑖𝑚𝑎 sicalcola lamedia\n𝑚𝑒𝑎𝑛𝑖eladeviazione standard 𝑠𝑡𝑑𝑑𝑒𝑣 𝑖esitrasformano ivalori\ncome :\n𝑥′=𝑥−𝑚𝑒𝑎𝑛𝑖/𝑠𝑡𝑑𝑑𝑒𝑣 𝑖\nDopo latrasformazione tutte lefeature hanno (sul training set)\nmedia 0edeviazione standard 1.\nAttenzione :iparametri della normalizzazione (es.minimi, massimi) si\ncalcolano sulsolo training setelatrasformazione siapplica siaatutti\nidati(training, validation ,test).\nLesemplici tecniche sopra descritte operano sulle singole feature\nindipendentemente .Una tecnica dinormalizzazione efficace (mapiù\ncostosa) che opera simultaneamente sututte lefeature tenendo\nconto della lorocorrelazione èlaWhitening transform .",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#35": "Normalizzazione\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#36": "Whitening Transform\n37prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneWhitening transform\nUn’efficace normalizzazione rispetto agli spazi divariazione, in\ngrado anche ditener conto delle correlazioni trafeature èpossibile :\npre\n-normalizzando lospazio delle feature attraverso\nWhitening transform (che vedremo meglio inseguito)\nutilizzando\n come metrica ladistanza diMahalanobis .\nLedue alternative sono equivalenti .Nelprimo casol’ellissoide\ncorrispondente allospazio delle feature viene“sfericizzato ”apriori\neviene inseguito usata ladistanza euclidea ;nelsecondo la\ndistanza diMahalanobis normalizza ogni componente sulla base\ndella matrice dicovarianza 6.\nDanon sottovalutare l’importanza della correlazione trafeatures\ncome aspetto negativo perlaclassificazione .Infatti,l’utilizzo di\nfeature correlate riduce (anche drasticamente) ilpotere\ndiscriminante .Nelcaso ideale tutte lefeature sono staticamente\nindipendenti (ellissoide assiparalleli aquelli cartesiani) .\nDue feature altamente discriminanti seprese individualmente, matraloro\nfortemente correlate, sono nelcomplesso meno discriminanti diuna terza\nfeature leggermente piùdidiscriminante diognuna delle precedenti .\nLadistanza diMahalanobis (olasfericizzazione dello spazio) tiene\nconto delle correlazioni epesa maggiormente feature non\ncorrelate .𝑥1𝑥2\ndistribuzione\noriginaledopo Whitening\ntransform",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#37": "Distanza Mahalanobis\n11prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistanza Mahalanobis\nLa\ndistanza diMahalanobis 𝑟tra𝐱e𝛍,definitadall’equazione :\n𝑟2=𝐱−𝛍𝑡Σ−1𝐱−𝛍\ndefinisce ibordi adensità costante inuna distribuzione\nmultinormale .Tale distanza viene spesso utilizzata in\nsostituzione della distanza euclidea ,essendo ingrado di\n“pesare”lediverse componenti tenendo conto deirelativi spazi\ndivariazione edella lorocorrelazione .\n𝑟=1\n𝑟=2\n𝑟=3\n𝑟=4\n𝑥1𝑥2\n11prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneDistanza Mahalanobis\nLa\ndistanza diMahalanobis 𝑟tra𝐱e𝛍,definitadall’equazione :\n𝑟2=𝐱−𝛍𝑡Σ−1𝐱−𝛍\ndefinisce ibordi adensità costante inuna distribuzione\nmultinormale .Tale distanza viene spesso utilizzata in\nsostituzione della distanza euclidea ,essendo ingrado di\n“pesare”lediverse componenti tenendo conto deirelativi spazi\ndivariazione edella lorocorrelazione .\n𝑟=1\n𝑟=2\n𝑟=3\n𝑟=4\n𝑥1𝑥2",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#38": "Whitening Transform\n37prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneWhitening transform\nUn’efficace normalizzazione rispetto agli spazi divariazione, in\ngrado anche ditener conto delle correlazioni trafeature èpossibile :\npre\n-normalizzando lospazio delle feature attraverso\nWhitening transform (che vedremo meglio inseguito)\nutilizzando\n come metrica ladistanza diMahalanobis .\nLedue alternative sono equivalenti .Nelprimo casol’ellissoide\ncorrispondente allospazio delle feature viene“sfericizzato ”apriori\neviene inseguito usata ladistanza euclidea ;nelsecondo la\ndistanza diMahalanobis normalizza ogni componente sulla base\ndella matrice dicovarianza 6.\nDanon sottovalutare l’importanza della correlazione trafeatures\ncome aspetto negativo perlaclassificazione .Infatti,l’utilizzo di\nfeature correlate riduce (anche drasticamente) ilpotere\ndiscriminante .Nelcaso ideale tutte lefeature sono staticamente\nindipendenti (ellissoide assiparalleli aquelli cartesiani) .\nDue feature altamente discriminanti seprese individualmente, ma traloro\nfortemente correlate, sono nelcomplesso meno discriminanti diuna terza\nfeature leggermente piùdidiscriminante diognuna delle precedenti .\nLadistanza diMahalanobis (olasfericizzazione dello spazio) tiene\nconto delle correlazioni epesa maggiormente feature non\ncorrelate .𝑥1𝑥2\ndistribuzione\noriginaledopo Whitening\ntransform",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#39": "10prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneRappresentazione grafica\nNormale Multivariata\nPer\n𝑑=2laforma della distribuzione èquella diun’ellisse .\n𝛍\n=𝜇1,𝜇2controlla laposizione delcentro .\n𝜎11e𝜎22determinano l’allungamento suidueassidell’ellisse .\n𝜎12=𝜎21controlla larotazione dell’ellisse rispetto agli assi\ncartesiani .\n•se=0(matrice dicovarianza diagonale ),ladistribuzione\nmultinormale èdefinita come prodotto di𝑑normali\nmonodimensionali .Intalcaso gliassidell’ellisse sono\nparalleli agliassicartesiani (es.Naive Bayes Classifier ).\n•Se >0(come nel caso della figura )𝑥1e𝑥2sono\npositivamente correlate (quando aumenta 𝑥1aumenta\nanche𝑥2).\n•Se<0𝑥1e𝑥2sono negativamente correlate (quando\naumenta 𝑥1cala𝑥2).\nGli\nassidell’ellisse sono paralleli agliautovettori diΣ.Le diverse ellissi \nindividuano \nluoghi di punti a \ndensità costante\n𝑥1𝑥2Distribuzione Normale Multivariata (Multinormale)",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#4": "Classiﬁcatore Nearest Neighbor (NN)\nQuando laprobabilità aposteriori diuna classe èvicina a1,la\nprobabilità dierrore Bayesiano P*èpiccola ,così come la\nprobabilità dierrore Pdella regola nearest neighbor .Quando\nciascuna classe èquasi ugualmente probabile ,siaBayes cheNN\nhanno untasso dierrore ~(1-1/c),con cnumero diclassi .Nel\nmezzo, iltasso dierrore NNèlimitato daltasso dierrore diBayes :\n!∗≤!≤!∗2−%\n%−1!∗(Eq.52)",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#40": "10prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneRappresentazione grafica\nNormale Multivariata\nPer\n𝑑=2laforma della distribuzione èquella diun’ellisse .\n𝛍\n=𝜇1,𝜇2controlla laposizione delcentro .\n𝜎11e𝜎22determinano l’allungamento suidueassidell’ellisse .\n𝜎12=𝜎21controlla larotazione dell’ellisse rispetto agli assi\ncartesiani .\n•se=0(matrice dicovarianza diagonale ),ladistribuzione\nmultinormale èdefinita come prodotto di𝑑normali\nmonodimensionali .Intalcaso gliassidell’ellisse sono\nparalleli agliassicartesiani (es.Naive Bayes Classifier ).\n•Se >0(come nel caso della figura )𝑥1e𝑥2sono\npositivamente correlate (quando aumenta 𝑥1aumenta\nanche𝑥2).\n•Se<0𝑥1e𝑥2sono negativamente correlate (quando\naumenta 𝑥1cala𝑥2).\nGli\nassidell’ellisse sono paralleli agliautovettori diΣ.Le diverse ellissi \nindividuano \nluoghi di punti a \ndensità costante\n𝑥1𝑥2\nDistribuzione Normale Multivariata (Multinormale)\nN.B. Per quanto l’assunzione chele variabili siano statisticamente indipendenti\n(!12=0) non siavera in generale , iClassificatori Naive Bayes sidimostrano lavorare\nbene sumolti dataset. Per tale motivo , taliclassificatori sono fraipiùpopolari",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#41": "Whitening Transform",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#42": "Metric Learning\n38prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneMetric Learning\nUnapproccio piùgenerale allascelta della metrica dautilizzare in\nunadeterminata applicazione, consiste nellearning supervisionato\ndella metrica stessa daidatideltraining set.\nObiettivo èdeterminare unatrasformazione degli input che:\n«avvicini »pattern della stessa classe\n«allontani »pattern diclassi diverse\nLadistanza euclidea nella spazio originale è:\n𝑑𝑖𝑠𝑡𝑎,𝑏=𝐚−𝐛𝑡𝐚−𝐛=𝐚−𝐛2\nUntipico approccio dimetric learning lineare determina (con\ntraining supervisionato) una matrice 𝐆che trasforma gliinput, e\ncontinuare adapplicare ladistanza euclidea agliinput trasformati\n𝑑𝑖𝑠𝑡𝑎,𝑏=𝐆𝐚−𝐆𝐛2\nVedremo una possibile soluzione diquesto problema nell’ambito\ndella riduzione didimensionalità con LDA (Linear Discriminant\nAnalysys ).\nSono anche possibili approcci nonlineari :\n𝑑𝑖𝑠𝑡𝑎,𝑏=𝐆𝜙𝐚−𝐆𝜙𝐛2\ndove𝜙èunafunzione nonlineare .",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#43": "Metric Learning\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#44": "Similarità /Distanza Coseno\n39prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneSimilarità Coseno e Distanza Coseno\nUna similarità/distanza piuttosto utilizzata inapplicazioni di\ninformation retrieval ,data mining etext mining èla\nsimilarità/distanza coseno .\nGeometricamente, dati due vettori𝐚e𝐛lasimilarità coseno\ncorrisponde alcosenodell’angolo tradiessi:\n𝐶𝑜𝑠𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 𝐚,𝐛=𝐚𝑡∙𝐛\n𝐚∙𝐛\nènoto infatti cheilprodotto scalare traduevettori è:\n𝐚𝑡∙𝐛=𝐚∙𝐛∙𝑐𝑜𝑠𝜃\nDue vettori identici hanno similarità 1eduevettori opposti -1.\nLadistanza coseno èsemplicemente :\n𝐶𝑜𝑠𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝐚,𝐛=1−𝐶𝑜𝑠𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 𝐚,𝐛\nEsempio Confronto ditesti:Untesto può essere codificato daun\nvettore numerico dove ogni dimensione contiene ilnumero di\noccorrenze diuna certa parola rispetto aundato dizionario .La\nsimilarità dicontenuto tradue testi non dipende dal numero\nassoluto diparole madalla frequenza relativa diciascuna diesse .\nLadistanza coseno «sconta» lalunghezza deivettori .\nLadistanza coseno non èuna metrica (es.non rispetta la\ndiseguaglianza triangolare ).Sesièinteressati aunametrica sipuò\npassare alladistanza angolare :\n𝐴𝑛𝑔𝑢𝑙𝑎𝑟𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝐚,𝐛=𝑐𝑜𝑠−1𝐶𝑜𝑠𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 𝐚,𝐛\n𝜋",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#45": "Similarità /Distanza Coseno\n39prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneSimilarità Coseno e Distanza Coseno\nUna similarità/distanza piuttosto utilizzata inapplicazioni di\ninformation retrieval ,data mining etext mining èla\nsimilarità/distanza coseno .\nGeometricamente, dati due vettori𝐚e𝐛lasimilarità coseno\ncorrisponde alcosenodell’angolo tradiessi:\n𝐶𝑜𝑠𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 𝐚,𝐛=𝐚𝑡∙𝐛\n𝐚∙𝐛\nènoto infatti cheilprodotto scalare traduevettori è:\n𝐚𝑡∙𝐛=𝐚∙𝐛∙𝑐𝑜𝑠𝜃\nDue vettori identici hanno similarità 1eduevettori opposti -1.\nLadistanza coseno èsemplicemente :\n𝐶𝑜𝑠𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝐚,𝐛=1−𝐶𝑜𝑠𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 𝐚,𝐛\nEsempio Confronto ditesti:Untesto può essere codificato daun\nvettore numerico dove ogni dimensione contiene ilnumero di\noccorrenze diuna certa parola rispetto aundato dizionario .La\nsimilarità dicontenuto tradue testi non dipende dal numero\nassoluto diparole madalla frequenza relativa diciascuna diesse .\nLadistanza coseno «sconta» lalunghezza deivettori .\nLadistanza coseno non èuna metrica (es.non rispetta la\ndiseguaglianza triangolare ).Sesièinteressati aunametrica sipuò\npassare alladistanza angolare :\n𝐴𝑛𝑔𝑢𝑙𝑎𝑟𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝐚,𝐛=𝑐𝑜𝑠−1𝐶𝑜𝑠𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 𝐚,𝐛\n𝜋",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#46": "Riferimenti\n!S.J. Russell & P. Norvig, Artificial Intelligence: A Modern \nApproach (3 ed.) , Pearson, 2009.\n!K. Fukunaga, Statistical Pattern Recognition , Academic Press, \n1990.\n!D. Maltoni , Machine Learning , Università di Bologna, 2017.\n!C.M. Bishop, Pattern Recognition and Machine Learning , \nSpringer, 2006.\n!K.P. Murphy, Machine Learning: A Probabilistic Perspective , The \nMIT Press, 2012.\n!R.O. Duda , P.E. Hart, and D.G. Stork. 2000. Pattern Classification \n(2nd Edition). Wiley -Interscience , New York, NY, USA. ",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#5": "Esempi NN\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#6": "18prof. Davide Maltoni –Università di Bologna\nML\nClassificazione…continua\nSupponendo di non avere altre informazioni, si possono stimare le \nprobabilità a priori come: 𝑃𝑤1=8/18, 𝑃𝑤2=10/18\n\u000b\f\n\u000b\f\u000b\f\u000b\f0.003321exp\nπ21|11\n1 1 2/1\n12/ 1  »¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010μxμx xt\ndwp\n\u000b\f\n\u000b\f\u000b\f\u000b\f 0045.021exp\nπ21|21\n2 2 2/1\n22/2  »¼º\n«¬ª\u0010¦\u0010\u0010\n¦ \u0010μxμx xt\ndw p\nPesoAltezza\n>@T168 ,57 x\n\u000b\f\u000b\f\u000b\f 0040.0 w w|is\n1ii   ¦\n P p p x x\n\u000b\f\u000b\f\u000b\f\n\u000b\f36.0w w||w1 1\n1 # xxxpP pP\n\u000b\f\u000b\f\u000b\f\n\u000b\f64.0w w||w2 2\n2 # xxxpP pPEsempi Bayes",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#7": "Esempi NN\n27prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneEsempi NN\nNell’esempio visto in precedenza, \n la regola NN assegna il \npattern 𝐱alla classe 𝑤1(maschi -blu)\nLa figura seguente mostra il partizionamento dello spazio \noperato dalla regola NN su un training set con 5 classi:\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#8": "Esempi Bayes\n19prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneBayes e confidenza di classificazione\nUngrande vantaggio delclassificatore diBayes ,rispetto adaltri\nclassificatori, èlegato alfatto cheesso produce unvalore dioutput\nprobabilistico (unvero eproprio valore diprobabilità tra0e1,con\nsomma 1sulle diverse classi )che può essere utilizzato come\nconfidenza (visualizzata nella figura come sfumatura colore ):\nInfatti, unclassificatore puòassegnare unpattern𝐱aunaclasse\n𝑤𝑖condiversi livelli dicertezza (oconfidenza) chepossono essere\nimpiegati per:\nscartare pattern in applicazioni open\n -setcon soglia\ncostruire \n un multi -classificatore\nSe non si è interessati alla confidenza, nella formula di Bayes non \nè necessario dividere per 𝑝𝐱il numeratore, e la regola di Bayes è \nsemplicemente:\n𝑏=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑖=1..𝑠𝑝𝐱𝑤𝑖∙𝑃𝑤𝑖\n",
    "data_test\\rootfolder\\università\\MachineLearning\\32-CBNN-sbloccato.pdf#9": "Da NN a k-NN\n",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Classiﬁcatore Bayesiano (Ex 13)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#1": "Sommario\n...",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#10": "Naive Bayes classiﬁer: step 2\nPer ogni dataset ricaviamo 2 statistiche: media e deviazione standard. \nLa media può essere ricavata così: \n μ\n = sum(x)/n * count(x) \n    \ndove x è la lista dei valori (o colonna) sui cui stiamo stimando la media.  \n# Calculate the mean of a list of numbers\ndef mean\n (\nnumbers\n)\n:\nreturn \nsum\n(\nnumbers\n)\n/\nfloat\n(\nlen\n(\nnumbers\n))\nPer la deviazione standard \n σ\n si ha: \n sqrt( \nΣ\ni\n(x\ni\n – \nμ\n(x))\n2\n / N-1)  \nfrom math import \n sqrt\n \n# Calculate the standard deviation of a list of numbers\ndef stdev\n (\nnumbers\n)\n:\navg\n = \nmean\n(\nnumbers\n)\nvariance\n  = \nsum\n([(\nx\n-\navg\n)\n**\n2 \nfor \nx \nin \nnumbers\n])\n / \nfloat\n(\nlen\n(\nnumbers\n)\n-\n1\n)\nreturn \nsqrt\n(\nvariance\n )\nMedia e deviazione standard devono essere calcolate per ogni feature e \nconsiderando tutte le istanze.\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#11": "Naive Bayes classiﬁer: step 2\nMedia e deviazione standard devono essere calcolate per ogni feature e considerando \ntutte le istanze. \nLa funzione \n zip(*...)\n  separa le colonne del dataset e restituisce una tupla per ogni \ncolonna contenente i relativi valori delle features. \n def summarize_dataset\n (\ndataset\n)\n:\nsummaries\n =\n[(\nmean\n(\ncolumn\n),\nstdev\n(\ncolumn\n),\nlen\n(\ncolumn\n)) \nfor \ncolumn \nin \nzip\n(\n*\ndataset\n)]\ndel\n(\nsummaries\n [\n-\n1\n])\nreturn \nsummaries\nAd esempio: \ndataset\n = \n[[\n3.393533211\n ,\n2.331273381\n ,\n0\n],\n[\n3.110073483\n ,\n1.781539638\n ,\n0\n],\n[\n1.343808831\n ,\n3.368360954\n ,\n0\n],\n[\n3.582294042\n ,\n4.67917911\n ,\n0\n],\n[\n2.280362439\n ,\n2.866990263\n ,\n0\n],\n[\n7.423436942\n ,\n4.696522875\n ,\n1\n],\n[\n5.745051997\n ,\n3.533989803\n ,\n1\n],\n[\n9.172168622\n ,\n2.511101045\n ,\n1\n],\n[\n7.792783481\n ,\n3.424088941\n ,\n1\n],\n[\n7.939820817\n ,\n0.791637231\n ,\n1\n]]\nsummary\n = \nsummarize_dataset\n (\ndataset\n)\nprint\n(\nsummary\n)\n> [(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#12": "Naive Bayes classiﬁer: step 3\nVogliamo ricavare le statistiche per ogni classe (o label). Sfruttiamo la funzione \nseparate_by_class()\n  deﬁnita in precedenza:  \ndef summarize_by_class\n (\ndataset\n)\n:\nseparated\n  = \nseparate_by_class\n (\ndataset\n)\nsummaries\n  = \ndict\n()\nfor \nclass_value\n , \nrows \nin \nseparated\n .\nitems\n()\n:\nsummaries\n [\nclass_value\n ]\n = \nsummarize_dataset\n (\nrows\n)\nreturn \nsummaries\nAd esempio:  \ndataset\n = \n[[\n3.393533211\n ,\n2.331273381\n ,\n0\n],\n[\n3.110073483\n ,\n1.781539638\n ,\n0\n],\n[\n1.343808831\n ,\n3.368360954\n ,\n0\n],\n[\n3.582294042\n ,\n4.67917911\n ,\n0\n],\n[\n2.280362439\n ,\n2.866990263\n ,\n0\n],\n[\n7.423436942\n ,\n4.696522875\n ,\n1\n],\n[\n5.745051997\n ,\n3.533989803\n ,\n1\n],\n[\n9.172168622\n ,\n2.511101045\n ,\n1\n],\n[\n7.792783481\n ,\n3.424088941\n ,\n1\n],\n[\n7.939820817\n ,\n0.791637231\n ,\n1\n]]\nseparated\n  = \nseparate_by_class\n (\ndataset\n)\nfor \nlabel \nin \nseparated\n :\nprint\n(\nlabel\n)\nfor \nrow \nin \nseparated\n [\nlabel\n]\n:\nprint\n(\nrow\n)\n13\n>>>\n0\n(2.7420144012, 0.9265683289298018, 5)\n(3.0054686692, 1.1073295894898725, 5)\n1\n(7.6146523718, 1.2344321550313704, 5)\n(2.9914679790000003, 1.4541931384601618, 5)",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#13": "Naive Bayes classiﬁer: step 4\nAssumiamo che la probabilità che un certo valore \n x\n osservato sia funzione \nda una distribuzione gaussiana, descritta interamente dai due valori: media \ne deviazione standard.  \nLa funzione di densità di probabilità sarà così ricavata (vedi lezione; la y \ncorrisponde alla media): \nf(x) = (1 / sqrt(2 * PI) * Σ) * exp(-((x-\n μ\n)^2 / (2 * Σ^2)))\nDove \n Σ\n è la matrice di covarianza (con d =1 coincide con la varianza). \nEsercizio\n : deﬁnire la funzione \n calculate_probability(x, mean, stdev) \n per il \ncalcolo della densità di probabilità.\n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#14": "Naive Bayes classiﬁer: step 4\nEsercizio\n : deﬁnire la funzione calculate_probability(x, mean, stdev) per il \ncalcolo della densità di probabilità. \nfrom math import sqrt\nfrom math import pi\nfrom math import \n exp\ndef calculate_probability\n (\nx\n, \nmean\n, \nstdev\n)\n:\nexponent\n  = \nexp\n(\n-\n((\nx\n-\nmean\n)\n**\n2\n / \n(\n2\n * \nstdev*\n*\n2 \n)))\nreturn \n(\n1\n / \n(\nsqrt\n(\n2\n * \npi\n)\n * \nstdev\n))\n * \nexponent\nprint\n(\ncalculate_probability\n (\n1.0\n, \n1.0\n, \n1.0\n))\nprint\n(\ncalculate_probability\n (\n2.0\n, \n1.0\n, \n1.0\n))\nprint\n(\ncalculate_probability\n (\n0.0\n, \n1.0\n, \n1.0\n))\n> \n0.3989422804014327\n> \n0.24197072451914337\n> \n0.24197072451914337\nNotare come per x=1, e media e varianza pari a 1, l'apice della campana \nassume valore 0.39. Per x=2 e x=0, e medesime statistiche, il valore è 0.24.\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#15": "Naive Bayes classiﬁer: step 5\nOra impieghiamo le statistiche ricavate dal training data per nuovi dati. La \nstima delle probabilità viene stimata per ogni classe. \nP(class|data) = P(X|class) * P(class) \nAttenzione: Avendo eliminato la frazione, il risultato non è strettamente \nuna probabilità.  \nVogliamo massimizzare tale valore, ovvero prendere la classe con valore di \nprobabilità massimo. \nL'approccio naive implica l'indipendenza, es: \nP(class=0|X1,X2) = P(X1|class=0) * P(X2|class=0) * P(class=0)\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#16": "Naive Bayes classiﬁer: step 5\nEsercizio\n : deﬁnire \n calculate_class_probabilities()\n  che prende in input le \nstatistiche restituite da \n summarize_by_class()\n  e valuta la probabilità per una \ncerta istanza data sempre in input. \nEsempio: \n# Test calculating class probabilities\ndataset\n = \n[[\n3.393533211\n ,\n2.331273381\n ,\n0\n],\n[\n3.110073483\n ,\n1.781539638\n ,\n0\n],\n[\n1.343808831\n ,\n3.368360954\n ,\n0\n],\n[\n3.582294042\n ,\n4.67917911\n ,\n0\n],\n[\n2.280362439\n ,\n2.866990263\n ,\n0\n],\n[\n7.423436942\n ,\n4.696522875\n ,\n1\n],\n[\n5.745051997\n ,\n3.533989803\n ,\n1\n],\n[\n9.172168622\n ,\n2.511101045\n ,\n1\n],\n[\n7.792783481\n ,\n3.424088941\n ,\n1\n],\n[\n7.939820817\n ,\n0.791637231\n ,\n1\n]]\nsummaries\n  = \nsummarize_by_class\n (\ndataset\n)\nprobabilities\n  = \ncalculate_class_probabilities\n (\nsummaries\n , \ndataset\n[\n0\n])\nprint\n(\nprobabilities\n )\n> {0: 0.05032427673372075, 1: 0.00011557718379945765}\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#17": "Naive Bayes classiﬁer: step 5\nEsercizio\n : deﬁnire \n calculate_class_probabilities()\n  che prende in input le \nstatistiche restituite da \n summarize_by_class()\n  e valuta la probabilità per una \ncerta istanza data sempre in input. \nCalcola il numero totale di istanze a partire dalle statistiche passate \ncome parametro. \nValuta il valore P(class) come frazione tra il numero di istanze per una \nclasse e il numero di istanze nel dataset \nStima la probabilità per ogni valore in input impiegando la funzione \ndensità di probabilità, e le statistiche per ogni colonna associata ad una \ncerta classe. Le probabilità saranno moltiplicate se associate alla stessa \nclasse. \nIl processo sarà ripetuto per ogni classe nel dataset. \nRestituire un dizionario classe->probabilità\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#18": "Naive Bayes classiﬁer: step 5\nEsercizio\n : deﬁnire \n calculate_class_probabilities()\n  che prende in input le \nstatistiche restituite da \n summarize_by_class()\n  e valuta la probabilità per una \ncerta istanza data sempre in input. \ndef calculate_class_probabilities\n (\nsummaries\n , \nrow\n)\n:\n \n# numero totale di istanze di training\n \ntotal_rows\n  = \nsum\n([\nsummaries\n [\nlabel\n][\n0\n][\n2\n] \nfor \nlabel \nin \nsummaries\n ])\n \n# output\nprobabilities\n  = \ndict\n()\n \n# per ogni chiave (classe) e valore (istanze di quella classe)\nfor \nclass_value\n , \nclass_summaries \n in \nsummaries\n .\nitems\n()\n:\n   \n# probabilità calcolata in base alle frequenze\nprobabilities\n [\nclass_value\n ]\n = \nsummaries\n [\nclass_value\n ][\n0\n][\n2\n]\n/\nfloat\n(\ntotal_rows\n )\n   \n# per ogni istanza in summaries associata ad una classe\nfor \ni \nin \nrange\n(\nlen\n(\nclass_summaries\n ))\n:\n      \n# ricava le statistiche di quella classe\nmean\n, \nstdev\n, \ncount\n = \nclass_summaries\n [\ni\n]\n      \n# aggiorna la probabilità per quella classe\nprobabilities\n [\nclass_value\n ]\n *= \ncalculate_probability\n (\nrow\n[\ni\n], \nmean\n, \nstdev\n)\nreturn \nprobabilities\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#19": "Naive Bayes classiﬁer: esercitazione\nConsiderare il dataset Kaggle Adult income dataset:  \nhttps://www.kaggle.com/datasets/wenruliu/adult-income-dataset  \nhttp://www.cs.toronto.edu/~delve/data/adult/adultDetail.html   \nContiene 16 colonne: \nTarget ﬁled: Income  \n-- The income is divide into two classes: <=50K and >50K   \nNumber of attributes: 14  \n-- These are the demographics and other features to describe a person \nAnalizza il dataset passo passo seguendo le considerazioni su: \nhttps://www.kaggle.com/code/prashant111/naive-bayes-classiﬁer-in-python/notebook  \nApplica l'algoritmo Naive Bayes classiﬁer per i suddetto dataset.  \nNota\n : alcuni attributi potrebbero dover essere normalizzati oppure convertiti in valori \nnumerici.\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#2": "Scikit-learn: Classiﬁcatori Naive Bayes\nUn approccio di classiﬁcazione molto veloce nell'addestramento, che non \nrichiede che il training set sia caricato interamente in memoria, anche se a \nvolte mostrano performance peggiori rispetto agli approcci lineare (es. \nLogisticRegression e LinearSVC). \nNaive\n  perché basato sull'assunzione che le feature siano indipendenti dal \npunto di vista statistico, spesso inesatta. \nEs. un problema cardiovascolare può dipendere dal colesterolo, peso, livelli di \ndiabete, etc; se presenti contemporaneamente possono aumentarne il rischio, ma \nl'approccio naive le valuta singolarmente. \nSi ricavano i parametri del modello analizzando le features singolarmente, e \ncollezionando statistiche per ogni feature per ogni classe. \nRicavare la classe più verosimile (con più alta probabilità \n a posteriori\n ) si \nottiene mediante il \n Teorema di Bayes\n . \nL'approccio naive (indipendenza tra features) ci porta a non interpretare la probabilità \nin output poiché risulta essere una approssimazione troppo grossolana rispetto a \nquella reale. \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#20": "Naive Bayes classiﬁer: esercitazione\nAlcune funzioni di supporto: \n# Load a CSV file\ndef \nload_csv\n (\nfilename\n ):\n  dataset = \n list\n()\n  \nwith \nopen\n(filename, \n 'r'\n) \nas \nfile\n:\n    csv_reader = reader(\n file\n)\n    \nfor\n row \nin\n csv_reader:\n      \nif \nnot\n row:\n        \n continue\n      dataset.append(row)\n  \nreturn\n dataset\n# Convert string column to float\ndef \nstr_column_to_float\n (\ndataset\n, \ncolumn\n):\n  \nfor\n row \nin\n dataset:\n    row[column] = \n float\n(row[column].strip())\n# Convert string column to integer\ndef \nstr_column_to_int\n (\ndataset\n, \ncolumn\n):\n  class_values = [row[column] \n for\n row \nin\n dataset]\n  unique = \n set\n(class_values)\n  lookup = \n dict\n()\n  \nfor\n i, value \n in \nenumerate\n (unique):\n    lookup[value] = i\n  \nfor\n row \nin\n dataset:\n    row[column] = lookup[row[column]]\n  \nreturn\n lookup\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#21": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017 \nTutorial \n https://machinelearningmastery.com/naive-bayes-classiﬁer-scratch-\npython/  \nDataset: \nhttps://www.kaggle.com/datasets/wenruliu/adult-income-dataset  \nhttp://www.cs.toronto.edu/~delve/data/adult/adultDetail.html  \nTesti di Riferimento\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#3": "Classiﬁcatori Naive Bayes: pregi e difetti\nSemplice implementazione (basata sulle occorrenze) \nPuò funzionare anche su dataset piccoli \nÈ veloce e richiede poca memoria \nGestiste il caso di valori mancanti nei dati  \nPoco sensibile a dati rumorosi \nL'assunzione dell'indipendenza statistica è raramente soddisfatta; il modello non \nconsidera le dipendenze tra features \nI dati nel continuo devono essere spesso rielaborati (es. binning) \nNon raggiunge prestazioni ottimali rispetto ad altri approcci \nNon supporta l'\n online learning\n : occorre riaddestrare il modello in presenza di \nnuovi dati. \nNon funziona correttamente se i dati nel test set non sono presenti nel training.\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#4": "Scikit-learn: Classiﬁcatori Naive Bayes\nCi sono vari classiﬁcatori implementati in Scikit-learn: \nGaussianNB: adatto a dati nel continuo \nCategoricalNB: features discrete distribuite su categorie predeﬁnite \nBernoulliNB: assume dati binari \nMultinomialNB: assume feature che accumulano valori (es. frequenza) \nComplementNB: variazione del Multinomial per correggere alcune \nassunzioni sui dati. \nBernoulliNB e MultinomialNB sono spesso usati per dati testuali. \nPer dataset di training molto grandi e sparsi si può usare il parametro \n partial_ﬁt\n  che \nriduce la richiesta di memoria. \nÈ una valida alternativa a \n logistic regression\n  e \ndecision trees\n .\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#5": "Scikit-learn: BernoulliNB\nConteggia quante volte una feature non è pari 0 per ogni classe. \nAd esempi, 4 istanze con 4 feature binarie ciascuna. La 1a e 3a istanza \nhanno classe '0', mentre la 2a e 4a hanno classe '1'. \nX \n= \nnp\n.\narray\n([[\n0\n, \n1\n, \n0\n, \n1\n],\n[\n1\n, \n0\n, \n1\n, \n1\n],\n[\n0\n, \n0\n, \n0\n, \n1\n],\n[\n1\n, \n0\n, \n1\n, \n0\n]])\ny \n= \nnp\n.\narray\n([\n0\n, \n1\n, \n0\n, \n1\n])\nEffettuando il conteggio per entrambe le classi si ha: \ncounts \n= \n{}\nfor \nlabel \nin \nnp\n.\nunique\n(\ny\n):\n# iterate over each class\n# count (sum) entries of 1 per feature\ncounts\n[\nlabel\n] \n= \nX\n[\ny \n== \nlabel\n]\n.\nsum\n(\naxis\n=\n0\n)\nprint\n(\n\"Feature counts:\\n{}\"\n .\nformat\n(\ncounts\n))\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#6": "Scikit-learn: MultinomialNB e GaussianNB\n ,\nMultinomialNB\n  tiene conto del valor medio per ogni feature per ogni \nclasse. \n GaussianNB\n  ricava valor medio e varianza. \nLa predizione su una istanza è ricavata valutando tutte le classi e \nscegliendo quella ottimale. \nMultinomialNB e BernoulliNB hanno un singolo parametro \n alpha\n , che \ndetermina la complessità del modello. Ai dati sono aggiunti \n alpha\n  istanze \nvirtuali che hanno valori positivi per tutte le features. Questo genera uno \n\"smoothing\" sulle statistiche calcolate.  \nValori elevati di \n alpha\n  creano smoothing elevati e modelli meno \ncomplessi.  \nGaussianNB\n  è più adatto a dataset con molte features. \n MultinomialNB\n  è \nmigliore rispetto a \n BernoulliNB\n  con dataset con un numero elevato di \nfeatures diverse da 0 (es. grandi documenti testuali).\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#7": "Naive Bayes classiﬁer da zero\nProviamo a fare l'implementazione del classiﬁcatore \nStep 1: Separate By Class.  \nStep 2: Summarize Dataset.  \nStep 3: Summarize Data By Class.  \nStep 4: Gaussian Probability Density Function.  \nStep 5: Class Probabilities \nImmaginiamo di impiegare il dataset \n Iris\n: \nlunghezza e larghezza sepalo (reali) \nlunghezza e larghezza petalo (reali) \nclasse di appartenenza = {Iris-setosa, Iris-versicolor, Iris-virginica}\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#8": "Naive Bayes classiﬁer: step 1\nCalcoliamo la probabilità di appartenenza di una istanza ad una certa \nclasse. \nSepariamo i dati in ingresso in base alla classe di appartenenza.  \n# Split the dataset by class values\n# Restituisce un dizionario classe -> lista di istanze\n# Funziona per ogni dataset il cui ultimo valore è la classe di appartenenza\ndef separate_by_class\n (\ndataset\n)\n:\nseparated\n  = \ndict\n()\nfor \ni \nin \nrange\n(\nlen\n(\ndataset\n))\n:\nvector\n = \ndataset\n[\ni\n]\nclass_value\n  = \nvector\n[\n-\n1\n]   # ultimo valore\nif \n(\nclass_value \n not \nin \nseparated\n )\n:\nseparated\n [\nclass_value\n ]\n = \nlist\n()\nseparated\n [\nclass_value\n ].\nappend\n(\nvector\n) \nreturn \nseparated\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\33-Ex_13 Esercitazione su Classificatore Bayesiano-sbloccato.pdf#9": "Naive Bayes classiﬁer: step 1\n# Iris dataset\ndataset\n = \n[[\n3.393533211\n ,\n2.331273381\n ,\n0\n],\n[\n3.110073483\n ,\n1.781539638\n ,\n0\n],\n[\n1.343808831\n ,\n3.368360954\n ,\n0\n],\n[\n3.582294042\n ,\n4.67917911\n ,\n0\n],\n[\n2.280362439\n ,\n2.866990263\n ,\n0\n],\n[\n7.423436942\n ,\n4.696522875\n ,\n1\n],\n[\n5.745051997\n ,\n3.533989803\n ,\n1\n],\n[\n9.172168622\n ,\n2.511101045\n ,\n1\n],\n[\n7.792783481\n ,\n3.424088941\n ,\n1\n],\n[\n7.939820817\n ,\n0.791637231\n ,\n1\n]]\nseparated\n  = \nseparate_by_class\n (\ndataset\n)\nfor \nlabel \nin \nseparated\n :\nprint\n(\nlabel\n)\nfor \nrow \nin \nseparated\n [\nlabel\n]\n:\nprint\n(\nrow\n)\n0\n[3.393533211, 2.331273381, 0]\n[3.110073483, 1.781539638, 0]\n[1.343808831, 3.368360954, 0]\n[3.582294042, 4.67917911, 0]\n[2.280362439, 2.866990263, 0]\n1\n[7.423436942, 4.696522875, 1]\n[5.745051997, 3.533989803, 1]\n[9.172168622, 2.511101045, 1]\n[7.792783481, 3.424088941, 1]\n[7.939820817, 0.791637231, 1]\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#0": "Machine Learning\nUniversità Roma Tre  \nDipartimento di Ingegneria \nAnno Accademico 2021 -2022\nSupport Vector Machine (SVM)",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#1": "Sommario\n!SVM Lineari: Pattern Linearmente Separabili e Non\n!SVM Non Lineari\n!SVM Multiclasse",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#10": "SVM50 CHAPTER 5. LINEAR DISCRIMINANT FUNCTIONS\ntransformation ϕ() that well separates the data — so the expected number of support\nvectors is small — then Eq. 107 shows that the expected error rate will be lower.\ny1y2\nR1\nR2\noptimal hyperplanemaximummargin b\nmaximummargin b\nFigure 5.19: Training a Support Vector Machine consists of ﬁnding the optimal hy-\nperplane, i.e., the one with the maximum distance from the nearest training patterns.\nThe support vectors are those (nearest) patterns, a distance bfrom the hyperplane.\nThe three support vectors are shown in solid dots.\n5.11.1 SVM training\nWe now turn to the problem of training an SVM. The ﬁrst step is, of course, to choose\nthe nonlinear ϕ-functions that map the input to a higher dimensional space. Often\nthis choice will be informed by the designer’s knowledge of the problem domain. In\nthe absense of such information, one might choose to use polynomials, Gaussians or\nyet other basis functions. The dimensionality of the mapped space can be arbitrarily\nhigh (though in practice it may be limited by computational resources).\nWe begin by recasting the problem of minimizing the magnitude of the weight\nvector constrained by the separation into an unconstrained problem by the method\nof Lagrange undetermined multipliers. Thus from Eq. 106 and our goal of minimizing\n||a||, we construct the functional\nL(a,α)=1\n2||a||2−n/summationdisplay\nk=1αk[zkatyk−1]. (108)\nand seek to minimize L() with respect to the weight vector a, and maximize it with\nrespect to the undetermined multipliers αk≥0. The last term in Eq. 108 expresses\nthe goal of classifying the points correctly. It can be shown using the so-called Kuhn-\nTucker construction (Problem 30) (also associated with Karush whose 1939 thesis\naddressed the same problem) that this optimization can be reformulated as maximiz-\ning\nL(α)=n/summationdisplay\nk=1αi−1\n2n/summationdisplay\nk,jαkαjzkzjyt\njyk, (109)\nsubject to the constraintsVedi Duda et al., Pattern Classification , 2000, pg. 262",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#11": "Sommario\n!SVM Lineari: Pattern Linearmente Separabili e Non\n!SVM Non Lineari\n!SVM Multiclasse",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#12": "SVM Lineari: Pattern Separabili\n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  ℜd: spazio vettoriale di d\ndimensioni ( d=3 in figura)\nxi: vettore di d componenti \nrelativo al pattern i-esimo \ndel TS\nyi: etichetta relativa al \npattern i-esimo del TS\nw: vettore che indica la \ndirezione ortogonale a tutti \ni vettori dell’iperpiano H\nb : coefficiente del termine \nnoto che compare nell’eq. \ndel iperpiano H",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#13": "Qualche Richiamo\n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n!Iperpiano : sottospazio inferiore di una dimensione allo spazio in cui è \ndefinito (e.g., nello spazio 3D gli iperpiani sono i piani)\n!Equazione cartesiana di un piano:\nIl luogo delle soluzioni (x,y,z) che verificano l’equazione è il luogo dei \npunti P = (x,y,z) che appartengono al piano\n!L’equazione del piano specifica due elementi\n!la terna (w1,w2,w3) dei coefficienti detti parametri direttori del piano\nche individua la direzione ortogonale a tutti i vettori del piano\n!il coefficiente del termine noto b\n!In sintesi, per individuare univocamente un piano nello spazio è \nsufficiente disporre della direzione ortogonale al piano we del \ncoefficiente bw1x+w2y+w3z+b=0,     con w1,w2,w3, b ∈ ℜ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#14": "Qualche Richiamo\n!Sia      uno spazio vettoriale di dimensione n sul campo    . \nIl prodotto scalare fra due vettori di      è un’operazione che \ngeneralmente si indica con il simbolo “ •” ed è definita come segue:\novvero associa ad una coppia di vettori x=(x 1,x2,...,x n)e y=(y 1,y2,...,y n) \nun numero reale così definito \nx∙y = <x,y> = x1y1+x 2y2,..., +x nyn\n!Alle volte il prodotto scalare è definito anche come\nx∙y = = < x,y> = xty\ndove xtyè il prodotto riga per colonna tra il vettore trasposto xte il \nvettore y\n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \nℜn\nℜn\n•: ℜn×ℜn→ℜℜ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#15": "Qualche Richiamo\n!La norma di un vettore                                    è un’applicazione che \nad un vettore associa un numero reale\nEssa è pari alla radice quadrata della somma del quadrato delle \ncomponenti del vettore o, equivalentemente, alla radice quadrata del \nprodotto scalare del vettore con se stesso\n!Fra le proprietà di cui gode la norma vi è quella di omogeneità : x=x1,x2,...,xn ( )∈ ℜn\n•: ℜn→ℜ\nx=x12+x22+...+xn2=x•x\nper ogni x∈ ℜn e per ogni λ∈ ℜ si ha\nλx=λx\n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#16": "Qualche Richiamo\n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n!Dato un piano P di equazione\nla sua distanza dall’origine degli assi è pari a\n!\n\"!\"+\"\"\"+\"#\"=!\n%\n!Si può dimostrare che la distanza !di un punto \"da un piano P è pari a\n&=%'(+!\n\"!\"+\"\"\"+\"#\"=%'(+!\n%=)(+)\n%\nmentre se il punto \"appartiene al piano, cioè se \"∈P, allora la \ndistanza !é per definizione zero w1x+w2y+w3z+b=0,     con w1,w2,w3, b ∈ ℜ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#17": "SVM Lineari: Pattern Separabili\n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \nIn altri termini, D(x) è la funzione distanza dall’iperpiano, cioè indica \nquanto il pattern xè distante dalla superficie decisionale ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#18": "SVM Lineari: Pattern Separabili\n182 4. LINEAR MODELS FOR CLASSIFICATION\nFigure 4.1 Illustration of the geometry of a\nlinear discriminant function in two dimensions.\nThe decision surface, shown in red, is perpen-\ndicular to w, and its displacement from the\norigin is controlled by the bias parameter w0.\nAlso, the signed orthogonal distance of a gen-\neral point xfrom the decision surface is given\nbyy(x)/∥w∥.x2\nx1wx\ny(x)\n∥w∥\nx⊥\n−w0\n∥w∥y=0\ny<0y>0\nR2R1\nan arbitrary point xand let x⊥be its orthogonal projection onto the decision surface,\nso that\nx=x⊥+rw\n∥w∥. (4.6)\nMultiplying both sides of this result by wTand adding w0, and making use of y(x)=\nwTx+w0andy(x⊥)=wTx⊥+w0=0, we have\nr=y(x)\n∥w∥. (4.7)\nThis result is illustrated in Figure 4.1.\nAs with the linear regression models in Chapter 3, it is sometimes convenient\nto use a more compact notation in which we introduce an additional dummy ‘input’\nvalue x0=1and then deﬁne /tildewidew=(w0,w)and/tildewidex=(x0,x)so that\ny(x)=/tildewidewT/tildewidex. (4.8)\nIn this case, the decision surfaces are D-dimensional hyperplanes passing through\nthe origin of the D+1-dimensional expanded input space.\n4.1.2 Multiple classes\nNow consider the extension of linear discriminants to K> 2classes. We might\nbe tempted be to build a K-class discriminant by combining a number of two-class\ndiscriminant functions. However, this leads to some serious difﬁculties (Duda and\nHart, 1973) as we now show.\nConsider the use of K−1classiﬁers each of which solves a two-class problem of\nseparating points in a particular class Ckfrom points not in that class. This is known\nas a one-versus-the-rest classiﬁer. The left-hand example in Figure 4.2 shows an4.1. Discriminant Functions 181\n(McCullagh and Nelder, 1989). Note, however, that in contrast to the models used\nfor regression, they are no longer linear in the parameters due to the presence of the\nnonlinear function f(·). This will lead to more complex analytical and computa-\ntional properties than for linear regression models. Nevertheless, these models are\nstill relatively simple compared to the more general nonlinear models that will be\nstudied in subsequent chapters.\nThe algorithms discussed in this chapter will be equally applicable if we ﬁrst\nmake a ﬁxed nonlinear transformation of the input variables using a vector of basis\nfunctions φ(x)as we did for regression models in Chapter 3. We begin by consider-\ning classiﬁcation directly in the original input space x, while in Section 4.3 we shall\nﬁnd it convenient to switch to a notation involving basis functions for consistency\nwith later chapters.\n4.1. Discriminant Functions\nA discriminant is a function that takes an input vector xand assigns it to one of K\nclasses, denoted Ck. In this chapter, we shall restrict attention to linear discriminants ,\nnamely those for which the decision surfaces are hyperplanes. To simplify the dis-\ncussion, we consider ﬁrst the case of two classes and then investigate the extension\ntoK>2classes.\n4.1.1 Two classes\nThe simplest representation of a linear discriminant function is obtained by tak-\ning a linear function of the input vector so that\ny(x)=wTx+w0 (4.4)\nwhere wis called a weight vector , andw0is abias (not to be confused with bias in\nthe statistical sense). The negative of the bias is sometimes called a threshold .A n\ninput vector xis assigned to class C1ify(x)/greaterorequalslant0and to class C2otherwise. The cor-\nresponding decision boundary is therefore deﬁned by the relation y(x)=0 , which\ncorresponds to a (D−1)-dimensional hyperplane within the D-dimensional input\nspace. Consider two points xAandxBboth of which lie on the decision surface.\nBecause y(xA)=y(xB)=0 ,w eh a v e wT(xA−xB)=0 and hence the vector wis\northogonal to every vector lying within the decision surface, and so wdetermines the\norientation of the decision surface. Similarly, if xis a point on the decision surface,\ntheny(x)=0 , and so the normal distance from the origin to the decision surface is\ngiven by\nwTx\n∥w∥=−w0\n∥w∥. (4.5)\nWe therefore see that the bias parameter w0determines the location of the decision\nsurface. These properties are illustrated for the case of D=2in Figure 4.1.\nFurthermore, we note that the value of y(x)gives a signed measure of the per-\npendicular distance rof the point xfrom the decision surface. To see this, consider\nVedi Bishop, Pattern Recognition and Machine Learning , 2006, pg. 182In questo caso la notazione è \ny(x) = wTx + w0  = D(x) = w · x + b\ncioè\nwTx =w · x = <w,x>\nw0 = b",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#19": "SVM Lineari: Pattern Separabili\n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n4 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili  \nDate  due classi  di pattern  (linearmente  separabili ), e un training  set \nTS contenente  𝑛 campioni  𝐱1,𝑦1 …𝐱𝑛,𝑦𝑛 , dove  𝐱𝑖∈𝑑 sono  i \npattern  multidimensionali  e 𝑦𝑖∈+1,−1 le etichette  delle  due classi,  \nesistono  diversi  iperpiani  in grado  di eseguire  la separazione  voluta . \nUn generico  iperpiano  è definito  dai parametri  (𝐰,𝑏): \n \n \n \n \n \n \n \n \n \n \nLa distanza  di un vettore  𝐱 dall’iperpiano  vale pertanto : 𝑟=𝐷𝐱\n𝐰 \nGli iperpiani  (𝐰,𝑏) che separano  i pattern  del TS, con distanza  \nminima  1/𝐰 su ogni lato, soddisfano,  per 𝑖=1…𝑛, le equazioni : \n𝐰∙𝐱𝒊+𝑏≥+1     𝑠𝑒    𝑦𝑖=+1 \n𝐰∙𝐱𝒊+𝑏≤−1     𝑠𝑒    𝑦𝑖=−1 \no in modo  più compatto : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1     𝑝𝑒𝑟  𝑖=1…𝑛 \n \n \n \n \nIperpiano  \n𝐷𝐱=𝐰∙𝐱+𝑏 \n𝐰: vettore normale all’iperpiano  \n𝑏/𝐰: distanza dall’origine  \n𝐷𝐱=0: luogo dei vettori sul piano  \n𝐱=𝐱𝑝+𝑟𝐰\n𝐰,𝐷𝐱𝑝=𝟎 \n \n \n𝐷𝐱=𝐰∙𝐱+𝑏=𝑟∙𝐰 \n \nper semplicità \nomettiamo il trasposto \nnel prodotto scalare  \n5 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (2)  \nLa minima  distanza  tra l’iperpiano  di separazione  e un pattern  del \ntraining  set è detta  margine  (W). \n \n \n \n \n \n \n \n \nLa distanza  dei punti  che giacciono  sull’iperpiano  𝐷𝐱=+1 \ndall’iperpiano  di separazione  (𝐷𝐱=0) è 1/𝐰; lo stesso  vale per i \npunti  sull’iperpiano  𝐷𝐱=−1. \nPertanto  il margine  è  W= 2/𝐰.  \nL’iperpiano  ottimo  secondo  SVM  è quello  soddisfa  i vincoli  di \nseparazione  dei pattern  e massimizza  il margine  W (o \nalternativamente  minimizza  il suo inverso) : \nMinimizza : 𝐰2/2 \nVincoli :  𝑦𝑖𝐰∙𝐱𝒊+𝑏−1≥0     𝑝𝑒𝑟  𝑖=1…𝑛 \nI pattern  del training  set che giacciono  sul margine  (cerchi  pieni  in \nfigura)  sono  detti support  vector . Tali pattern,  che costituiscono  i casi \npiù complessi,  definiscono  completamente  la soluzione  del \nproblema,  che può essere  espressa  come  funzione  di solo tali \npattern , indipendentemente  dalla  dimensionalità  dello  spazio  𝑑 e dal \nnumero  𝑛 di elementi  in TS.    \n 𝐷𝐱=+1 1/𝐰 \n𝐷𝐱=0 \n𝐷𝐱=−1 𝐷𝐱>+1 \n𝐷𝐱<−1 1/𝐰 Vincoli da soddisfare",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#2": "Dilemma\nReti Neurali a un solo strato :\nPro: algoritmo di apprendimento semplice ed efficiente\nCons : potere espressivo limitato (i.e., possono apprendere solo \n“confini” decisionali lineari nello spazio di input)\nReti Neurali multistrato :\nPro: potere espressivo elevato (i.e., possono rappresentare funzioni        \ngeneriche non lineari)\nCons : algoritmo di apprendimento complicato (a causa della \nabbondanza di minimi locali e dell’alto numero di dimensioni  \ndello spazio dei pesi)\n?:\nPro: potere espressivo elevato\nPro: algoritmo di apprendimento efficiente",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#20": "SVM Lineari: Pattern Separabili\n5prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneSVM lineari: Pattern Separabili (2)\nLaminima distanza tral’iperpiano diseparazione eunpattern del\ntraining setèdetta margine (W).\nLadistanza dei punti che giacciono sull’iperpiano 𝐷𝐱=+1\ndall’iperpiano diseparazione (𝐷𝐱=0)è1/𝐰;lostesso vale peri\npuntisull’iperpiano 𝐷𝐱=−1.\nPertanto ilmargine èW=2/𝐰.\nL’iperpiano ottimo secondo SVM èquello soddisfa ivincoli di\nseparazione dei pattern emassimizza ilmargine W(o\nalternativamente minimizza ilsuoinverso) :\nMinimizza :𝐰2/2\nVincoli :𝑦𝑖𝐰∙𝐱𝒊+𝑏−1≥0𝑝𝑒𝑟𝑖=1…𝑛\nIpattern deltraining setchegiacciono sulmargine (cerchi pieni in\nfigura) sono detti support vector .Talipattern, checostituiscono icasi\npiù complessi, definiscono completamente lasoluzione del\nproblema, che può essere espressa come funzione disolo tali\npattern ,indipendentemente dalla dimensionalità dello spazio𝑑edal\nnumero𝑛dielementi inTS.𝐷𝐱=+11/𝐰\n𝐷𝐱=0\n𝐷𝐱=−1𝐷𝐱>+1\n𝐷𝐱<−11/𝐰Laminima distanza trapattern del training set didue classi\ndifferenti piùvicini all’iperpiano diseparazione èdetta margine (-).",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#21": "SVM Lineari: Pattern Separabili\n5 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (2)  \nLa minima  distanza  tra l’iperpiano  di separazione  e un pattern  del \ntraining  set è detta  margine  (W). \n \n \n \n \n \n \n \n \nLa distanza  dei punti  che giacciono  sull’iperpiano  𝐷𝐱=+1 \ndall’iperpiano  di separazione  (𝐷𝐱=0) è 1/𝐰; lo stesso  vale per i \npunti  sull’iperpiano  𝐷𝐱=−1. \nPertanto  il margine  è  W= 2/𝐰.  \nL’iperpiano  ottimo  secondo  SVM  è quello  soddisfa  i vincoli  di \nseparazione  dei pattern  e massimizza  il margine  W (o \nalternativamente  minimizza  il suo inverso) : \nMinimizza : 𝐰2/2 \nVincoli :  𝑦𝑖𝐰∙𝐱𝒊+𝑏−1≥0     𝑝𝑒𝑟  𝑖=1…𝑛 \nI pattern  del training  set che giacciono  sul margine  (cerchi  pieni  in \nfigura)  sono  detti support  vector . Tali pattern,  che costituiscono  i casi \npiù complessi,  definiscono  completamente  la soluzione  del \nproblema,  che può essere  espressa  come  funzione  di solo tali \npattern , indipendentemente  dalla  dimensionalità  dello  spazio  𝑑 e dal \nnumero  𝑛 di elementi  in TS.    \n 𝐷𝐱=+1 1/𝐰 \n𝐷𝐱=0 \n𝐷𝐱=−1 𝐷𝐱>+1 \n𝐷𝐱<−1 1/𝐰 \n",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#22": "SVM Lineari: Pattern Separabili\n6 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (3)  \nIl problema  di ottimizzazione  precedente , può essere  risolto  \npassando  innanzitutto  a una formulazione  Lagrangiana  e \nsuccessivamente  a una formulazione  duale .  \nLa formulazione  Lagrangiana  prevede  di introdurre  un moltiplicatore  \n𝛼𝑖 (𝛼𝑖 ≥0) per ogni vincolo  nella  forma  𝑒𝑞𝑢𝑎𝑧𝑖𝑜𝑛𝑒  ≥0 e di sottrarre  \nil vincolo  moltiplicato  per 𝛼𝑖 dalla  funzione  obiettivo : \n𝑄𝐰,𝑏,𝛂=1\n2𝐰∙𝐰− 𝛼𝑖𝑛\n𝑖=1𝑦𝑖𝐰∙𝐱𝒊+𝑏−1 \nda minimizzare  rispetto  a 𝐰 e 𝑏 e massimizzare  rispetto  a 𝛼𝑖 ≥0. \n \nUtilizzando  le condizioni  di Karush -Kuhn -Tucker  (KKT), il problema  \npuò essere  posto  in forma  duale  esprimendo  i parametri  𝐰 e 𝑏 in \nfunzione  dei moltiplicatori  𝛼𝑖, e risolto  massimizzando  la nuova  \nfunzione  obiettivo  rispetto  ai soli 𝛼𝑖: \n𝑄𝛂= 𝛼𝑖\n𝑖=1…𝑛−1\n2 𝛼𝑖𝛼𝑗𝑦𝑖𝑦𝑗𝐱𝑖∙𝐱𝑗\n𝑖,𝑗=1…𝑛 \n \ncon vincoli     𝑦𝑖𝛼𝑖=0\n𝑖=1…𝑛      e      𝛼𝑖≥0   𝑝𝑒𝑟 𝑖=1…𝑛  \n \nPer approfondimenti  e derivazione  delle  equazioni : \nS. Gunn,  Support  Vector  Machines  for Classification  and Regression  \nC. Burges , A Tutorial  on Support  Vector  Machines  for Pattern  Recognition  \n \n \n5 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (2)  \nLa minima  distanza  tra l’iperpiano  di separazione  e un pattern  del \ntraining  set è detta  margine  (W). \n \n \n \n \n \n \n \n \nLa distanza  dei punti  che giacciono  sull’iperpiano  𝐷𝐱=+1 \ndall’iperpiano  di separazione  (𝐷𝐱=0) è 1/𝐰; lo stesso  vale per i \npunti  sull’iperpiano  𝐷𝐱=−1. \nPertanto  il margine  è  W= 2/𝐰.  \nL’iperpiano  ottimo  secondo  SVM  è quello  soddisfa  i vincoli  di \nseparazione  dei pattern  e massimizza  il margine  W (o \nalternativamente  minimizza  il suo inverso) : \nMinimizza : 𝐰2/2 \nVincoli :  𝑦𝑖𝐰∙𝐱𝒊+𝑏−1≥0     𝑝𝑒𝑟  𝑖=1…𝑛 \nI pattern  del training  set che giacciono  sul margine  (cerchi  pieni  in \nfigura)  sono  detti support  vector . Tali pattern,  che costituiscono  i casi \npiù complessi,  definiscono  completamente  la soluzione  del \nproblema,  che può essere  espressa  come  funzione  di solo tali \npattern , indipendentemente  dalla  dimensionalità  dello  spazio  𝑑 e dal \nnumero  𝑛 di elementi  in TS.    \n 𝐷𝐱=+1 1/𝐰 \n𝐷𝐱=0 \n𝐷𝐱=−1 𝐷𝐱>+1 \n𝐷𝐱<−1 1/𝐰 Funzione \nObiettivo",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#23": "6 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (3)  \nIl problema  di ottimizzazione  precedente , può essere  risolto  \npassando  innanzitutto  a una formulazione  Lagrangiana  e \nsuccessivamente  a una formulazione  duale .  \nLa formulazione  Lagrangiana  prevede  di introdurre  un moltiplicatore  \n𝛼𝑖 (𝛼𝑖 ≥0) per ogni vincolo  nella  forma  𝑒𝑞𝑢𝑎𝑧𝑖𝑜𝑛𝑒  ≥0 e di sottrarre  \nil vincolo  moltiplicato  per 𝛼𝑖 dalla  funzione  obiettivo : \n𝑄𝐰,𝑏,𝛂=1\n2𝐰∙𝐰− 𝛼𝑖𝑛\n𝑖=1𝑦𝑖𝐰∙𝐱𝒊+𝑏−1 \nda minimizzare  rispetto  a 𝐰 e 𝑏 e massimizzare  rispetto  a 𝛼𝑖 ≥0. \n \nUtilizzando  le condizioni  di Karush -Kuhn -Tucker  (KKT), il problema  \npuò essere  posto  in forma  duale  esprimendo  i parametri  𝐰 e 𝑏 in \nfunzione  dei moltiplicatori  𝛼𝑖, e risolto  massimizzando  la nuova  \nfunzione  obiettivo  rispetto  ai soli 𝛼𝑖: \n𝑄𝛂= 𝛼𝑖\n𝑖=1…𝑛−1\n2 𝛼𝑖𝛼𝑗𝑦𝑖𝑦𝑗𝐱𝑖∙𝐱𝑗\n𝑖,𝑗=1…𝑛 \n \ncon vincoli     𝑦𝑖𝛼𝑖=0\n𝑖=1…𝑛      e      𝛼𝑖≥0   𝑝𝑒𝑟 𝑖=1…𝑛  \n \nPer approfondimenti  e derivazione  delle  equazioni : \nS. Gunn,  Support  Vector  Machines  for Classification  and Regression  \nC. Burges , A Tutorial  on Support  Vector  Machines  for Pattern  Recognition  \n \n SVM Lineari: Pattern Separabili\nVincoliFunzione \nObiettivo",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#24": "SVM Lineari: Pattern Separabili\n6 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (3)  \nIl problema  di ottimizzazione  precedente , può essere  risolto  \npassando  innanzitutto  a una formulazione  Lagrangiana  e \nsuccessivamente  a una formulazione  duale .  \nLa formulazione  Lagrangiana  prevede  di introdurre  un moltiplicatore  \n𝛼𝑖 (𝛼𝑖 ≥0) per ogni vincolo  nella  forma  𝑒𝑞𝑢𝑎𝑧𝑖𝑜𝑛𝑒  ≥0 e di sottrarre  \nil vincolo  moltiplicato  per 𝛼𝑖 dalla  funzione  obiettivo : \n𝑄𝐰,𝑏,𝛂=1\n2𝐰∙𝐰− 𝛼𝑖𝑛\n𝑖=1𝑦𝑖𝐰∙𝐱𝒊+𝑏−1 \nda minimizzare  rispetto  a 𝐰 e 𝑏 e massimizzare  rispetto  a 𝛼𝑖 ≥0. \n \nUtilizzando  le condizioni  di Karush -Kuhn -Tucker  (KKT), il problema  \npuò essere  posto  in forma  duale  esprimendo  i parametri  𝐰 e 𝑏 in \nfunzione  dei moltiplicatori  𝛼𝑖, e risolto  massimizzando  la nuova  \nfunzione  obiettivo  rispetto  ai soli 𝛼𝑖: \n𝑄𝛂= 𝛼𝑖\n𝑖=1…𝑛−1\n2 𝛼𝑖𝛼𝑗𝑦𝑖𝑦𝑗𝐱𝑖∙𝐱𝑗\n𝑖,𝑗=1…𝑛 \n \ncon vincoli     𝑦𝑖𝛼𝑖=0\n𝑖=1…𝑛      e      𝛼𝑖≥0   𝑝𝑒𝑟 𝑖=1…𝑛  \n \nPer approfondimenti  e derivazione  delle  equazioni : \nS. Gunn,  Support  Vector  Machines  for Classification  and Regression  \nC. Burges , A Tutorial  on Support  Vector  Machines  for Pattern  Recognition  \n \n \nprodotto scalare fra \ncoppie di vettori del TS \nVincoliFunzione \nObiettivo",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#25": "SVM Lineari: Pattern Separabili\n7 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (4)  \nIl problema  di ottimizzazione  precedente  può essere  risolto  \nattraverso  un algoritmo  di programmazione  quadratica  (disponibile  in \nlibrerie  numeriche) . \nLa soluzione  consiste  nel derivare  i valori  ottimi  𝛼1∗,𝛼2∗…𝛼𝑛∗ \nLe condizioni  KKT assicurano  che 𝛼𝑖∗=0 per tutti i vettori  che non \nsono  support  vector . \nL’iperpiano  ottimo  è dunque  parametrizzato  da:  \n     𝐰∗= 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖 𝐱𝑖 \ne   𝑏∗=𝑦𝑠− 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖𝐱𝑖∙𝐱𝑠 \ndove  (𝐱𝑠, 𝑦𝑠) è uno dei support  vector .  \n \nLa funzione  distanza  dall’iperpiano  è: \n𝐷𝐱=𝐰∗∙𝐱+𝑏∗= 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖𝐱∙𝐱𝑖+𝑏∗ \n \nSi noti che: \nIl segno  della  funzione  𝐷𝐱 consente  di classificare  un generico  \npattern  𝐱.  \nLe sommatorie  sono  riducibili  ai soli support  vector . \nNel caso  lineare  non è necessario,  dopo  aver calcolato  𝐰∗ e   𝑏∗, \nconservare/memorizzare  i support  vectors .   \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#26": "SVM Lineari: Pattern Separabili\n7 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (4)  \nIl problema  di ottimizzazione  precedente  può essere  risolto  \nattraverso  un algoritmo  di programmazione  quadratica  (disponibile  in \nlibrerie  numeriche) . \nLa soluzione  consiste  nel derivare  i valori  ottimi  𝛼1∗,𝛼2∗…𝛼𝑛∗ \nLe condizioni  KKT assicurano  che 𝛼𝑖∗=0 per tutti i vettori  che non \nsono  support  vector . \nL’iperpiano  ottimo  è dunque  parametrizzato  da:  \n     𝐰∗= 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖 𝐱𝑖 \ne   𝑏∗=𝑦𝑠− 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖𝐱𝑖∙𝐱𝑠 \ndove  (𝐱𝑠, 𝑦𝑠) è uno dei support  vector .  \n \nLa funzione  distanza  dall’iperpiano  è: \n𝐷𝐱=𝐰∗∙𝐱+𝑏∗= 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖𝐱∙𝐱𝑖+𝑏∗ \n \nSi noti che: \nIl segno  della  funzione  𝐷𝐱 consente  di classificare  un generico  \npattern  𝐱.  \nLe sommatorie  sono  riducibili  ai soli support  vector . \nNel caso  lineare  non è necessario,  dopo  aver calcolato  𝐰∗ e   𝑏∗, \nconservare/memorizzare  i support  vectors .   \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#27": "SVM Lineari: Pattern Separabili\n8 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (4)  \nVantaggi  dell’approccio  SVM :  \nDefinizione  della  soluzione  sulla base  di un numero  ridotto  di \nsupport  vector  (solitamente  pochi) . \nIl numero  di support  vector  𝑛𝑠𝑣 indica  la complessità  del problema  \ne può essere  dimostrato  che l’errore  medio  (sui possibili  training  \nset) è limitato  da 𝑛𝑠𝑣/𝑛. \nSVM  «scala » molto  bene  rispetto  alla dimensionalità  𝑑 dello  \nspazio  delle  feature  (grazie  ai prodotti  scalari) . La complessità  \ncomputazionale  nel training  è quadratica  rispetto  al numero  𝑛 di \npattern  in TS. In pratica  il problema  può essere  risolto  per 𝑑=107 \ne per 𝑛 fino a 104. \n \n \nEsempio :  \ni support vectors \n(cerchiati ) \ndefiniscono  la \nsoluzione . ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#28": "SVM Lineari: Pattern Separabili\n8 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern Separabili (4)  \nVantaggi  dell’approccio  SVM :  \nDefinizione  della  soluzione  sulla base  di un numero  ridotto  di \nsupport  vector  (solitamente  pochi) . \nIl numero  di support  vector  𝑛𝑠𝑣 indica  la complessità  del problema  \ne può essere  dimostrato  che l’errore  medio  (sui possibili  training  \nset) è limitato  da 𝑛𝑠𝑣/𝑛. \nSVM  «scala » molto  bene  rispetto  alla dimensionalità  𝑑 dello  \nspazio  delle  feature  (grazie  ai prodotti  scalari) . La complessità  \ncomputazionale  nel training  è quadratica  rispetto  al numero  𝑛 di \npattern  in TS. In pratica  il problema  può essere  risolto  per 𝑑=107 \ne per 𝑛 fino a 104. \n \n \nEsempio :  \ni support vectors \n(cerchiati ) \ndefiniscono  la \nsoluzione . ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#29": "SVM Lineari: Pattern Non Separabili\n9 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern non Separabili  \nIn questo  caso  non tutti i pattern  possono  essere  separati  da un \niperpiano,  ed è necessario  rilassare  i vincoli  di separazione , per far \nsì che alcuni  pattern  (il minor  numero  possibile)  possano  valicare  il \nconfine  della  classe . \nA tal fine si introducono  𝑛 variabili  di slack  positive  ξ𝑖,𝑖=1…𝑛 e si \nmodificano  i vincoli  di separazione : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 \nPer ogni pattern  𝐱𝑖 del TS, la variabile  ξ𝑖 codifica  la deviazione  dal \nmargine . Per i pattern  separabili  del TS le corrispondenti  variabili  di \nslack  assumeranno  valore  0. \n \n \n \n \n \n \n \nL’iperpiano  ottimo  deve  in questo  caso  ancora  massimizzare  il \nmargine , ma allo stesso  tempo  minimizzare  il numero  di elementi  \nnon correttamente  classificati . La funzione  obiettivo,  e di \nconseguenza  il problema  di ottimizzazione  vengono  così modificati : \nMinimizza : 𝐰2\n2+𝐶  ξ𝑖 𝑖=1…𝑛 \nVincoli :  𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 Pattern erroneamente  \nclassificati: [ > 0 \n9 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern non Separabili  \nIn questo  caso  non tutti i pattern  possono  essere  separati  da un \niperpiano,  ed è necessario  rilassare  i vincoli  di separazione , per far \nsì che alcuni  pattern  (il minor  numero  possibile)  possano  valicare  il \nconfine  della  classe . \nA tal fine si introducono  𝑛 variabili  di slack  positive  ξ𝑖,𝑖=1…𝑛 e si \nmodificano  i vincoli  di separazione : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 \nPer ogni pattern  𝐱𝑖 del TS, la variabile  ξ𝑖 codifica  la deviazione  dal \nmargine . Per i pattern  separabili  del TS le corrispondenti  variabili  di \nslack  assumeranno  valore  0. \n \n \n \n \n \n \n \nL’iperpiano  ottimo  deve  in questo  caso  ancora  massimizzare  il \nmargine , ma allo stesso  tempo  minimizzare  il numero  di elementi  \nnon correttamente  classificati . La funzione  obiettivo,  e di \nconseguenza  il problema  di ottimizzazione  vengono  così modificati : \nMinimizza : 𝐰2\n2+𝐶  ξ𝑖 𝑖=1…𝑛 \nVincoli :  𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 Pattern erroneamente  \nclassificati: [ > 0 \nVi saranno quindi tante\nvariabili di slack (scarto)\nquanti sono ipattern del\nTraning Set(TS) .\nTali variabili saranno, però,\ndiverse dazero (>0)solo per\nipattern non separabili, cioè\nclassificati erroneamente",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#3": "Support Vector Machine (SVM)\nLeMacchine aVettori diSupporto oMacchine Kernel (Support Vector Machine,\nSVM) costituiscono uninsieme dimetodi diapprendimento supervisionato .\nPossono essere utilizzate siaperfare Classificazione ,siaperfare Regressione .\nInunbreve lasso temporale dalla loro prima implementazione hanno trovato\napplicazione inunnutrito numero dibranche scientifiche come Fisica, Biologia,\nChimica :\n!Preparazione difarmaci\n!Ricerca direlazioni quantitative sulle proprietà distrutture\n!Chemiometria\n!Sensoristica\n!Ingegneria chimica\n!Computer vision (e.g.,face detection erecognition inimmagini evideo)\n!...",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#30": "SVM Lineari: Pattern Non Separabili\n9 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern non Separabili  \nIn questo  caso  non tutti i pattern  possono  essere  separati  da un \niperpiano,  ed è necessario  rilassare  i vincoli  di separazione , per far \nsì che alcuni  pattern  (il minor  numero  possibile)  possano  valicare  il \nconfine  della  classe . \nA tal fine si introducono  𝑛 variabili  di slack  positive  ξ𝑖,𝑖=1…𝑛 e si \nmodificano  i vincoli  di separazione : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 \nPer ogni pattern  𝐱𝑖 del TS, la variabile  ξ𝑖 codifica  la deviazione  dal \nmargine . Per i pattern  separabili  del TS le corrispondenti  variabili  di \nslack  assumeranno  valore  0. \n \n \n \n \n \n \n \nL’iperpiano  ottimo  deve  in questo  caso  ancora  massimizzare  il \nmargine , ma allo stesso  tempo  minimizzare  il numero  di elementi  \nnon correttamente  classificati . La funzione  obiettivo,  e di \nconseguenza  il problema  di ottimizzazione  vengono  così modificati : \nMinimizza : 𝐰2\n2+𝐶  ξ𝑖 𝑖=1…𝑛 \nVincoli :  𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 Pattern erroneamente  \nclassificati: [ > 0 \n9 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern non Separabili  \nIn questo  caso  non tutti i pattern  possono  essere  separati  da un \niperpiano,  ed è necessario  rilassare  i vincoli  di separazione , per far \nsì che alcuni  pattern  (il minor  numero  possibile)  possano  valicare  il \nconfine  della  classe . \nA tal fine si introducono  𝑛 variabili  di slack  positive  ξ𝑖,𝑖=1…𝑛 e si \nmodificano  i vincoli  di separazione : \n𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 \nPer ogni pattern  𝐱𝑖 del TS, la variabile  ξ𝑖 codifica  la deviazione  dal \nmargine . Per i pattern  separabili  del TS le corrispondenti  variabili  di \nslack  assumeranno  valore  0. \n \n \n \n \n \n \n \nL’iperpiano  ottimo  deve  in questo  caso  ancora  massimizzare  il \nmargine , ma allo stesso  tempo  minimizzare  il numero  di elementi  \nnon correttamente  classificati . La funzione  obiettivo,  e di \nconseguenza  il problema  di ottimizzazione  vengono  così modificati : \nMinimizza : 𝐰2\n2+𝐶  ξ𝑖 𝑖=1…𝑛 \nVincoli :  𝑦𝑖𝐰∙𝐱𝒊+𝑏≥1−ξ𝑖      𝑝𝑒𝑟  𝑖=1…𝑛 Pattern erroneamente  \nclassificati: [ > 0 ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#31": "SVM Lineari: Pattern Non Separabili\n10 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern non Separabili (2)  \nIl coefficiente  𝐶 nel problema  di ottimizzazione  precedente,  indica  \nl’importanza  relativa  degli  errori  di classificazione  rispetto  \nall’ampiezza  del margine . Si tratta  di uno dei pochi  iperparametri  che \nl’utente  deve  scegliere  per il tuning  di SVM . \nPassando  attraverso  forma  lagrangiana/duale  otteniamo  un risultato  \nuguale  al caso  linearmente  separabile,  tranne  che per l’introduzione  \ndel limite  superiore  (𝐶) per i valori  dei moltiplicatori  𝛼𝑖 : \n𝑄𝛂= 𝛼𝑖\n𝑖=1…𝑛−1\n2 𝛼𝑖𝛼𝑗𝑦𝑖𝑦𝑗𝐱𝑖∙𝐱𝑗\n𝑖,𝑗=1…𝑛 \n \ncon vincoli     𝑦𝑖𝛼𝑖=0\n𝑖=1…𝑛      e      0≤𝛼𝑖≤𝐶   𝑝𝑒𝑟 𝑖=1…𝑛  \nIl metodo  di soluzione  (i.e. Progr . Quadratica)  e il modo  di derivare  \nl’iperpiano  dagli  𝛼𝑖 sono  gli stessi  del caso  linearmente  separabile . \nEsempi : \n𝐶=200 \n1 solo errore, margine minore  𝐶=10 \n2 errori, margine maggiore  -Se C ---> ∞  : non ammettiamo violazioni del margine (hard -margin SVM)\n-Se C è finito : ammettiamo violazioni del margine e pattern misclassificati \n(soft-margin SVM)VincoliFunzione \nObiettivo",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#32": "SVM Lineari: Pattern Non Separabili\n10 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM lineari: Pattern non Separabili (2)  \nIl coefficiente  𝐶 nel problema  di ottimizzazione  precedente,  indica  \nl’importanza  relativa  degli  errori  di classificazione  rispetto  \nall’ampiezza  del margine . Si tratta  di uno dei pochi  iperparametri  che \nl’utente  deve  scegliere  per il tuning  di SVM . \nPassando  attraverso  forma  lagrangiana/duale  otteniamo  un risultato  \nuguale  al caso  linearmente  separabile,  tranne  che per l’introduzione  \ndel limite  superiore  (𝐶) per i valori  dei moltiplicatori  𝛼𝑖 : \n𝑄𝛂= 𝛼𝑖\n𝑖=1…𝑛−1\n2 𝛼𝑖𝛼𝑗𝑦𝑖𝑦𝑗𝐱𝑖∙𝐱𝑗\n𝑖,𝑗=1…𝑛 \n \ncon vincoli     𝑦𝑖𝛼𝑖=0\n𝑖=1…𝑛      e      0≤𝛼𝑖≤𝐶   𝑝𝑒𝑟 𝑖=1…𝑛  \nIl metodo  di soluzione  (i.e. Progr . Quadratica)  e il modo  di derivare  \nl’iperpiano  dagli  𝛼𝑖 sono  gli stessi  del caso  linearmente  separabile . \nEsempi : \n𝐶=200 \n1 solo errore, margine minore  𝐶=10 \n2 errori, margine maggiore  \nAll’aumentare del valore di C\n!diminuisce il numero di support vector (i.e., complessità del problema)\n!diminuisce il numero di errori sul Traning Set\n!diminuisce il margine di separazione (i.e., capacità di generalizzazione)",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#4": "SVM\n!In1936 ,R.A.Fisher suggested the first algorithm forPattern Recognition\n(Fisher 1936 ).\n!Aronszajn (1950 )introduced the“Theory ofReproducing Kernels” .\n!In1957 Frank Rosenblatt invented alinear classifier called the perceptron (the\nsimplest kind offeedforward neural network) .\n!Vapnik and Lerner (1963 )introduced the Generalized Portrait algorithm (the\nalgorithm implemented by support vector machines isanonlinear\ngeneralization oftheGeneralized Portrait algorithm) .\n!Aizerman, Braverman and Rozonoer (1964 )introduced the geometrical\ninterpretation ofthekernels asinner products inafeature space .\n!Vapnik and Chervonenkis (1964 )further developed the Generalized\nPortrait algorithm .\n!...\n!SVMs close totheir current form were first introduced with apaper attheCOLT\n1992 conference (Boser, Guyon and Vapnik 1992 ).\n!In1995 thesoft margin classifier was introduced byCortes and Vapnik (1995 );\ninthe same year the algorithm was extended tothe case ofregression by\nVapnik (1995 )inThe Nature ofStatistical Learning Theory .\nfonte: https://www.svms.org/history.html",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#5": "SVM! SVMs (Vapnik, 1990’s) choose the linear separator with the \nlargest margin  \n• Good according to intuition, theory, practice  \n• SVM became famous when, using images as input, it gave \naccuracy comparable to neural-network with hand-designed \nfeatures in a handwriting recognition task Support Vector Machine (SVM) \nV. Vapnik Robust to \noutliers! \nA. Chervonenkis    XXV «                         »   * 1 \n1964 \n    5 1 9 . 9 5 \n                             \n .  .       ,  .  .             \n(      ) \n                                  ,                          -\n                    .                -                              \n                          .                                         -\n                                                                \n                           . \n1.          \n  1 9 5 7  .                                                     -\n                                   ,                            -\n      . \n                                              ,                \n                                                                     \n               ,        ,   -       ,                                -\n  ,     -       ,                                                    -\n     . \n  \n      \n   . 1 \n  1 9 5 7  .                                                  .   -\n                                  ,                                  \n                   .     -                                    . 1. \n                        ,                                        , \n                                  -        . \n   ,                                                ,        -\n        ,                                                        . \n               ,                                      ,             -\n    ,          % ,...,  ,                             ,              , \n                   .                                                \n        .                                                        \n[1].          [ 1]                                           ,           \n                                                     .               , \n                                                                     \n                                 ,                                  \n         .              ,                    U                      \n                   \n£*= ejx     §2   . . .   cnfn, \n112 \n                        \n1.              .  . ,              .  .                                     -\n             .                          ,  . X X I V ,   6, 1 9 6 3 . \n2. X        .  ,              .  ,              .  .                - 1 ,     \n                            .                        ,   4.    -          . \n     . , 1 9 6 2 . \n3.            .  .                                                      . \n .         ,        .            .    . ,  . 2,   2, 1 9 6 2 . \n4.                .    .                                                    -\n            .                        ,   4.    -          .      . , 1 9 6 2 . \n5.            .                                               .          -\n              ,   4.    -          .      . , 1 9 6 2 . \nON A P E R C E P T R O N C L A S S \nV. N . V A P N I K , A . Y A . C H E R V O N E N K I S \nA c l a s s of p e r c e p t r o n s d i f f e r i n g f r o m p e r c e p t r o n s in e x i s t e n c e w i t h t h e l e a r n i n g m e t -\nhod is c o n s i d e r e d . S u c h a p e r c e p t r o n is d e s c r i b e d , i t s b l o c k - s c h e m e a n d t h e l e a r n i n g m e t -\nhod s a r e p r o p o s e d . T h e a l g o r i t h m s f o r v a r i o u s c l a s s e s of p e r c e p t r o n s a r e c o m p a r e d w i t h \nthe t h e o r y of p a t t e r n r e c o g n i t i o n w i t h t h e h e l p of a g e n e r a l i z e d p o r t r a i t . Journal of Machine Learning Research 16 (2015) 2067-2080 Published 9/15\nAlexey Chervonenkis’s Bibliography\nAlex Gammerman alex@cs.rhul.ac.uk\nVladimir Vovk v.vovk@rhul.ac.uk\nComputer Learning Research Centre, Department of Computer Science\nRoyal Holloway, University of London\nThis bibliography does not contain Alexey’s patents (he has at least two), technical reports,\nunpublished manuscripts, and collections edited by him. \"NA\" indicates that a journal paper\nwas not assigned to a volume; e.g., it is common for Russian journals (such as Проблемы\nуправления and, in some years, Автоматика и телемеханика ) not to have volumes, and\nalso to have pages numbered separately inside each issue. All papers published by Alexey\nbefore 2001 (and afterwards in the case of papers whose original language was Russian) have\nauthor lists ordered according to the Cyrillic alphabetic order; for other papers the order\nmay reﬂect the authors’ contributions (people who contributed most tend to be listed ﬁrst)\nand administrative positions (bosses tend to be listed last).\nThe bibliography is given by the year of the original publication (which may be di ﬀerent\nfrom the year of the English translation, always given ﬁrst when available).\n1964\n[1] Vladimir N. Vapnik and Alexey Ya. Chervonenkis. On a class of perceptrons. Au-\ntomation and Remote Control ,2 5 ( 1 ) : 1 0 3 – 1 0 9 ,1 9 6 4 . R u s s i a no r i g i n a l : В.Н.Вапник ,\nА.Я.Червоненкис .Об одном классе персептронов .Автоматика и телемеханика ,\n25(1):112–120, 1964; with English summary entitled “On a perceptron class”. The orig-\ninal article submitted on 21 February 1963.\n[2] Vladimir N. Vapnik and Alexey Ya. Chervonenkis. On a class of pattern-recognition\nlearning algorithms. Automation and Remote Control ,2 5 ( 6 ) : 8 3 8 – 8 4 5 ,1 9 6 4 . R u s s i a n\noriginal: В.Н.Вапник ,А.Я.Червоненкис .Об одном классе алгоритмов обучения\nраспознаванию образов .Автоматика и телемеханика , 25(6):937–945, 1964; with\nEnglish summary entitled “A class of algorithms for pattern recognition learning”. The\nsubmission date is not given.\n[3] Vladimir N. Vapnik, Lyudmila M. Dronfort, and Alexey Ya. Chervonenkis. Some ques-\ntions of the self-organization of recognizing systems (in Russian). In Theory and Appli-\ncation of Automatic Systems (Russian), pages 172–177. Nauka, Moscow, 1964. In the\noriginal language: В.Н.Вапник ,Л.М.(Людмила Михайловна )Дронфорт ,А.Я.\nЧервоненкис .Некоторые вопросы самоорганизации распознающих устройств .\nТеория и применение автоматических систем ,сс.1 7 2 – 1 7 7 . Наука ,Москва ,1 9 6 4 .\nc\u00002015 Alex Gammerman and Vladimir Vovk.",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#6": "SVM\n2 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  Support Vector  Machines  (SVM)  \nLa teoria  che governa  i meccanismi  di funzionamento  di SVM  è stata  \nintrodotta  da Vapnik  a partire  dal 1965  (statistical  learning  theory ), e \nperfezionata  più recentemente  (1995 ) dallo  stesso  Vapnik  e altri. \nSVM  è uno degli  strumenti  più utilizzati  per la classificazione  di \npattern . \nInvece  di stimare  le densità  di probabilità  delle  classi,  Vapnik  \nsuggerisce  di risolvere  direttamente  il problema  di interesse  (che \nconsidera  più semplice),  ovvero  determinare  le superfici  decisionali  \ntra le classi  (classification  boundaries ). \n \nandiamo per gradi …  \nSVM  nasce  come  classificatore  binario  (2 classi),  estendibile  a più \nclassi . Affrontiamo  la trattazione  per gradi : \nSVM  lineare  (i.e., la superficie  di separazione  è un iperpiano ) e \npattern  del training  set linearmente  separabili  (i.e., esiste  per \nipotesi  almeno  un iperpiano  in grado  di separarli) . \nSVM  lineare  e pattern  non linearmente  separabili . Ci saranno  \ninevitabilmente  errori  di classificazione  nel training  set non \nesistendo  alcun  iperpiano  in grado  di separare  i pattern . \nSVM  non lineare  (i.e., superficie  di separazione  complessa ) \nsenza  ipotesi  sulla separabilità  dei pattern . \nEstensione  multiclasse . \n 1.  Use optimization to find solution (i.e. a hyperplane) \nwith few errors \n2.  Seek large margin separator to improve \ngeneralization \n3.  Use kernel trick to make large feature \nspaces computationally efficient Support vector machines: 3 key ideas ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#7": "SVM\nLe SVM si fondano su tre idee chiave\n!L’adozione di tecniche di ottimizzazione matematica per \nindividuare soluzioni (i.e., iperpiani) con un basso tasso di errori\n!La ricerca di un separatore con margine largo per migliorare la \ngeneralizzazione \n!L’impiego dello stratagemma del kernel (kernel trick) per rendere \ncomputazionalemente efficienti ampi spazi di feature1.  Use optimization to find solution (i.e. a hyperplane) \nwith few errors \n2.  Seek large margin separator to improve \ngeneralization \n3.  Use kernel trick to make large feature \nspaces computationally efficient Support vector machines: 3 key ideas ",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#8": "SVM\n2 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  Support Vector  Machines  (SVM)  \nLa teoria  che governa  i meccanismi  di funzionamento  di SVM  è stata  \nintrodotta  da Vapnik  a partire  dal 1965  (statistical  learning  theory ), e \nperfezionata  più recentemente  (1995 ) dallo  stesso  Vapnik  e altri. \nSVM  è uno degli  strumenti  più utilizzati  per la classificazione  di \npattern . \nInvece  di stimare  le densità  di probabilità  delle  classi,  Vapnik  \nsuggerisce  di risolvere  direttamente  il problema  di interesse  (che \nconsidera  più semplice),  ovvero  determinare  le superfici  decisionali  \ntra le classi  (classification  boundaries ). \n \nandiamo per gradi …  \nSVM  nasce  come  classificatore  binario  (2 classi),  estendibile  a più \nclassi . Affrontiamo  la trattazione  per gradi : \nSVM  lineare  (i.e., la superficie  di separazione  è un iperpiano ) e \npattern  del training  set linearmente  separabili  (i.e., esiste  per \nipotesi  almeno  un iperpiano  in grado  di separarli) . \nSVM  lineare  e pattern  non linearmente  separabili . Ci saranno  \ninevitabilmente  errori  di classificazione  nel training  set non \nesistendo  alcun  iperpiano  in grado  di separare  i pattern . \nSVM  non lineare  (i.e., superficie  di separazione  complessa ) \nsenza  ipotesi  sulla separabilità  dei pattern . \nEstensione  multiclasse . \n Iperpiano: sottospazio di dimensione inferiore di uno (n-1) rispetto allo spazio in \ncui è contenuto (n) (e.g., se lo spazio ha dimensione 3, i suoi iperpiani sono i piani)",
    "data_test\\rootfolder\\università\\MachineLearning\\34-SVM(1)-sbloccato.pdf#9": "3 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: l’idea  \nDate  due classi  di pattern  multidimensionali  linearmente  separabili,  \ntra tutti i possibili  iperpiani  di separazione,  SVM  determina  quello  in \ngrado  di separare  le classi  con il maggior  margine  possibile . \nIl margine  è la distanza  minima  di punti  delle  due classi  nel training  \nset dall’iperpiano  individuato . Definizione  formale  in seguito . \n \n \n \n \n \n \n \n \n \n \n \nLa massimizzazione  del margine  è legata  alla generalizzazione . Se i \npattern  del training  set sono  classificati  con ampio  margine  si può \n«sperare»  che anche  pattern  del test set vicini  al confine  tra le classi  \nsiano  gestiti  correttamente .  \n3 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: l’idea  \nDate  due classi  di pattern  multidimensionali  linearmente  separabili,  \ntra tutti i possibili  iperpiani  di separazione,  SVM  determina  quello  in \ngrado  di separare  le classi  con il maggior  margine  possibile . \nIl margine  è la distanza  minima  di punti  delle  due classi  nel training  \nset dall’iperpiano  individuato . Definizione  formale  in seguito . \n \n \n \n \n \n \n \n \n \n \n \nLa massimizzazione  del margine  è legata  alla generalizzazione . Se i \npattern  del training  set sono  classificati  con ampio  margine  si può \n«sperare»  che anche  pattern  del test set vicini  al confine  tra le classi  \nsiano  gestiti  correttamente .  \n3 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: l’idea  \nDate  due classi  di pattern  multidimensionali  linearmente  separabili,  \ntra tutti i possibili  iperpiani  di separazione,  SVM  determina  quello  in \ngrado  di separare  le classi  con il maggior  margine  possibile . \nIl margine  è la distanza  minima  di punti  delle  due classi  nel training  \nset dall’iperpiano  individuato . Definizione  formale  in seguito . \n \n \n \n \n \n \n \n \n \n \n \nLa massimizzazione  del margine  è legata  alla generalizzazione . Se i \npattern  del training  set sono  classificati  con ampio  margine  si può \n«sperare»  che anche  pattern  del test set vicini  al confine  tra le classi  \nsiano  gestiti  correttamente .  \nSVM",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#0": "Sommario\n!SVM Lineari: Pattern Linearmente Separabili e Non\n!SVM Non Lineari\n!SVM Multiclasse",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#1": "SVM Non Lineari\n11 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM Non lineari  \nSVM  prevede  un’importante  estensione  della  teoria  inizialmente  \nsviluppata  per iperpiani,  al caso  (non lineare ) di separazione  dei \npattern  con superfici  anche  molto  complesse . Tutto  ciò avviene  in \nmodo  molto  semplice :    \nViene  definito  un mapping  Φ non lineare  dei pattern  dallo  spazio  \ndi partenza  𝑑 verso  uno spazio  𝑚 a più alta dimensionalità  \n(𝑚>𝑑): \nΦ:𝑑→𝑚,Φ𝐱=𝑔1𝐱,𝑔2𝐱,…𝑔𝑚𝐱 \nNello  spazio  𝑚, dove  maggiori  sono  i gradi  di libertà , i pattern  \nΦ𝐱1,Φ𝐱2,…Φ𝐱𝑛 possono  essere  più facilmente  separati  da \nun iperpiano  utilizzando  la teoria  nota. Ciò equivale  a separare  i \npattern  𝐱1,𝐱2,…𝐱𝑛 in 𝑑 con superfici  arbitrariamente  complesse .  \nAnalizzando  la formulazione  del problema  lagrangiano -duale , si nota \nche i vettori  del training  set appaiono  solo in forma  di prodotti  scalari  \ntra coppie  di vettori . Questa  proprietà  (fondamentale ) permette  di \nevitare  la manipolazione  di vettori  nello  spazio  𝑚 (𝑚 può facilmente  \nraggiungere  dimensione  108 e anche  assumere  valore  infinito) . \nInfatti,  per opportuni  mapping  Φ è possibile  ricondurre  il prodotto  \nscalare  di due pattern  mappati  nello  spazio  𝑚 a una funzione  𝐾 \n(detta  Kernel ) dei due pattern  originali  nello  spazio  𝑑.  \nΦ𝐱∙Φ𝐱′=𝐾𝐱,𝐱′ \nCiò consente  di risolvere  il problema  di ottimizzazione  senza  \nparticolari  complicazioni  rispetto  al caso  lineare . Una volta  \ndeterminati  gli𝛼𝑖∗, la superficie  di separazione  (regola  di \nclassificazione)  è esprimibile  come :  \n𝐷𝐱= 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖 𝐾𝐱,𝐱𝑖+𝑏∗ \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#10": "SVM Non Lineari: Kernel Function\n12 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM Non lineari: Kernel  functions  \nPolinomio  di grado   𝑞 (iperparametro ): \n Le componenti  𝑔𝑖𝐱,𝑖=1..𝑚 sono  ottenute  come  tutte le \npossibili  combinazioni  di elevamento  a potenze  d 𝑞 delle  \ncomponenti  di 𝐱. Ad esempio  per 𝑑=2,𝑞=2: \nΦ𝐱=Φ𝑥1,𝑥2=1,𝑥1,𝑥2,𝑥1𝑥2,𝑥12,𝑥22,𝑥12𝑥2,𝑥1𝑥22,𝑥12𝑥22  \n e quindi  𝑚=9.  \n Si dimostra  che: \n𝐾𝐱,𝐱′=𝐱∙𝐱′+1𝑞 \nRadial  Basis  Function  (RBF) di ampiezza  𝜎 (iperparametro ): \n \n𝐾𝐱,𝐱′= 𝑒− 𝐱−𝐱′2\n2𝜎2 \n2-layer  Neural  Network  (meno  utilizzato) : \n \n𝐾𝐱,𝐱′=𝑡𝑎𝑛ℎ𝜈𝐱∙𝐱′+𝑎 \n \n𝜈 ed 𝑎 (iperparametri ) devono  essere  scelti  opportunamente : \nuna possibile  scelta  è: 𝜈=1,𝑎=1 \nIl numero  di hidden  units  e i pesi sono  determinati  \nautomaticamente  da SVM  \n \n \n \n \n \n \n \n \n Si può vedere che il kernel RBF (o gaussiano) equivale a eseguire \nil prodotto interno dei dati di input mappati in un feature space a \ndimensione infinita ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#11": "SVM Non Lineari: Kernel Function\n12 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM Non lineari: Kernel  functions  \nPolinomio  di grado   𝑞 (iperparametro ): \n Le componenti  𝑔𝑖𝐱,𝑖=1..𝑚 sono  ottenute  come  tutte le \npossibili  combinazioni  di elevamento  a potenze  d 𝑞 delle  \ncomponenti  di 𝐱. Ad esempio  per 𝑑=2,𝑞=2: \nΦ𝐱=Φ𝑥1,𝑥2=1,𝑥1,𝑥2,𝑥1𝑥2,𝑥12,𝑥22,𝑥12𝑥2,𝑥1𝑥22,𝑥12𝑥22  \n e quindi  𝑚=9.  \n Si dimostra  che: \n𝐾𝐱,𝐱′=𝐱∙𝐱′+1𝑞 \nRadial  Basis  Function  (RBF) di ampiezza  𝜎 (iperparametro ): \n \n𝐾𝐱,𝐱′= 𝑒− 𝐱−𝐱′2\n2𝜎2 \n2-layer  Neural  Network  (meno  utilizzato) : \n \n𝐾𝐱,𝐱′=𝑡𝑎𝑛ℎ𝜈𝐱∙𝐱′+𝑎 \n \n𝜈 ed 𝑎 (iperparametri ) devono  essere  scelti  opportunamente : \nuna possibile  scelta  è: 𝜈=1,𝑎=1 \nIl numero  di hidden  units  e i pesi sono  determinati  \nautomaticamente  da SVM  \n \n \n \n \n \n \n \n \n Il kernel 2-layer Neural Network è anche detto kernel Sigmoid",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#12": "SVM Non Lineari: Kernel Function\n!Inoltre spesso viene chiamato kernel lineare il kernel\nche equivale a utilizzare una funzione di mapping φtale \nche φ(x)=x, cioè a nonutilizzare un kernelK(x,x')=(x⋅x')",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#13": "SVM Non Lineari: Esempi\n13 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM Non lineari: Esempi  \nPolinomio 𝑞 = 2 Polinomio 𝑞 = 10 \nRBF V = 1 RBF V = 0.2  All’aumentare del valore dell’iperparametro q (grado del polinomio)\n!aumenta il numero di support vector (i.e., complessità del problema)\n!diminuisce il numero di errori sul Traning Set (da 1 a 0)\n!diminuisce il margine di separazione (i.e., capacità di generalizzazione)",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#14": "SVM Non Lineari: Esempi\n13 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM Non lineari: Esempi  \nPolinomio 𝑞 = 2 Polinomio 𝑞 = 10 \nRBF V = 1 RBF V = 0.2  \nAl diminuire del valore dell’iperparametro !(deviazione standard)\n!aumenta il numero di support vector (i.e., complessità del problema)\n!diminuisce il numero di errori sul Traning Set (da 1 a 0)\n!diminuisce il margine di separazione (i.e., capacità di generalizzazione)",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#15": "Sommario\n!SVM Lineari: Pattern Linearmente Separabili e Non\n!SVM Non Lineari\n!SVM Multiclasse",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#16": "SVM: Multiclasse\n14 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: estensione multiclasse  \nSVM  è in grado  di determinare  la superficie  di separazione  tra 2 \nclassi  di pattern ; come  gestire  allora  i problemi  con più di 2 classi  ? \nSi tratta  di un problema  ancora  aperto  anche  se esistono  diverse  \nsoluzioni ; le più utilizzate  sono : \n \nOne-Against -One: che studieremo  in seguito  nell’ambito  dei multi -\nclassificatori . \n \nOne-Against -All: \nDate  𝑠 classi , 𝑤1,𝑤2…𝑤𝑠 \nPer ogni classe  𝑤𝑘, si determina  con SVM  la superficie  di \nseparazione  tra i pattern  di 𝑤𝑘 (etichettati  +1) da una parte,  e i \npattern  di tutte le rimanenti  classi  𝑤ℎ,ℎ≠𝑘 (etichettati  -1) \ndall’altra,  ottenendo  la funzione  𝐷𝑘𝐱 che indica  quanto  𝐱 è \ndistante  dalla  superficie  decisionale  in direzione  di 𝑤𝑘. \nMaggiore  è 𝐷𝑘𝐱 più confidenti  siamo  dell’appartenenza  di 𝐱 a \n𝑤𝑘.  \nAl termine  del training,  si assegna  il pattern  𝐱 alla classe  𝑘∗ per \ncui è massima  la distanza  dalla  superficie  decisionale :  \n𝑘∗=𝑎𝑟𝑔 𝑚𝑎𝑥\n𝑘𝐷𝑘𝐱 \nNota : È necessario  eseguire  𝑠 training  SVM   \n \n \n !One-Against -One\n!One-Against -All",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#17": "One-Against -One\nE’ingenere piùaccurato diOne-Against -All(vedi dopo), anche se\nmeno efficiente inquanto richiede l’addestramento diunnumero\nmaggiore diclassificatori\n24 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  One-Against -One \nL’approccio  One-Against -One, consente  di risolvere  un problema  di \nclassificazione  multi -classe , attraverso  classificatori  binari . \nÈ l’approccio  adottato  dalla  libreria  LIBSVM  (usata  in BioLab ). \nSe 𝑠 sono  le classi  del problema,  si addestrano  \n𝑠×𝑠−1/2 classificatori  binari : tutte le possibili  coppie , \nindipendentemente  dall’ordine . \nDurante  la classificazione,  il pattern  𝐱 viene  classificato  da ogni \nclassificatore  binario,  che assegna  un voto alla classe  (tra le due) \npiù probabile .  \nAl termine  il pattern  𝐱 è assegnato  alla classe  che ha ricevuto  più \nvoti (majority  vote rule).  \n \nÈ in genere  più accurato  di One-Against -All (discusso  in precedenza  \nper SVM),  anche  se meno  efficiente  in quanto  richiede  \nl’addestramento  di un numero  maggiore  di classificatori . \n \n (Numero di combinazioni di classe k=2)",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#18": "14prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneSVM: estensione multiclasse\nSVM èingrado dideterminare lasuperficie diseparazione tra2\nclassi dipattern ;come gestire allora iproblemi conpiùdi2classi ?\nSitratta diunproblema ancora aperto anche seesistono diverse\nsoluzioni ;lepiùutilizzate sono :\nOne\n-Against -One:chestudieremo inseguitonell’ambito deimulti -\nclassificatori .\nOne\n-Against -All:\nDate\n𝑠classi ,𝑤1,𝑤2…𝑤𝑠\nPer\n ogni classe𝑤𝑘,sidetermina con SVM lasuperficie di\nseparazione traipattern di𝑤𝑘(etichettati +1)daunaparte, ei\npattern ditutte lerimanenti classi𝑤ℎ,ℎ≠𝑘(etichettati -1)\ndall’altra, ottenendo lafunzione 𝐷𝑘𝐱cheindica quanto𝐱è\ndistante dalla superficie decisionale indirezione di𝑤𝑘.\nMaggiore è𝐷𝑘𝐱piùconfidenti siamodell’appartenenza di𝐱a\n𝑤𝑘.\nAl\ntermine deltraining, siassegna ilpattern𝐱allaclasse𝑘∗per\ncuièmassima ladistanza dalla superficie decisionale :\n𝑘∗=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑘𝐷𝑘𝐱\nNota :Ènecessario eseguire 𝑠training SVMSVM: Multiclasse\n(con x pattern da classificare \ne k=1, 2 ... s)  \n14prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneSVM: estensione multiclasse\nSVM èingrado dideterminare lasuperficie diseparazione tra2\nclassi dipattern ;come gestire allora iproblemi conpiùdi2classi ?\nSitratta diunproblema ancora aperto anche seesistono diverse\nsoluzioni ;lepiùutilizzate sono :\nOne\n-Against -One:chestudieremo inseguitonell’ambito deimulti -\nclassificatori .\nOne\n-Against -All:\nDate\n𝑠classi ,𝑤1,𝑤2…𝑤𝑠\nPer\nogni classe𝑤𝑘,sidetermina con SVM lasuperficie di\nseparazione traipattern di𝑤𝑘(etichettati +1)daunaparte, ei\npattern ditutte lerimanenti classi𝑤ℎ,ℎ≠𝑘(etichettati -1)\ndall’altra, ottenendo lafunzione 𝐷𝑘𝐱cheindica quanto𝐱è\ndistante dalla superficie decisionale indirezione di𝑤𝑘.\nMaggiore è𝐷𝑘𝐱piùconfidenti siamodell’appartenenza di𝐱a\n𝑤𝑘.\nAl\ntermine deltraining, siassegna ilpattern𝐱allaclasse𝑘∗per\ncuièmassima ladistanza dalla superficie decisionale :\n𝑘∗=𝑎𝑟𝑔𝑚𝑎𝑥\n𝑘𝐷𝑘𝐱\nNota :Ènecessario eseguire 𝑠training SVM",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#19": "SVM: Implementazione\n15 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: implementazione  \n \nIl training  di SVM,  richiede  algoritmi  numerici  non banali  in grado  di \nrisolvere  un problema  di programmazione  quadratica .  \nAlcune  implementazioni  sono  disponibili  on-line. Ad esempio : \nLIBSVM  (wrapped  da BioLab ) \nhttp://www .csie.ntu.edu.tw/~cjlin/libsvm  \nAttenzione  i Kernel  (RBF,  ecc.) sono  parametrizzati  in modo  \ndiverso  da quello  comune  (vedi  file Readme .txt di LibSvm  e [1]). \nIn particolare  si fa uso del generico  parametro  gamma  (𝛾) per \nregolare  la complessità  della  superficie  decisionale . \nAumentando  γ la superficie  può assumere  forme  più \ncomplesse . \nN.B. Con kernel  RBF γ opera  in modo  inverso  rispetto  a 𝜎. \nInserito  γ anche  nel kernel  polinomiale  (oltre  al grado  polinomio  e \nCoef 0) \nPer la classificazione  multiclasse  utilizza  internamente  One-\nAgainst -One [2] (accurato  ma inefficiente  per molte  classi ). \n[1] C.W. Hsu, C.C. Chang,  and C.J. Lin, A Practical  Guide  to Support  Vector  \nClassification,  disponibile  sul sito web di LIBSVM  \n[2] C.C. Chang  and C.J. Lin. LIBSVM : a library  for support  vector  machines . \nACM  Transactions  on Intelligent  Systems  and Technology,  2:27:1--27:27, \n2011 , disponibile  sul sito web di LIBSVM  \n \nLIBLINEAR  - https ://www .csie.ntu.edu.tw/~cjlin/liblinear / \nStessi  autori  di LIBSVM,  consigliata  nel caso  lineare  per elevata  \ndimensionalità  ed elevato  numero  di pattern . \nSVM -light - http://svmlight .joachims .org \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#2": "Kernel Trick\nSVM Classification\nOvviamente le SVM possono essere\nusate per separare classi che non\npotrebbero essere separate con un\nclassificatore lineare, altrimenti la loro\napplicazione a casi di reale interesse\nnon sarebbe possibile. In questi casi le\ncoordinate degli oggetti sono mappate\nin uno spazio detto “feature space”\nutilizzando funzioni non lineare,\nchiamate “feature function” ϕ.Ilfeature\n chiamate “feature function” ϕ.Ilfeature\nspace è uno spazio fortemente\nmultidimensionale in cui le due classi\npossono essere separate con un\nclassificatore lineare.\nQuindi lo spazio iniziale viene rimappato\nnel nuovo spazio, a questo punto viene\nidentificato il classificatore che poi viene\nriportato nello spazio iniziale, come\nillustrato in figura.Fonte: Stefano Cavuoti\nSVM Classification\nLa funzione ϕcombina quindi lo spazio iniziale (le \ncaratteristiche originali degli oggetti) nello spaz io \ndelle features che potrebbe in linea di principio \navere anche dimensione infinita. A causa del fatto \nche questo spazio ha molte dimensioni non \nsarebbe pratico utilizzare una funzione generica \nper trovare l’iperpiano di separazione, quindi \nvengono usate delle funzioni dette “kernel” e si \nidentifica la funzione ϕtramite una combinazione \ndi funzioni di kernel.\nFonte: http://www.ivanciuc.org/\ndi funzioni di kernel.\nL’implementazione più famosa delle SVM (libSVM) \nusa quattro possibili kernel:\nFonte: http://www.imtech.res.in/raghava/rbpred/svm. jpg\nKernel trick–(1)\n•Possiamo trasformare i dati nell' input space in un nuovo \nspazio, detto feature space , a più alta dimensionalità\n•I vettori che prima non erano linearmente separabili hanno più \nprobabilità di esserlo in uno spazio a più dimensioni\n25\nIdea:trasformare idati nell’Input Space inunnuovo spazio, detto\nFeature Space ,apiùaltadimensionalità .\nIpattern che prima non erano linearmente separabili nello spazio di\npartenza hanno piùprobabilità diesserlo inuno spazio apiùdimensioni,\nessendo ilnumero digradi dilibertà piùelevato .",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#20": "SVM: Implementazione\n15 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: implementazione  \n \nIl training  di SVM,  richiede  algoritmi  numerici  non banali  in grado  di \nrisolvere  un problema  di programmazione  quadratica .  \nAlcune  implementazioni  sono  disponibili  on-line. Ad esempio : \nLIBSVM  (wrapped  da BioLab ) \nhttp://www .csie.ntu.edu.tw/~cjlin/libsvm  \nAttenzione  i Kernel  (RBF,  ecc.) sono  parametrizzati  in modo  \ndiverso  da quello  comune  (vedi  file Readme .txt di LibSvm  e [1]). \nIn particolare  si fa uso del generico  parametro  gamma  (𝛾) per \nregolare  la complessità  della  superficie  decisionale . \nAumentando  γ la superficie  può assumere  forme  più \ncomplesse . \nN.B. Con kernel  RBF γ opera  in modo  inverso  rispetto  a 𝜎. \nInserito  γ anche  nel kernel  polinomiale  (oltre  al grado  polinomio  e \nCoef 0) \nPer la classificazione  multiclasse  utilizza  internamente  One-\nAgainst -One [2] (accurato  ma inefficiente  per molte  classi ). \n[1] C.W. Hsu, C.C. Chang,  and C.J. Lin, A Practical  Guide  to Support  Vector  \nClassification,  disponibile  sul sito web di LIBSVM  \n[2] C.C. Chang  and C.J. Lin. LIBSVM : a library  for support  vector  machines . \nACM  Transactions  on Intelligent  Systems  and Technology,  2:27:1--27:27, \n2011 , disponibile  sul sito web di LIBSVM  \n \nLIBLINEAR  - https ://www .csie.ntu.edu.tw/~cjlin/liblinear / \nStessi  autori  di LIBSVM,  consigliata  nel caso  lineare  per elevata  \ndimensionalità  ed elevato  numero  di pattern . \nSVM -light - http://svmlight .joachims .org \n \n \n 15 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: implementazione  \n \nIl training  di SVM,  richiede  algoritmi  numerici  non banali  in grado  di \nrisolvere  un problema  di programmazione  quadratica .  \nAlcune  implementazioni  sono  disponibili  on-line. Ad esempio : \nLIBSVM  (wrapped  da BioLab ) \nhttp://www .csie.ntu.edu.tw/~cjlin/libsvm  \nAttenzione  i Kernel  (RBF,  ecc.) sono  parametrizzati  in modo  \ndiverso  da quello  comune  (vedi  file Readme .txt di LibSvm  e [1]). \nIn particolare  si fa uso del generico  parametro  gamma  (𝛾) per \nregolare  la complessità  della  superficie  decisionale . \nAumentando  γ la superficie  può assumere  forme  più \ncomplesse . \nN.B. Con kernel  RBF γ opera  in modo  inverso  rispetto  a 𝜎. \nInserito  γ anche  nel kernel  polinomiale  (oltre  al grado  polinomio  e \nCoef 0) \nPer la classificazione  multiclasse  utilizza  internamente  One-\nAgainst -One [2] (accurato  ma inefficiente  per molte  classi ). \n[1] C.W. Hsu, C.C. Chang,  and C.J. Lin, A Practical  Guide  to Support  Vector  \nClassification,  disponibile  sul sito web di LIBSVM  \n[2] C.C. Chang  and C.J. Lin. LIBSVM : a library  for support  vector  machines . \nACM  Transactions  on Intelligent  Systems  and Technology,  2:27:1--27:27, \n2011 , disponibile  sul sito web di LIBSVM  \n \nLIBLINEAR  - https ://www .csie.ntu.edu.tw/~cjlin/liblinear / \nStessi  autori  di LIBSVM,  consigliata  nel caso  lineare  per elevata  \ndimensionalità  ed elevato  numero  di pattern . \nSVM -light - http://svmlight .joachims .org \n \n \n15 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM: implementazione  \n \nIl training  di SVM,  richiede  algoritmi  numerici  non banali  in grado  di \nrisolvere  un problema  di programmazione  quadratica .  \nAlcune  implementazioni  sono  disponibili  on-line. Ad esempio : \nLIBSVM   \nhttp://www .csie.ntu.edu.tw/~cjlin/libsvm  \nAttenzione  i Kernel  (RBF,  ecc.) sono  parametrizzati  in modo  \ndiverso  da quello  comune  (vedi  file Readme .txt di LibSvm  e [1]). \nIn particolare  si fa uso del generico  parametro  gamma  (𝛾) per \nregolare  la complessità  della  superficie  decisionale . \nAumentando  γ la superficie  può assumere  forme  più \ncomplesse . \nN.B. Con kernel  RBF γ opera  in modo  inverso  rispetto  a 𝜎. \nInserito  γ anche  nel kernel  polinomiale  (oltre  al grado  polinomio  e \nCoef 0) \nPer la classificazione  multiclasse  utilizza  internamente  One-\nAgainst -One [2] (accurato  ma inefficiente  per molte  classi ). \n[1] C.W. Hsu, C.C. Chang,  and C.J. Lin, A Practical  Guide  to Support  Vector  \nClassification,  disponibile  sul sito web di LIBSVM  \n[2] C.C. Chang  and C.J. Lin. LIBSVM : a library  for support  vector  machines . \nACM  Transactions  on Intelligent  Systems  and Technology,  2:27:1--27:27, \n2011 , disponibile  sul sito web di LIBSVM  \n \nLIBLINEAR  - https ://www .csie.ntu.edu.tw/~cjlin/liblinear / \nStessi  autori  di LIBSVM,  consigliata  nel caso  lineare  per elevata  \ndimensionalità  ed elevato  numero  di pattern . \nSVM -light - http://svmlight .joachims .org \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#21": "SVM: Implementazione\n27LIBSVM :AL i b r a r yf o rS u p p o r tV e c t o rM a c h i n e s\nCHIH-CHUNG CHANG and CHIH-JEN LIN ,N a t i o n a lT a i w a nU n i v e r s i t y\nLIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package\nsince the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained\nwide popularity in machine learning and many other areas. In this article, we present all implementation\ndetails of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass\nclassiﬁcation probability estimates and parameter selection are discussed in detail.\nCategories and Subject Descriptors: I.5.2 [ Pattern Recognition ]: Design Methodology— Classiﬁer design\nand evaluation ;G . 1 . 6[ Numerical Analysis ]: Optimization— Quadratic programming methods\nGeneral Terms: Algorithms, Performance, Experimentation\nAdditional Key Words and Phrases: Classiﬁcation LIBSVM optimization regression support vector machines\nSVM\nACM Reference Format:\nChang, C.-C. and Lin, C.-J. 2011. LIBSVM : A library for support vector machines. ACM Trans. Intell. Syst.\nTechnol. 2, 3, Article 27 (April 2011), 27 pages.\nDOI=10.1145/1961189.1961199 http://doi.acm.org/10.1145/1961189.1961199\n1. INTRODUCTION\nSupport Vector Machines (SVMs) are a popular machine learning method for clas-\nsiﬁcation, regression, and other learning tasks. Since the year 2000, we have been\ndeveloping the package LIBSVM as a library for support vector machines.1LIBSVM is\ncurrently one of the most widely used SVM software. In this article,2we present all\nimplementation details of LIBSVM .H o w e v e r ,t h i sa r t i c l ed o e sn o ti n t e n dt ot e a c ht h e\npractical use of LIBSVM . For instructions of using LIBSVM ,s e et h e README ﬁle included\nin the package, the LIBSVM FAQ ,3and the practical guide by Hsu et al. [2003].\nLIBSVM supports the following learning tasks.\n(1) SVC: support vector classiﬁcation (twoclass and multiclass);\n(2) SVR: support vector regression.\n(3) One-class SVM.\n1The Web address of the package is at http://www.csie.ntu.edu.tw/ ∼cjlin/libsvm.\n2This LIBSVM implementation document was created in 2001 and has been maintained at\nhttp://www.csie.ntu.edu.tw/ ∼cjlin/papers/libsvm.pdf.\n3LIBSVM FAQ :h t t p : / / w w w . c s i e . n t u . e d u . t w / ∼cjlin/libsvm/faq.html.\nThis work was supported in part by the National Science Council of Taiwan via the grants NSC 89-2213-E-\n002-013 and NSC 89-2213-E-002-106.\nAuthors’ addresses: C.-C. Chang and C.-J. Lin (corresponding author), Department of Computer Science,\nNational Taiwan University, Taipei 106, Taiwan; email: cjlin@csie.ntu.edu.tw.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted\nwithout fee provided that copies are not made or distributed for proﬁt or commercial advantage and that\ncopies show this notice on the ﬁrst page or initial screen of a display along with the full citation. Copyrights for\ncomponents of this work owned by others than ACM must be honored. Abstracting with credit is permitted.\nTo copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this\nwork in other works requires prior speciﬁc permission and/or a fee. Permissions may be requested from\nPublications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1( 2 1 2 )\n869-0481, or permissions@acm.org.\nc⃝2011 ACM 2157-6904/2011/04-ART27 $10.00\nDOI 10.1145/1961189.1961199 http://doi.acm.org/10.1145/1961189.1961199\nACM Transactions on Intelligent Systems and Technology, Vol. 2, No. 3, Article 27, Publication date: April 2011.\n15prof. Davide Maltoni –Università di Bologna\nML\nClassificazioneSVM: implementazione\nIltraining diSVM, richiede algoritmi numerici nonbanali ingrado di\nrisolvere unproblema diprogrammazione quadratica .Alcune\nimplementazioni sono disponibili on-line:\nLIBSVM\n -http://www .csie.ntu.edu.tw/~cjlin/libsvm\nAttenzione\n iKernel (RBF, ecc.)sono parametrizzati inmodo\ndiverso daquello comune (vedi Readme .txtdiLibSvm e[1]).In\nparticolare sifausodelparametro gamma (𝛾)perregolare la\ncomplessità della superficie decisionale .Aumentando γla\nsuperficie puòassumere forme piùcomplesse .\nN.B.Con kernel RBFγopera inmodo inverso rispetto a𝜎.\nInserito\nγanche nelkernel polinomiale (oltre algrado polinomio e\nCoef 0)\nPer\n laclassificazione multiclasse utilizza internamente One-\nAgainst -One [2](accurato mainefficiente permolte classi ).\nWrapped\n daScikit -Learn→sklearn .svm.SVC\n[1]C.W.Hsu, C.C.Chang, andC.J.Lin,APractical Guide toSupport Vector\nClassification, disponibile sulsitoweb diLIBSVM\n[2]C.C.Chang andC.J.Lin.LIBSVM :alibrary forsupport vector machines .\nACM Transactions onIntelligent Systems andTechnology, 2:27:1--27:27,\n2011 ,disponibile sulsitoweb diLIBSVM\nLIBLINEAR\n -https ://www .csie.ntu.edu.tw/~cjlin/liblinear /\nStessi\n autori diLIBSVM, consigliata nelcaso lineare perelevata\ndimensionalità edelevato numero dipattern .\nWrapped\n daScikit -Learn→sklearn .svm.LinearSVC\nSVM\n -light -http://svmlight .joachims .org",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#22": "Esempi LIBSVM\n16 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  Esempi LibSvm  (1) \n«maschi -femmine»  \n \nLineare , 𝐶=10 Lineare , 𝐶=500 \nPolinomio  𝑞=3,𝐶=10 Polinomio  𝑞=3,𝐶=500 \nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezzaSVM Lineare",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#23": "Esempi LIBSVM\n16 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  Esempi LibSvm  (1) \n«maschi -femmine»  \n \nLineare , 𝐶=10 Lineare , 𝐶=500 \nPolinomio  𝑞=3,𝐶=10 Polinomio  𝑞=3,𝐶=500 \nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nSVM Non Lineare (Kernel Polinomiale)",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#24": "Esempi LIBSVM\n17 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  RBF, γ=5,𝐶=10 RBF,γ=5,𝐶=500 \nRBF,γ=10,𝐶=10 RBF,γ=10,𝐶=500 \nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezzaEsempi LibSvm  (2) \n«maschi -femmine»  \n \nSVM Non Lineare (Kernel RBF)",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#25": "Esempi LIBSVM\n17 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  RBF, γ=5,𝐶=10 RBF,γ=5,𝐶=500 \nRBF,γ=10,𝐶=10 RBF,γ=10,𝐶=500 \nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezza\nPesoAltezzaEsempi LibSvm  (2) \n«maschi -femmine»  \n \nSVM Non Lineare (Kernel RBF)",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#26": "Esempi LIBSVM\n18 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  Lineare , 𝐶=100 \nPolinomio  𝑞=7,𝐶=100 RBF,γ=5,𝐶=100 Polinomio  𝑞=2,𝐶=100 \nEsempi LibSvm  (3) \nmulticlasse  \n \nSVM Lineare e Non Lineare (Kernel Polinomiale)\nCaso Multiclasse",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#27": "Esempi LIBSVM\n18 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  Lineare , 𝐶=100 \nPolinomio  𝑞=7,𝐶=100 RBF,γ=5,𝐶=100 Polinomio  𝑞=2,𝐶=100 \nEsempi LibSvm  (3) \nmulticlasse  \n \nSVM Non Lineare (Kernel Polinomiale e Kernel RBF)\nCaso Multiclasse",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#28": "Esempi LIBSVM\nUna semplicissima applicazione sviluppata\ndaicreatori della libreria LIBSVM che ne\nillustra ilfunzionamento èdisponibile allink\nseguente :\nhttps://www.csie.ntu.edu.tw/~cjlin/libsvm/\nInparticolare, cliccando col mouse si\ntracciano dei punti sullo schermo,\npremendo suChange sicambia laclasse (il\ncolore deipunti relativi) ;infine, premendo\nsuRun, una semplice SVM attribuisce al\npiano l’appartenenza alle varie classi\nmostrandole colorate inmaniera diversa .\nLIBSVM is an integrated software for support vector classification, (C -SVC, nu -SVC), \nregression (epsilon -SVR, nu -SVR) and distribution estimation (one -class SVM). \nIt supports multi -class classification. ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#29": "Esempi LIBSVM\nThis isasimple graphical interface which\nshows how SVM separate data inaplane .\nYou canclick inthewindow todraw data\npoints .Use \"change\" button tochoose\nclass 1,2or3(i.e.,uptothree classes are\nsupported), \"load\" button toload data from\nafile,\"save\" button tosave data toafile,\n\"run\" button toobtain anSVM model, and\n\"clear\" button toclear thewindow .Youcan\nenter options inthebottom ofthewindow,\nthesyntax ofoptions isthesame as`svm -\ntrain' .Note that\"load\" and\"save\" consider\ndata inthe classification but not the\nregression case .Each data point hasone\nlabel (the color) which must be1,2,or3\nand two attributes (x-axis and y-axis\nvalues) in[0,1].Type `make' inrespective\ndirectories tobuild them ...",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#3": "!Dato un insieme              , una funzione                           è un kernel se \nrisulta che  \ndove                     e   è uno spazio di Hilbert\n!Lo Spazio di Hilbert è uno spazio vettoriale che generalizza la nozione \ndi Spazio Euclideo\n!φ è la funzione di mapping dall’Input Space al Feature Space\n!Si può dimostrare che una funzione                           è un kernel se, e \nsolo se, comunque si scelgano r elementi x1, x 2, ..., x r∈X, la matrice \nK=[k(x i,xj)]i,j=1,...,r è simmetrica e semidefinita positiva\n!Ogni matrice simmetrica semidefinita positiva ha tutti gli autovalori non \nnegativik(x,y)=φ(x),φ(y)   ∀x,y∈XX⊂ ℜ k:X×X→ℜ\nϕ:X→ΗΗ\nk:X×X→ℜKernel",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#30": "Esempi LIBSVM",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#31": "Esempi LIBSVM\nQ:What isthedifference between\nnu-SVC and C-SVC? Basically they\narethesame thing butwith different\nparameters .The range ofCisfrom\nzero toinfinity but nu isalways\nbetween [0,1].Anice property ofnuis\nthatitisrelated totheratio ofsupport\nvectors and theratio ofthetraining\nerror.\nAdditionally one-class SVM type is\nsupported fordistribution estimation .\nThe one-class SVM type gives the\npossibility tolearn from justone class\nofexamples and later ontest ifnew\nexamples match theknown ones .",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#32": "SVM in pratica\n19 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM in pratica  \nLineare  o Non-lineare?   \nse la dimensionalità  𝑑 dello  spazio  è molto  elevata  (es. 5000  \nfeature ) si utilizza  generalmente  SVM  lineare . Infatti  in uno \nspazio  così grande  i pattern  sono  tipicamente  molto  sparsi  e \nanche  «semplici»  iperpiani  sono  in grado  di separare  le \nclassi  efficacemente . Il solo iperparametro  da tarare  è 𝐶.  \nper bassa  dimensionalità  (es. 20 feature ) la scelta  primaria  è \nSVM  non lineare  con kernel  RBF. Gli iperparametri  da tarare  \nsono  𝐶 e V (o γ se si utilizza  LIBSVM ). \nPer media  dimensionalità  (es. 200 features ) in genere  si \nprovano  entrambe  le tipologie  (i.e., anche  questa  scelta  \ndiventa  un iperparametro ). \nCome  sempre  gli iperparametri  si tarano  su un validation  set \nseparato,  oppure  attraverso  cross -validation  sul training  set. \n \nCome  gestire  il caso  multi -classe?   \nTipicamente  ci si affida  alla soluzione  disponibile  nella  libreria  \nutilizzata  (One-Agaist -One per LIBSVM ). \nSe però il numero  di classi  è molto  elevato,  il costo  può \ndiventare  inaccettabile  per certe  applicazioni . In questo  caso  \nOne-Against -All diventa  la scelta  obbligata .  \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#33": "SVM in pratica\n",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#34": "SVM in sintesi\n!Vantaggi\n!Si basa su una teoria ben fondata\n!Presenta eccellenti proprietà di generalizzazione\n!La funzione obiettivo non presenta minimi locali\n!Può essere impiegata per individuare funzioni discriminanti non lineari\n!La complessità del classificatore è caratterizzata dal numero di su pport \nvector piuttosto che dalla dimensionalità dello spazio trasformato\n!Svantaggi\n!Tende ad essere più lenta rispetto ad altri metodi\n!La programmazione quadratica è computazionalmente onerosa ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#35": "!S.J. Russell & P. Norvig, Artificial Intelligence: A Modern Approach (4 ed.) , \nPearson, 2020.\n!C. Burges, A Tutorial on Support Vector Machines for Pattern Recognition , \n1998.\n!S. Gunn, Support Vector Machines for Classification and Regression , 1998.\n!D. Maltoni, Machine Learning , Università di Bologna, 2017.\n!C.W. Hsu, C.C. Chang, and C.J. Lin, A Practical Guide to Support Vector \nClassification , Last updated: May 19, 2016.\n!C.C. Chang and C.J. Lin, LIBSVM: A Library for Support Vector Machines , ACM \nTransactions on Intelligent Systems and Technology, 2:27:1 —27:27, 2011.\n!G. Raiconi, Support Vector Machines: Concetti ed Esempi , Università di \nSalerno 2016.\n!R.O. Duda, P.E. Hart, and D.G. Stork. 2000. Pattern Classification (2nd \nEdition). Wiley -Interscience, New York, NY, USA. \n!C.M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.Riferimenti",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#4": "Kernel\n!Uno spazio di Hilbert     è uno spazio vettoriale Hreale o \ncomplesso sul quale è definito un prodotto interno tale che, detta d \nla distanza indotta da       su H, lo spazio metrico ( H, d) sia completo\n!Uno spazio metrico è un insieme di elementi, detti punti , nel quale è \ndefinita una distanza , detta anche metrica (lo spazio metrico più \ncomune è lo spazio euclideo di dimensione 1, 2 o 3)\n!Uno spazio metrico completo è uno spazio metrico in cui tutte le \nsuccessioni di Cauchy sono convergenti ad un elemento dello spazio\n!Una successione di Cauchy è una successione tale che, comunque si \nfissi una distanza arbitrariamente piccola ε> 0, da un certo punto in poi \ntutti gli elementi della successione hanno distanza reciproca inferiore \nad ε=(H,⋅,⋅) Η\n⋅,⋅\n⋅,⋅",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#5": "SVM Non Lineari\n11 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM Non lineari  \nSVM  prevede  un’importante  estensione  della  teoria  inizialmente  \nsviluppata  per iperpiani,  al caso  (non lineare ) di separazione  dei \npattern  con superfici  anche  molto  complesse . Tutto  ciò avviene  in \nmodo  molto  semplice :    \nViene  definito  un mapping  Φ non lineare  dei pattern  dallo  spazio  \ndi partenza  𝑑 verso  uno spazio  𝑚 a più alta dimensionalità  \n(𝑚>𝑑): \nΦ:𝑑→𝑚,Φ𝐱=𝑔1𝐱,𝑔2𝐱,…𝑔𝑚𝐱 \nNello  spazio  𝑚, dove  maggiori  sono  i gradi  di libertà , i pattern  \nΦ𝐱1,Φ𝐱2,…Φ𝐱𝑛 possono  essere  più facilmente  separati  da \nun iperpiano  utilizzando  la teoria  nota. Ciò equivale  a separare  i \npattern  𝐱1,𝐱2,…𝐱𝑛 in 𝑑 con superfici  arbitrariamente  complesse .  \nAnalizzando  la formulazione  del problema  lagrangiano -duale , si nota \nche i vettori  del training  set appaiono  solo in forma  di prodotti  scalari  \ntra coppie  di vettori . Questa  proprietà  (fondamentale ) permette  di \nevitare  la manipolazione  di vettori  nello  spazio  𝑚 (𝑚 può facilmente  \nraggiungere  dimensione  108 e anche  assumere  valore  infinito) . \nInfatti,  per opportuni  mapping  Φ è possibile  ricondurre  il prodotto  \nscalare  di due pattern  mappati  nello  spazio  𝑚 a una funzione  𝐾 \n(detta  Kernel ) dei due pattern  originali  nello  spazio  𝑑.  \nΦ𝐱∙Φ𝐱′=𝐾𝐱,𝐱′ \nCiò consente  di risolvere  il problema  di ottimizzazione  senza  \nparticolari  complicazioni  rispetto  al caso  lineare . Una volta  \ndeterminati  gli𝛼𝑖∗, la superficie  di separazione  (regola  di \nclassificazione)  è esprimibile  come :  \n𝐷𝐱= 𝛼𝑖∗\n𝑖=1…𝑛𝑦𝑖 𝐾𝐱,𝐱𝑖+𝑏∗ \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#6": "SVM Non Lineari\n!In altri termini, possiamo calcolare w*, b*e funzione di \ndecisione in maniera analoga a quanto visto in precedenza\n!Utilizzando il kernel evitiamo l’operazione costosa di \ntrasformazione e prodotto interno nello spazio trasformato, \nessendo il kernel una funzione dei pattern originali definiti \nnell’Input Space ℜd\n!Possiamo inoltre effettuare trasformazioni in spazi a \ndimensione infinita\n!In sintesi, le proprietà del prodotto scalare consentono di \nesprimere il prodotto scalare dei pattern immagine (\"(x) ∈ℜm) \ncorrispondenti ai pattern in input (x ∈ℜd) semplicemente come \nfunzioni kernel dei pattern in input (x ∈ℜd)",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#7": "Kernel Trick\n!Pattern linearmente non separabili possono diventare \nlinearmente separabili se trasformati, o mappati, in uno spazio \ndimensionale superiore\n!Il calcolo della matematica vettoriale (cioè i prodotti scalari) in \nuno spazio dimensionale assai elevato è costoso dal punto di \nvista computazionale\n!Il trucco del kernel consente di calcolare in modo efficiente \nprodotti scalari di dimensioni molto elevate\n!Esso consente di mappare in pattern in input in modo implicito \nin uno spazio dimensionale più elevato (possibilmente infinito) \ncon un overhead computazionale ridotto\n!“In modo implicito”, in quanto i vettori a dimensionalità \nsuperiore non sono mai effettivamente costruiti",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#8": "Kernel Function: Esempio\n!Supponiamo di avere i seguenti due vettori bidimensionali \n\"=$!,$\"e &='!,'\"\n!La funzione seguente ((*)mappa vettori bidimensionali in vettori \ntridimensionali\n!Il modo standard per calcolare\nè prima mappare i pattern in input nello spazio delle feature e poi \neseguire il prodotto scalare nello spazio a dimensione più elevata\n!Tuttavia, il prodotto scalare può essere effettuato interamente nello \nspazio originale a due dimensioni(,=-!\"\n2-!-\"\n-\"\"\n(\"*(&\n(\"*(&=$!\"'!\"+2$!$\"'!'\"+$\"\"'\"\"=\"*&\"",
    "data_test\\rootfolder\\università\\MachineLearning\\35-SVM(2)-sbloccato.pdf#9": "SVM Non Lineari: Kernel Function\n12 prof. Davide Maltoni  – Università di Bologna  \nML \nClassificazione  SVM Non lineari: Kernel  functions  \nPolinomio  di grado   𝑞 (iperparametro ): \n Le componenti  𝑔𝑖𝐱,𝑖=1..𝑚 sono  ottenute  come  tutte le \npossibili  combinazioni  di elevamento  a potenze  d 𝑞 delle  \ncomponenti  di 𝐱. Ad esempio  per 𝑑=2,𝑞=2: \nΦ𝐱=Φ𝑥1,𝑥2=1,𝑥1,𝑥2,𝑥1𝑥2,𝑥12,𝑥22,𝑥12𝑥2,𝑥1𝑥22,𝑥12𝑥22  \n e quindi  𝑚=9.  \n Si dimostra  che: \n𝐾𝐱,𝐱′=𝐱∙𝐱′+1𝑞 \nRadial  Basis  Function  (RBF) di ampiezza  𝜎 (iperparametro ): \n \n𝐾𝐱,𝐱′= 𝑒− 𝐱−𝐱′2\n2𝜎2 \n2-layer  Neural  Network  (meno  utilizzato) : \n \n𝐾𝐱,𝐱′=𝑡𝑎𝑛ℎ𝜈𝐱∙𝐱′+𝑎 \n \n𝜈 ed 𝑎 (iperparametri ) devono  essere  scelti  opportunamente : \nuna possibile  scelta  è: 𝜈=1,𝑎=1 \nIl numero  di hidden  units  e i pesi sono  determinati  \nautomaticamente  da SVM  \n \n \n \n \n \n \n \n \n ",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#0": "MATLAB (MATrix LABoratory)\n!Per installare il sofftware, accedere all’Area Sistemi Informativi di Roma Tre \ndisponibile al seguente indirizzo: http://asi.uniroma3.it/ ---> cliccare su ‘servizi \nagli studenti’ ---> scorrere fino in fondo e cliccare su ‘MathWorks’\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#1": "MATLAB\nManualetto di Matlabr\nL. Scuderi\n1 Comandi d’avvio\nPer avviare Matlab in ambiente Windows ` e su éciente selezionare con il mouse l’icona corrispon-\ndente. In ambiente MsDos o in ambiente Unix basta digitare matlab e premere il tasto di invio (o\nenter, return, ...). Il simbolo >>che compare, ` e il prompt di Matlab . Per eseguire un comando\ndigitato occorre premere il tasto di invio. Per terminare la sessione di lavoro occorre digitare il\ncomando exit oppure quit .\nTabella 1. Alcuni comandi per gestire una sessione di lavoro.\nComando Signiﬁcato\nhelp per visualizzare tutti gli argomenti presenti\nhelp arg per visualizzare informazioni su arg\ndoc arg per visualizzare dettagliate informazioni su arg\nclc per cancellare il contenuto della ﬁnestra di lavoro\n; per non visualizzare il risultato di un’istruzione\n... per continuare a scrivere un’istruzione nella riga successiva\nwho per visualizzare le variabili poste in memoria\nwhos per visualizzare informazioni sulle variabili poste in memoria\nclear per cancellare tutte le variabili dalla memoria\nclear var1 var2 per cancellare le variabili var1 evar2 dalla memoria\n2 Le variabili in Matlab\nI nomi delle variabili possono essere lunghi al massimo 32 caratteri. I caratteri utilizzabili sono\nle lettere (maiuscole e minuscole), i numeri e il carattere “ _” (underscore). Un nome di variabile\ndeve cominciare con un carattere alfabetico (a-z, A-Z). Matlab distingue tra lettere maiuscole\ne minuscole (ad esempio i nomi a1edA1rappresentano variabili diverse). La variabile si crea\nautomaticamente nel momento in cui si assegna ad essa un valore o il risultato di un’espressione.\nL’assegnazione avviene mediante il simbolo =secondo la seguente sintassi\n>> nome_variabile=espressione\nSe la variabile che si vuole creare ` e di tipo stringa occorre racchiudere espressione tra una\ncoppia di apici. Nella tabella 2 abbiamo riportato alcune variabili scalari predeﬁnite.\nMatlab lavora con sedici cifre signiﬁcative. Tuttavia, in output una variabile intera viene\nvisualizzata generalmente in un formato privo di punto decimale, mentre una variabile reale (non\nintera) viene visualizzata solo con quattro cifre decimali. Se si vuole modiﬁcare il formato di output\nsi pu` o utilizzare uno dei comandi della tabella 3. Per visualizzare tutte le sedici cifre impiegate da\nMatlab ` e necessario attivare il comando format long e .\nNella tabella 4 abbiamo riportato le principali operazioni eseguibili sulle variabili scalari. Oltre\nalle operazioni di base, in Matlab sono presenti anche le funzioni predeﬁnite riportate nella tabella\n5.\nGli elementi di un vettore vanno digitati tra parentesi quadre; gli elementi di un vettore riga\nvanno separati con uno spazio oppure una virgola, quelli di un vettore colonna con un punto e virgola\n1!Per avviare Matlab in ambiente Windows o Mac è sufficiente selezionare con \nil mouse l’icona corrispondente!In ambiente MsDos o in ambiente Unix basta digitare matlab e premere il \ntasto Invio (o Enter, Return, ... )!Il simbolo >> che compare nella Command Window è il prompt di Matlab!Per eseguire un comando digitato occorre premere il tasto Invio!Per terminare la sessione di lavoro occorre digitare il comando exit o quit!Seguono alcuni comandi per gestire una sessione di lavoro",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#10": "Esempi in MATLAB\n!Overfitting con SVM Non Lineare (Kernel Gaussiano)\nσ=1/15, C=106Esempi in MATLAB –(7)\n49\n•Vediamo invece un esempio di overfitting utilizzando il kernel\ngaussiano con 𝜎=1/15e 𝐶=106:\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#11": "Esempi in MATLAB\n% … caricamento dei dati come nel caso precedente …%\nload fisheriris; % carica le osservazioni in ‘meas’ e le label in ‘species’\nX=meas(:,3:4); % estrae lunghezza e larghezza dei petali\ny = ~strcmp(species,'virginica'); % label 0 se Iris viginica, 1 se altre specie\n% … caricamento dei dati come nel caso precedente …%\nSVMModel=fitcsvm(X,y, 'BoxConstraint' ,1e6,'KernelFunction' ,'gaussian' ,'KernelScale' ,\n1/15);\n% … disegno dello scatter plot come nel caso precedente …%\nfigure\ngscatter(X(:,1),X(:,2),y); % scatter plot dei vettori di training\nhold on\nsv = SVMModel.SupportVectors;\nplot(sv(:,1),sv(:,2),'ko','MarkerSize',10) % cerchia i vettori di supporto\nlegend('Iris virginica','Altre specie','Support vectors','Location','southeast')\naxis manual\n% … disegno dello scatter plot come nel caso precedente …%\nd=0.02; % intervallo utilizzato per generare la griglia di punti\n[x1Grid,x2Grid]=meshgrid(min(X(:,1)):d:max(X(:,1)),min(X(:,2)):d:max(X(:,2))); % \ngenerazione della griglia\nxGrid=[x1Grid(:),x2Grid(:)];\n[~,scores1]=predict(SVMModel,xGrid); % valutiamo l’output del modello nei punti \ndella griglia\ncontour(x1Grid,x2Grid,reshape(scores1(:,2),size(x1Grid)),[0 0],'k'); % plot del \nconfine di decisione",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#12": "Esempi in MATLAB\nσ=1/15, C=106\n% … caricamento dei dati come nel caso precedente …%\nSVMModel=fitcsvm(X,y, 'BoxConstraint' ,1e6, 'KernelFunction' ,'gaussian' ,'KernelScale' ,1/15);\n% … disegno dello scatter plot come nel caso precedente …%!Overfitting con SVM Non Lineare (Kernel Gaussiano)",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#13": "Esempi in MATLAB\nσ=1/15, C=106\n% … caricamento dei dati come nel caso precedente …%\nSVMModel=fitcsvm(X,y, 'BoxConstraint' ,1e6, 'KernelFunction' ,'gaussian' ,'KernelScale' ,1/15);\n% … disegno dello scatter plot come nel caso precedente …%\nC σ!Overfitting con SVM Non Lineare (Kernel Gaussiano)",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#14": "Esempi in MATLAB\nσ=1/15, C=106Esempi in MATLAB –(8)\n50\n𝐶=106,𝜎=1/15\n!Overfitting con SVM Non Lineare (Kernel Gaussiano)",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#15": "Esempi in MATLAB\n!Esempio di SVM Non Lineare (Kernel Gaussiano)\nσ=5, C=100\n% … caricamento dei dati come nel caso precedente …%\nSVMModel=fitcsvm(X,y, 'BoxConstraint' ,100, 'KernelFunction' ,'gaussian' ,'KernelScale’ ,5);\n% … disegno dello scatter plot come nel caso precedente …%",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#16": "Esempi in MATLAB\nσ=5, C=100\n% … caricamento dei dati come nel caso precedente …%\nSVMModel=fitcsvm(X,y, 'BoxConstraint' ,100, 'KernelFunction' ,'gaussian' ,'KernelScale’ ,5);\n% … disegno dello scatter plot come nel caso precedente …%\nC σ!Esempio di SVM Non Lineare (Kernel Gaussiano)",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#17": "Esempi in MATLAB\nσ=5, C=100\nEsempi in MATLAB –(8)\n51\n𝐶=100,𝜎=5!Esempio di SVM Non Lineare (Kernel Gaussiano)",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#18": "Classification Learner\nThe Classification Learner app lets you train models toclassify data using supervised\nmachine learning .\nUsing Classification Learner, you can perform common machine learning tasks such\nasinteractively exploring your data, selecting features, specifying validation schemes,\ntraining models, and assessing results .Choose from several classification types\nincluding decision trees ,support vector machines (SVM) ,and k-nearest neighbors ,\nand select from ensemble methods such asbagging, boosting, and random subspace .\nClassification Learner helps you choose thebest model foryour data byletting you\nperform model assessment and model comparisons using confusion matrices and\nROC curves .Export classification models tothe MATLAB workspace togenerate\npredictions onnew data, orgenerate MATLAB code tointegrate models into\napplications such ascomputer vision, signal processing, and data analytics .",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#19": "Classification Learner\nMatrice diConfusione (oTabella diErrata Classificazione) :rappresentazione\ndell'accuratezza diclassificazione statistica .\nOgni colonna della matrice rappresenta ivalori predetti, mentre ogni riga\nrappresenta ivalori reali (i.e.,l'elemento sulla riga iesulla colonna jèilnumero di\ncasi incui ilclassificatore haclassificato laclasse \"vera\" icome classe j).\nAttraverso questa matrice èosservabile seviè\"confusione\" nella classificazione di\ndiverse classi .\nCurve ROC (Receiver Operating Characteristic) :schemi grafici perunclassificatore\nbinario .\nLungo idue assi sipossono rappresentare lasensibilità e(1-specificità), come True\nPositive Rate (vero positivo) eFalse Positive Rate (falso positivo) .Inaltri termini, si\nstudiano irapporti fraveri allarmi (hitrate) efalsi allarmi alvariare diuna soglia .",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#2": "Esempi in MATLAB\n!Dataset multivariato Iris (Fisher's 1936 iris data)\n!50 esemplari di Iris Setosa , 50 di Iris Versicolor , 50 di Iris Virginica\n!4 feature: lunghezza e larghezza del sepalo, lunghezza e larghezza \ndel petalo\n!Sepalo: in botanica, ciascuno degli elementi, simili a foglioline verdi, \nche formano il calice del fiore\n!Utilizzeremo solo lunghezza e larghezza del sepalo (per poter \nvisualizzare i dati)\n!Funzione fitcsvm di MATLAB (metodo di ottimizzazione adottato: \nSequential Minimal Optimization (SMO))\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#20": "Classification Learner\n-Avviare Matlab\n-Scaricare il file ClassificationLearner_Example_Datasets.mat\n-Doppio clic sul file ClassificationLearner_Example_Datasets.mat\n-Selezionare i dati che si desidera importare \n[e.g., FisherIris (Numero di feature (predittori): 4, Numero di pattern (osservazioni): 150, Numero di \nclassi: 3); tabella di 150 righe (osservazioni) e 5 colonne (4 valori delle feature + classe)]\n-Digitare Finish per importarli nel Workspace di Matlab\n-Avviare l’app Classification Learner (vedi scheda APPS o digitare al prompt il \ncomando classificationLearner )",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#21": "Classification Learner\n-New Session —> From Workspace\n-Selezionare le feature e l’eventuale metodo di validazione\n-Start Session\n-Selezionare uno o più algoritmi di classificazione dalla barra superiore (per \nselezionare i parametri vedi Advanced)\n-Train e buon divertimento!\n-per ulteriori dettagli: https://it.mathworks.com/help/stats/classificationlearner -app.html",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#22": "CL Features\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#23": "CL Features\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#24": "CL Features\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#25": "CL Features\n-Linear SVM\n-Fine Gaussian SVM (Kernel Scale=0.35)\n-Medium Gaussian SVM (Kernel Scale=1.4)\n-Coarse Gaussian SVM (Kernel Scale=5.7)\n-Quadratic SVM (Kernel Polinomiale con grado=2)\n-Cubic SVM (Kernel Polinomiale con grado=3)\n-Kernel scale parameter, specified as the comma -separated pair consisting of 'KernelScale' and 'auto' or a positive \nscalar. The software divides all elements of the predictor matrix X by the value of KernelScale. Then, the software \napplies the appropriate kernel norm to compute the Gram matrix.\n-per ulteriori dettagli digitare al prompt il comando doc fitcsvm",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#26": "CL Features\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#27": "CL Features\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#28": "Altri Dataset\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#29": "Altri Dataset",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#3": "Dataset Fisher Iris\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#30": "Altri Dataset",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#31": "Altri Dataset",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#32": "Altri Dataset",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#33": "Classification Learner\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#34": "Classification Learner\n“Plotting the fisheriris data, you can see that sepal length\nand sepal width separate one ofthe classes well\n(setosa) .You need toplot other predictors (features) to\nsee ifyou can separate theother two classes .”",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#35": "Classification Learner\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#36": "Classification Learner\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#37": "Classification Learner\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#38": "Classification Learner\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#39": "Classification Learner\n“Plotting thefisheriris data, you can seethat petal length and petal\nwidth arethefeatures that separate theclasses best.”",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#4": "Esempi in MATLAB\nfitcsvm trains or cross -validates a support vector machine (SVM) model for \ntwo-class (binary) classification on a low -dimensional or moderate -\ndimensional predictor data set. fitcsvm supports mapping the predictor data \nusing kernel functions, and supports sequential minimal optimization (SMO), \niterative single data algorithm (ISDA), or L1 soft -margin minimization via \nquadratic programming for objective -function minimization. Matlab Help\nThe support vectors are observations that occur on or beyond their estimated \nclass boundaries. \nYou can adjust the boundaries (and, therefore, the number of support \nvectors) by setting a box constraint during training using the 'BoxConstraint'\nname -value ( C) pair argument.",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#40": "L'errore di classificazione è uguale, ma i classificatori sono molto diversi fra loro!Vera Assegnata\n1 1\n1 1\n1 1\n1 1\n1 1\n1 1\n1 1\n2 1\n2 1\n2 1Vera Assegna ta\n1 1\n1 2\n1 2\n1 1\n1 1\n1 1\n1 1\n2 2\n2 2\n2 1Classificatore 1  \n(assegn aun \noggetto sempre \nalla prima classe )\nErrore: 3/10 = 0.3 \n(30%)Classificatore 2Valutazione Prestazioni\nErrore: 3/10 = 0.3\n(30%)Il sempice errore di classificazione (i.e., numero di errori / numero totale di classificazioni) \nnon sempre ci permette di capire o confrontare completamente due classificatori.\nEsempio:",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#41": "Vera Assegna ta\n1 1\n1 2\n1 2\n1 1\n1 1\n1 1\n1 1\n2 2\n2 2\n2 1Elementi della classe 1 classificati  \ncome appartenenti alla classe 1\nElementi della classe 1classificati \ncome appartenenti alla classe 2\nElementi della classe 2 classificati  \ncome appartenenti alla classe 2Matrice di Confusione\nElementi della classe 2classificati  \ncome appartenenti alla classe 15 2\n1 2EsempioMatrice Mche ci dice come un classificatore opera rispetto alle diverse classi \nm(i,j) = numero di elementi della classe iclassificati come elementi della classe j\nIn altri termini, indice di riga i:valore reale , indice di colonna j:valore predetto",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#42": "Matrice di Confusione\nL'errore di classificazione può essere calcolato facilmente dalla matrice di \nconfusione\nLa somma di tutti gli elementi non appartenenti alla diagonale principale\nO, meglio, può essere calcolato come “1 -Accuracy”\nAccuracy: somma elementi diagonale principale / numero elementi totali",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#43": "Nel caso di problema a due classi la matrice di \nconfusione  assume una forma particolare (2 classi, \npositivi vs negativi)\nTrue  \nPositive  \n(TP)False  \nNegative  \n(FN)\nFalse  \nPositive  \n(FP)True  \nNegative  \n(TN)ESEMPIO: classificazione tra malati (positivi) e sani(negativi)\nCLASSIFICAZIONE CORRETTA:\nVeri positivi: pazienti malati classificati come malati\nVeri negativi: pazienti sani classificati come sani\nCLASSIFICAZIONE ERRATA:\nFalsi positivi: pazienti sani classificati come malati\nFalsi negativi: pazienti malati classificati come saniMatrice di Confusione",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#44": "Dalla matrice di confusione possono essere calcolati diversi indici\nIndice Formula Intuizione\nAccuracy Percentuale di classificazioni corrette\nPrecision Percentuale di classificazioni positive che  \nsono corrette\nRecall (Sensitivity) Percentuale di elementi positivi del tes tset \nche sono stati classificati come positivi\nSpecificity Percentuale di elementi negativi del test set \nche sono stati classificati come negativi\nPrecision: se dico “positivo”, è corretto ?\nRecall: riesco a trovare tuttii positivi del testing set?Matrice di Confusione\nTN\nTN+FPTP\nTP+FNTP+TN\nTP+FP+TN+FN\nTP\nTP+FP",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#45": "Matrice di Confusione\nVera Assegnata\n1 1\n1 1\n1 1\n1 1\n1 1\n1 1\n1 1\n2 1\n2 1\n2 1Vera Assegnata\n1 1\n1 2\n1 2\n1 1\n1 1\n1 1\n1 1\n2 2\n2 2\n2 1Matrice diconfusione\nAccuracy: 7/10 (0.7)\nPrecision: 7/10 (0.7)\nRecall: 7/7 (1)\nSpecificity: 0/3 (0)TP:7 FN:0\nFP:3 TN:0Indice Formula\nAccuracy\nPrecision\nRecall (Sensitivity)\nSpecificity\nMatrice diconfusione\nAccuracy: 7/10 (0.7)\nPrecision: 5/6 (0.83)\nRecall: 5/7 (0.71)\nSpecificity: 2/3 (0.66)TP:5 FN:2\nFP:1 TN:2TP+TN\nTP+FP+TN+FN\nTP\nTP+FP\nTP\nTP+FN\nTN\nTN+FP",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#46": "blu50 10\n20 20Supponiamo diaver addestrato unmodello utilizzando 100\nesempi ditraining .Diquesti 100,60sono diclasse “rosso” e\n40sono diclasse “blu”.\nIlmodello haeffettuato leseguenti classificazioni :\nEtichette predette\nrosso blu\nrosso\nEtichette\nrealiMatrice di Confusione",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#47": "rosso\nblu50 10\n20 20Etichette predette\nrosso blu\nNB: la somma è sempre  \n100 (pari al numero di  \npattern ditraining)Etichetta reale Etichetta predetta Tipo predizione\nrosso rosso True positive (TP) (50)\nrosso blu False negative (FN) (10)\nblu blu True negative (TN) (20)\nblu rosso False positive (FP) (20)\nMatrice di confusioneEtichette\nrealiMatrice di Confusione",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#48": "rosso\nbluTP FN\nFP TNEtichette predette\nrosso blu\nNB: la somma è sempre  \n100 (pari al numero di  \npattern ditraining)Etichetta reale Etichetta predetta Tipo predizione\nrosso rosso True positive (TP) (50)\nrosso blu False negative (FN) (10)\nblu blu True negative (TN) (20)\nblu rosso False positive (FP) (20)\nMatrice di confusioneEtichette\nrealiMatrice di Confusione",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#49": "Precision: =50 / (50+20) =0.7142\nTP +FPEtichetta reale Etichetta predetta Tipo predizione\nrosso rosso True positive (TP) (50)\nrosso blu False negative (FN) (10)\nblu blu True negative (TN) (20)\nblu rosso False positive (FP) (20)\nMisure di performance :\nPrecision: \"Quanti dei pattern che ho predetto di tipo rosso sono\ndavvero pattern di classe rosso ?\"\nTPMatrice di Confusione",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#5": "Esempi in MATLAB\nload fisheriris; % carica le osservazioni in ‘meas’ e le label in ‘species’\nX=meas(:,3:4); % estrae lunghezza e larghezza dei sepali\ny = ~strcmp(species, 'setosa' ); % label 0 se Iris setosa, 1 se altre specie\nSVMModel=fitcsvm(X,y, 'BoxConstraint' ,+Inf); % C=+Inf —> hard-margin linear SVM\nfigure\ngscatter(X(:,1),X(:,2),y); % scatter plot dei vettori di training\nhold on\nsv = SVMModel.SupportVectors;\nplot(sv(:,1),sv(:,2), 'ko','MarkerSize' ,10) % cerchia i vettori di supporto\nlegend('Iris setosa' ,'Altre specie' ,'Support vectors' ,'Location' ,'southeast' )\naxis manual\nx1 = linspace( -5,5);\nf=@(x)(-SVMModel.Bias -SVMModel.Beta(1)*x)/SVMModel.Beta(2); % iperpiano di \nseparazione\nplot(x1,f(x1))\nf1=@(x)( -SVMModel.Bias+1 -SVMModel.Beta(1)*x)/SVMModel.Beta(2); % margine \npositivo\nplot(x1,f1(x1))\nf2=@(x)( -SVMModel.Bias -1-SVMModel.Beta(1)*x)/SVMModel.Beta(2); % margine \nnegativo\nplot(x1,f2(x1))",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#50": "Recall \n(Sensi tivity) :\nTP\nSensitivity: =50 / (50+10) =0.8333Etichetta reale Etichetta predetta Tipo predizione\nrosso rosso True positive (TP) (50)\nrosso blu False negative (FN) (10)\nblu blu True negative (TN) (20)\nblu rosso False positive (FP) (20)\nMisure di performance :\nTP + FN\nsomma dei positivi nel training setMatrice di Confusione\n“Dei pattern che dovrei predire come rosso (positivo) \nquanti ne ho predetti di classe rosso (positivo)?”",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#51": "Specificity: = 20 / (20+20) =0.5Etichetta reale Etichetta predetta Tipo predizione\nrosso rosso True positive (TP) (50)\nrosso blu False negative (FN) (10)\nblu blu True negative (TN) (20)\nblu rosso False positive (FP) (20)\nMisure di performance :\nSpecifici ty:Matrice di Confusione\nTN“Dei pattern che dovrei predire come blu(negativo) \nquanti ne ho predetti di classe blu(negativo)?”\nTN+FP\nsomma dei negativi nel training set",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#52": "Lacurva ROC (Receiver Operating Characteristic) èuna\ntecnica statistica attualmente utilizzata inuna grande varietà di\ncampi scientifici .\nQuesta tecnica trae origine nell'ambito della teoria della\nrilevazione delsegnale .Sitratta diunametodologia cheèstata\nadottata per laprima volta daalcuni ingegneri, durante la\nseconda guerra mondiale, perl'analisi delle immagini radar elo\nstudio delrapporto segnale/disturbo .\nE'possibile usare lacurva ROC anche pervalutare leprestazioni\ndiunmodello diclassificazione .Curva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#53": "Sistema molto utilizzato per valutare un classificatore binario\nbasato susoglia\nLegenda: \nPositivo = Disease (Malati) \nNegativo = Normal (Sani)\nClassificazione:\nNegativo < !\nPositivo > !\nVariando il valore di soglia \n!(cut-off)si ottengono \ndiversi valori di TP, TN, FP,\nFN\nEsempio: con ilvalore  di !\nin figura i Falsi Positivi \nsono azeroCurva ROC\n!",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#54": "La curva ROC mette in relazione la specificity (recall) \nconla sensitivity al variare della soglia\nFissata una soglia, quanti sono i veri positivi rispetto ai falsipositivi? \nCome si calcola:\nSi fa variare la soglia calcolando i  \ncorrispondenti veri positivi e falsi  \npositivi, che rappresentano un  \npunto della curva\nIl valore minimo/massimo della  \nsoglia è quello per cui sono tutti  \nfalsi positivi o tutti veri positiviCurva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#55": "Fissatauna sogliat,\nperi tutti gliesempi  \nquali il\n(classificatore)modello\ngenera\nunoscore >tvengono  \npredetti positivi.\nQuesto cipermette di\nquantificare, per ogni\nscelta del valore di\nsoglia ,TP,TN, FPe\nFN.\nTP\nFPCurva ROC\nTN\nFN",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#56": "Fissato unvalore dit\npossiamo calcolare, ad\nesempio :\nTP=0.5  \nFN=0.5  \nFP=0.12  \nFN=0.88\nPossiamo  \nidentificare un  \npunto sulla curva  \n(associato al  \nvalore di tscelto)Curva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#57": "Possiamo calcolare TP  \ne FP per una serie di  \nvalori  di t (es. 0.1,0.2,\n0.3, 0.4, 0.5, 0.6, …,\n1.0). In questo modo  \notteniamo diversi punti  \nche compongono la  \ncurva ROC.\nCurva ROC\nIl valore degli score \ngenerati dal \nclassificatore può \nvariare tra 0 e 1.",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#58": "A seconda di come si vuole operare si sceglie lasoglia\nEsempio 1 (curva) : Sensitivity al 95%, si \nottiene un corrispondente valore di Specificity\n(70%)\nEsempio 2 (segmento tratteggiato) : \nSensitivity = 100-Specificity, si chiama \nEqual Error Rate\nCurva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#59": "Ciò che èimportante\nperò non èlacurva di\npersé,mal’area sotto\nla curva . Questa\nquantità èindicata con\niltermine Area Under\ntheROC Curve (AUC )\nCurva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#6": "Esempi in MATLAB Esempi in MATLAB –(3)\n45\n𝐶=+∞C=+∞\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#60": "AUC = 1:\nIl classificatore è  \nperfetto\nAUC = 0.5 :\nIl classificatore è\ntotalmente casuale\n(lancio diunamoneta)\nCurva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#61": "Rispetto a TP e FP :\nTP:0, FP:0\nPredice sempre «negativo»\nTP:1, FP:1\nPredice sempre «positivo»\nTP:1, FP:0\nClassificatore ideale\nCurva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#62": "Si possono confrontare curve ROC calcolando l'area\nsotto la curva (AUC –Area Under theCurve)\nUn AUC più grande \nimplica unclassificatore\nmigliore\nCurva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#63": "A B CA migliore di B migliore di C (C=lancio moneta)Curva ROC",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#64": "Join the protest!!!\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#7": "Esempi in MATLAB\nC=+∞Esempi in MATLAB –(4)\n46\n𝐶=+∞\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#8": "Esempi in MATLAB\nC=1Esempi in MATLAB –(5)\n47\n𝐶=1\n",
    "data_test\\rootfolder\\università\\MachineLearning\\36-SVM(3)-sbloccato.pdf#9": "Esempi in MATLAB\nC=10−6Esempi in MATLAB –(6)\n48\n𝐶=10−6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: SVM (Ex 14)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#1": "Sommario\nScikit-learn e SVM \nSVM e Iris dataset \nUse case: Stock forecasting \nUse case: Sentiment Analysis",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#10": "Esercitazione: stock forecasting\nfrom\n sklearn.svm \n import\n SVC\nfrom\n sklearn.metrics \n import\n accuracy_score\n  \nimport\n pandas \n as\n pd\nimport\n numpy \nas\n np\n  \nimport\n matplotlib.pyplot \n as\n plt\nplt.style.use(\n 'seaborn-darkgrid'\n )\n  \nimport\n warnings\nwarnings.filterwarnings(\n \"ignore\"\n )\ndf = pd.read_csv(\n 'RELIANCE.csv'\n )\nd\nf.index = pd.to_datetime(df[\n 'Date'\n])\ndf = df.drop([\n 'Date'\n], axis=\n 'columns'\n )\n... (completa)\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#11": "Esercitazione: stock forecasting\nfrom\n sklearn.svm \n import\n SVC\nfrom\n sklearn.metrics \n import\n accuracy_score\n  \nimport\n pandas \n as\n pd\nimport\n numpy \nas\n np\n  \nimport\n matplotlib.pyplot \n as\n plt\nplt.style.use(\n 'seaborn-darkgrid'\n )\n  \nimport\n warnings\nwarnings.filterwarnings(\n \"ignore\"\n )\ndf = pd.read_csv(\n 'RELIANCE.csv'\n )\nd\nf.index = pd.to_datetime(df[\n 'Date'\n])\ndf = df.drop([\n 'Date'\n], axis=\n 'columns'\n )\ndf[\n'Open-Close'\n ] = df.Open - df.Close\ndf[\n'High-Low'\n ] = df.High - df.Low\n  \n# per ora uso solo 2 valori\nX = df[[\n 'Open-Close'\n , \n'High-Low'\n ]]\ny = np.where(df[\n 'Close'\n].shift(\n -1\n) > df[\n'Close'\n], \n1\n, \n0\n)\n>> [1 1 1 ... 1 0 0]\n... (segue)\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#12": "Esercitazione: stock forecasting\nsplit_percentage = \n 0.8\nsplit = \n int\n(split_percentage*\n len\n(df))\n  \n# Train data set\nX_train = X[:split]\ny_train = y[:split]\n  \n# Test data set\nX_test = X[split:]\ny_test = y[split:]\ncls = SVC().fit(X_train, y_train)\ndf[\n'Predicted_Signal'\n ] = cls.predict(X)\ndf[\n'Return'\n ] = df.Close.pct_change()\ndf[\n'Strategy_Return'\n ] = df.Return *df.Predicted_Signal.shift(\n 1\n)\ndf[\n'Cum_Ret'\n ] = df[\n'Return'\n ].cumsum()\ndf[\n'Cum_Strategy'\n ] = df[\n'Strategy_Return'\n ].cumsum()\nimport\n matplotlib.pyplot \n as\n plt\n%matplotlib \n inline\n  \nplt.plot(df[\n 'Cum_Ret'\n ],color=\n 'red'\n)\nplt.plot(df[\n 'Cum_Strategy'\n ],color=\n 'blue'\n)\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#13": "Esercitazione: stock forecasting\n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#14": "Esercitazione: stock forecasting\nL'algoritmo genera un ritorno del 18.87% in un 1 anno, rispetto al 5.97% \ndel titolo azionario. \nLa funzione accuracy_score() restituisce una accuracy del 62.07% sul train \nset e 50.67 sul test set. \nEsercizi: (1) crea il target value a distanza di più giorni dall'istanza \ncorrente; (2) usa gli ultimi 15 valori Close come istanza di input per predire \nil successivo; (3) impiega altri kernel (es. rbf).\n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#15": "Esercitazione: Sentiment Analysis\nTecnica molto popolare per classiﬁcare brani di testo, micropost o frasi in \nlinguaggio naturale in base al sentimento (es. positivo, negativo, neutro). \nSupponiamo di impiegare le review di ﬁlm, es: \nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/  \nMovie-review data for use in sentiment-analysis experiments. Available are collections of \nmovie-review documents labeled with respect to their overall \n sentiment polarity\n  (positive \nor negative) or \n subjective rating\n  (e.g., \"two and a half stars\") and sentences labeled with \nrespect to their \n subjectivity status\n  (subjective or objective) or \n polarity \nimport pandas as pd  \ntrainData = pd.read_csv(\"\n https://raw.githubusercontent.com/Vasistareddy/\nsentiment_analysis/master/data/train.csv\n \") \ntestData = pd.read_csv(\"\n https://raw.githubusercontent.com/Vasistareddy/\nsentiment_analysis/master/data/test.csv\n \") \ntrainData.sample(frac=1).head(5) # shuffle the df and pick first 5  \n      \nContent                                             \n Label \n56\n    jarvis cocker of pulp once said that he wrote ...   pos  \n1467\n  david spade has a snide , sarcastic sense of h...   neg  \n392\n   upon arriving at the theater during the openin...   pos  \n104\n   every once in a while , a film sneaks up on me...   pos  \n1035\n  susan granger's review of \" american outlaws \"...   neg\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#16": "Esercitazione: Sentiment Analysis\nPer impiegare il testo come input agli algoritmi di ML spesso si effettua una \npipeline di processamento per trasformare parole o frasi in vettori numerici. \nPer dettagli: \n https://medium.com/@vasista/preparing-the-text-data-with-\nscikit-learn-b31a3df567e\n   e  \nhttps://scikit-learn.org/stable/modules/\ngenerated/sklearn.feature_extraction.text.TﬁdfVectorizer.html   \nfrom sklearn.feature_extraction.text import TfidfVectorizer  \n# ignora i termini che compaiono in meno di 5 documenti \n# e i termini che compaiono in > 80% dei documenti; \n# abilita l'inverse document frequency per pesare i termini \nvectorizer = TfidfVectorizer(min_df = 5,  \n                             max_df = 0.8,  \n                             sublinear_tf = True,  \n                             use_idf = True)  \ntrain_vectors = vectorizer.fit_transform(trainData['Content'])  \ntest_vectors = vectorizer.transform(testData['Content']) \nEsercizio\n : completa il codice impiegando SVM lineare e valutane \nl'accuratezza. Testa la predizione su review di Amazon.\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#17": "Esercitazione: Sentiment Analysis\nfrom sklearn import svm \nfrom sklearn.metrics import classification_report \nclassifier_linear = svm.SVC(kernel='linear') \nclassifier_linear.fit(train_vectors, trainData['Label']) \nprediction_linear = classifier_linear.predict(test_vectors) \ntime_linear_train = t1-t0 \ntime_linear_predict = t2-t1 \nreport = classification_report(testData['Label'], prediction_linear, \noutput_dict=True) \nprint('positive: ', report['pos']) \nprint('negative: ', report['neg']) \npositive:  {'precision': 0.9191919191919192, 'recall': 0.91, 'f1-score': \n0.9145728643216081, 'support': 100} \nnegative:  {'precision': 0.9108910891089109, 'recall': 0.92, 'f1-score': \n0.9154228855721394, 'support': 100} \nreview = \"\"\"Very good picture and sound, very glad I chose this unit\"\"\" \nreview_vector = vectorizer.transform([review]) # vectorizing \nprint(classifier_linear.predict(review_vector))  \n18",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#18": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and TensorFlow: \nConcepts, Tools, and Techniques to Build Intelligent Systems\n . O'Reilly Media \n2017 \nhttps://www.kaggle.com/code/parulpandey/getting-started-with-time-series-\nusing-pandas/notebook   \nhttps://medium.com/@vasista/preparing-the-text-data-with-scikit-learn-\nb31a3df567e    \nhttps://scikit-learn.org/stable/modules/generated/\nsklearn.feature_extraction.text.TﬁdfVectorizer.html  \nTutorial: \n https://www.geeksforgeeks.org/predicting-stock-price-direction-using-\nsupport-vector-machines/?ref=rp  \nTutorial: \n https://medium.com/@vasista/sentiment-analysis-using-\nsvm-338d418e3ff1\nTesti di Riferimento\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#2": "Support Vector Machines\nL'algoritmo SVM è impiegato in ambito di classiﬁcazione e regressione. \nHa molti vantaggi tra cui: \nEfﬁcace in spazi con molte dimensioni (cioè features) \nPuò trattare casi in cui le dimensioni sono maggiori delle istanze \nÈ efﬁciente in termini di spazio di memoria richiesto \nAttenzione: \nSe le dimensioni sono molto maggiori delle istanze, la scelta della \nfunzione kernel e la regolarizzazione sono fondamentali. \nSVM non restituisce direttamente probabilità.\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#3": "Scikit-learn: Support Vector Machines\nI dati in input supportati in scikit-learn sono sia \n dense\n  (es. \nnumpy.ndarray\n , \nnumpy.asarray\n ) sia sparsi (qualsiasi \n scipy.sparse\n ) \n>>> \nfrom \nsklearn \nimport\n svm \n>>> \nX \n=\n [[\n0\n, \n0\n], [\n1\n, \n1\n]] \n>>> \ny \n=\n [\n0\n, \n1\n] \n>>> \nclf \n=\n svm\n.\nSVC() \n>>> \nclf\n.\nfit(X, y) \nSVC() \n>>> \nclf\n.\npredict([[\n 2.\n, \n2.\n]]) \narray([1]) \n>>> \n# support vectors  \n>>> \nclf\n.\nsupport_vectors_ \narray([[0., 0.],  \n       [1., 1.]])  \n>>> \n# indici dei support vectors  \n>>> \nclf\n.\nsupport_ \narray([0, 1]...)  \n>>> \n# numero dei support vectors per ogni classe  \n>>> \nclf\n.\nn_support_ \narray([1, 1]...)\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#4": "Scikit-learn: Support Vector Machines\nEsempio IRIS dataset: \n# carico il dataset IRIS\niris \n=\n load_iris()\n# uso solo le prime due features\nX \n=\n iris.data[:, :\n 2\n]\nY \n=\n iris.target\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\n# equivale a SVC(kernel\n =\n\"linear\"\n )\nsvm \n=\n LinearSVC()\nsvm.fit(X_train_std, Y_train)\n \nprint\n(\n\"Accuracy Train Set:\"\n , svm.score(X_train_std, Y_train))\nprint\n(\n\"Accuracy Test Set:\"\n , svm.score(X_test_std, Y_test))\n>> Accuracy Train Set: 0.8285714285714286\n>> \nAccuracy Test Set: 0.6888888888888889\nCosa possiamo dire?\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#5": "Scikit-learn: Support Vector Machines\n>> Accuracy Train Set: 0.8285714285714286\n>> \nAccuracy Test Set: 0.6888888888888889\nCosa possiamo dire? \nIl modello soffre di overﬁtting. \nEsercizio: prova ad impiegare tutte le features del dataset.\n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#6": "Scikit-learn: Support Vector Machines\nEsercizio: prova ad impiegare tutte le features del dataset. \n>> Accuracy Train Set: 0.9428571428571428\n>> \nAccuracy Test Set: 0.9555555555555556\nEsercizio: cambia il parametro \n kernel\n  di SVC() e testa le altre funzioni oltre \nalla \n linear\n  cioè \n rbf\n, \nsigmoid\n  e \npoly\n impiegando sempre 2 features.\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#7": "Scikit-learn: Support Vector Machines\nEsercizio: prova ad impiegare tutte le features del dataset. \n>> Accuracy Train Set: 0.9428571428571428\n>> \nAccuracy Test Set: 0.9555555555555556\nEsercizio: cambia il parametro \n kernel\n  di SVC() e testa le altre funzioni oltre \nalla \n linear\n  cioè \n rbf\n, \nsigmoid\n  e \npoly\n impiegando sempre 2 features. \n8\nAccuracy Train Set: 0.81\nAccuracy Test Set: 0.78Accuracy Train Set: 0.72\nAccuracy Test Set: 0.8Accuracy Train Set: 0.76\nAccuracy Test Set: 0.67",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#8": "Stock forecasting\nAlcuni servizi web rendono disponibili gli andamenti di titoli azionari via \nAPIs, es: \nOpen: Starting price at which a stock is traded in a day. \nClose: Closing price. \nHigh: The highest price of equity symbol in a day. \nLow: The lowest price of the share in a day \nVWAP: Volume weighted average price \nVolume: Total volume of stocks traded on a particular day.  \nI dati possono essere interpretati come \n time series\n , cioè sequenze di valori \nordinati temporalmente. \nPer approfondimenti:  \nhttps://www.kaggle.com/code/parulpandey/getting-started-with-time-series-using-pandas/notebook  \nIl task della stock price forecasting è predire il valore futuro (es. intraday, \ngiornalieri, mensile, etc). di un titolo in base ai valori passati.\n9",
    "data_test\\rootfolder\\università\\MachineLearning\\37-Ex_14 Esercitazione SVM-sbloccato.pdf#9": "Esercitazione: stock forecasting\nAl seguente indirizzo trovi i dati storici del titolo RELIANCE: \nhttps://storage.googleapis.com/kaggle-forum-message-attachments/894813/16059/RELIANCE.csv \nPossiamo impiegare l'istanza attuale come input e tentare di fare predizione \nsul comprare (+1) oppure no (0). \nIn ambito azionario è utile deﬁnire nuove features che combinano quelle \nattuali, es. Open-Close o High-Low: \ndf[\n'Open-Close'\n ] = df.Open - df.Close\nLa variabile target puoi essere approssimare nel seguente modo: \ny = np.where(df[\n 'Close'\n].shift(\n -1\n) > df[\n'Close'\n], \n1\n, \n0\n)\nIl ritorno cumulato può essere ottenuto nel seguente modo: \ndf[\n'Return'\n ] = df.Close.pct_change() \n # variazione percentuale rispetto al prec\ndf[\n'Strategy_Return'\n ] = df.Return * df.Predicted_Signal.shift(\n 1\n)\ndf[\n'Cum_Ret'\n ] = df[\n'Return'\n ].cumsum()\ndf[\n'Cum_Strategy'\n ] = df[\n'Strategy_Return'\n ].cumsum()\nEsercizio\n : impiega l'algoritmo SVM per la predizione e valuta l'accuratezza.\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#0": "Machine Learning\nUniversità Roma Tre  \nDipartimento di Ingegneria \nAnno Accademico 2021 -2022\nRiduzione di Dimensionalità",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#1": "Sommario\n!Introduzione\n!Definizioni\n!Le Principali Tecniche\n!PCA vs LDA\n!Principal Component Analysis (PCA)\n!Linear Discriminant Analysis (LDA)\n!t-SNE",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#10": "PCA: Retro -Proiezione\n6prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: proiezione e retroproiezione\nProiezione\n (o):una volta determinato lospazio PCA, la\nproiezione diunpattern sutale spazio èsemplicemente la\nproiezione geometrica diunvettoresull’iperpiano chedefinisce\nlospazio .Inrealtà lavera proiezione geometrica èunvettore\nchehalastessa dimensionalità delvettore originale mentre in\nquesto contesto indichiamo con proiezione ilvettore (ridotto)\nnello spazio PCA.Matematicamente questa operazione è\neseguita come prodotto della matrice diproiezione trasposta\nperilpattern alquale èpreventivamente sottratta lamedia .\n𝑃𝐶𝐴:𝑑→𝑘\n𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘=Φ𝑘𝑡𝐱−ത𝐱\nRetro\n -proiezione (m):Dato unvettore nello spazio PCA, lasua\nretro-proiezione verso lospazio originale siottiene moltiplicando\nilvettore perlamatrice diproiezione esommando ilvettore\nmedio .Questa trasformazione non sposta spazialmente il\nvettore, che giace ancora sullo spazio PCA,maopera un\ncambiamento dicoordinate che nepermette lacodifica in\ntermini delle𝑑componenti dello spazio originale .\n𝑃𝐶𝐴:𝑘→𝑑\n𝑃𝐶𝐴𝐲,𝑆ത𝐱,Φ𝑘=Φ𝑘𝐲+ത𝐱𝐱\n𝑑=3,𝑘=2\n𝑆ത𝐱,Φ𝑘𝐲=𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘\n6prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: proiezione e retroproiezione\nProiezione\n (o):una volta determinato lospazio PCA, la\nproiezione diunpattern sutale spazio èsemplicemente la\nproiezione geometrica diunvettoresull’iperpiano chedefinisce\nlospazio .Inrealtà lavera proiezione geometrica èunvettore\nchehalastessa dimensionalità delvettore originale mentre in\nquesto contesto indichiamo con proiezione ilvettore (ridotto)\nnello spazio PCA.Matematicamente questa operazione è\neseguita come prodotto della matrice diproiezione trasposta\nperilpattern alquale èpreventivamente sottratta lamedia .\n𝑃𝐶𝐴:𝑑→𝑘\n𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘=Φ𝑘𝑡𝐱−ത𝐱\nRetro\n -proiezione (m):Dato unvettore nello spazio PCA, lasua\nretro-proiezione verso lospazio originale siottiene moltiplicando\nilvettore perlamatrice diproiezione esommando ilvettore\nmedio .Questa trasformazione non sposta spazialmente il\nvettore, che giace ancora sullo spazio PCA,maopera un\ncambiamento dicoordinate che nepermette lacodifica in\ntermini delle𝑑componenti dello spazio originale .\n𝑃𝐶𝐴:𝑘→𝑑\n𝑃𝐶𝐴𝐲,𝑆ത𝐱,Φ𝑘=Φ𝑘𝐲+ത𝐱𝐱\n𝑑=3,𝑘=2\n𝑆ത𝐱,Φ𝑘𝐲=𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#11": "PCA: Esempio Riduzione 2 --->1\n7prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: esempio riduzione 2 o1\nL’\nellisse rappresenta la distribuzione dei pattern nel training set .\n𝝋1e 𝝋2sono gli autovettori della matrice di covarianza.\nGli \nautovalori 𝜆1e 𝜆2sono le varianze della distribuzione lungo \ngli assi 𝝋1e 𝝋2.\n𝑦1e 𝑦2sono le proiezioni di 𝐱sugli assi 𝝋1e 𝝋2.\nSe \n𝜆2è piccolo, 𝐱può essere approssimato con 𝐱′\n(retroproiezione di 𝐲) senza perdite significative di informazione. \nSi \npuò dimostrare che tra tutte le riduzioni di dimensionalità\nlineari PCA è quella che preserva al massimo l’informazione dei \nvettori originali.0\nSpazio iniziale\n(𝑑=2)Spazio KL\n(𝑘=1)\n𝐱′=𝑃𝐶𝐴𝐲,𝑆ത𝐱,Φ𝑘𝐲=𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘\n𝜆2\n𝜆1𝑥2\n𝑥1 𝑥1𝑥2ത𝐱𝑦2𝑦1𝐱Φ1=𝝋1,Φ1∈2×1\n𝐲=𝑦1,𝐲∈1\nത𝐱=𝑥1,𝑥2𝑡,𝐱∈2\n𝐱=𝑥1,𝑥2𝑡,𝐱∈2Φ=𝝋1,𝝋2,Φ∈2×2𝝋1𝝋2\n7prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: esempio riduzione 2 o1\nL’\nellisse rappresenta la distribuzione dei pattern nel training set .\n𝝋1e 𝝋2sono gli autovettori della matrice di covarianza.\nGli \nautovalori 𝜆1e 𝜆2sono le varianze della distribuzione lungo \ngli assi 𝝋1e 𝝋2.\n𝑦1e 𝑦2sono le proiezioni di 𝐱sugli assi 𝝋1e 𝝋2.\nSe \n𝜆2è piccolo, 𝐱può essere approssimato con 𝐱′\n(retroproiezione di 𝐲) senza perdite significative di informazione. \nSi \npuò dimostrare che tra tutte le riduzioni di dimensionalità\nlineari PCA è quella che preserva al massimo l’informazione dei \nvettori originali.0\nSpazio iniziale\n(𝑑=2)Spazio KL\n(𝑘=1)\n𝐱′=𝑃𝐶𝐴𝐲,𝑆ത𝐱,Φ𝑘𝐲=𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘\n𝜆2\n𝜆1𝑥2\n𝑥1 𝑥1𝑥2ത𝐱𝑦2𝑦1𝐱Φ1=𝝋1,Φ1∈2×1\n𝐲=𝑦1,𝐲∈1\nത𝐱=𝑥1,𝑥2𝑡,𝐱∈2\n𝐱=𝑥1,𝑥2𝑡,𝐱∈2Φ=𝝋1,𝝋2,Φ∈2×2𝝋1𝝋2",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#12": "PCA: Esempio Riduzione 2 --->1\n7prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: esempio riduzione 2 o1\nL’\nellisse rappresenta la distribuzione dei pattern nel training set .\n𝝋1e 𝝋2sono gli autovettori della matrice di covarianza.\nGli \nautovalori 𝜆1e 𝜆2sono le varianze della distribuzione lungo \ngli assi 𝝋1e 𝝋2.\n𝑦1e 𝑦2sono le proiezioni di 𝐱sugli assi 𝝋1e 𝝋2.\nSe \n𝜆2è piccolo, 𝐱può essere approssimato con 𝐱′\n(retroproiezione di 𝐲) senza perdite significative di informazione. \nSi \npuò dimostrare che tra tutte le riduzioni di dimensionalità\nlineari PCA è quella che preserva al massimo l’informazione dei \nvettori originali.0\nSpazio iniziale\n(𝑑=2)Spazio KL\n(𝑘=1)\n𝐱′=𝑃𝐶𝐴𝐲,𝑆ത𝐱,Φ𝑘𝐲=𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘\n𝜆2\n𝜆1𝑥2\n𝑥1 𝑥1𝑥2ത𝐱𝑦2𝑦1𝐱Φ1=𝝋1,Φ1∈2×1\n𝐲=𝑦1,𝐲∈1\nത𝐱=𝑥1,𝑥2𝑡,𝐱∈2\n𝐱=𝑥1,𝑥2𝑡,𝐱∈2Φ=𝝋1,𝝋2,Φ∈2×2𝝋1𝝋2",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#13": "PCA: Scelta di k\n8prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: scelta di 𝑘\nTalvolta\n lascelta di𝑘èobbligata :adesempio per la\nvisualizzazione 2Do3Ddeidati.\nQuando\n invecel’obiettivo èquello discartare informazione\ninutile edati correlati mantenendo gran parte delcontenuto\ninformativo sipuòscegliere 𝑘nelmodo seguente :\nFissata\n una percentuale 𝑡delcontenuto informativo chesi\nvuole preservare (es.𝑡=95%)sisceglie ilminimo valore di\n𝑘percuilasomma deipiùgrandi𝑘autovalori ,rispetto alla\nsomma dituttigliautovalori ,èmaggiore ouguale a𝑡.\nConsiderando\n gliautovalori ordinati inordine decrescente :\n𝑘=𝑎𝑟𝑔𝑚𝑖𝑛\n𝑧σ𝑖=1…𝑧𝜆𝑖\nσ𝑖=1…𝑑𝜆𝑖≥𝑡\nInfatti, ricordando che gliautovalori denotano lavarianza\nlungo idiversi assi, ilrapporto nella formula indica lavarianza\n«conservata» rispetto allavarianza totale .",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#14": "PCA: Codifica di Immagini\n9prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: codifica di un’immagine\n𝐱∈16500\n𝐲∈15\n𝐱′∈16500Immagine\noriginaleRicostruzione\n(retroproiezione)\nproiezione retro-proiezioneiprimi 8 autovettori o componenti principali\n(denominati eigenfaces nell’applicazione al riconoscimento volto )\n𝐲=𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ15𝐱′=𝑃𝐶𝐴𝐲,𝑆ത𝐱,Φ15\n-2532 2193 -2179 2099 491\n427 -324 961 35 -40\n-149 -624 317 -158 -142",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#15": "Calcolo PCA in Pratica\n10prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàCalcolo PCA in pratica\nPer𝑑elevato (tipico nelcaso diimmagini, audio, ecc.)lamatrice di\ncovarianza puòessere molto grande .\nper\n𝑑=16500 ,𝚺∈16500×16500,oltre 272milioni divalori !\nse\n𝑛≪𝑑,èconveniente calcolare lamatrice diproiezione (primi\n𝑘autovettori) attraverso decomposizione SVD della matrice\nrettangolare deipattern centralizzati 𝐗∈𝑑×𝑛senza passare\nperlamatrice dicovarianza (vedi [1]).\n𝐗=𝐱1−ത𝐱𝐱2−ത𝐱⋯𝐱𝑛−ത𝐱\ndecomposizone SVD per𝑑>𝑛:𝐗=𝐔𝚪𝐕𝑡,con𝐔∈𝑑×𝑛\nortonormale, con𝐕∈𝑛×𝑛ortonormale, 𝚪∈𝑛×𝑛diagonale .\n𝚺=1\n𝑛𝐗𝐗𝑡=1\n𝑛𝐔𝚪𝐕𝑡𝐕𝚪𝐔𝑡=1\n𝑛𝐔𝚪2𝐔𝑡\nAutovettori eautovalori di𝚺possono dunque essere ottenuti\ndalle colonne di𝐔(vettori singolari sinistri di𝐗)ecorrispondenti\nelementi diagonali di𝚪alquadrato (valori singolari alquadrato\ndi𝐗).\n[1] R. Madsen, L. Hansen, O. Winther, “Singular Value Decomposition and Principal Component \nAnalysis”, http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4000/pdf/imm4000.pdfAttenzione\nformato trasposto rispetto a \n𝐗usata in regressione.\nOgni pattern una colonna.\ndecomposizione \nspettrale della \nmatrice quadrata 𝚺\n10prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàCalcolo PCA in pratica\nPer𝑑elevato (tipico nelcaso diimmagini, audio, ecc.)lamatrice di\ncovarianza puòessere molto grande .\nper\n𝑑=16500 ,𝚺∈16500×16500,oltre 272milioni divalori !\nse\n𝑛≪𝑑,èconveniente calcolare lamatrice diproiezione (primi\n𝑘autovettori) attraverso decomposizione SVD della matrice\nrettangolare deipattern centralizzati 𝐗∈𝑑×𝑛senza passare\nperlamatrice dicovarianza (vedi [1]).\n𝐗=𝐱1−ത𝐱𝐱2−ത𝐱⋯𝐱𝑛−ത𝐱\ndecomposizone SVD per𝑑>𝑛:𝐗=𝐔𝚪𝐕𝑡,con𝐔∈𝑑×𝑛\nortonormale, con𝐕∈𝑛×𝑛ortonormale, 𝚪∈𝑛×𝑛diagonale .\n𝚺=1\n𝑛𝐗𝐗𝑡=1\n𝑛𝐔𝚪𝐕𝑡𝐕𝚪𝐔𝑡=1\n𝑛𝐔𝚪2𝐔𝑡\nAutovettori eautovalori di𝚺possono dunque essere ottenuti\ndalle colonne di𝐔(vettori singolari sinistri di𝐗)ecorrispondenti\nelementi diagonali di𝚪alquadrato (valori singolari alquadrato\ndi𝐗).\n[1] R. Madsen, L. Hansen, O. Winther, “Singular Value Decomposition and Principal Component \nAnalysis”, http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4000/pdf/imm4000.pdfAttenzione\nformato trasposto rispetto a \n𝐗usata in regressione.\nOgni pattern una colonna.\ndecomposizione \nspettrale della \nmatrice quadrata 𝚺",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#16": "Singular Value Decomposition (SVD)\n!LaDecomposizione aiValori Singolari (Singular Value\nDecomposition, SVD) èuna importante fattorizzazione per\nmatrici avalori reali ocomplessi chesiavvale diautovalori\neautovettori\n!Ogni matrice M∈!m×npuò essere fattorizzata in \nM=U\"V*\ndove\n!Uèuna matrice m×munitaria (cioè UUt=Im)\n!#èuna matrice m×ndiagonale rettangolare con soli elementi\nreali non negativi\n!V*èlatrasposta coniugata diuna matrice n×nunitaria V",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#17": "Singular Value Decomposition (SVD)\n!Glielementi della diagonale di\"sono detti valori singolari diM\n!Lemcolonne diUsono dette vettori singolari sinistri diM\n!Lencolonne diVsono dette vettori singolari destri diM\n!Vale quanto segue\n!Ivettori singolari sinistri diMsono gliautovettori diM∙M*\n!Ivettori singolari destri diMsono gliautovettori diM*∙M\n!Ivalori singolari diMsono leradici quadrate degli autovalori non\nnulli diM∙M* eM*∙M",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#18": "Calcolo PCA in Pratica\n10prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàCalcolo PCA in pratica\nPer𝑑elevato (tipico nelcaso diimmagini, audio, ecc.)lamatrice di\ncovarianza puòessere molto grande .\nper\n𝑑=16500 ,𝚺∈16500×16500,oltre 272milioni divalori !\nse\n𝑛≪𝑑,èconveniente calcolare lamatrice diproiezione (primi\n𝑘autovettori) attraverso decomposizione SVD della matrice\nrettangolare deipattern centralizzati 𝐗∈𝑑×𝑛senza passare\nperlamatrice dicovarianza (vedi [1]).\n𝐗=𝐱1−ത𝐱𝐱2−ത𝐱⋯𝐱𝑛−ത𝐱\ndecomposizone SVD per𝑑>𝑛:𝐗=𝐔𝚪𝐕𝑡,con𝐔∈𝑑×𝑛\nortonormale, con𝐕∈𝑛×𝑛ortonormale, 𝚪∈𝑛×𝑛diagonale .\n𝚺=1\n𝑛𝐗𝐗𝑡=1\n𝑛𝐔𝚪𝐕𝑡𝐕𝚪𝐔𝑡=1\n𝑛𝐔𝚪2𝐔𝑡\nAutovettori eautovalori di𝚺possono dunque essere ottenuti\ndalle colonne di𝐔(vettori singolari sinistri di𝐗)ecorrispondenti\nelementi diagonali di𝚪alquadrato (valori singolari alquadrato\ndi𝐗).\n[1] R. Madsen, L. Hansen, O. Winther, “Singular Value Decomposition and Principal Component \nAnalysis”, http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4000/pdf/imm4000.pdfAttenzione\nformato trasposto rispetto a \n𝐗usata in regressione.\nOgni pattern una colonna.\ndecomposizione \nspettrale della \nmatrice quadrata 𝚺\n10prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàCalcolo PCA in pratica\nPer𝑑elevato (tipico nelcaso diimmagini, audio, ecc.)lamatrice di\ncovarianza puòessere molto grande .\nper\n𝑑=16500 ,𝚺∈16500×16500,oltre 272milioni divalori !\nse\n𝑛≪𝑑,èconveniente calcolare lamatrice diproiezione (primi\n𝑘autovettori) attraverso decomposizione SVD della matrice\nrettangolare deipattern centralizzati 𝐗∈𝑑×𝑛senza passare\nperlamatrice dicovarianza (vedi [1]).\n𝐗=𝐱1−ത𝐱𝐱2−ത𝐱⋯𝐱𝑛−ത𝐱\ndecomposizone SVD per𝑑>𝑛:𝐗=𝐔𝚪𝐕𝑡,con𝐔∈𝑑×𝑛\nortonormale, con𝐕∈𝑛×𝑛ortonormale, 𝚪∈𝑛×𝑛diagonale .\n𝚺=1\n𝑛𝐗𝐗𝑡=1\n𝑛𝐔𝚪𝐕𝑡𝐕𝚪𝐔𝑡=1\n𝑛𝐔𝚪2𝐔𝑡\nAutovettori eautovalori di𝚺possono dunque essere ottenuti\ndalle colonne di𝐔(vettori singolari sinistri di𝐗)ecorrispondenti\nelementi diagonali di𝚪alquadrato (valori singolari alquadrato\ndi𝐗).\n[1] R. Madsen, L. Hansen, O. Winther, “Singular Value Decomposition and Principal Component \nAnalysis”, http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4000/pdf/imm4000.pdfAttenzione\nformato trasposto rispetto a \n𝐗usata in regressione.\nOgni pattern una colonna.\ndecomposizione \nspettrale della \nmatrice quadrata 𝚺\n",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#19": "PCA Whitening\n11prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA Whitening\nÈunatecnica dipre-normalizzazione deidati, che:\nRimuove\n lecorrelazioni traledimensioni, ruotando lanuvola di\npunti per allineare gliassi divariazione principale deidati\n(autovettori )agliassicartesiani .\nSfericizza\n l’ellissoide, uniformando levarianze (denotate dagli\nautovalori )a1lungo tuttigliassi\nDopo aver proiettato ipattern sullo spazio PCA (definito daiprimi𝑘\nautovettori )èsufficiente dividere ogni dimensione perlaradice\nquadrata dell’autovalore corrispondente (deviazione standard) .\nLamatrice dicovarianza deidati normalizzati èl’identità .𝝋1\n𝝋2 𝝋1𝝋2\n𝝋1𝝋2\n𝝋1𝝋2Ricordiamo, infatti, che gliautovettori della matrice dicovarianza !\nsono paralleli agli assi dell’ellisse che rappresenta ladistribuzione dei\npattern delTraning Set(TS)",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#2": "Definizioni\n2prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàDefinizioni\nObiettivo dei metodi per lariduzione didimensionalità\n(dimensionality reduction )èquello dieseguire unmapping dallo\nspazio iniziale𝑑aunospazio didimensione inferiore 𝑘,𝑘<𝑑.\nPuò essere vista come unaforma dicompressione (con perdita di\ninformazione) .Obiettivo èscartare leinformazioni non rilevanti o\nmeno rilevanti perilproblema diinteresse :\nallevia\n iproblemi collegati allacurse ofdimensionality :operare\ninspazi adelevata dimensionalità ,acausa delfatto che i\npattern sono molto sparsi, richiede ingenti moli didati per\nl’addestramento .\noperare\n inspazi adimensionalità inferiore rende piùsemplice\naddestrare algoritmi dimachine learning .Scartando dati\nridondanti (informazioni correlate) erumorosi talvolta si\nmigliorano anche leprestazioni .\nAttenzione :riduzione didimensionalità non significa mantenere\nalcune «dimensioni» ecancellarne altre, ma «combinare »le\ndimensioni inmodo opportuno .",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#20": "PCA Whitening\n11prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA Whitening\nÈunatecnica dipre-normalizzazione deidati, che:\nRimuove\n lecorrelazioni traledimensioni, ruotando lanuvola di\npunti per allineare gliassi divariazione principale deidati\n(autovettori )agliassicartesiani .\nSfericizza\n l’ellissoide, uniformando levarianze (denotate dagli\nautovalori )a1lungo tuttigliassi\nDopo aver proiettato ipattern sullo spazio PCA (definito daiprimi𝑘\nautovettori )èsufficiente dividere ogni dimensione perlaradice\nquadrata dell’autovalore corrispondente (deviazione standard) .\nLamatrice dicovarianza deidati normalizzati èl’identità .𝝋1\n𝝋2 𝝋1𝝋2\n𝝋1𝝋2\n𝝋1𝝋2",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#21": "Linear Discriminant Analysis (LDA)\n12prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàLinear Discriminant Analysis (LDA)\nRiduzione didimensionalità lineare esupervisionata ilcui\nobiettivo èmassimizzare laseparazione traleclassi (che nelTS\nsono etichettate ).L’esempio seguente mostra chealfinedella\ndiscriminazione lasoluzione ottimale può essere anche molto\ndiversa dalla soluzione PCA.\nPer\n formulare ilcriterio diottimizzazione dimassima\nseparazione traleclassi sono definite leseguenti matrici di\nscattering (initaliano“sparpagliamento” ):\nwithin\n -class𝐒𝑤:indica come ivettori sono scattered rispetto\nalcentro delle classi (ciascuno rispetto allapropria classe) .\nbetween\n -class𝐒𝑏:indica come icentri delle classi sono\nscattered rispetto alcentro generale della distribuzione\n(ovvero quanto leclassi sono scattered ).\nUna matrice discatter sicalcola come unamatrice dicovarianza\nsenza normalizzare perilnumero dipatternaltezzapesoPCA\nLDAuomini\ndonne",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#22": "Linear Discriminant Analysis (LDA)\n12prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàLinear Discriminant Analysis (LDA)\nRiduzione didimensionalità lineare esupervisionata ilcui\nobiettivo èmassimizzare laseparazione traleclassi (che nelTS\nsono etichettate ).L’esempio seguente mostra chealfinedella\ndiscriminazione lasoluzione ottimale può essere anche molto\ndiversa dalla soluzione PCA.\nPer\n formulare ilcriterio diottimizzazione dimassima\nseparazione traleclassi sono definite leseguenti matrici di\nscattering (initaliano“sparpagliamento” ):\nwithin\n -class𝐒𝑤:indica come ivettori sono scattered rispetto\nalcentro delle classi (ciascuno rispetto allapropria classe) .\nbetween\n -class𝐒𝑏:indica come icentri delle classi sono\nscattered rispetto alcentro generale della distribuzione\n(ovvero quanto leclassi sono scattered ).\nUna matrice discatter sicalcola come unamatrice dicovarianza\nsenza normalizzare perilnumero dipatternaltezzapesoPCA\nLDAuomini\ndonne",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#23": "Calcolo LDA\n",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#24": "Richiami\n!Latraccia diuna matrice èdefinita solo per lematrici\nquadrate edèlasomma degli elementi presenti sulla\ndiagonale principale .\n!Traccia eautovalori diuna matrice :latraccia diuna\nmatrice èuguale allasomma deisuoi autovalori moltiplicati\nperlerispettive molteplicità algebriche ,cioè seλ1,λ2,...,λp\nsono gliautovalori distinti diuna matrice Adiordine n,\ndette m1,m2,...,mplerispettive molteplicità algebriche, se\nm1+m2+...+mp=n,allora\ntr(A) =m1λ1+m2λ2+...+mpλp",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#25": "Richiami\n!SiaAuna matrice quadrata diordine nesiaλ0unsuo\nautovalore .Sidice molteplicità algebrica dell’autovalore λ0,\nesiindica conma(λ0),ilnumero cheesprime quante volte\nl’autovalore λ0annulla ilpolinomio caratteristico .\n!Ricordiamo che ilpolinomio caratteristico associato auna\nmatrice quadrata Aèildeterminante della matrice A-λIn,\ndove Aèlamatrice inesame, λèun’incognita eInèla\nmatrice identità dello stesso ordine di A.\nInformule :\npA(λ):=det(A-λIn)",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#26": "Calcolo LDA\n13prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàCalcolo LDA\nDato untraining setTScontenente 𝑛pattern 𝐱1,𝑦1…𝐱𝑛,𝑦𝑛,\ndove𝐱𝑖∈𝑑sono ipattern multidimensionali e𝑦𝑖∈[1…𝑠]le\netichette delle𝑠classi .Siano𝑛𝑖eഥ𝐱𝑖ilnumero dipattern eilvettore\nmedio della classe i-esima .Allora lematrici discattering sono\ndefinite come :\nwithin\n -class :\n𝐒𝑤=෍\n𝑖=1…𝑠𝐒𝑖,𝐒𝑖=෍\n𝐱𝑗|𝑦𝑗=𝑖𝐱𝑗−ഥ𝐱𝑖𝐱𝑗−ഥ𝐱𝑖𝑡\nbetween\n -class :\n𝐒𝑏=෍\n𝑖=1…𝑠𝑛𝑖∙ഥ𝐱𝑖−𝐱0ഥ𝐱𝑖−𝐱0𝑡,𝐱0=1\n𝑛෍\n𝑖=1…𝑠𝑛𝑖∙ഥ𝐱𝑖\nTra idiversi criteri diottimizzazione possibili quello più\nfrequentemente utilizzato èlamassimizzazione della quantità :\n𝐽1=𝑡𝑟𝐒𝑤−1𝐒𝑏=෍\n𝑖=1…𝑑𝜆𝑖\ndove𝑡𝑟èlatraccia (somma degli autovalori )della matrice .Il\ncriterio èintuitivo inquanto cerca dimassimizzare loscattering tra\nleclassi (𝐒𝑏)minimizzando alcontempo (matrice inversa 𝐒𝑤−1)\nquelloall’interno diogni classe .\nSidimostra che perlamassimizzazione di𝐽1lospazio LDA è\ndefinito (analogia con PCA)dagli autovettori relativi aiprimi𝑘(𝑘<\n𝑛,𝑘<𝑠,𝑘<𝑑)autovalori della matrice𝐒𝑤−1𝐒𝑏.𝑝𝑎𝑡𝑡𝑒𝑟𝑛𝑑𝑒𝑙𝑙𝑎𝑐𝑙𝑎𝑠𝑠𝑒𝑖−𝑒𝑠𝑖𝑚𝑎\n𝑚𝑒𝑑𝑖𝑎𝑔𝑙𝑜𝑏𝑎𝑙𝑒\nvaloremassimo di𝑘=𝑠−1\n13prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàCalcolo LDA\nDato untraining setTScontenente 𝑛pattern 𝐱1,𝑦1…𝐱𝑛,𝑦𝑛,\ndove𝐱𝑖∈𝑑sono ipattern multidimensionali e𝑦𝑖∈[1…𝑠]le\netichette delle𝑠classi .Siano𝑛𝑖eഥ𝐱𝑖ilnumero dipattern eilvettore\nmedio della classe i-esima .Allora lematrici discattering sono\ndefinite come :\nwithin\n -class :\n𝐒𝑤=෍\n𝑖=1…𝑠𝐒𝑖,𝐒𝑖=෍\n𝐱𝑗|𝑦𝑗=𝑖𝐱𝑗−ഥ𝐱𝑖𝐱𝑗−ഥ𝐱𝑖𝑡\nbetween\n -class :\n𝐒𝑏=෍\n𝑖=1…𝑠𝑛𝑖∙ഥ𝐱𝑖−𝐱0ഥ𝐱𝑖−𝐱0𝑡,𝐱0=1\n𝑛෍\n𝑖=1…𝑠𝑛𝑖∙ഥ𝐱𝑖\nTra idiversi criteri diottimizzazione possibili quello più\nfrequentemente utilizzato èlamassimizzazione della quantità :\n𝐽1=𝑡𝑟𝐒𝑤−1𝐒𝑏=෍\n𝑖=1…𝑑𝜆𝑖\ndove𝑡𝑟èlatraccia (somma degli autovalori )della matrice .Il\ncriterio èintuitivo inquanto cerca dimassimizzare loscattering tra\nleclassi (𝐒𝑏)minimizzando alcontempo (matrice inversa𝐒𝑤−1)\nquelloall’interno diogni classe .\nSidimostra che perlamassimizzazione di𝐽1lospazio LDA è\ndefinito (analogia con PCA)dagli autovettori relativi aiprimi𝑘(𝑘<\n𝑛,𝑘<𝑠,𝑘<𝑑)autovalori della matrice𝐒𝑤−1𝐒𝑏.𝑝𝑎𝑡𝑡𝑒𝑟𝑛𝑑𝑒𝑙𝑙𝑎𝑐𝑙𝑎𝑠𝑠𝑒𝑖−𝑒𝑠𝑖𝑚𝑎\n𝑚𝑒𝑑𝑖𝑎𝑔𝑙𝑜𝑏𝑎𝑙𝑒\nvaloremassimo di𝑘=𝑠−1",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#27": "t-distributed Stochastic Neighbor Embedding (t -SNE)\n14prof. Davide Maltoni –Università di Bologna\nML\nRiduzione Dimensionalitàt-distributed Stochastic Neighbor \nEmbedding (t-SNE)\nÈunatecnica nonlineare (nonsupervisionata )perlariduzione\ndidimensionalità introdotta nel2008 daVan derMaaten e\nHinton [1].\nRappresenta\n lostatodell’arte perlavisualizzazione 2Do3Ddi\ndati multidimensionali .Implementazione disponibile inmolti\nlinguaggi in[2].\nAnche\n PCA (con𝑘=2o𝑘=3)può essere utilizzata atale\nscopo, madaticondistribuzioni spiccatamente nonmultinormali\nnon possono essere efficacemente «ridotti» attraverso un\nmapping lineare .\nEsempio\n :visualizzazione 2DdiMNIST (digit scritti amano) .\n[1]L.J.P.vanderMaaten and G.E.Hinton .Visualizing High-Dimensional Data Using t-\nSNE.Journal ofMachine Learning Research ,2008 .\n[2]https ://lvdmaaten .github .io/tsne/\n14prof. Davide Maltoni –Università di Bologna\nML\nRiduzione Dimensionalitàt-distributed Stochastic Neighbor \nEmbedding (t-SNE)\nÈunatecnica nonlineare (nonsupervisionata )perlariduzione\ndidimensionalità introdotta nel2008 daVan derMaaten e\nHinton [1].\nRappresenta\n lostatodell’arte perlavisualizzazione 2Do3Ddi\ndati multidimensionali .Implementazione disponibile inmolti\nlinguaggi in[2].\nAnche\n PCA (con𝑘=2o𝑘=3)può essere utilizzata atale\nscopo, madaticondistribuzioni spiccatamente nonmultinormali\nnon possono essere efficacemente «ridotti» attraverso un\nmapping lineare .\nEsempio\n :visualizzazione 2DdiMNIST (digit scritti amano) .\n[1]L.J.P.vanderMaaten and G.E.Hinton .Visualizing High-Dimensional Data Using t-\nSNE.Journal ofMachine Learning Research ,2008 .\n[2]https ://lvdmaaten .github .io/tsne/\n14prof. Davide Maltoni –Università di Bologna\nML\nRiduzione Dimensionalitàt-distributed Stochastic Neighbor \nEmbedding (t-SNE)\nÈunatecnica nonlineare (nonsupervisionata )perlariduzione\ndidimensionalità introdotta nel2008 daVan derMaaten e\nHinton [1].\nRappresenta\n lostatodell’arte perlavisualizzazione 2Do3Ddi\ndati multidimensionali .Implementazione disponibile inmolti\nlinguaggi in[2].\nAnche\n PCA (con𝑘=2o𝑘=3)può essere utilizzata atale\nscopo, madaticondistribuzioni spiccatamente nonmultinormali\nnon possono essere efficacemente «ridotti» attraverso un\nmapping lineare .\nEsempio\n :visualizzazione 2DdiMNIST (digit scritti amano) .\n[1]L.J.P.vanderMaaten andG.E.Hinton .Visualizing High-Dimensional Data Using t-\nSNE.Journal ofMachine Learning Research ,2008 .\n[2]https ://lvdmaaten .github .io/tsne/\n",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#28": "14prof. Davide Maltoni –Università di Bologna\nML\nRiduzione Dimensionalitàt-distributed Stochastic Neighbor \nEmbedding (t-SNE)\nÈunatecnica nonlineare (nonsupervisionata )perlariduzione\ndidimensionalità introdotta nel2008 daVan derMaaten e\nHinton [1].\nRappresenta\n lostatodell’arte perlavisualizzazione 2Do3Ddi\ndati multidimensionali .Implementazione disponibile inmolti\nlinguaggi in[2].\nAnche\n PCA (con𝑘=2o𝑘=3)può essere utilizzata atale\nscopo, madaticondistribuzioni spiccatamente nonmultinormali\nnon possono essere efficacemente «ridotti» attraverso un\nmapping lineare .\nEsempio\n :visualizzazione 2DdiMNIST (digit scritti amano) .\n[1]L.J.P.vanderMaaten and G.E.Hinton .Visualizing High-Dimensional Data Using t-\nSNE.Journal ofMachine Learning Research ,2008 .\n[2]https ://lvdmaaten .github .io/tsne/\nt-SNE: Esempio",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#29": "t-SNE in Scikit -Learn\nPer maggiori dettagli vedi documentazione online della libreria\nopen source scikit -learn :sklearn .manifold .TSNE!Esempio :visualizzazione 2Ddiundataset contenente immagini\n(non controllate) di1000 Cani +1000 Gatti, utilizzando come\nfeature diinput leHOG (Histogram ofOriented Gradients)\ninvece deipixel .\n15prof. Davide Maltoni –Università di Bologna\nML\nRiduzione Dimensionalitàt-SNE in sklearn\nEsempio\n :visualizzazione 2Ddiundataset contente immagini\n(non controllate) di1000 Cani +1000 Gatti, utilizzando come\nfeature diinput leHOG (Histogram ofOriented Gradients )\ninvece deipixel.\nVedi\n https ://distill .pub/2016 /misread -tsne/perconsigli sucome\ntarare iparametri (es.perplexity )ditsne.\nI due cluster sono \nmolto sovrapposti. I \nclassificatori \ntradizionali (es. SVM) \nraggiungono il 70% \ncirca, sfruttando la \nmaggiore densità dei \npatter verdi (gatti) in \nuna certa regione.",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#3": "Le Principali Tecniche\n3prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàLe principali tecniche\nLepiùnote tecniche diriduzione didimensionalità (che vedremo)\nsono :\nPrincipal\n Component Analysis (PCA):trasformazione non-\nsupervisionata nota anche come Karhunen Loeve (KL)\ntransform .Esegue unmapping lineare conl’obiettivo di\npreservare almassimo l’informazione deipattern .\nLinear\n Discriminant Analysis (LDA):ilmapping èancora lineare ,\nmainquesto caso èsupervisionato .Mentre PCA privilegia le\ndimensioni cherappresentano almeglio ipattern, LDA privilegia\nledimensioni chediscriminano almeglio ipattern delTS.\nt-distributed Stochastic Neighbor Embedding (t-SNE):\ntrasformazione non lineare e non supervisionata ,\nspecificatamente ideata per ridurre dimensionalità a2o3\ndimensioni onde poter visualizzare datimultidimensionali .\nAltre tecniche diinteresse :\nIndependent\n Component Analysis (ICA):trasformazione lineare\norientata aproiettare ipattern suuna base dicomponenti\n(statisticamente indipendenti ).\nKernel\n PCA:simile aPCA mapiùpotente perché ilmapping è\nnon-lineare .Utilizza un«trucco» simile aquello chepermette di\npassare daSVM lineare aSVM nonlineare .\nLocal\n Linear Embedding (LLE):trasformazione non-lineare che\ninvece dicalcolare unmapping «globale», considera relazioni\ntragruppi dipattern vicini .",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#30": "15prof. Davide Maltoni –Università di Bologna\nML\nRiduzione Dimensionalitàt-SNE in sklearn\nEsempio\n :visualizzazione 2Ddiundataset contente immagini\n(non controllate) di1000 Cani +1000 Gatti, utilizzando come\nfeature diinput leHOG (Histogram ofOriented Gradients )\ninvece deipixel.\nVedi\n https ://distill .pub/2016 /misread -tsne/perconsigli sucome\ntarare iparametri (es.perplexity )ditsne.\nI due cluster sono \nmolto sovrapposti. I \nclassificatori \ntradizionali (es. SVM) \nraggiungono il 70% \ncirca, sfruttando la \nmaggiore densità dei \npatter verdi (gatti) in \nuna certa regione.\n15prof. Davide Maltoni –Università di Bologna\nML\nRiduzione Dimensionalitàt-SNE in sklearn\nEsempio\n :visualizzazione 2Ddiundataset contente immagini\n(non controllate) di1000 Cani +1000 Gatti, utilizzando come\nfeature diinput leHOG (Histogram ofOriented Gradients )\ninvece deipixel.\nVedi\n https ://distill .pub/2016 /misread -tsne/perconsigli sucome\ntarare iparametri (es.perplexity )ditsne.\nI due cluster sono \nmolto sovrapposti. I \nclassificatori \ntradizionali (es. SVM) \nraggiungono il 70% \ncirca, sfruttando la \nmaggiore densità dei \npatter verdi (gatti) in \nuna certa regione.I due cluster appaiono molto \nsovrapposti. \nI classificatori tradizionali \n(e.g., SVM) raggiungono una \naccuratezza di circa il 70%, \nsfruttando la maggiore \ndensità dei pattern verdi \n(gatti) in una certa regione.t-SNE in Scikit -Learn",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#31": "!Matlab Toolbox for Dimensionality Reduction:\nhttps://lvdmaaten.github.io/drtoolbox/\nt-SNE in Matlab",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#32": "Riferimenti\n!S.J. Russell & P. Norvig, Artificial Intelligence: A Modern \nApproach (4 ed.) , Pearson, 2020.\n!K. Fukunaga, Statistical Pattern Recognition , Academic Press, \n1990.\n!D. Maltoni , Machine Learning , Università di Bologna, 2017.\n!C.M. Bishop, Pattern Recognition and Machine Learning , \nSpringer, 2006.\n!K.P. Murphy, Machine Learning: A Probabilistic Perspective , The \nMIT Press, 2012.\n!R.O. Duda , P.E. Hart, and D.G. Stork. 2000. Pattern Classification \n(2nd Edition). Wiley -Interscience , New York, NY, USA. ",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#4": "Le Principali Tecniche\n3prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàLe principali tecniche\nLepiùnote tecniche diriduzione didimensionalità (che vedremo)\nsono :\nPrincipal\n Component Analysis (PCA):trasformazione non-\nsupervisionata nota anche come Karhunen Loeve (KL)\ntransform .Esegue unmapping lineare conl’obiettivo di\npreservare almassimo l’informazione deipattern .\nLinear\n Discriminant Analysis (LDA):ilmapping èancora lineare ,\nmainquesto caso èsupervisionato .Mentre PCA privilegia le\ndimensioni cherappresentano almeglio ipattern, LDA privilegia\nledimensioni chediscriminano almeglio ipattern delTS.\nt-distributed Stochastic Neighbor Embedding (t-SNE):\ntrasformazione non lineare e non supervisionata ,\nspecificatamente ideata per ridurre dimensionalità a2o3\ndimensioni onde poter visualizzare datimultidimensionali .\nAltre tecniche diinteresse :\nIndependent\n Component Analysis (ICA):trasformazione lineare\norientata aproiettare ipattern suuna base dicomponenti\n(statisticamente indipendenti ).\nKernel\n PCA:simile aPCA mapiùpotente perché ilmapping è\nnon-lineare .Utilizza un«trucco» simile aquello chepermette di\npassare daSVM lineare aSVM nonlineare .\nLocal\n Linear Embedding (LLE):trasformazione non-lineare che\ninvece dicalcolare unmapping «globale», considera relazioni\ntragruppi dipattern vicini .",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#5": "Esempio PCA vs LDA\n4prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàEsempio PCA vs LDA\nInfigura dueesempi diriduzione didimensionalità da𝑑=2a𝑘=\n1dimensione :\nIl\nsegmento nero cheidentifica lasoluzione PCA èl’iperpiano\nsulquale proiettando ipattern (indipendentemente dalla loro\nclasse) conserviamo almassimo l’informazione .\nIl\nsegmento verde cheidentifica lasoluzione LDA èl’iperpiano\nsulquale proiettando ipattern siamo ingrado didistinguere al\nmeglio ledueclassi (pattern rossi contro blu).\nEntrambi sono mapping lineari2→1malasoluzione (retta) è\nprofondamente diversa .\n4prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàEsempio PCA vs LDA\nInfigura dueesempi diriduzione didimensionalità da𝑑=2a𝑘=\n1dimensione :\nIl\nsegmento nero cheidentifica lasoluzione PCA èl’iperpiano\nsulquale proiettando ipattern (indipendentemente dalla loro\nclasse) conserviamo almassimo l’informazione .\nIl\nsegmento verde cheidentifica lasoluzione LDA èl’iperpiano\nsulquale proiettando ipattern siamo ingrado didistinguere al\nmeglio ledueclassi (pattern rossi contro blu).\nEntrambi sono mapping lineari2→1malasoluzione (retta) è\nprofondamente diversa .\n",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#6": "Principal Component Analysis (PCA)\n5prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPrincipal Component Analysis \n(PCA)\nDato untraining set𝑇𝑆=𝐱𝑖∈𝑑,𝑖=1…𝑛,siano :\nത𝐱=1\n𝑛෍\n𝑖=1…𝑛𝐱𝑖\n𝚺=1\n𝑛−1෍\n𝑖=1…𝑛𝐱𝑖−ത𝐱𝐱𝑖−ത𝐱𝑡\nilvettore medio∈𝑑elamatrice dicovarianza ∈𝑑×𝑑\n(definizioni simili aquelle usate per ilclassificatore diBayes\nparametrico con multinormali .Ladivisione per𝑛−1invece di\n𝑛dovuta acorrezione diBessel percaso unbiased ).\nallora perundato𝑘(𝑘<𝑑,𝑘<𝑛,𝑘>0),lospazio𝑘dimensionale\n(𝑆ത𝐱,Φ𝑘)èunivocamente definito dalvettore medio edalla matrice di\nproiezione Φ𝑘∈𝑑×𝑘lecui colonne sono costituite dagli\nautovettori di𝚺corrispondenti ai𝑘piùgrandi autovalori :\nΦ𝑘=𝝋𝑖1,𝝋𝑖2…𝝋𝑖𝑘con  𝜆𝑖1≥𝜆𝑖2≥⋯𝜆𝑖𝑘≥⋯𝜆𝑖𝑑\n𝝋𝑖𝑟autovettore di 𝚺corrispondente all’ autovalore 𝜆𝑖𝑟, 𝑟=1…𝑑\n𝝋𝒊𝟏indica la direzione di \nmaggior varianza nel \ntraining set TS𝝋𝒊𝟏\n𝝋𝒊𝟐\nI primi 𝑘autovettori sono \ndetti componenti principali",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#7": "Principal Component Analysis (PCA)\n5prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPrincipal Component Analysis \n(PCA)\nDato untraining set𝑇𝑆=𝐱𝑖∈𝑑,𝑖=1…𝑛,siano :\nത𝐱=1\n𝑛෍\n𝑖=1…𝑛𝐱𝑖\n𝚺=1\n𝑛−1෍\n𝑖=1…𝑛𝐱𝑖−ത𝐱𝐱𝑖−ത𝐱𝑡\nilvettore medio∈𝑑elamatrice dicovarianza ∈𝑑×𝑑\n(definizioni simili aquelle usate per ilclassificatore diBayes\nparametrico con multinormali .Ladivisione per𝑛−1invece di\n𝑛dovuta acorrezione diBessel percaso unbiased ).\nallora perundato𝑘(𝑘<𝑑,𝑘<𝑛,𝑘>0),lospazio𝑘dimensionale\n(𝑆ത𝐱,Φ𝑘)èunivocamente definito dalvettore medio edalla matrice di\nproiezione Φ𝑘∈𝑑×𝑘lecui colonne sono costituite dagli\nautovettori di𝚺corrispondenti ai𝑘piùgrandi autovalori :\nΦ𝑘=𝝋𝑖1,𝝋𝑖2…𝝋𝑖𝑘con  𝜆𝑖1≥𝜆𝑖2≥⋯𝜆𝑖𝑘≥⋯𝜆𝑖𝑑\n𝝋𝑖𝑟autovettore di 𝚺corrispondente all’ autovalore 𝜆𝑖𝑟, 𝑟=1…𝑑\n𝝋𝒊𝟏indica la direzione di \nmaggior varianza nel \ntraining set TS𝝋𝒊𝟏\n𝝋𝒊𝟐\nI primi 𝑘autovettori sono \ndetti componenti principali",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#8": "Principal Component Analysis (PCA)\n!Riassumendo\n!Dagli npattern delTraining Set (TS) sicalcolano vettore medio e\nmatrice dicovarianza !\n!Dalla matrice dicovarianza sicalcolano idautovalori eautovettori\n!Dei dautovalori siconsiderano solo ikautovalori con valore\nmaggiore (inordine decrescente)\n!Lamatrice diproiezione \"ksarà unmatrice (d×k)lecuikcolonne\nsono costituite dagli autovettori relativi aikautovalori calcolati\ncome sopra\n!Gli autovettori !idella matrice di \ncovarianza \"sono paralleli agli \nassi dell’ ellisse che rappresenta \nladistribuzione dei pattern nel TS\n!Gli autovalori #isono le varianze \nlungo gli assi !i\n5prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPrincipal Component Analysis \n(PCA)\nDato untraining set𝑇𝑆=𝐱𝑖∈𝑑,𝑖=1…𝑛,siano :\nത𝐱=1\n𝑛෍\n𝑖=1…𝑛𝐱𝑖\n𝚺=1\n𝑛−1෍\n𝑖=1…𝑛𝐱𝑖−ത𝐱𝐱𝑖−ത𝐱𝑡\nilvettore medio∈𝑑elamatrice dicovarianza ∈𝑑×𝑑\n(definizioni simili aquelle usate per ilclassificatore diBayes\nparametrico con multinormali .Ladivisione per𝑛−1invece di\n𝑛dovuta acorrezione diBessel percaso unbiased ).\nallora perundato𝑘(𝑘<𝑑,𝑘<𝑛,𝑘>0),lospazio𝑘dimensionale\n(𝑆ത𝐱,Φ𝑘)èunivocamente definito dalvettore medio edalla matrice di\nproiezione Φ𝑘∈𝑑×𝑘lecui colonne sono costituite dagli\nautovettori di𝚺corrispondenti ai𝑘piùgrandi autovalori :\nΦ𝑘=𝝋𝑖1,𝝋𝑖2…𝝋𝑖𝑘con  𝜆𝑖1≥𝜆𝑖2≥⋯𝜆𝑖𝑘≥⋯𝜆𝑖𝑑\n𝝋𝑖𝑟autovettore di 𝚺corrispondente all’ autovalore 𝜆𝑖𝑟, 𝑟=1…𝑑\n𝝋𝒊𝟏indica la direzione di \nmaggior varianza nel \ntraining set TS𝝋𝒊𝟏\n𝝋𝒊𝟐\nI primi 𝑘autovettori sono \ndetti componenti principali",
    "data_test\\rootfolder\\università\\MachineLearning\\38-RD-sbloccato.pdf#9": "PCA: Proiezione\n6prof. Davide Maltoni –Università di Bologna\nML\nRiduzione DimensionalitàPCA: proiezione e retroproiezione\nProiezione\n (o):una volta determinato lospazio PCA, la\nproiezione diunpattern sutale spazio èsemplicemente la\nproiezione geometrica diunvettoresull’iperpiano chedefinisce\nlospazio .Inrealtà lavera proiezione geometrica èunvettore\nchehalastessa dimensionalità delvettore originale mentre in\nquesto contesto indichiamo con proiezione ilvettore (ridotto)\nnello spazio PCA.Matematicamente questa operazione è\neseguita come prodotto della matrice diproiezione trasposta\nperilpattern alquale èpreventivamente sottratta lamedia .\n𝑃𝐶𝐴:𝑑→𝑘\n𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘=Φ𝑘𝑡𝐱−ത𝐱\nRetro\n -proiezione (m):Dato unvettore nello spazio PCA, lasua\nretro-proiezione verso lospazio originale siottiene moltiplicando\nilvettore perlamatrice diproiezione esommando ilvettore\nmedio .Questa trasformazione non sposta spazialmente il\nvettore, che giace ancora sullo spazio PCA,maopera un\ncambiamento dicoordinate che nepermette lacodifica in\ntermini delle𝑑componenti dello spazio originale .\n𝑃𝐶𝐴:𝑘→𝑑\n𝑃𝐶𝐴𝐲,𝑆ത𝐱,Φ𝑘=Φ𝑘𝐲+ത𝐱𝐱\n𝑑=3,𝑘=2\n𝑆ത𝐱,Φ𝑘𝐲=𝑃𝐶𝐴𝐱,𝑆ത𝐱,Φ𝑘",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Riduzione dimensionalità (Ex 15)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#1": "Sommario\nRichiami riduzione dimensionalità, projection, mainfold learning \nPCA in Python \nScikit-learn e PCA \nPCA e compressione \nRandomized PCA \nKernel PCA \nLocally Linear Embedding LLE \nEsercitazione",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#10": "PCA: Step by step in Python\nUna volta ottenute le componenti, deﬁniamo il nuovo iperspazio con \n d \ndimensioni. Prendiamo le prime d colonne di \n V\n e proiettiamo le istanze nel \nnuovo spazio: \nX\nd-proj\n = X W\n d \nIn Python: \nW2 \n= \nVt\n.\nT\n[:, :\n2\n]\nX2D \n= \nX_centered\n .\ndot\n(\nW2\n)\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#11": "Scikit-learn e PCA\nScikit-learn implementa la PCA nel seguente modo: \nfrom \nsklearn.decomposition \n import \nPCA\npca \n= \nPCA\n(\nn_components \n = \n2\n)\nX2D \n= \npca\n.\nfit_transform\n (\nX\n)\nLa variabile components_ contiene i vettori. Per accedere al primo vettore:  \npca.components_.T[:,0] \nOgni componente è associata alla relativa varianza che si può analizzare \ncon la variabile \n explained_variance_ratio_\n . Considerando l'esempio \nprecedente si ottiene: \n>>> \npca\n.\nexplained_variance_ratio_\narray([0.84248607, 0.14631839])\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#12": "Scikit-learn e PCA\nLa scelta del numero di dimensioni dipende dalla varianza: si tende a \nscegliere il numero che garantisce sempre elevata varianza (es. 95%). \nEccezione se vogliamo fare un diagramma dei campioni, in tal caso ci \noccorrono 2 o 3 dimensioni. \nIl seguente codice ricava il numero di dimensioni che preservano il 95% \ndella varianza sul training set: \npca \n= \nPCA\n()\npca\n.\nfit\n(\nX_train\n)\ncumsum \n= \nnp\n.\ncumsum\n(\npca\n.\nexplained_variance_ratio_\n )\nd \n= \nnp\n.\nargmax\n(\ncumsum \n>= \n0.95\n) \n+ \n1\nUn modo alternativo per speciﬁcare la varianza: \npca \n= \nPCA\n(\nn_components\n =\n0.95\n)\nX_reduced \n = \npca\n.\nfit_transform\n (\nX_train\n)\n(continua)\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#13": "Scikit-learn e PCA\nUlteriore metodo per ﬁssare d è fare un graﬁco del cumsum della varianza. \nIl \ngomito\n  della curva è dove la varianza interrompe la crescita veloce.  \nNell'esempio una dimensionalità inferiore a 100 è un valore ideale:\n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#14": "PCA e compressione\nUna volta ottenuto il dataset proiettato sulle componenti, la dimensione del \ndataset si riduce sensibilmente. \nLa funzione \n inverse_transform\n () di PCA permette di ricostruire una \napprosimazione del dataset con le dimensioni originali a partire dalle \nistanze nello spazio ridotto. \nX\nrecovered\n  = X\n d-proj\n W\nd\nT \nEsercizio\n : (1) impiega il dataset MNIST (cifre, 784 dimensioni) e applica la \nPCA riducendo le dimensioni a 154, e valuta le dimensioni del nuovo \ndataset. (2) visualizza le cifre ricostruite con \n inverse_transform(). \nSuggerimento per visualizzare le cifre\n : \nimport\n matplotlib.pyplot \n as\n plt\nplt.imshow(X_train[\n 0\n].reshape((\n 28\n, \n28\n)), cmap=\n 'gray'\n)\nplt.show()\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#15": "PCA e compressione\nEsercizio\n : (1) impiega il dataset MNIST (cifre, 784 dimensioni) e applica la \nPCA riducendo le dimensioni a 154, e valuta le dimensioni del nuovo \ndataset \nfrom\n sklearn.decomposition \n import\n PCA\nfrom\n sklearn \n import\n datasets\nfrom\n sklearn.model_selection \n import\n train_test_split\nfrom\n sklearn.datasets \n import\n fetch_openml\nfrom\n sklearn.preprocessing \n import\n StandardScaler\nmnist = fetch_openml(\n 'mnist_784'\n )\nX_train, X_test, y_train, y_test  = train_test_split\n        (mnist.data, mnist.target, test_size=\n 1\n/\n7.0\n, random_state=\n 0\n)\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\npca = PCA(n_components = \n 154\n)\nX_reduced = pca.fit_transform(X_train)\nX_recovered = pca.inverse_transform(X_reduced)\n(continua)\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#16": "PCA e compressione\noriginal_len = X_train.flatten().size\nprint\n(original_len)\nred_len = X_reduced.flatten().size\nprint\n(red_len)\nrec_len = X_recovered.flatten().size\nprint\n(rec_len)\npct = (red_len - original_len) * \n 100\n / original_len\nprint\n (pct)\n47040000\n9240000\n47040000\n-80.35714285714286 \nDataset ridotto al 20% della dimensione originale! \n17",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#17": "PCA e compressione\nEsercizio\n : ... (2) visualizza le cifre ricostruite con \n inverse_transform().  \nimport\n matplotlib.pyplot \n as\n plt\nplt.imshow(X_train[\n 0\n].reshape((\n 28\n, \n28\n)), cmap=\n 'gray'\n)\nplt.show()\nplt.imshow(X_recovered[\n 0\n].reshape((\n 28\n, \n28\n)), cmap=\n 'gray'\n)\nplt.show()\n18\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#18": "Scikit-learn: Randomized PCA\nImpiegando il parametro \n svd_solver\n =\n\"randomized\", impieghiamo \nl'algoritmo Randomized PCA che riduce notevolmente il tempo di \nesecuzione essendo d << n: \nO(\nm × n\n2\n) + O(\n n\n3\n)  --->   O(\n m × d\n2\n) + O(\n d\n3\n) \nPer default sciki-learn usa il solver \n auto\n, che impiega la versione \nrandomized se \n m\n o \nn\n sono maggiori di 500 e d è minore del 80% rispetto a \nm\n o \nn\n.  \nPer forzare l'impiego della versione esatta, impiegare l'opzione \n full\n. \n19",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#19": "Scikit-learn: Incremental PCA\nL'algoritmo PCA richieda che l'intero dataset sia presente in memoria. \nL'algoritmo Incremental PCA accetta il dataset suddiviso in mini-batch, e \npuò essere impiegato \n online\n . \nfrom \nsklearn.decomposition \n import \nIncrementalPCA\nn_batches \n = \n100\ninc_pca \n = \nIncrementalPCA\n (\nn_components\n =\n154\n)\nfor \nX_batch \n in \nnp\n.\narray_split\n (\nX_train\n, \nn_batches\n ):\n    \ninc_pca\n.\npartial_fit\n (\nX_batch\n)\nX_reduced \n = \ninc_pca\n.\ntransform\n (\nX_train\n)\nCon la classe memmap di NumPy possiamo leggere gli array \nincrementalmente da ﬁle binary: \nX_mm \n= \nnp\n.\nmemmap\n(\nfilename\n , \ndtype\n=\n\"float32\"\n , \nmode\n=\n\"readonly\"\n , \nshape\n=\n(\nm\n, \nn\n))\nbatch_size \n = \nm \n// \nn_batches\ninc_pca \n = \nIncrementalPCA\n (\nn_components\n =\n154\n, \nbatch_size\n =\nbatch_size\n )\ninc_pca\n.\nfit\n(\nX_mm\n)\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#2": "Richiami: riduzione di dimensionalità\nIn casi reali i dati da analizzare possono essere sempliﬁcati riducendo il \nnumero di features. L'obiettivo è velocizzare il processo di training senza \ninﬂuire troppo sulle performance, e ridurre l'eventuale rumore, es: \nNel MNIST dataset i pixel nei bordi sono sempre bianchi, possiamo \npensare di rimuoverli \nSpesso i pixel neri sono correlati, cioè appaiono vicini. È possibile \nfonderli facendone una media.  \nSe creassimo a caso immagini, in rarissimi casi potrebbero assomigliare \nalle cifre nel dataset. Perciò i gradi di libertà disponibili nella creazione \ndi istanze sono notevolmente ridotti rispetto a quelli potenziali in uno \nspazio con le medesime dimensioni. \nInoltre la riduzione di dimensionalità permette di creare graﬁci nel caso in \ncui le dimensioni siano 2 o 3.\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#20": "Scikit-learn: Kernel PCA (kPCA)\nIn modo simile ai kernel del SVM è possibile fare proiezioni non lineari \ncomplesse per la riduzione di dimensionalità. \nSimilmente al SVM, ha il vantaggio di mantenere cluster di istanze dopo la \nproiezione, oppure operare \n unrolling\n  di forme complesse. \nIn scikit-learn impiegando il kernel rbf si ha: \nfrom \nsklearn.decomposition \n import \nKernelPCA\nrbf_pca \n = \nKernelPCA\n (\nn_components \n = \n2\n, \nkernel\n=\n\"rbf\"\n, \ngamma\n=\n0.04\n)\nX_reduced \n = \nrbf_pca\n.\nfit_transform\n (\nX\n)\nDi seguito alcuni esempi di proiezioni a 2 dimensioni con vari kernel:\n21\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#21": "Scikit-learn: Kernel PCA (kPCA)\nLa scelta del kernel dipende dal task. Se il task è supervisionato, si può fare \nuna semplice grid search sui kernel e i relativi iperparametri per ottenere \nperformance migliori. \nNel seguente codice si impiega una pipeline per il task di regressione: \nfrom \nsklearn.model_selection \n import \nGridSearchCV\nfrom \nsklearn.linear_model \n import \nLogisticRegression\nfrom \nsklearn.pipeline \n import \nPipeline\nclf \n= \nPipeline\n ([\n(\n\"kpca\"\n, \nKernelPCA\n (\nn_components\n =\n2\n)),\n(\n\"log_reg\"\n , \nLogisticRegression\n ())\n])\nparam_grid \n = \n[{\n\"kpca__gamma\"\n : \nnp\n.\nlinspace\n (\n0.03\n, \n0.05\n, \n10\n),\n\"kpca__kernel\"\n : [\n\"rbf\"\n, \n\"sigmoid\"\n ]\n}]\ngrid_search \n = \nGridSearchCV\n (\nclf\n, \nparam_grid\n , \ncv\n=\n3\n)\ngrid_search\n .\nfit\n(\nX\n, \ny\n)\n>>> \nprint\n(\ngrid_search\n .\nbest_params_\n )\n{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#22": "Scikit-learn: Kernel PCA (kPCA)\nSe il task è unsupervised, è possibile comunque fare un tuning dei \nparametri?\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#23": "Scikit-learn: Kernel PCA (kPCA)\nSe il task è unsupervised, è possibile comunque fare un tuning dei \nparametri? \nSe ricostruiamo i dati originali con le componenti PCA possiamo valutare \nle performance con una misura di discostamento (es. distanza quadratica). \nAttenzione\n : l'impiego dei kernel implica che la ricostruzione generi un \nfeature space inﬁnito-dimensionale. Perciò non è possibile confrontare \ndirettamente le istanze con lo spazio originale. Con la tecnica \nrecontruction pre-image è possibile trovare il punto nello spazio originale \nche è vicino alla istanza \n ricostruita\n .\n24\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#24": "Scikit-learn: Kernel PCA (kPCA)\nIl parametro ﬁt_inverse_transform=True adotta questa strategia: \nrbf_pca \n = \nKernelPCA\n (\nn_components \n = \n2\n, \nkernel\n=\n\"rbf\"\n, \ngamma\n=\n0.0433\n,\n              \n fit_inverse_transform\n =\nTrue\n)\nX_reduced \n = \nrbf_pca\n.\nfit_transform\n (\nX\n)\nX_preimage \n = \nrbf_pca\n.\ninverse_transform\n (\nX_reduced\n )\n>>> \nfrom \nsklearn.metrics \n import \nmean_squared_error\n>>> \nmean_squared_error\n (\nX\n, \nX_preimage\n )\n32.786308795766132\n25",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#25": "Scikit-learn: Locally Linear Embedding (LLE)\nÈ una ulteriore tecnica non lineare, ma non si basa sulle proiezioni. \nIn sintesi, nella prima fase analizza le similarità tra una istanza e le istanze \nvicine (neighbors), e successivamente crea un iperspazio con meno \ndimensioni che mantiene queste tali relazioni.  \nÈ un approccio ideale per fare unrolling e in presenza di poco rumore. \nfrom \nsklearn.manifold \n import \nLocallyLinearEmbedding\nlle \n= \nLocallyLinearEmbedding\n (\nn_components\n =\n2\n, \nn_neighbors\n =\n10\n)\nX_reduced \n = \nlle\n.\nfit_transform\n (\nX\n)\nLo spazio risultate è correttamente dispiegato, anche se le distanze relative \nnon sono mantenute coerenti.\n26\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#26": "Esercitazione\nCarica il dataset MNIST e suddividilo in 60K training e 10K test set. \nAddestra un classiﬁcatore Random Forest, calcola il tempo di training e le \nperformance. \nApplica PCA con una \n explained variance ratio\n  del 95%. \nAddestra un nuovo classiﬁcatore Random Forest, calcola il tempo di \ntraining e le performance, e confronta i valori con i valori precedenti.\n27",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#27": "Esercitazione\nfrom\n sklearn.datasets \n import\n fetch_openml\nmnist = fetch_openml(\n 'mnist_784'\n )\nX_train = mnist[\n 'data'\n][:\n60000\n]\ny_train = mnist[\n 'target'\n ][:\n60000\n]\nX_test = mnist[\n 'data'\n][\n60000\n:]\ny_test = mnist[\n 'target'\n ][\n60000\n:]\nfrom\n sklearn.ensemble \n import\n RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=\n 100\n, random_state=\n 42\n)\nimport\n time\nt0 = time.time()\nrnd_clf.fit(X_train, y_train)\nt1 = time.time()\nprint\n(\n\"Training took {:.2f}s\"\n .\nformat\n(t1 - t0))\n28",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#28": "Esercitazione\nfrom\n sklearn.metrics \n import\n accuracy_score\ny_pred = rnd_clf.predict(X_test)\naccuracy_score(y_test, y_pred)\nfrom\n sklearn.decomposition \n import\n PCA\npca = PCA(n_components=\n 0.95\n)\nX_train_reduced = pca.fit_transform(X_train)\nrnd_clf2 = RandomForestClassifier(n_estimators=\n 100\n, random_state=\n 42\n)\nt0 = time.time()\nrnd_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\nprint\n(\n\"Training took {:.2f}s\"\n .\nformat\n(t1 - t0))\nX_test_reduced = pca.transform(X_test)\ny_pred = rnd_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)\n29",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#29": "Esercitazione\nfrom\n sklearn.linear_model \n import\n LogisticRegression\nlog_clf = LogisticRegression(multi_class=\n \"multinomial\"\n , solver=\n \"lbfgs\"\n, \nrandom_state=\n 42\n)\nt0 = time.time()\nlog_clf.fit(X_train, y_train)\nt1 = time.time()\nprint\n(\n\"Training took {:.2f}s\"\n .\nformat\n(t1 - t0))\ny_pred = log_clf.predict(X_test)\nprint(\naccuracy_score(y_test, y_pred))\nlog_clf2 = LogisticRegression(multi_class=\n \"multinomial\"\n , solver=\n \"lbfgs\"\n, \nrandom_state=\n 42\n)\nt0 = time.time()\nlog_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\nprint\n(\n\"Training took {:.2f}s\"\n .\nformat\n(t1 - t0))\ny_pred = log_clf2.predict(X_test_reduced)\nprint(\naccuracy_score(y_test, y_pred))\n30",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#3": "Osservazione\nPresi 2 punti a caso in un quadrato di dimensioni 1x1, la distanza media è \n0.52. In un cubo 3d è 0.66. In un ipercubo di 1M dimensioni è 408,25. \nIn dataset con alta dimensionalità il rischio di data sparity è elevato. \nUna nuova istanza è molto probabile che sia lontana dalle altre già \nincontrate in precedenza; probabile overﬁtting durante il test. \nSe incrementiamo le istanze nel training set riduciamo il problema, ma il \ntempo di addestramento si allungano, e a volte non è possibile \ncollezionare nuove istanze. \nIl numero di istanze da includere cresce esponenzialmente col numero \ndi dimensioni del dataset.\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#30": "Esercitazione\n>>>\nTraining took 57.60s\nTraining took 124.90s\n0.9255\nTraining took 55.04s\nTraining took 15.68s\n0.9201\n31",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#31": "Andreas C. Müller, Sarah Guido. \n Introduction to Machine Learning with \nPython: A Guide for Data Scientists\n . O'Reilly Media 2016  \nAurélien Géron. \n Hands-On Machine Learning with Scikit-Learn and \nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n . \nO'Reilly Media 2017\nTesti di Riferimento\n32",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#4": "Projection\nSpesso le features sono correlate tra loro, oppure assumono spesso valori in \npiccoli intervalli. \nNell'esempio seguente si può notare come molti punti sono vicini ad un \npiano (2d). Se proiettiamo questi punti sul piano (sottospazio) otteniamo un \ndataset di dimensioni ridotte, con 2 nuove features, cioè le coordinate nel \npiano.\n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#5": "Projection\nIn altri casi la proiezione è difﬁcile o impossibile. \nNel dataset toy Swiss roll, semplici piani non permettono di mantenere la \ncoerenza spaziale originale delle istanze. Solo \"dispiegando il rotolo\" di \npunti è possibile avere un piano coerente.\n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#6": "Manifold learning\nNell'esempio precedente, il \n roll\n è una struttura che assomiglia a un piano \n2d (d=2) che è disposta (rotolata) in uno spazio 3d (n=3 con n>d). \nManifold assumption\n : l'ipotesi che molti dataset reali possono essere \nrappresentati in spazi con dimensioni ridotte. Inoltre suppone che nello \nspazio ridotto sia più facile risolvere problemi di classiﬁcazione, \nregressione, etc.  \nQuest'ultima assunzione non è sempre vera, es:\n7\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#7": "Richiami: PCA\nPer proiettare i dati in uno iperspazio con meno dimensioni, occorre prima \ndeﬁnirlo. Nell'esempio, il piano con linea continua mantiene la varianza \nmassima, mentre i 2 piani tratteggiati hanno varianze più basse. \nQuale piano sceglieresti?\n8\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#8": "Richiami: PCA\nPer proiettare i dati in uno iperspazio con meno dimensioni, occorre prima \ndeﬁnirlo. Nell'esempio, il piano con linea continua mantiene la varianza \nmassima, mentre i 2 piani tratteggiati hanno varianze più basse. \nIl piano con massima varianza potenzialmente riduce la perdita di \ninformazione durante la proiezione. \nPCA mira a identiﬁcare tali assi.\n9\n",
    "data_test\\rootfolder\\università\\MachineLearning\\39-Ex_15 Riduzione dimensionalita-sbloccato.pdf#9": "PCA: Step by step in Python\nAttenzione\n : se perturbiamo leggermente il training set, i \n principal \ncomponent \n (gli assi) possono cambiare, ad esempio invertendo la \ndirezione, anche se il piano deﬁnito da essi rimane generalmente lo stesso. \nPer trovare le compomenti si impiega la Singular Value Decomposition \n(SVD) che permette di ottenere la seguente decomposizione: \nX = U \n Σ\n V\nT \ndove le colonne di V rappresentano le componenti che stiamo cercando. \nIn Python possiamo ottenerle nel seguente modo: \nX_centered \n = \nX \n- \nX\n.\nmean\n(\naxis\n=\n0\n)\nU\n, \ns\n, \nVt \n= \nnp\n.\nlinalg\n.\nsvd\n(\nX_centered\n )\nc1 \n= \nVt\n.\nT\n[:, \n0\n]\nc2 \n= \nVt\n.\nT\n[:, \n1\n]\nAttenzione\n : è sempre consigliabile centrare i dati nell'origine impiegando \nlo \nStandardScaler\n ().\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nRegressione:  \nValutazione delle prestazioni\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#1": "Sommario\nIntroduzione alla Valutazione delle Prestazioni nella \nRegression \nLoss Function e le tre misure di Loss: \n•\n Training Error, Generalization Error, Test Error \nOverﬁtting \nLe tre fonti di errore: Noise, Bias, Variance\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#10": "Training Error vs.  \nComplessità del Modello\n \n11E’ interessante vedere come può variare tale errore in base \nalla complessità del modello.  \ncaso di modello costante:  \ny\nArea xPrezzo\nComplessità del modelloErrore ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#11": "Training Error vs.  \nComplessità del Modello\n \n12caso di modello lineare:  \ny\nArea xPrezzo\nComplessità del modelloErrore ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#12": "Training Error vs.  \nComplessità del Modello\n \n13\ncaso di modello quadratico:  \nComplessità del modelloErrore \ny\nArea xPrezzo\n",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#13": "Training Error vs.  \nComplessità del Modello\n \n14\ncaso di modello polinomiale:  \nComplessità del modelloErrore \ny\nArea xPrezzo\n",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#14": "Training Error vs.  \nComplessità del Modello\n \n15L’andamento dell’errore ha dunque in genere la seguente \nforma: \nComplessità del modelloErrore ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#15": "Training Error vs.  \nComplessità del Modello\n \n16Il training error non è una buona misura della predictive \nperformance: \nEsso è eccessivamente ottimistico, proprio perché il vettore \nŵ è calcolato afﬁnché il modello si adatti ai dati di training. y\nArea xPrezzo\nxt\n",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#16": "Generalization Error\n \n17Sarebbe interessante conoscere il “loss” prendendo in \nconsiderazione tutte le possibili coppie ( x, y), ossia, per il \nnostro esempio degli appartamenti, tutte le possibili case della \nzona presa in considerazione, calcolando la media della \nfunzione loss su tali appartamenti. \nIn genere però nel nostro data set abbiamo soltanto un \nnumero limitato di osservazioni ( x, y). ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#17": "Generalization Error\n \n18Per effettuare una stima di tale costo dovremmo cercare di \npesare le varie coppie ( x, y) in base alla probabilità che hanno \ndi essere presenti nella zona d’interesse.  \nE’ dunque utile prendere in considerazione la distribuzione \ndegli appartamenti in base al valore della loro area:\nArea",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#18": "Generalization Error\n \n19Potremmo inoltre considerare la distribuzione delle case in \nbase al loro prezzo, a parità di area:\nPrezzo",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#19": "Generalization Error\n \n20Formalmente, possiamo deﬁnire il Generalization (o True) \nError come segue:\nGeneralization Error = Ex,y[L(y,f ˆw(x))]\nossia come l’average value della funzione Loss, calcolato su \ntutte le possibili coppie ( x, y) pesate in base alla loro \nprobabilità di comparire nella zona.",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#2": "La valutazione delle prestazioni è di importanza cruciale \nper poter apprezzare il metodo che stiamo utilizzando per \nle nostre previsioni. \nEssa ci aiuta nella scelta tra modelli di diversa complessità \na nostra disposizione. \nA tal ﬁne occorre deﬁnire una metrica che ci consenta di \nvalutare quanto perdiamo (loss) quando facciamo una \ncerta previsione. \n \n3\nIntroduzione alla  \nValutazione delle Prestazioni",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#20": "Generalization Error\n \n21Vediamo ora di intuire come tale errore possa variare in base \nalla complessità del modello. \nPer far questo ci avvarremo della rappresentazione che segue, \ndove la regione in blu rappresenta, con le diverse gradazioni \nnei vari punti, la distribuzione di probabilità di avere una casa \nnel nostro data set (la parte bianca rappresenta le più alte \nprobabilità):\ny\nArea xPrezzo",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#21": "Generalization Error\n \n22Per valutare l’errore consideriamo come la “ﬁtted function” f \n(in verde nella ﬁgura precedente e in quelle successive), che \nsi adatta alle osservazioni del training set, possa predire i \nvalori delle case non presenti nel training set, pesate dalle \nloro probabilità. \nOssia dobbiamo vedere quanto la f sia “vicina” all’area in \nbianco della distribuzione rappresentata in ﬁgura. \nNei lucidi che seguono cercheremo di intuire l’andamento del \nGeneralization Error a fronte di diverse complessità del \nmodello (costante, lineare, quadratico, ecc.).",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#22": "Generalization Error vs.  \nComplessità del Modello\n \n23caso di modello costante: \nPrezzo\nAreaErrore \nComplessità del modello",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#23": "Generalization Error vs.  \nComplessità del Modello\n \n24caso di modello lineare: \nAreaPrezzoErrore \nComplessità del modello",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#24": "Generalization Error vs.  \nComplessità del Modello\n \n25caso di modello quadratico: \nAreaPrezzoErrore \nComplessità del modello",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#25": "Generalization Error vs.  \nComplessità del Modello\n \n26caso di modello polinomiale: \nAreaPrezzoErrore \nComplessità del modello",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#26": "Generalization Error vs.  \nComplessità del Modello\n \n27caso di modello “High level”: \nAreaPrezzoErrore \nComplessità del modello",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#27": "Generalization Error vs.  \nComplessità del Modello\n \n28L’andamento dell’errore è dunque in genere il seguente: \nComplessità del ModelloErrore",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#28": "Generalization Error\n \n29Ricordiamoci però che, a differenza di ciò che accade per \nil training error, NON è possibile calcolare il \nGeneralization Error. \nPer calcolarlo, dovremmo conoscere la “true distribution” \ndelle probabilità vista prima, cosa che non sappiamo fare. ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#29": "Test Error\nDeﬁnito come average loss sui punti dell’insieme di test:\n \n30\nTest Error =1\nNtest·X\ni2testL[yi,fˆw(xi)]\nAreaPrezzo",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#3": "Loss Function\n \n4Possiamo indicare una funzione di Loss come segue:\ndove y è l’actual value, mentre la funzione f ci fornisce il \nvalore previsto ŷ.  \nTale funzione L rappresenta il costo che abbiamo se usiamo \nla f con il vettore dei pesi ŵ a fronte dell’input x.L[y, f ˆw(x)]",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#30": "Andamento degli errori  vs.  \nComplessità del Modello\n \n31Training Error: \nTraining Error\nComplessità del modelloErrore ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#31": "Andamento degli errori vs.  \nComplessità del Modello\n \n32Generalization Error: \nGeneralization Error\nTraining Error\nComplessità del modelloErrore ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#32": "Andamento degli errori vs.  \nComplessità del Modello\n \n33Test Error: approssima il Generalization Error \nGeneralization Error\nTraining ErrorTest Error\nComplessità del modelloErrore ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#33": "Overﬁtting\n \n34Dato un modello con parametri ŵ, si ha overﬁtting se esiste un \nmodello con i parametri stimati w’ tale che: \n 1. training error( ŵ) < training error(w’) \n 2. true error( ŵ) > true error(w’) \nGeneralization (true) Error\nTraining ErrorTest Error\nŵw’\nComplessità del modelloErrore ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#34": "Training/Test Split\nUn importante problema da considerare ai ﬁni \ndell’addestramento e della valutazione di un modello è la \nsuddivisione delle osservazioni disponibili tra training set \ne test set:\n \n35\nTraining Set Test Set",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#35": "Training/Test Split\nSe per il training set abbiamo poche osservazioni \nrischiamo di non stimare in modo adeguato il modello, \ncosa che potrebbe comportare previsioni imprecise da \nparte dello stesso.\n \n36\nTraining Set Test Set\nTroppo pochi ➝ ŵ non stimato adeguatamente  ",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#36": "Training/Test Split\nD’altro canto, se abbiamo poche osservazioni per il test \nset rischiamo di non avere una rappresentazione adeguata \ndei dati che stiamo analizzando (e.g., tutte le case in \nvendita) \n \n37\nTraining Set Test Set\nTroppo pochi ➝ true error non stimato  \n                       adeguatamente dal test error",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#37": "Training/Test Split\nPurtroppo non esiste una formula che ci dica come \nsuddividere esattamente i dati in training set e test set. \nUna regola pratica da poter seguire consiste nell’usare un \nnumero sufﬁciente di punti per il test set per consentire \nuna ragionevole approssimazione del true error: \n \n38\nTraining Set Test Set\nSe ciò lascia troppi pochi punti per il training set,  ci \npossiamo avvalere di altri metodi che vedremo \nsuccessivamente (\n cross validation\n ).",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#38": "Le tre sorgenti di errore\nNoise \nBias  \nVariance\n \n39Test SetE’ estremamente utile analizzare le diverse cause che possono \nportare ad un errore nelle previsioni. Esse sono le seguenti:\nCominciamo a vedere in modo intuitivo di cosa si tratta.",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#39": "Noise\nCome sappiamo, in genere i dati sono intrinsecamente \n“rumorosi”. \nNel nostro caso, possiamo ipotizzare che esista una “true \nfunction” che lega \n x\n a y. Essa però non è una descrizione \nperfetta di tale legame. Ci sono infatti altri fattori che \nmagari non abbiamo tenuto in conto (altri attributi, ecc.)\n \n40\nTutto ciò comporta un “rumore” intrinseco, che possiamo \nrappresentare con il termine \n ε\n, che ha media uguale a zero.y\nArea xPrezzo\n",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#4": "Esempi di Loss Function\n \n5La funzione di Loss può essere ad esempio deﬁnita come \nErrore Assoluto (Absolute Error):\noppure come Errore Quadratico (Squared Error):L[y, f ˆw(x)] = |y\u0000fˆw(x)|\nL[y, f ˆw(x)] = [ y\u0000fˆw(x)]2\nTali esempi di funzione assumono che il “loss” per \nunderpredicting sia uguale a quello di overpredicting.",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#40": "Noise\nTale rumore comporta in genere uno scostamento (spread) \nrispetto alla funzione vera. \nPossiamo dunque prendere in considerazione la varianza \ndi tale variabile:\n \n41varianza di ε\nIl rumore in questione è chiamato “\n irreducible error\n ”.y\nArea xPrezzo\n",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#41": "Bias\nIl rumore prima descritto non lo possiamo controllare. \nPossiamo però controllare il bias e la variance. \nPer deﬁnire il bias, consideriamo ad es. un modello \ncostante addestrato con diversi training set:\n \n42y\nArea xPrezzo\ny\nArea xPrezzo\n",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#42": "Bias\nConsideriamo adesso la funzione media (quella tratteggiata) \ndelle varie funzioni f stimate:\n \n43y\nArea xPrezzo\ny\nArea xPrezzo\nf¯w(xt),Etrain [fˆw(xt)]",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#43": "Bias\nIl bias è deﬁnito come la differenza tra la funzione media \ne la true function:\n \n44\nE’ in sostanza una valutazione di quanto il mio modello si \nadatti alla true function.\nlow complexity → high biasy\nArea xPrezzo\nbias( fˆw(xt)) = fw(true) (xt)\u0000f¯w(xt)",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#44": "Variance\nPer introdurre il concetto di varianza nella regression, \ndobbiamo considerare quanto le varie funzioni f stimate \ndifferiscono dalla funzione media. Vediamo ad esempio il \ncaso di modello costante: \n \n45Non abbiamo una variazione elevata  \nper le diverse funzioni stimate.\nlow complexity → low variancey\nArea xPrezzo\ny\nArea xPrezzo\ny\nArea xPrezzo\nvar(fˆw(xt)) =Etrain [(fˆw(xt)\u0000f¯w(xt))2]",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#45": "Variance\nPer un modello polinomiale di grado elevato le cose \nvanno in modo diverso:\n \n46\nPrezzo\nPrezzo\nArea Area",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#46": "Variance\nSe consideriamo le funzioni f stimate per i possibili \ntraining set:\n \n47\nQuesta volta la variazione è elevata.\nhigh complexity → high varianceArea AreaPrezzo\nPrezzo\nvar(fˆw(xt)) =Etrain [(fˆw(xt)\u0000f¯w(xt))2]",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#47": "Bias per  \nhigh-complexity models\nIl bias per modelli “high order” è invece in genere basso:\n \n48high complexity → low biasy\nArea xPrezzo\nf¯w(xt),Etrain [fˆw(xt)]\nbias( fˆw(xt)) = fw(true) (xt)\u0000f¯w(xt)",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#48": " \n49\nBias-Variance Tradeoff\nbias variance\nComplessità del Modello\nsweet spotMSE = bias + variance2",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#49": " \n50\nBias-Variance Tradeoff\ngoal in ML →  trovare il cosiddetto “sweet spot”\npurtroppo non possiamo calcolare bias e variance!\nPer calcolarle dovremmo avere a disposizione la true \nfunction e tutti i possibili training set. \nVedremo in seguito come poter operare in pratica per \nottimizzare il tradeoff.",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#5": "Valutare la funzione Loss \n \n6Ai ﬁni della valutazione del “loss” occorre deﬁnire i \nseguenti tipi di errore: \n• Training Error \n• Generalization Error (True Error) \n• Test Error",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#50": " \n51\nErrori vs. numerosità dei dati \n(con un modello di ﬁssata complessità)\nbias + noise\n#data points nel training settrue errorErrore\ntraining error",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#51": " \n52Andamento del True Error:\nSe abbiamo pochi punti nel training set l’errore è alto, perché la \nfunzione f (ﬁtted function) non stima bene la “true relationship” \ntra \nx\n e y. \nAumentando i punti l’errore diminuisce. \nAl limite, esso tende ad un valore uguale a: bias + noise. Questo \nperché, anche se avessimo tutte le osservazioni possibili, il \nmodello potrebbe non essere sufﬁcientemente ﬂessibile per \ncatturare perfettamente la “true relationship” (questa è la nostra \ndeﬁnizione di bias). \nA ciò si aggiunge il noise che, come sappiamo, non possiamo \ncontrollare. \nErrori vs. numerosità dei dati \n(con un modello di ﬁssata complessità)",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#52": " \n53Andamento del Training Error:\nSe abbiamo pochi punti nel training set l’errore è basso, \nperché la funzione f (ﬁtted function) può approssimare più \nfacilmente la “true relationship” tra \n x\n e y. \nAumentando i punti l’errore aumenta. \nAl limite, anch’esso tende ad un valore uguale a: bias + noise. \nQuesto perché, se avessimo tutte le osservazioni possibili, \nl’errore calcolato sarebbe proprio il true error che, come \nabbiamo visto, tende al valore bias + noise. \nErrori vs. numerosità dei dati \n(con un modello di ﬁssata complessità)",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#53": "Workﬂow per Regression  \n(e per il ML in generale)\nI due task importanti che dobbiamo attuare nella \nregression sono: \n1.\n Scelta del modello di regressione (\n model selection\n ) \n2.\n Valutazione del modello (\n model assessment\n )\n \n54",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#54": "Workﬂow per Regression (e ML)\n1.\n Model selection \nSpesso dobbiamo scegliere dei parametri di tuning \n λ \nche controllano la complessità del modello (e.g., grado \ndel polinomio) \n2.\n Model assessment \nUna volta scelto il modello, dobbiamo effettuare la \nvalutazione del Generalization Error.\n \n55",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#55": "Workﬂow per Regression (e ML)\n1.\n Model selection \nPer ogni modello di complessità \n λ\n: \n• \nstima dei parametri \n ŵ\nλ\n sui training data \n•\n valutazione delle prestazioni sui test data \n•\nscelta del parametro \n λ\n (\nλ\n*) che comporta il più basso test error  \n2.\n Model assessment \nConsiderare il test error calcolato su \n ŵ\nλ\n*\n (ﬁtted model per la \ncomplessità scelta \n λ\n*) per approssimare il Generalization \nError.\n \n56Un approccio ingenuo  al problema potrebbe essere il seguente:",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#56": "Workﬂow per Regression (e ML)\n \n57Attenzione! \nSi è veriﬁcato un Peaking!!!\n•E’ accaduto che l’ipotesi (i.e., la funzione stimata) è stata \nselezionata  in base alle sue prestazioni sull’insieme di test . \n•L’informazione che avrebbe dovuto rimanere conﬁnata in \ntale insieme si è, per così dire, “inﬁltrata” nell’algoritmo di \napprendimento.",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#57": "Una soluzione è quella di considerare non solo due data \nset, ossia il training set e il test set: \n \n58\nTraining \nSetTest \nSet\nma considerarne tre (a patto di avere dati sufﬁcienti): \nTraining \nSetTest \nSet\nValidation\n Set\nWorkﬂow per Regression (e ML)",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#58": "Workﬂow per Regression (e ML)\n1.\n Model selection \nPer ogni modello di complessità \n λ\n: \n•\n stima dei parametri \n ŵ\nλ \nsul training set \n•\n valutazione delle prestazioni sul validation set \n•\nscelta del parametro \n λ\n (\nλ\n*) che comporta il più basso errore sul \nvalidation set \n2.\n Model assessment \nCalcolo del test error (usando dunque il test set) con \n ŵ\nλ\n*\n per \napprossimare il Generalization Error.\n \n59L’approccio in questo caso è il seguente:",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#59": "Tipici split \n \n60Non c’è una regola generale per suddividere le \nosservazioni disponibili tra i tre data set. \nTipici split sono i seguenti:\nTraining \nSetTest \nSet\nValidation\n Set\n80%      10 %   10%\n50%      25%   25%",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#6": "Training Error\n \n7Consideriamo ancora l’esempio relativo ai prezzi degli \nappartamenti. Supponiamo di avere a disposizione le \nosservazioni come rappresentate in ﬁgura:\ny\nAreaxPrezzo\n",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#60": "Riferimenti\n \n61\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , Apogeo, 2008. \nMachine Learning: Regression\n , University of Washington - Coursera, 2015. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. \nMurphy, K.P. \n Machine Learning - A Probabilistic Approach\n , The MIT Press, 2012.",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#7": "Training Error\n \n8y\nAreaxPrezzo\nCome sappiamo, dobbiamo decidere il modello da \nutilizzare (lineare, quadratico, ecc.) e scegliere un \nsottoinsieme delle osservazioni per effettuare la fase di \ntraining del modello:",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#8": "Training Error\n \n9y\nAreaxPrezzo\nPossiamo ad esempio scegliere un modello quadratico e \ncalcolare i pesi w tali da minimizzare la funzione RSS:",
    "data_test\\rootfolder\\università\\MachineLearning\\4-Regression-Valutazione-sbloccato.pdf#9": "Calcolo del Training Error\n \n10Una volta stimati i parametri del modello, possiamo \nvalutare il training error di tale modello stimato come \nsegue: \n1. Deﬁnizione di una Loss Function (absolute error, squared \nerror, ecc.) \n2. Calcolo del Training Error come “average loss”, deﬁnito \nsugli N punti di training: \nTraining Error =1\nN·NX\ni=1L[yi,fˆw(xi)]",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#0": "Machine Learning\nUniversità Roma Tre  \nDipartimento di Ingegneria \nAnno Accademico 2021 -2022\nReinforcement Learning",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#1": "Acknowledgements, sources and links\nOrganization, content and images of the slides are extracted from  the \nfollowing sources:\n•Reinforcement Learning: An Introduction . Richard S. Sutton  \nand Andrew G. Barto, second edition, 2018.\n•Implementation of Reinforcement Learning algorithms, from \nSutton -Barto’s book . Denny Britz, GitHub project, 2016.\n•Tutorial: Introduction to Reinforcement Learning with \nFunction Approximation . Richard S. Sutton, 2016.\n•UCL Course, Reinforcement Learning, videos and slides .  \nDavid Silver, 2015.\n•UCL course, Advanced Deep Learning & Reinforcement \nLearning, videos and slides . DeepMind, 2018.",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#10": "First actor: the agent\nA never -ending loop\n•...we(the agent) receive Rtand observe Ot...\n•...wechoose the action At∼π(·,f(Ot,Rt,At−1,Ot−1,Rt−1,...))...\n•...and because ofour action At, the environment send usareward\n•Rt+1 and a new state , that we observe as Ot+1. ..",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#11": "First actor: the agent\nA never -ending loop\n•...we(the agent) receive Rtand observe Ot...\n•...wechoose the action At∼π(·,f(history ))...\n•...and because ofour action At, the environment send usareward\n•Rt+1 and a new state , that we observe as Ot+1. ..",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#12": "Not alone! Second actor: the environment\nAgent, step t\n•Receives observation Ot\n•Receives scalar reward Rt \n•Computes his own state !!\"\n•Executes action At.\nEnvironment, step t\n•Receives action At \n•Computes his own state !!#$%\n•Emits observation Ot+1\n•Emits scalar reward Rt+1",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#13": "1What is Reinforcement Learning?\nThe RL setup: problem and actors\n 2\n3 What do we know? State and observability\n4What can we do? Policy and value -andmodel?\n5The never -ending control loop: prediction =improvement",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#14": "History , agent state , environment state\nNotation\n•History : the sequence of observations, actions, rewards up \nto  time step t:\nHt:=O1,R1,A1,...,At−1,Ot,Rt\n•The agent selects actions, and the environment answers with\nobservations andrewards\n•State : the information used (by the agent and the  \nenvironment) to determine what happens next\n•State is naturally a sequence St\n•Agent state is a function of history: St := f (Ht)\n•Environment state \"!\"is different from agent state \"!#",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#15": "Markov state\nUncertainty\nSince we have no control of environment, everything is a random  \nvariable\nDefinition\nA sequence of states (random variables) is Markov if and only if\nPr(St+1|St)=Pr(St+1|S1,...,St)\n•The future is independent of the past given the present:\nSt  →Ht+1:+ ∞\n•Once the state is known, the history may be thrown away: the  \nstate is a sufficient statistic of the future\nExercise\nIs the environment state \"!\"Markov? Is the history HtMarkov?",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#16": "Fully observable environments\n•Agent observes environment\nstate: Ot = !!\"=!!%\n•Agent state andenvironment  \nstate coincides!\nAnever -ending loop\n•...we(the agent) receive Rt\nand observe St...\n•...and thus wedecide todo  \naction At∼π(·,St)...\n•...andenvironment answers  \nAt with a new reward, state  \npair Rt+1,St+1...",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#17": "Example: themaze\nExercise\nDiscuss this example in terms of the language you have  \nlearned up tonow.",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#18": "1What is Reinforcement Learning?\nThe RL setup: problem and actors\n 2\n3 What do we know? State and observability\n4 What can we do? Policy and value -and model?\n5 The never -ending control loop: prediction =improvement",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#19": "Strategy Policy\nArrows represent the policy π: which action to take from  \nevery state.Example: a policy for themaze\n",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#2": "1What is Reinforcement Learning?\nThe RL setup: problem and actors\n 2\n3What do we know? State and observability\n4What can we do? Policy and value -andmodel?\n5The never -ending control loop: prediction =improvement",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#20": "Example: values for the policy of the maze\n•Let πbe the optimal policy\n•Value vπ (s) for every state s\nExercise\nChoose a state s and compute vπ (s) by yourself. If sj denote s\nthe successor state of s, can the value vπ (sj) help with this  \ncomputation?",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#21": "1What is Reinforcement Learning?\nThe RL setup: problem and actors\n 2\n3What do we know? State and observability\n4What can we do? Policy and value -and model?\n5 The never -ending control loop: prediction =improvement",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#22": "Prediction , improvement and control\nThe prediction problem inRL\nForecast the future: can you say from each state how much will be  \nyour return? It depends on the policy!\nThe improvement problem inRL\nChange the future: can you find a different policy that will give  \nyou a better return?\nThe control problem inRL\nChange the future: can you find the best policy atall?\nExercise\nState formally the prediction, the improvement and the control  \nproblem.",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#23": "Gridworld example: prediction\nExercise\nCompute the value function for the uniform random policy.",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#24": "Gridworld example: improvement\nExercise\nFind an improvement of the uniform policy.",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#25": "Gridworld example: optimal control\nExercise\nCompute the optimal value function over all possible policies.  \nGiven the optimal value v∗as above, find the optimal policy.  Is \nthe optimal policy unique?",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#26": "Wrapping up\nLearning goals\n•Understand the RL problem, and how RL differs from  \nsupervised learning\n•Understand reward, return and how they are used to \nmake  decisions\n•Understand actions, states and rewards in term \nof  agent/environment interactions\n•Understand the optimal control problem",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#27": "Wrapping up\nWhat we (hopefully) have learnt\n•Reinforcement Learning (RL) is concerned with goal -directed  \nlearning and decision -making. In RL an agent learns from  \nexperiences it gains by interacting with the environment. In  \nsupervised learning we cannot affect theenvironment\n•In RL rewards are often delayed in time and the agent tries to  \nmaximize the cumulative sum of rewards, called return .Return \nis a long -term goal. For example, one may need to  make \nseemingly suboptimal moves to reach a winning position in a \ngame\n•An agent interacts with the environment via actions. The  \nenvironment answers with states and rewards ... and so on in \na  loop, that can finish after a certain number of steps or go \non  forever\n•Optimal control can be achieved by a prediction -improvement  \nloop",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#28": "Exercises\n•Can every decision task be represented as an optimization  \nproblem with respect to a suitable reward?\n•Is the reward an intrinsic datum of the decision task?\n•Make an example where the greedy policy isoptimal.\n•Make an example of a decision task with non-Markov  \nenvironment state.\n•Make an example of a decision task with non-Markov agent  \nstate.",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#29": "Exercises\nThe generic definition of policy is a time -dependant stochastic  \nfunction of the history: πt (At|Ht ) = Pr(At |Ht ). Give a definition  of the \npolicy in the following cases, and find a corresponding task.\n•Fully observable environment, stochastic and non-stationary  \npolicy.\n•Partially observable environment, stochastic and stationary  \npolicy.\n•Fully observable environment, deterministic and stationary  \npolicy.",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#3": "1What is Reinforcement Learning?\nThe RL setup: problem and actors\n 2\n3What do we know? State and observability\n4What can we do? Policy and value -andmodel?\n5The never -ending control loop: prediction =improvement",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#4": "The RLproblem\nImportant points\n•Trying to reach a goal\n•Interactions: active decision -making agent vs environment\n•Uncertainty about theenvironment\n•Effects of actions cannot be fully predicted: \nadaptation required ( learning )\nThe RL reward hypothesis\nAll goals can be described by the maximization of some expected  \ncumulative reward\n•Is it true? Interesting analysis at  \nhttp://incompleteideas.net/rlai.cs.ualberta.ca/ \nRLAI/rewardhypothesis.html\n•Related with the expected utility hypothesis from von  \nNeumann -Morgenstern utility theory",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#5": "The RLproblem\nRL main task\nDecision problem: we would like to choose actions that maximize  \nthe return , i.e. the total future reward\nSequential decision making\nActions may have long term consequences\nUncertainty\nThe best we can aim for is maximizing the value , i.e.the\nexpected total future reward\nExercise\nFind an example of a deterministic task, that is, a task where  your \nactions gives a fixed outcome (that you may or may not know  in \nadvance)",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#6": "The RLproblem\nTo be greedy can bewrong\n•A financial investment (may take months to mature)\n•Refuelling a helicopter (might prevent a crash in \nseveral  hours)\n•Blocking opponent moves (might help winning chances \nmany moves from now)\nExercise\nDiscuss the difference between return and value",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#7": "The RLproblem\nExamples of reward\n•Games: RT := −1, 0, +1 (win, draw, lose). More generally,\n•RT can be the final score\n•Games: RT := 0, +1 (win, lose). In this case, the value is the  \nprobability of winning. Why?\n•Atari games: Rt is the immediate score increment at step t\n•Walking robot: Rt := +1 for every step he doesn’t fall\nhttps://www.youtube.com/watch?v=gn4nRCC9TwQ .\n•Financial investment: Rt is the money increment in the last  \ntime step in portfolio\n•Maze and Gridworld: +100 for reaching the exit, 0 otherwise.  \nWrong. Why?",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#8": "The RLproblem\nExamples of reward\n•Games: RT := −1, 0, +1 (win, draw, lose). More generally,\n•RT can be the final score\n•Games: RT := 0, +1 (win, lose). In this case, the value is the  \nprobability of winning. Why?\n•Atari games: Rt is the immediate score increment at step t\n•Walking robot: Rt := +1 for every step he doesn’t fall\nhttps://www.youtube.com/watch?v=gn4nRCC9TwQ .\n•Financial investment: Rt is the money increment in the last  \ntime step in portfolio\n•Maze and Gridworld: +100 for reaching the exit, 0 otherwise.  \nWrong. Why?\n•Maze and Gridworld: −1 for every move. Correct. Why?",
    "data_test\\rootfolder\\università\\MachineLearning\\40-RL(1)-sbloccato.pdf#9": "Examples\nGames\nTD-Gammon, 1995, ACM Communications .  \nAtari’s family (video ).\n•49 out of 57: DQN, 25 Feb 2015, Nature .\n•52 out of 57: R2D2, Sep 2018, ICLR 2019 .\n•51 out of 57: MuZero, Nov 2019, arXiv .\n•57 out of 57: Agent57, Mar 2020, arXiv .\nAlphaGo’s family (video ).\n•AlphaGo, 27 Jan 2016, Nature .\n•AlphaGo Zero, Oct 2017, Nature . \n•AlphaZero, Dec 2018, Science .\n•MuZero, Nov 2019, arXiv .\nStarCraft II (video ).AlphaStar, Nov 2019, Nature .\nProtein folding\nHow a protein’s amino acid sequence dictates its three -dimensional  \nstructure? AlphaFold :Oct 2019, PROTEINS ;Jan 2020, Nature .",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#0": "1Distribution model\n2Decisions andreturn\n3Value functions\n4 Bellman equationsMDP: Markov Decision Processes\n",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#1": "Basic block: state , action , model ,reward\nuintermediate\nor initial state\n vintermediate\nor final state\nw\nr =+3\np(v,+3|u,a)=0.7\nr =−1\np(w,−1|u,a)=0.3intermediate\nor final stateaction a",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#10": "1Distribution model\n2Decisions andreturn\n3Value functions\n4 Bellman equations",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#11": "The D in MDP: decisions\nWhere are thedecisions?\n•In any state s, the agent must choose between available  \nactions a\n•When choosing a from s, the environment answers s’ \nwith probability #!!!\". Environment decision.\n•The agent behaviour is given by probabilities π(a|s): ”how  \nlikely I’m going to choose a from s?”.  Agent decision.\nDefinition\nA policy π is a probability distribution over actions given states:\nπ(a|s) := Pr(At = a|St =s)",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#12": "Example: uniform stochastic policy\nA\n1\nB\n1\n0.5\n0.5\n0.8,+10\n 0.2,+3\n0.1,+2\n 0.9,−30.5\n0.5\n0.1,+39\n0.9,+42\n0.1,−2\n0.9,+3\n2 2\nWhat can wedo?\nAt every step, we choose the action according to the probability.",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#13": "Example: deterministic policy\nA\n1\n2B\n1\n2\n0.8,+10\n 0.2,+3\n0.1,+2\n 0.9,−3\n0.1,+39\n0.9,+42\n0.9,+3\n0.1,−2\nWhat can wedo?\nAt every step, we choose the given action.",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#14": "Tabular representation\nS and Aarefinite\nA policy can be represented by a table: every line in the table  \ncorresponds to a state.\nStochastic policy\nA[0.5,0.5]\nB[0.5,0.5]\nDeterministic policy\nA2\nB1",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#15": "The return : towards the goal\nDefinition\n•Total return ofanepisode ending attime T:thevalue ofthe  \nrandom variable Gt:=Rt+1+Rt+2+···+RTfortheepisode\n•If the MDP is continuing, we need a discount factor :\nWhy?\n•Transforming theterminal state inabsorbing with reward 0,we  \ncan use a unified notation for episodic and continuing MDP:\n•In episodic tasks we can use γ = 1, in continuing tasks we  \nmust use γ <1!!≔#!\"#+%#!\"$+%$#!\"%+…=(\n&'(\")\n%&#!\"&\"#\n!!≔#!\"#+%#!\"$+%$#!\"%+…=(\n&'(\")\n%&#!\"&\"#",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#16": "The return : towards the goal\nWhy the discount\n•The discount factor measures how much do we care about \nrewards far in thefuture\n•A reward r after k + 1 time -steps is worth “only” γkr : wesay\nmyopic evaluation if γ ∼0, far-sighted evaluation if γ ∼1\n•Convenience: avoids infinite returns in cyclic MDP\n•We shouldn’t trust our model too much: uncertainty about  \nthe future may not be fully represented\n•If the reward is financial, immediate rewards may earn more  \ninterest than delayed rewards\n•Animal and human behaviour shows preference for immediate  \nreward",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#17": "1Distribution model\n2Decisions andreturn\n3Value functions\n4 Bellman equations",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#18": "How much are states and actions worth?\nRemark\nThe total return Gt at time t is a random variable:\nThus, it makes sense to compute its expected value.\nDefinition :state -value function\nThe state -value function vπ(s)foraMDP isthe return wecan\nexpect toaccumulate starting from state s,following thepolicy π:\nvπ(s):=\"π[Gt|St=s]\nExercise\nIs the above definition/notation correct?!!≔#!\"#+%#!\"$+%$#!\"%+%%#!\"*+…=(\n&'(\")\n%&#!\"&\"#",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#19": "How much are states and actions worth?\nTotal return\nState -value function\nvπ(s)=\"π[Gt|St=s]\nDefinition: action -value function\nThe action -value function qπ (s,a) for a MDP is the return we can \nexpect to accumulate starting from a state s, choosing action a, \nand then following the policy π:\nqπ(s,a):=Eπ[Gt|St=s,At=a]!!≔#!\"#+%#!\"$+%$#!\"%+%%#!\"*+…=(\n&'(\")\n%&#!\"&\"#",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#2": "Basic block: state , action , model ,reward\nu\nv\nwa\nr = +3, p =0.7\nr=−1,p=0.3",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#20": "Example\nExercise\nCompute qπ(A,1),qπ(A,2),qπ(B,1) and qπ(B,2) forthe uniform policy π.",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#3": "Markov Decision Process: MDP\nMarkov decision process data\n•Asetofstates S,asetofactions Aandasetofrewards R\n•For each state s ∈S and action a∈A, a probability  \ndistribution p(·,·|s,a)over S×R\n•A discount factor γ ∈[0,1]\nDistribution model\nThe probability distribution p is called distribution model , or  \nsimply model, of the MDP\nFocus on finite MDP\nFrom now on, assume that S, Aand Rarefinite",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#4": "MDP: meaning of the model\nMarkov decision process data\n•Asetofstates S,asetofactions Aandasetofrewards R\n•For each state s ∈S and action a∈A, a probability  \ndistribution p(·,·|s,a)over S×R\n•A discount factor γ ∈[0,1]\nFrom distribution model to random variables St andRt\nThe probability distribution p of the MDP gives the next state and  \nreward:\nPr(St=s',Rt=r|St−1=s,At−1=a):=p(s',r|s,a)",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#5": "MDP: meaning of the model\nExercises\n•Explain what St,At and Rtare\n•Given p, give a formula for Pr(St = s'|St−1 = s, At−1 = a)\n•Given p, give a formula for \"[Rt|St−1 = s, At−1 =a]\n•Given p, give a formula for \"[Rt|St−1 = s, At−1 = a,St = s']",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#6": "The M in MDP: Markov property\nTabular representation: transitions\nAn action a ∈Agives a transition probability from a state s toa\nstate s' :\nThus, we have a transition matrix Pa for each action a, and a  \ncorresponding underlying Markov stochastic process.\nTabular representation: rewards\nAn action a ∈A gives an average reward for any state s:\nThus, we have an average reward vector Ra for any action a.!!!!\"≔#$#|$,'==!)*$=$#|*$%&=$,+$%&='\n,!\"=-,$|*$%&=$,+$%&='",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#7": "Example\nu\n1\n2v\n1\n2\n0.8,+10\n 0.2,+3\n0.1,+2\n 0.9,−3\n0.1,+39\n0.9,+42\n0.9,+3\n0.1,−2",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#8": "Example\nu1\n2v1\n0.8,+10 0.2,+3\n0.1,+2 0.9,−30.1,+39\n0.9,+42\n0.9,+30.1,−2\n2\n= distribution model",
    "data_test\\rootfolder\\università\\MachineLearning\\41-RL(2)-sbloccato.pdf#9": "Episodic MDP\n•If there is a special terminal  \nstate reachable from every  \nstate, the MDP is episodic\n•Otherwise, the MDP is\ncontinuing\n•Episode : any sample\nS0,A0,R1,S1,...terminating  \nin the final state\nExercise\n•Write an episode, and compute its probability \nof  happening. Hint: tricky question.",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Reinforcement Learning (Ex 16)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#1": "Sommario\nRichiami RL e Q-learning \nEsempio taxi  \nAmbiente OpenAI Gym \nApproccio non RL \nApproccio Q-Learning \nApproccio Epsilon-Greedy Q-Learning \nValutazione e iperparametri",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#10": "OpenAI Gym\nSono delle API in Python che permettono di sperimentare approcci RL. \nhttps://www.gymlibrary.ml  \nLa libreria include già l'ambiente Taxi già costruito. \n!\npip install cmake \n 'gym[atari]'\n  scipy\nimport\n gym\n# carichiamo l'environment taxi\nenv = gym.make(\n \"Taxi-v3\"\n ).env\nenv.render()\n>>\n+---------+\n|R: | : :\n G\n|\n| : | : : |\n| : : : : |\n| | : | \n: |\n|\nY\n| : |B: |\n+---------+\n11",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#11": "OpenAI Gym\n...\nenv.reset() \n # reset environment to a new, random state\nenv.render()\nprint\n(\n\"Action Space {}\"\n .\nformat\n(env.action_space))\nprint\n(\n\"State Space {}\"\n .\nformat\n(env.observation_space))\n>>\n+---------+\n|R: | : :\n G\n|\n| : | : : |\n| : : \n: : |\n| | : | : |\n|Y| : |\n B\n: |\n+---------+\n>> Action Space Discrete(6)\n>> State Space Discrete(500)\n...\n12",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#12": "OpenAI Gym\nLe azioni sono codiﬁcate con interi:  \n0 = south, 1 = north, 2 = east, 3 = west, 4 = pickup, 5 = dropoff \nstate, reward, done, info \n =\n env.step(\n 0\n) # azione: verso south\nenv.render()\n+---------+\n|R: | : :\n G\n|\n| : | : : |\n| : : : : |\n| | : \n| : |\n|Y| : |\n B\n: |\n+---------+\nstate, reward, done, info \n =\n env.step(\n 0\n) # azione: verso south\nenv.render()\n+---------+\n|R: | : :\n G\n|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : \n|\nB\n: |\n+---------+\n13",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#13": "OpenAI Gym\nSono delle API in Python che permettono di sperimentare approcci RL. \n# parametri (taxi row, taxi column, passenger index, destination index) \nstate = env.encode(\n 3\n, \n1\n, \n2\n, \n0\n)\nprint\n(\n\"State:\"\n , state)\nenv.s = state\nenv.render()\n>> State: 328\n+---------+\n|\nR\n: | : :G|\n| : | : : |\n| : : : : |\n| | \n: | : |\n|\nY\n| : |B: |\n+---------+\n14",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#14": "OpenAI Gym\nPossiamo rappresentare un certo stato dell'environment esplicitamente. \nAllo stato sarà associato un id numerico (328). \nstate \n=\n env.\nencode\n(\n3\n, \n1\n, \n2\n, \n0\n)  \n# (taxi row, taxi column, passenger index, destination index)\nprint\n(\n\"State:\"\n , state)\nenv.\ns \n=\n state\nenv.\nrender\n()\nState: 328\n+---------+\n|\nR\n: | : :G|\n| : : : : |\n| : : : : |\n| | \n: | : |\n|\nY\n| : |B: |\n+---------+\n15",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#15": "OpenAI Gym\nLa reward table rappresenta coppie stati x azioni. \nAd esempio, per lo stato 328 otteniamo il seguente dizionario: \nenv.P[\n328\n]\n>>\n{0: [(1.0, 428, -1, False)],\n 1: [(1.0, 228, -1, False)],\n 2: [(1.0, 348, -1, False)],\n 3: [(1.0, 328, -1, False)],\n 4: [(1.0, 328, -10, False)],\n 5: [(1.0, 328, -10, False)]}\nDove il dizionario ha la struttura:  \n{action: [(probability \n sempre_1\n , next-state, reward, done)]}.\n16",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#16": "Esercizio Taxi: senza RL\nSupponi che l'agente abbia accesso unicamente la \n reward table\n  P per \ndecidere quale azioni compiere. Perciò non apprende dall'esperienza \nacquista nel passato. \nCrea un loop che prosegua ﬁnché il cliente non sia arrivato a destinazione. \nSuggerimento: la funzione \n env.action_space\n .\nsample\n ()\n restituisce una \nazione in modo casuale.\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#17": "Esercizio Taxi: senza RL\nenv.\ns \n= \n328  \n# stato iniziale\nepochs \n= \n0\npenalties, reward \n = \n0\n, \n0\nframes \n=\n [] \n# per animazione\ndone \n= \nFalse\nwhile \nnot\n done:\n    action \n =\n env.\naction_space\n .\nsample\n()\n    state, reward, done, info \n =\n env.\nstep\n(action)\n    \nif\n reward \n == \n-\n10\n:\n        penalties \n += \n1\n    \n    frames.\n append\n({\n        \n 'frame'\n: env.\nrender\n(mode\n=\n'ansi'\n),\n        \n 'state'\n: state,\n        \n 'action'\n : action,\n        \n 'reward'\n : reward\n        }\n    )\n    epochs \n += \n1\n    \nprint\n(\n\"Timesteps taken: {}\"\n .\nformat\n(epochs))\nprint\n(\n\"Penalties incurred: {}\"\n .\nformat\n(penalties))\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#18": "Esercizio Taxi: senza RL\nPer l'animazione: \nfrom\n IPython.\n display \nimport\n clear_output\nfrom\n time \nimport\n sleep\ndef\n print_frames(frames):\n    \nfor\n i, frame \n in \nenumerate\n (frames):\n        clear_output(wait\n =\nTrue\n)\n        \n print\n(frame[\n'frame'\n].\ngetvalue\n ())\n        \n print\n(\nf\"Timestep: \n {i \n+ \n1\n}\n\"\n)\n        \n print\n(\nf\"State: \n {frame[\n'state'\n]}\n\"\n)\n        \n print\n(\nf\"Action: \n {frame[\n'action'\n ]}\n\"\n)\n        \n print\n(\nf\"Reward: \n {frame[\n'reward'\n ]}\n\"\n)\n        sleep(\n .1\n)\n        \nprint_frames(frames)\nL'algoritmo può impiegare molti step (oltre 1000) incorrendo in molte \npenalty (oltre 300).\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#19": "Esercizio: Q-learning in Python\nEsercizio\n : modiﬁca il codice precedente implementando l'algoritmo  \nQ-learning. \n20",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#2": "Richiami: Reinforcement Learning\nNell'ambito del Reinforcement Learning (RL), la \n policy\n  è la strategia per \nscegliere una azione nello stato corrente che determini la massima \nricompensa (\n reward)\n . \nIl \nQ-learning\n  è un algoritmo che mira a determinare col tempo la migliore \nazione (\n best action\n ), dato lo stato corrente (\n current state), \n in base alla stima \ndi \nreward\n  attesa. \nMisura la bontà di una combinazione stato-azione in termini di \n reward\n . \nImpiega una \n Q-table\n  aggiornata dopo ogni episodio, dove la riga \ncorrisponde allo stato e la colonna all'azione. Il Q-value dentro la tabella \nindicano quanto una azione è stata buona (alto reward) in passato. \nÈ un algoritmo model-free, poiché l'agente non conosce il valore di una \nazione prima di effettuarla.  \nNon segue un approccio greedy poiché scegliere sempre l'azione con \nreward immediato massimo potrebbe determinare sequenze di azioni non \nottime.\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#20": "Esercizio: Q-learning in Python\nEsercizio\n : modiﬁca il codice precedente implementando l'algoritmo  \nQ-learning. \n%%time   # stampa il tempo trascorso al termine dell'esecuzione\nimport\n numpy \nas\n np\nimport\n random\nfrom\n IPython.display \n import\n clear_output\n# iperparametri\nalpha = \n 0.1\ngamma = \n 0.6\n# per il report\nall_epochs = []\nall_penalties = []\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n...\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#21": "Esercizio: Q-learning in Python\n...\nfor\n i \nin \nrange\n(\n1\n, \n100001\n):\n    state = env.reset()\n    epochs, penalties, reward, = \n 0\n, \n0\n, \n0\n    done = \n False\n    \n    \nwhile \nnot\n done:\n        action = np.argmax(q_table[state])\n        next_state, reward, done, info = env.step(action) \n        \n        old_value = q_table[state, action]\n        next_max = np.\n max\n(q_table[next_state])\n        \n # eq Bellman \n        new_value = (\n 1\n - alpha) * old_value + alpha * \n                    (reward + gamma * next_max)\n        q_table[state, action] = new_value\n...\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#22": "Esercizio: Q-learning in Python\n...\n        \n if\n reward == \n -10\n:\n            penalties += \n 1\n        state = next_state\n        epochs += \n 1\n        \n    \nif\n i % \n100\n == \n0\n:\n        clear_output(wait=\n True\n)\n        \n print\n(\nf\n\"Episode: \n {i}\n\"\n)\nprint\n(\n\"Training finished.\\n\"\n )\n>> Episode: 100000\n>> Training finished.\n>> CPU times: user 1min 25s, sys: 15 s, total: 1min 40s\n>> Wall time: 1min 29s\nq_table[\n 328\n] # l'azione migliore è north -2.27\n>> array([-2.31436727, -2.27325184, -2.31164458, -2.3090025 , \n          -2.8816    , -2.8816    ])\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#23": "Esercizio: Q-learning in Python\nEsercizio\n : valuta nuovamente l'algoritmo con le best action ricavate dalla \nQ-table. \n24",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#24": "Esercizio: Q-learning in Python\nValutazione dell'algoritmo con le best action ricavate dalla Q-table: \ntotal_epochs, total_penalties \n = \n0\n, \n0\nepisodes \n = \n100\nfor\n _ \nin \nrange\n(episodes):\n    state \n =\n env.\nreset\n()\n    epochs, penalties, reward \n = \n0\n, \n0\n, \n0\n    \n    done \n = \nFalse\n    \nwhile \nnot\n done:\n        \n action \n=\n np.\nargmax\n(q_table[state])\n        state, reward, done, info \n =\n env.\nstep\n(action)\n        \n if\n reward \n == \n-\n10\n:\n            penalties \n += \n1\n        epochs \n += \n1\n    total_penalties \n +=\n penalties\n    total_epochs \n +=\n epochs\nprint\n(\nf\"Results after \n {episodes}\n  episodes:\"\n )\nprint\n(\nf\"Average timesteps per episode: \n {total_epochs  \n/ \nepisodes}\n \"\n)\nprint\n(\nf\"Average penalties per episode: \n {total_penalties  \n/ \nepisodes}\n \"\n)\n>> Results after 100 episodes:\n>> Average timesteps per episode: 13.01     \n>> Average penalties per episode: 0.0       <-- nessuna penalty con 100 clienti \n25",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#25": "Epsilon-Greedy Q-learning\nCon l'approccio Epsilon-greedy Q-learning introduciamo il bilanciamento \ntra \nexploration\n  e \nexploitation\n .  \nNei modelli model-free è fondamentale esplorare l'ambiente per ottenere \ninformazioni su cui basare le successive decisioni informate. \nNella versione Espilon-greedy, con probabilità epsilon l'agente sceglie una \nazione in modo casuale (esplorazione) e segue l'azioni valutata migliore \nnell'altro caso (1-epsilon).  \nEsercizio\n : modiﬁca il codice introducendo questa versione.\n26\n",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#26": "Epsilon-Greedy Q-learning\nEsercizio\n : modiﬁca il codice introducendo questa versione. \n... \n# iperparametri\nalpha = \n 0.1\ngamma = \n 0.6\nepsilon = \n 0.1\n...\nwhile \nnot\n done:\n        \n if\n random.uniform(\n 0\n, \n1\n) < epsilon:\n            action = env.action_space.sample() \n # Explore action space\n        \n else\n:\n            action = np.argmax(q_table[state]) \n # Exploit learned values\n...\n>> Results after 100 episodes:\n>> Average timesteps per episode: 12.81     <-- invece di 13.01 \n>> Average penalties per episode: 0.0       <-- nessuna penalty con 100 clienti \n27",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#27": "Valutazione approccio RL\nAlcune metriche da considerare nella valutazione sono: \nNumero medio di \n penalità\n  per episodio (ideale --> 0) \nNumero medio di \n timesteps\n  per percorso  \nValore medio di \n reward\n  per mossa\n28\n",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#28": "Iperparametri\nAlpha\n : da decrementare con l'incremento dell'esperienza acquisita \nGamma\n : se ci avviciniamo all'obiettivo dobbiamo ridurre l'importanza \ndella reward a lungo termine \nEpsilon\n : con l'accumularsi dei tentativi, epsilon deve ridursi. \nEsercizio: applica un approccio \n grid search\n  per ricavare una \napprossimazione degli iperparametri nello scenario del taxi che guida da \nsolo.\n29",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#29": "OpenAI Gym \n https://www.gymlibrary.ml\nTesti di Riferimento\n30",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#3": "Richiami: Reinforcement Learning\nStep #1: inizializzo la Q-table con valori pari a 0, ogni azioni e \nequiprobabile. \nStep #2: scegli l'azione in modo random, o sfrutta l'eventuale informazione \nche hai al principio \nStep #3: esegui l'azioni e colleziona il reward \nStep #4: aggiorna la Q-table di conseguenza \n4\n",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#4": "Richiami: Reinforcement Learning\nL'equazione di \n Bellman\n  aggiorna i Q-values determinando il valore \nmassimo di \n reward\n  atteso per ogni stato nella Q-table.  \nIl primo termine \n Q()\n indica il valore dell'azione corrente nello stato \ncorrente. Il secondo combina il reward corrente e il valore discount dello \nstato futuro caratterizzato da reward massima.  \nIl \ndiscount factor lambda\n  [0,1] permette di ridurre il reward col tempo e \nindica quanta importanza assegnamo ai futuri reward: valori vicini allo 0 \nindicano che l'agente si limita a valutare i reward immediati, vicini al 1 \npermettono di valutare l'effetto a lungo termine dei reward. \nIl valore \n alpha\n  (learning rate (0,1]) determina l'importanza che assegniamo \nai valori futuri rispetto a quelli attuali.\n5\n",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#5": "Esempio: taxi che guida da solo\nDeﬁniamo un ambiente (environment) sempliﬁcato dove un taxi deve \nprendere un cliente in una certa locazione e lasciarlo in un'altra. \nVogliamo altresì: \nLasciare il cliente nel luogo giusto \nMinimizzare il tempo per il trasporto \nSeguire le regole della strada \nDobbiamo deﬁnire: rewards, states, actions. \nQuali puoi ipotizzare?\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#6": "Taxi che guida da solo: reward\nPer i reward possiamo ipotizzare: \nAlto reward se il cliente viene lasciato correttamente. \nPenalizzazione se il cliente viene lasciato nella location sbagliata. \nPer ogni istante di tempo trascorso, una piccola penalità.\n7",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#7": "Taxi che guida da solo: state space\nLo state space corrisponde a tutte le possibili situazioni in cui un taxi si può \ntrovare. Ogni stato deve contenere abbastanza informazioni per permettere \nall'agente di decidere una azione. \nSupponiamo il taxi sia l'unico veicolo. \nSuddividiamo l'ambiente in una griglia 5x5  \nPosizione corrente (3,1) \n4 location per il pick up e drop off: R,G,Y,B;  \ncioè [(0,0), (0,4), (4,0), (4,3)] \nIl cliente è in Y e vuole andare in R. \nUno stato aggiuntivo che rappresenta  \nil cliente all'interno del taxi. \nQuanti sono il numero dei possibili stati?\n8\n",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#8": "Taxi che guida da solo: state space\nLo state space corrisponde a tutte le possibili situazioni in cui un taxi si può \ntrovare. Ogni stato deve contenere abbastanza informazioni per permettere \nall'agente di decidere una azione. \nSupponiamo il taxi sia l'unico veicolo. \nSuddividiamo l'ambiente in una griglia 5x5  \nPosizione corrente (3,1) \n4 location per il pick up e drop off: R,G,Y,B;  \ncioè [(0,0), (0,4), (4,0), (4,3)] \nIl cliente è in Y e vuole andare in R. \nUno stato aggiuntivo che rappresenta  \nil cliente all'interno del taxi. \nPossibili stati: 5 x 5 x 5 x 4 = 500\n9\n",
    "data_test\\rootfolder\\università\\MachineLearning\\42-Ex_16 Esercitazione RL 1-sbloccato.pdf#9": "Taxi che guida da solo: action space\nL'agente può in ogni stato fare una delle seguenti azioni: \nmuoversi a nord \nmuoversi a su \nmuoversi a est \nmuoversi a ovest \nprendere il cliente \nlasciare il cliente \nSe l'agente non può fare una certa azione in uno stato (es. presenza di un \nmuro) possiamo assegnare una penalità di -1.\n10",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Reinforcement Learning (Ex 17)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#1": "Sommario\nOpenAI GYM: Ambienti  \nDeep Q-learning \nLibreria Baseline3",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#10": "OpenAI GYM: Environments\n..\nenv = gym.make(\n \"Qbert-v0\"\n )\ndirectory = \n './video'\nenv = Recorder(env, directory)\n...\n11\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#11": "OpenAI GYM: Environments\n12\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#12": "Deep Q-learning\nPer ambienti complessi (es. 10K stati e 1K azioni), la Q-table associata ad \nogni observation può risultare complessa (10M di celle). La stima di un \ncerto valore a partire da quelli esplorati in passato richiede: \nmolta memoria per la Q-table \ntempo necessario per esplorare tutti gli stati e ricavare i valori \nNel Deep Q-learning usiamo una rete neurale per stimare i Q-value, dove \nin output abbiamo il valore stimato per ogni azione.\n13\nQ learning Deep Q learning",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#13": "Deep Q-learning: Temporal Difference\nLa rete neurale ha bisogno di un valore di loss. \nDeﬁniamo la \n Temporal Difference:   \nQ(s,a)\n  è il Q-value per una certa azione a. Dopo aver eseguito l'azione avremo \nun reward R(s,a). \n Q\nt-1\n(s,a)\n è il Q-value precedente . \nIdealmente le due parti devono coincidere, essendo la prima impiegata per \nricavare la seconda, ma la casualità dell'ambiente e il tempo di apprendimento \ncreano discostamenti. \nIl valore del loss è determinato dal discostamento (\n Temporal Difference target)  \ndal target: Q-value - Q* \nImpiegando il learning rate alpha, usiamo la TD per ricavare il nuovo Q-value.\n14\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#14": "Deep Q-learning\nNel Deep Q-learning l'azione da eseguire è determinata dal valore \nmassimo in output dalla rete (non esiste la Q-table tradizionale). Ogni \nnodo di output è una azione possibile. \nOtteniamo un problema di regressione, senza però conoscere il valore del \ntarget\n  (nell'equazione in verde) non essendoci Q-table. \nLa nuova eq per il Q-learning diverrà: \nil brackpropagation tenderà a convergere i Q-value e i reward. \n15\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#15": "Deep Q-learning: Neural Fitted Q Iteration \nLa rete ci restituisce i Q-value target per ogni azione.  \nLa loss sarà così deﬁnita: \nNota: nel Q-learning tradizionale, il Q-value viene aggiornato ad ogni \ntransizione di stato. Nel Deep Q-learning il processo è più complesso...\n16\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#16": "Deep Q-learning: Neural Fitted Q Iteration \nLa rete deve valutare sia il valore \n predetto\n  sia il \n target\n . Per tale motivo se ne \nusano 2 con stessa architettura ma pesi distinti. \nUna rete per i valori \n target\n  con i parametri \"ﬁssi\". Ad ogni C iterazioni (es. \n100) i parametri della \n prediction\n  network (aggiornata spesso, es. ogni 4 \nsteps) saranno copiati nella \n target\n  network. Questo rende il training più \nstabile poiché mantiene la funzione target stabile. Approccio \"\n Neural Fitted \nQ Iteration (NFQ)\n \" \n17\nLa target nework stima il Temporal difference target",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#17": "Experience replay\nData la numerosità degli stati possibili, conviene salvarli per poi \"rigiocarci\" \nin seguito. È una \n off-line policy\n , cioè si sfrutta l'esperienza acquisita nelle \nazioni fatte nel passato per aggiornare i parametri attuali. \nDare in input lunghe sequenze di stati correlati (es. foto di percorsi \nautostradali rettilinei) può creare bias nella rete e non permettere di \nadattarsi in altre situazione. \nA differenza del Q-learning, durante l'esecuzione i dati [state, action, \nreward, next_state] sono salvati in un buffer chiamato \n experience replay\n . \nSupponendo che l'ambiente sia un gioco, durante il training possiamo \ncampionare periodicamente (es. ogni 4 step) in modo casuale 64 frames \n(batch) dei 100K possibili in modo che abbiano scarsa correlazione tra \nloro. Questo permette di non introdurre bias dovuti alla particolare subset \ndi istanze considerate.\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#18": "Deep Q-learning: step principali\nRicava i Q-values dalla rete per ogni azione dando in input lo stato \ncorrente (es. screenshot). \nSeleziona una azione con \n epsilon-greedy policy\n , cioè random con \nprobabilità \n epsilon\n , altrimenti l'azione con Q-value massimo. \nValuta l'azione che genera il nuovo stato \n s'\n, e ricava il reward \ncorrispondente. Lo stato \n s'\n corrisponde allo screen successivo. La \ntransizione \n <s,a,r,s’>\n  è salvata nel replay buffer. \nCampiona casualmente batch di transizioni dal replay buffer e ricava la loss \ncorrispondente. \nEsegui il \n gradient descent\n  con la differenza tra \n target Q\n  e \npredicted Q \nimpiegando la rete e i parametri attuali minimizzando la loss. \nOgni C iterazioni trasferisci i parametri della rete alla rete target. \nRipeti il procedimento M episodi.\n19",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#19": "Deep Q-learning in Python: CartPole\n# codice tratto da \n https://github.com/mswang12/minDQN/blob/main/minDQN.py\nimport\n gym\nimport\n tensorflow \n as\n tf\nimport\n numpy \nas\n np\nfrom\n tensorflow \n import\n keras\nfrom\n collections \n import\n deque\nimport\n time\nimport\n random\nRANDOM_SEED = \n 5\ntf.random.set_seed(RANDOM_SEED)\nenv = gym.make(\n 'CartPole-v1'\n )\nenv.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nprint\n(\n\"Action Space: {}\"\n .\nformat\n(env.action_space))\nprint\n(\n\"State space: {}\"\n .\nformat\n(env.observation_space))\n# An episode a full game\ntrain_episodes = \n 300\ntest_episodes = \n 100\n...\n20",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#2": "OpenAI GYM: Environments\nLa classe Env implementa il simulatore dell'ambiente in cui l'agente si \nmuove.  \nAlcuni esempi di environment disponibili in Gym: \nimport\n gym\nenv = gym.make(\n 'MountainCar-v0'\n )\nEsempio\n : nel MointainCar, un carrello deve incrementare l'inerzia per \nriuscire a passare la collina.\n3\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#20": "Deep Q-learning in Python: CartPole\n...\ndef \nagent\n(\nstate_shape\n , \naction_shape\n ):\n    # l'output è il Q-value stimato per ogni azione\n    learning_rate = \n 0.001\n    init = tf.keras.initializers.HeUniform()\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(\n 24\n, input_shape=state_shape, activation=\n 'relu'\n, \nkernel_initializer=init))\n    model.add(keras.layers.Dense(\n 12\n, activation=\n 'relu'\n, kernel_initializer=init))\n    model.add(keras.layers.Dense(action_shape, activation=\n 'linear'\n , \nkernel_initializer=init))\n    model.\n compile\n(loss=tf.keras.losses.Huber(), \noptimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\n 'accuracy'\n ])\n    \nreturn\n model\ndef \nget_qs\n(\nmodel\n, \nstate\n, \nstep\n):\n    \nreturn\n model.predict(state.reshape([\n 1\n, state.shape[\n 0\n]]))[\n0\n]\n...\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#21": "Deep Q-learning in Python: CartPole\n...\ndef \ntrain\n(\nenv\n, \nreplay_memory\n , \nmodel\n, \ntarget_model\n , \ndone\n):\n    learning_rate = \n 0.7 \n# Learning rate\n    discount_factor = \n 0.618\n    MIN_REPLAY_SIZE = \n 1000\n    \nif \nlen\n(replay_memory) < MIN_REPLAY_SIZE:\n        \n return\n    batch_size = \n 64\n * \n2\n    mini_batch = random.sample(replay_memory, batch_size)\n    current_states = np.array([transition[\n 0\n] \nfor\n transition \n in\n mini_batch])\n    current_qs_list = model.predict(current_states)\n    new_current_states = np.array([transition[\n 3\n] \nfor\n transition \n in\n mini_batch])\n    future_qs_list = target_model.predict(new_current_states)\n    X = []\n    Y = []\n...\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#22": "Deep Q-learning in Python: CartPole\n...\n    \nfor\n index, (observation, action, reward, new_observation, done) \n in \n                                       \n enumerate\n (mini_batch):\n        \n if \nnot\n done:\n            max_future_q = reward + discount_factor * \n        np.\n max\n(future_qs_list[index])\n        \n else\n:\n            max_future_q = reward\n        current_qs = current_qs_list[index]\n        current_qs[action] = (\n 1\n - learning_rate) * current_qs[action] + \n                 learning_rate * max_future_q\n        X.append(observation)\n        Y.append(current_qs)\n    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=\n 0\n, \n        shuffle=\n True\n)\n ...\n23",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#23": "Deep Q-learning in Python: CartPole\n...\ndef \nmain\n():\n    epsilon = \n 1 \n# inizializzato ad 1, cioè ogni azione è random\n    max_epsilon = \n 1\n    min_epsilon = \n 0.01 \n# al valore minimo, 1% sarà ancora esplorazione\n    decay = \n 0.01\n    \n# 1. Initializzazione Target e Main models\n    \n# Main Model (updated every 4 steps)\n    model = agent(env.observation_space.shape, env.action_space.n)\n    \n# Target Model (updated every 100 steps)\n    target_model = agent(env.observation_space.shape, env.action_space.n)\n    target_model.set_weights(model.get_weights())\n    replay_memory = deque(maxlen=\n 50\n_\n000\n)\n    target_update_counter = \n 0\n    \n# X = states, y = actions\n    X = []\n    y = []\n...\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#24": "Deep Q-learning in Python: CartPole\n...\n    steps_to_update_target_model = \n 0\n    \nfor\n episode \n in \nrange\n(train_episodes):\n        total_training_rewards = \n 0\n        observation = env.reset()\n        done = \n False\n        \n while \nnot\n done:\n            steps_to_update_target_model += \n 1\n            \n if True:            # Su Colab può dare problemi\n            \n    env.render()\n            random_number = np.random.rand()\n            \n # 2. Esplora con Epsilon Greedy Exploration Strategy\n            \n if\n random_number <= epsilon:\n                \n # Explore\n                action = env.action_space.sample()\n            \n else\n:\n                \n # Exploit best known action\n                \n # model dims are (batch, env.observation_space.n)\n                encoded = observation\n                encoded_reshaped = encoded.reshape([\n 1\n, encoded.shape[\n 0\n]])\n                predicted = model.predict(encoded_reshaped).flatten()\n                action = np.argmax(predicted)\n            new_observation, reward, done, info = env.step(action)\n            replay_memory.append([observation, action, reward, new_observation, done])\n...\n25",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#25": "Deep Q-learning in Python: CartPole\n...\n           \n # 3. Aggiorna la rete con la Bellman Equation\n            \n if\n steps_to_update_target_model % \n 4\n == \n0 \nor\n done:\n                train(env, replay_memory, model, target_model, done)\n            observation = new_observation\n            total_training_rewards += reward\n            \n if\n done:\n                \n print\n(\n'Total training rewards: {} after n steps = {} with final \n                      reward = {}'\n .\nformat\n(total_training_rewards, episode, reward))\n                total_training_rewards += \n 1\n                \n if\n steps_to_update_target_model >= \n 100\n:\n                    \n print\n(\n'Copying main network weights to the target network \n                           weights'\n )\n                    target_model.set_weights(model.get_weights())\n                    steps_to_update_target_model = \n 0\n                \n break\n        epsilon = min_epsilon+(max_epsilon - min_epsilon)*np.exp(-decay * episode)\n    env.close()\nif\n __name__ == \n '__main__'\n :\n    main()\n26",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#26": "Action Space: Discrete(2)\nState space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\nTotal training rewards: 19.0 after n steps = 0 with final reward = 1.0\n...\nCopying main network weights to the target network weights\nTotal training rewards: 184.0 after n steps = 291 with final reward = 1.0\nCopying main network weights to the target network weights\nTotal training rewards: 152.0 after n steps = 292 with final reward = 1.0\nCopying main network weights to the target network weights\nTotal training rewards: 47.0 after n steps = 293 with final reward = 1.0\nTotal training rewards: 20.0 after n steps = 294 with final reward = 1.0\nTotal training rewards: 121.0 after n steps = 295 with final reward = 1.0\nCopying main network weights to the target network weights\nTotal training rewards: 10.0 after n steps = 296 with final reward = 1.0\nTotal training rewards: 124.0 after n steps = 297 with final reward = 1.0\nCopying main network weights to the target network weights\nTotal training rewards: 272.0 after n steps = 298 with final reward = 1.0\nCopying main network weights to the target network weights\nTotal training rewards: 41.0 after n steps = 299 with final reward = 1.0\nDeep Q-learning in Python: CartPole\n27",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#27": "Deep Q-learning: Stable Baseline3\nStable Baselines3 (SB3) implementa algoritmi di RL in PyTorch.  \nPossono essere impiegati in OpenAI GYM. \nGithub repository: \n https://github.com/DLR-RM/stable-baselines3  \nLa classe DQN implementa l'approccio descritto in precedenza \nhttps://stable-baselines3.readthedocs.io/en/master/modules/dqn.html   \nhttps://stable-baselines3.readthedocs.io/en/master/guide/examples.html  \n28",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#28": "Deep Q-learning: Stable Baseline3\nAlgoritmi implementati:\n29\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#29": "Deep Q-learning: CartPole in GYM\n# Anaconda: conda install -c conda-forge stable-baselines3 \n!\npip install stable-baselines3[extra]\nimport\n gym\nfrom\n stable_baselines3 \n import\n DQN\nenv = gym.make(\n \"CartPole-v0\"\n )\nmodel = DQN(\n \"MlpPolicy\"\n , env, verbose=\n 1\n)\nmodel.learn(total_timesteps=\n 10000\n, log_interval=\n 4\n)\nmodel.save(\n \"dqn_cartpole\"\n )\ndel\n model \n# remove to demonstrate saving and loading\nmodel = DQN.load(\n \"dqn_cartpole\"\n )\nobs = env.reset()\nwhile \nTrue\n:\n    action, _states = model.predict(obs, deterministic=\n True\n)\n    obs, reward, done, info = env.step(action)\n    env.render()                 \n # Su colab può dare problemi\n    \nif\n done:\n      obs = env.reset()\n30",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#3": "OpenAI GYM: Environments\nLa variabile \n observation_space\n  deﬁnisce la struttura e gli stati permessi \ndell'ambiente.  \nPer esempio, posizione  rispetto all'orgine e velocità del carrello \nCartPole, rappresentati come vettore numerico. \nMa in casi più complessi è possibile fare un rendering dello stato (es. \nuno screenshot di un arcade) è impiegare una matrice di pixel come \nstato. \nLa variabile \n action_space\n  consiste nelle azioni permesse nell'ambiente. \nGym fornisce diverse strutture per rappresentare osservazioni e stati (es. \ndiscrete action space, continuous action space, ecc). \n4",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#30": "Deep Q-learning: CartPole in GYM\nL'output dipende dall'approccio RL scelto, es. per agenti PPO - Proximal \nPolicy Optimization algorithm che combina il A2C multiple workers, e \nTRPO:\n31\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#31": "Deep Q-learning: CartPole in GYM\n32\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#32": "Deep Q-learning: CartPole in GYM\n33\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#33": "Deep Q-learning: CartPole in GYM\n34\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#34": "Esercitazione\nDeﬁnisci una misura di prestazioni per confrontare diverse approcci RL \nImpiega la classe Deep Q Network (DQN) \nSperimenta diversi iperparametri e valuta le differenze: \nlearning_rate \nexploration_initial_eps  e  exploration_ﬁnal_eps \nbuffer_size \nbatch_size \ngamma \ntrain_freq \nUsa altri ambienti, es: Atlantis-v0, MountainCar-v0, o ambienti Atari.\n35",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#35": "OpenAI Gym \n https://www.gymlibrary.ml\nTesti di Riferimento\n36",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#4": "OpenAI GYM\nCome ispezionare l'ambiente. \nimport\n gym\ndef \nquery_environment\n (\nname\n):\n    env = gym.make(name)\n    spec = gym.spec(name)\n    \nprint\n(\nf\n\"Action Space: \n {env.action_space}\n \"\n)\n    \nprint\n(\nf\n\"Observation Space: \n {env.observation_space}\n \"\n)\n    \nprint\n(\nf\n\"Max Episode Steps: \n {spec.max_episode_steps}\n \"\n)\n    \nprint\n(\nf\n\"Nondeterministic: \n {spec.nondeterministic}\n \"\n)\n    \nprint\n(\nf\n\"Reward Range: \n {env.reward_range}\n \"\n)\n    \nprint\n(\nf\n\"Reward Threshold: \n {spec.reward_threshold}\n \"\n)\nquery_environment(\n \"MountainCar-v0\"\n )\n3 azioni: Accelerate forward, decelerate, backward\n>> Action Space: Discrete(3)\n2 ﬂoat: velocità e posizione; (2,) indica la struttura del dato\n>> Observation Space: \n                 Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n200 step disponibili\n>> Max Episode Steps: 200\n>> Nondeterministic: False\nPer il reward occorre ispezionare il codice (i.e., nessun reward tranne quando il carrello riesce ad uscire)\n>> Reward Range: (-inf, inf)\n>> Reward Threshold: -110.0\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#5": "OpenAI GYM\nquery_environment(\n \"CartPole-v1\"\n )\n2 valori: spingi a sinistra, spingi a destra\n>> Action Space: Discrete(2)\n4 valori: Cart Position, Cart Velocity, Pole Angle, Pole Velocity At Tip\n>> Observation Space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), \nfloat32)\n>> Max Episode Steps: 500\n>> Nondeterministic: False\nreward +1 per ogni step, e termino quando il palo cade, o se mi sposto di >2.4 unità dal centro\n>> Reward Range: (-inf, inf)\n>> Reward Threshold: 475.0\nquery_environment(\n \"MountainCarContinuous-v0\"\n )\n1 valore: quanta forza imprimere (a sinistra o destra)\n>> Action Space: Box(-1.0, 1.0, (1,), float32)\n>> Observation Space: Box(-1.2000000476837158, 0.6000000238418579, (2,), \nfloat32)\n>> Max Episode Steps: 999\n>> Nondeterministic: False\n>> Reward Range: (-inf, inf)\n>> Reward Threshold: 90.0\n6",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#6": "OpenAI GYM\nNell'ambiente Atari breakout, l'observation space può corrispondere alle \ndimensioni dell screen (210x160) o alla RAM dell'elaboratore (128 bytes). \nquery_environment(\n \"Breakout-v0\"\n )\nAction Space: Discrete(4)\nObservation Space: Box(0, 255, (210, 160, 3), uint8)\nMax Episode Steps: 10000\nNondeterministic: False\nReward Range: (-inf, inf)\nReward Threshold: None\nquery_environment(\n \"Breakout-ram-v0\"\n )\nAction Space: Discrete(4)\nObservation Space: Box(0, 255, (128,), uint8)\nMax Episode Steps: 10000\nNondeterministic: False\nReward Range: (-inf, inf)\nReward Threshold: None\n7\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#7": "OpenAI GYM: Environments\nCome applicare un'azione sull'ambiente. \n# reset \nobs = env.reset()\nprint\n(\n\"The initial observation is {}\"\n .\nformat\n(obs))\nrandom_action = env.action_space.sample()\n# Applichiamo l'azione all'ambiente\nnew_obs, reward, done, info = env.step(random_action)\nprint\n(\n\"The new observation is {}\"\n .\nformat\n(new_obs))\n>> OUTPUT:\n>> \nThe initial observation \n is\n [\n-0.48235664   \n0\n.]\n>> \nThe new observation \n is\n [\n-0.48366517  \n-0.00130853\n ]\n8",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#8": "OpenAI GYM: Environments\nPer visualizzare lo stato a video su Colab inserire il seguente codice: \n# vedi \nhttps://github.com/ryanrudes/colabgymrender  \n!\npip install gym pyvirtualdisplay > /dev/null \n 2\n>&\n1\n!\napt-get install -y xvfb python-opengl ffmpeg > /dev/null \n 2\n>&\n1\n!\npip install colabgymrender==\n 1.0.2\nimport\n gym\nfrom\n colabgymrender.recorder \n import\n Recorder\nenv = gym.make(\n \"MountainCar-v0\"\n )\ndirectory = \n './video'\nenv = Recorder(env, directory)\nobservation = env.reset()\nterminal = \n False\nwhile \nnot\n terminal:\n# Azione random\n  action = env.action_space.sample()\n  observation, reward, terminal, info = env.step(action)\nenv.play()\n9\n",
    "data_test\\rootfolder\\università\\MachineLearning\\43-Ex_17 Esercitazione RL 2-sbloccato.pdf#9": "OpenAI GYM: Environments\n...\nenv = gym.make(\n \"Atlantis-v0\"\n )\ndirectory = \n './video'\nenv = Recorder(env, directory)\n...\n10\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#0": "Machine Learning \nUniversità Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nEsercitazione: Introduzione al Deep Learning  \n(Ex 18/19)\n1",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#1": "Sommario\nIntroduzione \nIl concetto di Deep Learning \nEsempi di applicazioni \nTranslational simmetry \nConvolutional Neural network \n•\nConvolutional layer \n•\nLocal receptive ﬁeld \n•\nStride e Padding \n•\nFilters e Feature Maps \n•\nPooling Layer \nArchitettura LeNet-5 \nArchitettura AlexNet \nArchitettura GoogleNet e Inception Module \nArchitettura Residual Network (ResNet)",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#10": "Esercizio\nRealizzo un classiﬁcatore binario che mi identiﬁca se in una \nporzione di immagine c’è un pedone. \nScorro l’immagine per trovare una porzione con un pedone  \n \n        \n11\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#11": "Object recognition\nYolo https://pjreddie.com/darknet/yolov2/ \nhttps://www.youtube.com/watch?v=VOC3huqHrss \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#12": "Pose estimation\nZhe Cao , Tomas Simon, Shih-En Wei, Yaser Sheikh   \nhttps://www.youtube.com/watch?v=pW6nZXeWlGM \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#13": "Object Tracking\n Beijing DeepGlint  https://www.youtube.com/watch?v=xhp47v5OBXQ  \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#14": "Activity Recognition\n MIT CS & AI Lab http://relation.csail.mit.edu\nhttps://www.youtube.com/watch?v=JBwSk6nJOyM \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#15": "Tesla Self Driving Demo 2016\nTesla   https://www.youtube.com/watch?v=VG68SKoG7vE \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#16": "Introduzione\nAlcune soﬁsticate \n architetture ML\n  sono riuscite a ottenere \n performance superiori a \nquelle umane\n  (es. il gioco degli scacchi con IBM Deep Blue). Ma solo intorno al \n2000\n  si sono ottenute\n  buone performance per  \ntask apparentemente più semplici\n , \ncome: \n•\nRiconoscere un giocattolo in una immagine \n•\nSpeech recognition - riconoscimento vocale  \nPer noi sono task semplici perché l'evoluzione ha portato il cervello a costruire \nstrutture con funzioni speciﬁche.  \nQuando le informazioni arrivano alle parti deputate al ragionamento ad alto \nlivello, sono già arricchite di features ad alto livello elaborate da queste strutture. \n•\nSebbene siamo coscienti che esiste un giocattolo, non sappiamo spiegare quale \nprocesso abbiamo seguito per identiﬁcarlo. \n•\nLe architetture \n Convolutional Neural Networks (CNN)\n  sono state sviluppate negli \nanni '80 in base agli studi della zona della corteccia deputata al riconoscimento \nvisivo. Nelle ultime 2 decadi si sono diffuse grazie alla presenza di \n GPU\n .\n17",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#17": "L'architettura della Visual cortex\nNegli anni '60 \n Hubel e Wiesel\n  hanno dimostrato che  \n•\nmolti neuroni nella parte di corteccia deputata al riconoscimento di \nimmagini possiedono un piccolo \n Local receptive ﬁeld (LRF)\n , cioè possono \nreagire agli stimoli situati in regioni limitate del campo visuale. \n•\nsebbene condividano il LRF, \n alcuni neuroni si attivano \n solo\n in presenza di \nlinee orizzontali\n , \naltri \nsolo \ncon quelle \n verticali\n . \n•\nalcuni neuroni hanno LRF più estesi\n  e \nsi attivano in presenza di certe \nconﬁgurazioni di più caratteristiche a basso livello\n .  \n•\nsi può desumere che l'attivazione di neuroni ad alto livello é basata \nsull'output di neuroni a basso-livello che sono ritenuti \"vicini\". \nAumentando la complessità, ripetendo più volte in cascata i passi riportati, \npossiamo riconoscere \n patterns visuali \n anche molto \n complessi\n . \nNota\n : il resto della lezione suppone di considerare \n immagini\n  come istanze di \ninput, ma le tecnologie introdotte possono essere usate anche per altri input.\n18",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#18": "L'architettura della Visual cortex\n19\nSecondo te è una MLP?Ad ogni livello saliamo di astrazione \nnei pattern individuati\nLocal receptive ﬁelds",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#19": "L'architettura della Visual cortex\n20\nÈ simile a una MLP , \nma ogni nodo e connesso solo  \na un piccolo insieme di neuroni viciniAd ogni livello saliamo di astrazione \nnei pattern individuati\nLocal receptive ﬁelds",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#2": "Deep Learning - Cos’è?\nInvece di una singola rete con molti parametri da individuare \ntutti insieme, si suddividono le elaborazione in più moduli \ndistinti a cascata. \nSviluppato negli anni ’80 (Geoff Hinton, Yann Lecun, Yoshua \nBengio, Jürgen Schmidhuber) ispirandosi ai risultati sulla \ncognizione umana. \nMa al tempo non c’erano le infrastrutture hardware e software \nadatte (GPU-enabled). \nUtile in scenari con grosse moli di dati complessi. \nUno degli obiettivi è ignorare la (noiosa) fase di deﬁnizione di \nfeature ad-hoc per lo speciﬁco problema da esaminare e lasciare \nalle reti neurali identiﬁcare le features più adatte.\n3",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#20": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti?\n21",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#21": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti? \n1.\nA causa dell'\n elevato numero di parametri da stimare  \n•\nSupponiamo di avere in input una piccola immagine di 100x100 pixel \n•\nCreiamo un primo layer di appena 1000 nodi, che perciò ﬁltra \nnotevolmente le informazioni passata ai successivi layer. \n•\nPer questo primo strato abbiamo già \n 10 milioni di parametri da stimare\n .\n22",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#22": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti? \n1.\nA causa dell'\n elevato numero di parametri da stimare  \n•\nSupponiamo di avere in input una piccola immagine di 100x100 pixel \n•\nCreiamo un primo layer di appena 1000 nodi, che perciò ﬁltra notevolmente \nle informazioni passata ai successivi layer. \n•\nPer questo primo strato abbiamo già \n 10 milioni di parametri \n da stimare. \n2.\nSupponiamo che \n certi nodi \n del primo strato \n si specializzino su un certo task\n , es. \nriconoscere linee orizzontali. \n•\nI neuroni specializzati sono attivati se il pattern da identiﬁcare è localizzato in \nuna certa zona.  \n•\nMa vorremmo poter identiﬁcare lo stesso pattern indipendentemente da dove \ncompare. \nCon \ntranslational symmetry\n  intendiamo che lo stesso output deve essere \nprodotto anche a seguito di operazioni di traslazione sulla istanza in input. \n23",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#23": "MLP - fully connected\nPerché non usiamo una NN MLP per riconoscere gli oggetti? \n3. Le reti \n MLP \nnon riescono a codiﬁcare esplicitamente l'organizzazione \nspaziale delle features\n . \n•\nNel Visual cortex i neuroni degli strati più vicini all'input identiﬁcano \nfeatures analizzando piccole aree dell'immagine. \n•\nI neuroni \"ad alto livello\" combinano tali features per identiﬁcare features \nspazialmente più estese.\n24",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#24": "Esempi di \n translational simmetry\n25\nNel task a lato, per addestrare una MLP dovremmo avere \nun training set con: \n•stessa specie animale che compare in varie \nposizioni, angolazioni e dimensioni. \n•specie visualizzate parzialmente (es. sul bordo). \n•casi di overlap tra specie diverse di animali",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#25": "Ulteriore considerazione: \n sparsity\n26Per riconoscere certe caratteristiche speciﬁche analizziamo informazioni \"locali\" o ravvicinate, cioè con una \ndistanza relativa limitata. Non c'è bisogno di considerare l'intera immagine iniziale. \nUn output associato ad una certa feature (es. occhio o naso) è associato solo un certo numero di pixel in input.  ",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#26": "Convolutional NN\nLe architetture \n Convolutional NN (CNN)\n  consistono in varie tecniche \nispirate al funzionalmente del cervello.  \nIl blocco più importante è il \n convolutional layer\n  così costituito: \n•\nI nodi nel primo layer \n non sono connessi con tutti i pixel\n  dell'immagine in \ninput, ma \n solo in una regione\n  (es. un rettangolo). Tale regione è chiamata \nLocal receptive ﬁeld (LRF)\n . \n•\nQuesto permette alla rete di \n specializzarsi\n  su caratteristiche a basso \nlivello che saranno poi elaborate in caratteristiche a più alto livello nei \nsuccessivi hidden layer. \n•\nUna rete CNN è \n gerarchica\n , con più convolutional layer nascosti che \nindividuano via via caratteristiche più astratte.\n27",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#27": "CNN - Convolutional layer e LRF\n28\nnodo\ninput per il successivo hidden layer\n25x2521x21\nEsempio di input: \nimmagine 25x25 pixel\nin bianco e neroOutput dopo il primo  \nlayer convolutivo.local receptive ﬁeldOgni nodo è attivato in base \nall'input determinato \nda una certa posizione del \nLRF che scorre lungo l'input.input\nConvolutional layer\nnotiamo la riduzione della  \ndimensione rispetto all'inputmatrice delle attivazioni\nelaborazione",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#28": "CNN - Struttura gerarchica\n29Input layer: \nÈ un layer costituito da unità \na cui viene associato il valore \ndei singoli pixel dell'immagine.  \nNon c'è reale elaborazione.Primo convolutional layer\nSecondo convolutional layerDato una instanza in input, nodi vicini nel convolutional layer layer saranno attivati in base \nalle features estratte da una certa zona dell'input. Astrazione delle features\nNota: Nelle tradizionali MLP, input bidimensionali [N, M] (es. immagini in bianco e nero) sono \ncomunemente ridimensionati a vettori, ovvero matrici di dimensioni [NxM, 1].  \nNelle CNN tale ridimensionamento è controproducente poiché si perderebbe l'informazione relativa alla \nvicinanza delle features in input. Struttura gerarchica\n Nell'input layer le features \ncorrispondono ai singoli pixel",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#29": "CNN - LRF\n30Output layer precedente•In un certo layer convoluzionale, un nodo con indice (i, j) prende in input gli output dei nodi \ndel layer precedente posizionati all'interno del LRF .\n•la regione LRF va dalla riga i alla riga i+f h-1, e dalla colonna j alla colonna j+f w-1\n•fh e f w corrispondono all'altezza e larghezza del LRF. \n•i e j sono indici che scorrono da 1 a dim x-fh-1 e dim y-fh-1\nIl convolutional layer è \nrappresentato da una \ngriglia bidimensionale \nche contiene il risultato \ndelle attivazioni.forward propagation\nEsempio con LRF 3x3 \ncon stride pari a 1.<------ padding ------>\n<------ padding ------><------ dim x ------>\n<--- dimy -->\nPadding \n(discusso più avanti)",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#3": "Deep Learning - Cos’è? (2)\nIn estrema sintesi: \nSi compongono più strutture di reti neurali in cascata il cui \nscopo è analizzare l’input ed estrarre ad ogni passo un \ninsieme di features (in automatico). \nL’output di una rete neurale è l’input della successiva. \nTipicamente l’input iniziale è low-level (es. gruppi di pixel di \nuna immagine) e ogni rete genera rappresentazioni più ad \nalto livello (es. contorno viso, bocca, bocca sorridente etc.). \nLe elaborazioni degli strati intermedi sono tipicamente \nunsupervised.\n4",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#30": "CNN - Stride\n31•La distanza s tra due LRF adiacenti è chiamata stride. \n•Finora abbiamo visto stride di 1 pixel, ma la LRF può scorrere di più pixel. \nOutput layer precedenteLayer convoluzionale\n<------ padding ------>\n<------ padding ------>\nLRF di 3x3 \nStride = 2",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#31": "CNN: Padding\n32•Supponendo stride > 1, può accadere che il convolutional layer (comunque ridotto di fw-1 e \nfh-1 a causa del LRF) non abbia le stesse dimensioni del layer precedente poiché la LRF non \npuò scorrere l'intera instanza in input. \n•Il padding aggiunge dimensioni ai dati in input. Normalmente i dati inseriti sono valori nulli \n(0-padding). Si hanno i seguenti vantaggi:\n•Permettere alla LRF di scorrere per intero l'immagine in input senza ignorarne delle parti.\n•Un LRF potrebbe \"imparare\" a riconosce una certa feature quando è centrata \nnell'immagine. Se la feature è posizionata molto vicino al bordo, senza padding potrebbe \nessere ignorata.\n0-padding\n✓LRF\nOutput layer precedente senza padding Output layer precedente con padding",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#32": "CNN - Esempio di attivazione di un nodo \nL'attivazione di un nodo in un layer convoluzionale si ottiene \nanalizzando l’output dal layer precedente per mezzo del \n LRF\n. \nEsempio: la funzione d’attivazione (\n σ\n) per il nodo <\n l\n,\nk\n> si valuta \nconsiderando il bias \n b\n e la matrice \n W\n di dimensione \n f\nh  \nf\nw \nassociati al \nLRF, in questo caso pari a 3\n 3. \n \nW\n e \nb\n sono i parametri da determinare.  \ni\n e \nj\n sono gli offset riferiti al \n LRF\n. \nSe la ﬁnestra scorre un passo alla volta allora \n l\n e \nk\n fanno riferimento \nall’origine della ﬁnestra del \n LRF\n.\n×\n×\nσ\n(\nb\n+\n2\n∑\ni\n=\n0\n2\n∑\nj\n=\n0\nw\ni\n,\nj\n⋅\nx\ni\n+\nl\n,\nj\n+\nk\n)\nijlk\nLRF",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#33": "CNN: Riduzione dimensionalità e Stride\n34Output layer precedente•La presenza di stride > 1 altera gli indici iniziali e ﬁnali che identiﬁcato il LRF associato ad \nun certo nodo. \n•L'attivazione di un nodo nella posizione (i, j) di un certo layer è determinato dagli output dei \nnodi nel layer precedente posizionati nella righe da i × s h  a  i × s h + f h - 1, e nelle colonne da  \nj × s w  a  j × s w + f w - 1.\n•Per s pari a 1, si torna alla formulazione già vista.\n•Stride > 1 riducono la dimensione del layer convoluzionale a scapito della precisione.\nLayer convoluzionale\n<------ padding ------>\n<------ padding ------>stride verticale\nstride orizzontaleLRF di 3x3 \nStride = 2",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#34": "CNN: Filters\n35Filters•Supponiamo di poter rappresentare graﬁcamente i pesi associati a un certo nodo, usati per \nil calcolo della sua attivazione. Tali pesi prendono il nome di ﬁlters o convolution kernels  \n(o kernels )\n•Ad esempio, una LRF 7 7 corrisponderà ad un ﬁltro con medesime dimensioni. ×\nNell'esempio ci sono due ﬁltri Vertical ﬁlter e Horizontal ﬁlter entrambi con matrice tutta di 0, \ntranne una colonna di 1 e una riga di 1, rispettivamente.\nInput",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#35": "Esempi di ﬁltri e attivazioni (1)\n36Esempio di input \nimmagine 25x25 pixel\nOutput dopo il primo  \nlayer convolutivo.\nImmagine in input\nImmagine in inputOutput\nOutputFiltro\nFiltroAttivazioni\nAttivazioni",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#36": "Esempi di ﬁltri e attivazioni (2)\nhttp://brohrer.github.io/how_convolutional_neural_networks_work.html\n1-1-1\n-11-1\n-1-11\n0.33 -0.11 0.55 0.33 0.11 -0.11 0.77\n-0.11 0.11 -0.11 0.33 -0.11 1.00 -0.11\n0.55 -0.11 0.11 -0.33 1.00 -0.11 0.11\n0.33 0.33 -0.33 0.55 -0.33 0.33 0.33\n0.11 -0.11 1.00 -0.33 0.11 -0.11 0.55\n-0.11 1.00 -0.11 0.33 -0.11 0.11 -0.11\n0.77 -0.11 0.11 0.33 0.55 -0.11 0.33-1-1-1-1-1-1-1-1-1\n-11-1-1-1-1-11-1\n-1-11-1-1-11-1-1\n-1-1-11-11-1-1-1\n-1-1-1-11-1-1-1-1\n-1-1-11-11-1-1-1\n-1-11-1-1-11-1-1\n-11-1-1-1-1-11-1\n-1-1-1-1-1-1-1-1-1=0.77 -0.11 0.11 0.33 0.55 -0.11 0.33\n-0.11 1.00 -0.11 0.33 -0.11 0.11 -0.11\n0.11 -0.11 1.00 -0.33 0.11 -0.11 0.55\n0.33 0.33 -0.33 0.55 -0.33 0.33 0.33\n0.55 -0.11 0.11 -0.33 1.00 -0.11 0.11\n-0.11 0.11 -0.11 0.33 -0.11 1.00 -0.11\n0.33 -0.11 0.55 0.33 0.11 -0.11 0.77\n-1-11\n-11-1\n1-1-11-11\n-11-1\n1-110.33 -0.55 0.11 -0.11 0.11 -0.55 0.33\n-0.55 0.55 -0.55 0.33 -0.55 0.55 -0.55\n0.11 -0.55 0.55 -0.77 0.55 -0.55 0.11\n-0.11 0.33 -0.77 1.00 -0.77 0.33 -0.11\n0.11 -0.55 0.55 -0.77 0.55 -0.55 0.11\n-0.55 0.55 -0.55 0.33 -0.55 0.55 -0.55\n0.33 -0.55 0.11 -0.11 0.11 -0.55 0.33=\n=-1-1-1-1-1-1-1-1-1\n-11-1-1-1-1-11-1\n-1-11-1-1-11-1-1\n-1-1-11-11-1-1-1\n-1-1-1-11-1-1-1-1\n-1-1-11-11-1-1-1\n-1-11-1-1-11-1-1\n-11-1-1-1-1-11-1\n-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1\n-11-1-1-1-1-11-1\n-1-11-1-1-11-1-1\n-1-1-11-11-1-1-1\n-1-1-1-11-1-1-1-1\n-1-1-11-11-1-1-1\n-1-11-1-1-11-1-1\n-11-1-1-1-1-11-1\n-1-1-1-1-1-1-1-1-1ﬁltroattivazioni\nﬁltro\nﬁltro",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#37": "CNN: Feature Maps\n38Filters•Le LRF scorrono sulla immagine in input. Supponiamo di mantenere costante i valori del \nﬁltro usato per il calcolo dell'attivazione. Tale approccio prende il nome di shared weights. \n•L'insieme delle attivazioni ottenute con lo stesso ﬁltro viene chiamato feature map. Esse \npossono essere visualizzate come una immagine.\nNell'esempio si nota che il Vertical ﬁlter crea una feature map dove le zone dell'input simili a una \nlinea verticale sono più evidenziate (cioè più attivazione), mentre le zone  meno simili saranno \npiù scure e sfocate. Discorso duale per il ﬁltro Horizontal ﬁlter.Feature maps\nInput",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#38": "CNN: Stacking feature maps\nIn \nogni layer\n  possiamo contemplare \n più ﬁltri con le medesime dimensioni\n . \nOgni ﬁltro produrrà una diversa feature map. Ogni layer sarà così costituito \nda una sequenza di matrici di attivazioni, perciò una \n struttura 3d\n . \nDurante il \n forward propagation\n  è fondamentale che i ﬁltri, cioè i parametri \npesi\n e \nbias\n che costituiscono il layer convoluzionale, rimangano costanti, \nsebbene il valore delle attivazioni, ovvero la \n feature map\n , cambiano in base \nalla posizione del \n LRF\n. Questo permette di: \n•\nAvere un numero molto minore di parametri da stimare rispetto a un layer \nMLP. \n•\nDurante la backpropagation, adattare ogni ﬁltro ad una particolare \ncaratteristica saliente.  \n•\nLa possibilità di usare lo stesso ﬁltro in diverse zone dell'immagine garantisce la \ntranslational simmetry\n , cioè possiamo riconoscere la caratteristica in diverse \nposizioni. Una rete Fully connected (\n FC\n) potrebbe riconoscere una caratteristica \nin una posizione stimando certi parametri, ma non sarebbe in grado di \nriutilizzarli in altre posizioni.\n39",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#39": "CNN: Feature Maps\n40...Input\nConvolutional layer 2\nConvolutional layer 1\nUna immagine a colori con 3 matrici \nassociate ai canali RGB.Possiamo deﬁnire un certo numero di ﬁltri (es. 12) per riconoscere diverse caratteristiche salienti dell'immagine iniziale. I ﬁltri analizzano contemporaneamente 3 canali RGB, perciò i ﬁltri saranno deﬁniti con matrici a 3 dimensioni. Un ﬁltro applicato all'immagine in input produce un singolo convolutional layer.I successivi layer convoluzionali analizzato le attivazioni di più ﬁltri contemporaneamente. I ﬁltri di questo layer riconosceranno caratteristiche più astratte.depth = 3 depth = 12 depth = 7",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#4": "Deep Learning - Principali vantaggi\nRispetto ad altri approcci: \nSi può sviluppare un unico framework computazionale che \npuò essere implementato ed eseguito su varie piattaforme \nhardware e cloud. \nIl framework offre funzionalità valide per molte architetture di \nreti e tasks (es. natural language processing, computer vision, \nspeech recognition, etc.) \nSi possono condividere e riutilizzare i parametri ottenuti \ndurante l’apprendimento per uno speciﬁco task in altri \ncontesti.\n5",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#40": "TensorFlow: Padding\nTensorFlow fornisce il parametro \n padding\n  che può assumere due valori: \n•\n\"\nVALID\n \" nel caso in cui si voglia ignorare il padding  \n•\n\"\nSAME\n \" per aggiungere automaticamente righe e colonne composte da \nvalori 0 in modo bilanciato per garantire che il LRF scorra l'intera matrice \nin input.\n4101234567891011121300 12345678910111213\nsenza padding ('VALID') con padding ('SAME')ignorati\nstride=5padding P=+3",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#41": "Tuning delle CNN\nRispetto a una MLP abbiamo \n molti più iperparametri da stimare\n : \nNumero di ﬁltri per layer (o \n depth\n ) \nDimensione del LRF \nStride e padding \nInvece di usare tecniche automatiche per il tuning,\n  ci si ispira ad \narchitetture già studiate \n in letteratura per avere una conﬁgurazione \nverosimilmente già ottimizzata.\n42",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#42": "Risorse di memoria: considerazioni\nLa backpropagation richiede di memorizzare tutti i valor intermedi calcolati \ndurante la forward propagation\n . \n•\nAd esempio, \n convolutional layer \n con ﬁltri 5\n 5 e con 200 feature maps di \ndimensione 150\n 100 con stride 1 e padding SAME: se in input abbiamo \nimmagini RGB 150\n 100, il numero di \n parametri\n  è (5\n 5\n3+1)\n 200 = \n 15.200 \n•\nNella \n MLP\n, un layer 150\n 100 completamente connesso col layer in input \nrichiederebbe 150\n 100\n 150\n 100\n 3 = \n67.5M di parametri\n . \n•\nOgnuna delle 200 mappe contiene 150\n 100 nodi, ed ogni nodo ricava \nl'attivazione valutando 5\n 5\n3 input, che corrispondono a \n 225 milioni di \nmoltiplicazioni\n  in virgola mobile. \n•\nCon ﬂoat di \n 32bit\n  il layer di output impiega 200\n 150\n 100\n 32 = \n 11.5Mb \ncirca\n  per ogni istanza. Per 100 istanze il layer occuperebbe più di un \n 1Gb\n. \nIn produzione, le attivazioni di un layer possono essere dimenticate appena i \ncalcoli sul layer successivo sono terminati, richiedendo molta meno memoria \n(cioè al massimo quella di 2 layer contemporaneamente). \n×\n×\n×\n ×\n×\n ×\n×\n×\n ×\n ×\n ×\n×\n×\n×\n×\n ×\n ×\n43",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#43": "Pooling layer (1)\nI \nlayer di pooling \n ha lo scopo di \n ridurre il numero di parametri \n operando un \ncampionamento\n  (o \ndown-sampling\n ) dei dati. I vantaggi sono i seguenti: \n•\nMeno complessità computazione \n•\nMeno risorse di memoria \n•\nMeno parametri (e ridurre l'overﬁtting come effetto collaterale) \nCome nel convolutional layer, \n ogni nodo è connesso con un numero limitato di \nnodi del layer precedente \n posizionati in un certo LRF. \n•\nOccorre deﬁnire dimensione, stride e padding \nIl \npooling layer non ha parametri.\n  Opera semplicemente una \"\n aggregazione\n \" dei \nvalori associati ai nodi, ad esempio calcolando \n media\n  o \nvalore massimo\n . \nSpesso il calcolo è fatto per ogni canale in input, cioè su un singolo strato alla volta \nrispetto all'intera profondità del layer precedente (es. sul canale R, G e B \nseparatamente).  \n•\nLa profondità (numero di layer) in uscita corrisponderà a quella che si ha in \ningresso. \n44",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#44": "Pooling layer (2)\nNon ha parametri da inferire\n , ma solo iperparametri, cioè dimensione del \nﬁeld (\n pooling size\n ), il \npooling stride\n , e tipo di aggregazione. \n•\nSpesso pooling size e stride corrispondono. \nIn molti scenari \n non è fondamentale la posizione esatta di una certa \ncaratteristica\n , ma il fatto che esista in una certa zona, o che sia identiﬁcata \nuna certa sequenza (o pattern) di features senza considerare esattamente le \nrispettive distanze reciproche. \n•\nAd esempio, nella face detection ho interesse a riconoscere due occhi \nvicini, ma non mi interessa la distanza esatta. \nEsistono \n due tipi principali di aggregazione\n : \n•\nmax-pooling:  \nun nodo assume l’attivazione massima tra i valori presenti \nnel ﬁeld considerato. \n•\naverage pooling:\n  considero il valor medio nel ﬁeld.\n45",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#45": "Esempio: Pooling layer\nNell'esempio il pooling kernel è di 2\n 2, lo stride pari a 2, padding VALID e \naggregazione max. \n•\nIl layer di output contiene il 75% in meno dei valori del layer precedente.\n×\n46\nA causa del padding VALID \nil valore di alcuni nodi sarà ignorato.\nSe in input abbiamo un canale con un layer NN,  f po è il pooling size, s po il pooling stride,  \n \nuna dimensione del layer di output è:  ×\nN−fpo\nspo+1",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#46": "CNN: Convolutional layer e dimensione output\nLa dimensione dell'output di un \n convolutional layer\n  si ricava a \npartire dalla dimensione dell'input e dal valore degli iperparametri. \nSe per semplicità assumiamo input \n N\nN\n, e la dimensione del \n LRF \n \nf\nh\n = f\n w\n = \nf\n, lo stride \n s,\n e le righe (o colonne) \n p\n aggiunte come \npadding, allora una delle due dimensione del layer di output è la \nseguente: \n \nLa dimensione in output perciò corrisponde a \n O\nO.\n×\nO\n=\nN\n−\nf\n+\np\ns\n+\n1\n×",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#47": "AlexNet\n  (2012) è una delle prime architetture di reti neurali che combina CNN e \nGPU nell'ambito della classiﬁcazione degli oggetti.\nEsempio: calcolo parametri AlexNetoutput depth = 96input depth = 3\nRicordiamoci  che il local receptive ﬁeld  \nha una profondità pari a quella dell’inputEsempi di calcolo dei parametri nel primo layer \nhidden: \n•Dim. immagine in Input = 227 227 3 \n•Dim. LRF = 11 11 \n•Stride = 4; padding VALID \n•Numero ﬁltri (o depth) = 96 \n•L’output per ogni ﬁltro avrà dimensione di lato (227-11)/4 + 1 = 55. Cioè 55 55 per ﬁltro. \n•Considerando la profondità si ha: 55x55x96 =290.400 nodi. \n•L'attivazione di un nodo si ricava considerando 11x11x3 nodi del layer precedente.  \n•In una MLP si avrebbero 105.415.200 parametri. \n•Per la proprietà degli shared weights, nella CNN il numero di parametri sarà 11x11x3x96 + 96 = 34.944. × ×\n×\n×\nfeature mapscomputazione",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#48": "Architettura LeNet-5 per OCR\nLeNet-5\n  (1989) è una delle prime architetture CNN.  \n•\nE' stata ideata per fare OCR garantendo un errore <1% su MNIST. \nCombina layers \n CNN\n  con una rete tradizionale \n MLP\n a valle.  \n•\nLo scopo è di impiegare le caratteristiche salienti identiﬁcate dalle CNN \nper fare classiﬁcazione per mezzo della MLP. \n•\nUna rete interamente \n MLP fully connected avrebbe richiesto molti più \nparametri\n  per ottenere le stesse prestazioni.",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#49": "Demo LeNet-5\nda http://yann.lecun.com/exdb/lenet/ ",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#5": "DeepMind e Atari Breakout\nGoogle Deepminds https://deepmind.com \nhttps://www.youtube.com/watch?v=eG1Ed8PTJ18 ",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#50": "Architettura LeNet-5\nconvolutional layer#1 conv. layer\nfeature maps:  \n28x28, depth 6#3 conv. layer \nfeature maps: \n10x10, depth 16\navg.  \npoolingconv. layeravg.  \npooling#2 pooling layer\nfeature maps: \n14x14, depth 6#4 pooling layer\nfeature maps: \n5x5, depth 16\nconv. layer#6 fully connected layer \nnodi 84#5 conv. layer \nfeature maps:  \n1x1, depth 120\nImmagini  \n32x32x1 (gray scale)LRF\nL'output dell'ultimo \nconvolution layer è \nconvertito in un vettore \n120x1, adatto come input di \nun fully connected layer.\nLa ReLU non era ancora \nstata approfondita ai tempi di \nLeNet-5. Si è impiegata la \npiù tradizionale tanh.#7 fully connected layer \nnodi 10\nLa conﬁgurazione degli \niperparametri e la dimensione \ndell'input non necessita di \nimpiegare il padding.",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#51": "LeNet-5 in Keras\nCon Keras l'implementazione di LeNet-5 per il dataset MNIST è rapida poiché i \nlayer di pooling e di convoluzione sono già fatti:  \nmodel = keras.Sequential()\nmodel.add(layers.Conv2D(filters=\n 6\n, kernel_size=(\n 3\n, \n3\n), activation=\n 'relu'\n, \n                     input_shape=(\n 32\n,\n32\n,\n1\n)))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Conv2D(filters=\n 16\n, kernel_size=(\n 3\n, \n3\n), activation=\n 'relu'\n))\nmodel.add(layers.AveragePooling2D())\n# cambio il formato da matrice a vettore\nmodel.add(layers.Flatten())\n# layer fully connected o denso\nmodel.add(layers.Dense(units=\n 120\n, activation=\n 'relu'\n))\nmodel.add(layers.Dense(units=\n 84\n, activation=\n 'relu'\n))\nmodel.add(layers.Dense(units=\n 10\n, activation = \n 'softmax'\n ))",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#52": "CNN - Esercizio\nSe le tue GPU non hanno memoria sufﬁciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema?\n53",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#53": "CNN - Esercizio\nSe le tue GPU non hanno memoria sufﬁciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema? \n1.\nRidurre la \n dimensione del mini-batch\n .\n54",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#54": "CNN - Esercizio\nSe le tue GPU non hanno memoria sufﬁciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema? \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers. \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere.\n55",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#55": "CNN - Esercizio\nSe le tue GPU non hanno memoria sufﬁciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema? \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers. \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere. \n3.\nCambiare l'architettura \n rimuovendo un layer\n .\n56",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#56": "CNN - Esercizio\nSe le tue GPU non hanno memoria sufﬁciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema? \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers. \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere. \n3.\nCambiare l'architettura \n rimuovendo un layer\n . \n4.\nUsare rappresentazioni\n  ﬂoat a 16\n  bit invece che 32.\n57",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#57": "CNN - Esercizio\nSe le tue GPU non hanno memoria sufﬁciente per una rete CNN, quali sono le \n5 cose che puoi fare per risolvere il problema? \n1.\nRidurre la \n dimensione del mini-batch\n . \n2.\nRidurre la \n dimensionalità nella rete\n , ad esempio con stride > 1 in uno o più \nlayers, o con pooling layers. \n•\nAnche un convolutional layer riduce la dimensione del layer precedente ma, a \ndifferena del pooling layer, introduce ulteriori parametri da apprendere. \n3.\nCambiare l'architettura \n rimuovendo un layer\n . \n4.\nUsare rappresentazioni\n  ﬂoat a 16\n  bit invece che 32. \n5.\nDistribuire la computazione\n  su più elaboratori.\n58",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#58": "Architettura AlexNet\nArchitettura CNN vincitrice della challenge object detection ILSVRC 2012 con un \ntop-5 error del 17% (il secondo ha ottenuto 26%).  \nE' molto simile a \n LeNet-5\n  ma con più profondità, con stacking dei pooling layer \nuno dopo l'altro (senza strato di pooling). \n•\nPrimo tentativo di sfruttare piattaforme hardware GPU-enabled per addestrare reti \ncomplesse.\nDopo i 5 convolutional \nlayers (11x11, 5x5 e 3x3) \nc'è il max pooling, e una \nrete FC da 3 layer. \nImpiega ReLI, SGD e \nmomentum. \n \nLa doppia pipeline è \ndovuta all’hardware \nimpiegato per \nl’addestramento (2 gpu)",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#59": "Architettura AlexNet (2)\nImpiega \n dropout\n  sugli strati FC, e \n data augumentation\n . Nei layer C1 e C3 impiega \nla \nLocal response normalization:\n  se un nodo riceve una attivazione signiﬁcativa, \nsi inibiscono i nodi nella stessa posizione ma in altre feature maps.  \n•\nL'ipotesi è quella di favorire la competitività, specializzando ogni feature map su \ncaratteristiche distinte.\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#6": "Robot Learns to Flip Pancakes\nPetar Kormushev (IIT): https://www.youtube.com/watch?v=W_gxLKSsSIE \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#60": "CNN: Alcune problematiche \nNei seguenti esempi riconosciamo un cane, ma la posizione e \ndimensione dell’animale sono molto diverse tra loro.  \n•\nNon è facile determinare la giusta dimensione (e il numero) dei \nﬁltri negli strati iniziali. \nE nonostante le tecnologie di apprendimento introdotte, in \narchitetture molto deep (con molti strati) può sempre riproporsi il \nvanishing gradient problem\n . \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#61": "CNN: Inception module (GoogleNet)\nL'\ninception module\n  si basa sulla ipotesi che \n combinare\n  le \ninformazioni provenienti da diverse pipeline di processamento basate \nconvolutional layer permetta di estendere le caratteristiche salienti \nidentiﬁcate. \n•\nPiù convolution layer in parallelo\n , ognuno con una \n diversa \ndimensione dei ﬁltri\n . Gli output dei convolution layers sono \n\"combinati\" in una singola struttura che consisterà nell'input per il \nlayer successivo. \n•\nSi impiegano ﬁltri con dimensioni pari a \n 1x1\n, \n3x3\n e \n5x5\n, tutti con \nstride 1\n , \nSAME\n  padding e \n ReLU\n  activation function. \nIn pratica si processa lo stesso input contemporaneamente \nconsiderando più dimensioni di LRF.  \nL'inception module è stato impiegato per la prima volta \nnell'architettura \n GoogleLeNet\n .",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#62": "Inception module\nL'input è dato contemporaneamente a \n 3 convolution layers\n  e un \n 3x3 \nmax pooling\n .  \n•\nLe \n1x1 convolution \n \"\ncomprimono\n \" la profondità dell'input, utili \nsoprattutto per \n sempliﬁcare i dati in input \n alle convoluzioni 3x3 e \n5x5 che richiedono risorse computazionali. \n•\nLa combinazione \n 1x1+3x3\n  e \n1x1+5x5\n  hanno più possibilità di \nrappresentare \n feature più complesse \n rispetto ai singoli 3x3 e 5x5. \n•\nSperimentalmente si nota come gli inception module sono più \nefﬁcienti se usati negli higher layers. \nInception module\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#63": "Architettura GoogLeNet v1\nL’architettura vincitrice della object detection challenge ILSRC 2014 \nraggiungendo un top-5 error < 7%.  \nLa principale caratteristica è la profondità: \n 22 layer\n  (27 considerando anche i \npooling layers) con 9 \n inception module\n  in cascata.  \n•\nDopo ogni \n inception module\n  si opera una average pooling per ridurre il \nnumero di parametri. \n•\nSebbene più profonda, possiede 1/10 dei parametri di AlexNet (6 milioni \ncirca)\nAltre tecniche impiegate: batch \nnormalization, image distortions e RMSprop?? inception module",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#64": "Architettura GoogLeNet v1 (2)\nL’\noutput di due inception module intermedi (3º e 6º inception module) è valutato \npreliminarmente nel task della classiﬁcazione \n per mezzo di una softmax. \nSi affrontare il problema del \n vanishing gradient problem\n , dato che si generano \ngradienti addizionali negli hidden layer lontani dall'ultimo layer. \nIl valore della loss intermedia è chiamato \n auxiliary loss\n . Durante il training \nviene combinato linearmente (scalato del 70%) con la loss dell'intera rete.  \nIn produzione e nel test set non vengono impiegati. \nNota\n : le versioni v2, v3 e v4 di GoogleNet introducono molti espedienti per \nrendere più efﬁciente il training e migliorare l’accuracy.\nauxiliary classiﬁerauxiliary classiﬁer",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#65": "GoogLeNet: esempio di ﬁltri\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#66": "Architettura Residual Network (ResNet)\nResNet\n  è stata presentata a ILSVRC 2015 (top-5: 3.6%) e consiste in 152 layers. \nUna rete con tale profondità non potrebbe essere addestrata a causa del \nvanishing gradient problem. \nSi introducono le \n skip connections\n , che propagano l'output di un certo layer \nnell'input di un layer che è posizionato più a valle.   \n•\nL'ipotesi è di rendere \n più semplice propagare segnali \n su varie parti della rete. \n•\nNelle fasi iniziali (comportamento random) si obbliga parti della rete ad \ncomportarsi in modo da riproporre i valori in input, rendendo \n più veloce \nl'apprendimento\n .\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#67": "ResNet: Residual learning\nAddestrare una rete neurale può essere interpretato come approssimare una \nfunzione h(\n x\n). Se aggiungi un valore x all'output della rete, allora la rete è \nobbligata a modellare la funzione f(\n x\n) = h(\n x\n) - \nx\n. Tale approccio è chiamato \nresidual learning\n . \nDal punto di vista operativo, è sufﬁciente combinare l'output di un layer con \nl'output di un layer posizionato più a monte prima di valutare la funzione di \nattivazione (ReLU).\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#68": "Architetture CNN\nPrincipali architetture CNN per le immagini, complessità, numero di operazioni \nrichieste per l'addestramento e accuratezza. \n69\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#7": "Chess Game\nStati possibili: ~1047\n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#8": "Driverless cars\nStati possibili: ?Microsoft AirSim simulator \nhttps://www.youtube.com/watch?v=fv-oFP AqSZ4 \n",
    "data_test\\rootfolder\\università\\MachineLearning\\44-Ex18 Intro DL-sbloccato.pdf#9": "Esercizio\nVoglio sapere se c’è un pedone di fronte a me analizzando \nl’immagine di una camera dalla mia auto, come procedo? \n10\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nRidge Regression \nCross Validation\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#1": "Sommario\nOverﬁtting nella polynomial  regression \nSintomi dell’Overﬁtting \nFunzione di Costo nella Ridge Regression \nMinimizzazione della Funzione di Costo \nForma Chiusa \nGradient Descent \nK-fold Cross Validation\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#10": "Overﬁtting e #input\n \n11\nAnche il numero di input inﬂuenza l’overﬁtting: \n1 input (e.g., sq.ft)  → per evitare l’overﬁtting servono \nosservazioni che sono molto “dense” sull’asse delle ascisse. \nServono in sostanza osservazioni rappresentative di tutte le \ncoppie (x, y), cosa  difﬁcile da ottenere.  \nd input (e.g., sq.ft, #bagni, #camere-letto, anno di costruzione, \necc.) → è ancora  più difﬁcile avere molte osservazioni \nrappresentative delle coppie (x, y) .\nAreaPrezzoy\nx",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#11": "Funzione di Costo \nnella Ridge Regression\n \n12\nL’idea alla base della Ridge Regression è quella di limitare \nil valore assoluto dei coefﬁcienti wi deﬁnendo come \nsegue la funzione di costo totale (da minimizzare nella \nfase di training): \ncosto_ridge  = misura del “ﬁt” + misura della grandezza dei coefﬁcienti\nPer misura del “ﬁt” intendiamo una funzione come la RSS. \nLa misura dei coefﬁcienti possiamo deﬁnirla in vari modi. ",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#12": "Misura dei Coefﬁcienti  \ndi Regressione\n \n13\nSomma dei valori:                                                  \nSomma dei valori assoluti ( L1 norm ): \nSomma dei quadrati (quadrato della L2 norm ): w0+w1+w2+···+wD\n|w0|+|w1|+|w2|+···+|wD|=DX\nj=0|wj|,kwk1\n👍\n👎\n👍\nw2\n0+w2\n1+w2\n2+···+w2\nD=DX\nj=0w2\nj,kwk2\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#13": "Funzione di Costo \nnella Ridge Regression\n \n14\nLa Ridge Regression usa la somma dei quadrati ( L2 \nRegularization ). \nLa funzione che rappresenta il costo totale nella Ridge è \ndunque la seguente:    \ndove il parametro λ (tuning parameter ) serve per \nbilanciare i due termini.                                              costo ridge(w)=R S S ( w)+\u0000·kwk2\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#14": " \n15Vediamo cosa accade a fronte di diversi valori del parametro λ: \nSe λ = 0: \nci riconduciamo alla vecchia soluzione, ossia minimizzazione \ndell’ RSS( w) → ŵLS \nSe λ → ∞: \nper soluzioni dove ŵ ≠ 0, il costo totale → ∞ \nl’unica soluzione per minimizzare il costo è: ŵ = 0 \nSe 0 < λ < ∞: \nFunzione di Costo \nnella Ridge Regression\n0kˆwk2\n2kˆwLSk2\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#15": "Bias-Variance tradeoff\n \n16\nParametro λ elevato: \nhigh bias, low variance  (e.g., ŵ = 0 per λ = ∞) \nParametro λ piccolo: \nlow bias, high variance  (e.g., standard least squares ﬁt \nper polinomi di grado elevato) ",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#16": "Esempio di polynomial ﬁt \nrivisitato\n \n17\nRivediamo ora la demo relativa al polinomio di grado 16, \napplicando però l’approccio della Ridge Regression con \ndiversi valori del parametro λ. ",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#17": "Polinomio di grado 16 \nlambda = 1.00e-25\n \n18\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#18": "Polinomio di grado 16 \nlambda = 1.00e-10\n \n19\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#19": "Polinomio di grado 16 \nlambda = 1.00e-06\n \n20\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#2": "Il problema dell’Overﬁtting \nnella polynomial regression\n \n3\nRicordiamo il nostro modello nella polynomial  \nregression:\nA seconda del grado del polinomio possiamo avere \ndiverse situazioni:\noverﬁt\nyi=w0+w1xi+w2x2\ni+···+wpxp\ni+✏i\ny\nArea xPrezzo\nAreaPrezzoy\nx",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#20": "Polinomio di grado 16 \nlambda = 1.00e-03 \n \n21\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#21": "Polinomio di grado 16 \nlambda = 1.00e+02\n \n22\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#22": " \n23costo ridge(w)=R S S ( w)+\u0000·kwk2\n2=(y\u0000\u0000w)T(y\u0000\u0000w)+\u0000·wTw\nkwk2\n2=w2\n0+w2\n1+w2\n2+···+w2\nD=wT·w\nGradiente della Funzione di Costo \nPer il calcolo del gradiente della funzione di costo, \nriscriviamo tale funzione in notazione matriciale:\npoiché:",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#23": "Gradiente della Funzione di Costo \n \n24rcosto ridge(w)= r[(y\u0000\u0000w)T(y\u0000\u0000w)+\u0000·wTw]=\n=r[(y\u0000\u0000w)T(y\u0000\u0000w)] +\u0000·r[wTw]=\n=\u00002\u0000T(y\u0000\u0000w)+\u0000·2w\nIl gradiente della funzione è il seguente:",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#24": "Algoritmi per adattare il modello \n[caso della Ridge Regression]\nAnche nel caso della Ridge Regression, una volta calcolato \nil gradiente della funzione \n costo_ridge\n  ci sono due possibili \napprocci per minimizzare la funzione di costo: \n“\nForma chiusa\n ”: Si uguaglia il gradiente a zero e si risolvono le equazioni \n(non sempre è possibile o conveniente dal punto di vista computazionale) \nAlgoritmo di Discesa del Gradiente (\n Gradient Descent\n ) (richiede la \ndeﬁnizione del criterio di convergenza e dello “step size”)\n \n25",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#25": "Forma Chiusa \n[caso della Ridge Regression] \n \n26Poniamo il gradiente uguale a zero:\nrcosto ridge( w)=\u00002\u0000T(y\u0000\u0000w)+2 ·\u0000w=0\n\u00002\u0000Ty+2\u0000T\u0000ˆw+2 ·\u0000ˆw=0\n\u0000\u0000Ty+\u0000T\u0000ˆw+\u0000Iˆ w =0\n\u0000T\u0000ˆw+\u0000Iˆ w =\u0000Ty\n(\u0000T\u0000+\u0000I)ˆw=\u0000Ty\n(\u0000T\u0000+\u0000I)\u00001(\u0000T\u0000+\u0000I)ˆw=(\u0000T\u0000+\u0000I)\u00001\u0000Ty\nIˆ w =(\u0000T\u0000+\u0000I)\u00001\u0000Ty\nˆwridge=(\u0000T\u0000+\u0000I)\u00001\u0000Tyda cui si ha:",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#26": "Gradient Descent (1/4) \n[caso della Ridge Regression] \n \n27w(t+1) w(t)\u0000↵·rcosto ridge(w(t))\nw(t+1)\n0 w(t)\n0\u0000↵·@costo ridge(w(t))\n@w0\nw(t+1)\n1 w(t)\n1\u0000↵·@costo ridge(w(t))\n@w1\n······ ·····················\nw(t+1)\nj w(t)\nj\u0000↵·@costo ridge(w(t))\n@wj\n······ ·····················\nw(t+1)\nD w(t)\nD\u0000↵·@costo ridge(w(t))\n@wD\nI singoli pesi devono pertanto essere aggiornati come segue:\nDobbiamo aggiornare il vettore dei pesi in modo tale da \nspostarci nel verso opposto al gradiente:",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#27": "Gradient Descent (2/4) \n[caso della Ridge Regression] \n \n28@costo ridge(w(t))\n@wj=@RSS(w(t))\n@wj+\u0000·@(w(t)T·w(t))\n@wjcosto ridge(w)=R S S ( w)+\u0000·kwk2\n2=R S S ( w)+\u0000·wTw\nPer il calcolo delle derivate parziali precedenti, consideriamo \ndi nuovo l’espressione della funzione costo_ridge:\nLa derivata parziale della funzione di costo rispetto al \ngenerico peso w j è dunque:",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#28": "Gradient Descent (3/4) \n[caso della Ridge Regression] \n \n29@RSS(w(t))\n@wj=\u00002NX\ni=1\u0000j(xi)[yi\u0000ˆyi(w(t))]\n@(w(t)T·w(t))\n@wj=2 ·w(t)\nj\nPoiché:\nabbiamo:",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#29": "Gradient Descent (4/4) \n[caso della Ridge Regression] \n \n30\nL’aggiornamento del generico peso w j:\ndiventa:\nossia:",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#3": "Sintomi dell’Overﬁtting\n \n4\nSpesso, quando il fenomeno dell’overﬁtting si manifesta, \naccade che i valori assoluti dei parametri stimati ŵ \nassumono valori molto alti. \nVediamo ora una semplice demo in cui si mostra questo \nfenomeno.",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#30": "Algoritmo di Gradient Descent \n[caso della Ridge Regression] \n \n31w(1)= 0 (oppure lo inizializziamo in modo casuale)\nt=1\nwhilekrcosto ridge( w(t))k2>✏\nfor j=0,1,. . . ,D\nderivata parziale RSS[ j]=\u00002NX\ni=1\u0000j(xi)[yi\u0000ˆyi(w(t))]\nw(t+1)\nj (1\u00002↵\u0000)w(t)\nj\u0000↵⇤derivata parziale RSS[ j]\nt t+1",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#31": "Andamento Coefﬁcienti \nRidge\n \n32\nλ ",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#32": " \n33\nSelezione dei parametri  \nvia Cross Validation \nUn problema importante è quello relativo alla scelta del \nparametro λ. Lo affrontiamo per la Ridge, ma le \nconsiderazioni sono più generali. \nCome è stato detto in precedenza, per ogni valore di λ che \nvogliamo considerare possiamo addestrare il nostro modello \nsui dati di training, valutarlo sul validation set e scegliere il \nvalore di λ che ottiene i migliori risultati (validation error più \nbasso). \nPossiamo poi valutare le prestazioni del modello selezionato \nsul test set.",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#33": "Tutto ciò è certamente possibile, a patto di avere un numero \nsufﬁciente di dati:\n \n34\nTraining \nSetTest \nSet\nValidation\n Set\nﬁt ŵλ \ntest prestazioni di ŵλ  \nper selezionare λ* \nvalutare il  \ntrue error di ŵλ*  \nSelezione dei parametri  \nvia Cross Validation ",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#34": "La domanda che dobbiamo porci è però la seguente: cosa \naccade se non abbiamo dati sufﬁcienti per dividerli nei tre \nsottoinsiemi necessari? \n \n35\nDati disponibili\nResto dei dati Test \nSet\nSelezione dei parametri  \nvia Cross Validation \nSupponiamo dunque di trovarci in questa situazione, in cui i \ndati disponibili sono pochi:\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#35": " \n36\nDopo aver scorporato il test set di dimensione adeguata, \nvediamo come poter gestire il resto dei dati da utilizzare  per \nil training e la validation.\nUn uso ingenuo potrebbe essere il seguente:\nSelezione dei parametri  \nvia Cross Validation \nResto dei dati \nTraining \nSet\nValidation\n Set\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#36": " \n37\nIl metodo però non funzionerebbe perché, con pochi dati a \ndisposizione, il validation set non sarebbe sufﬁcientemente \nampio per consentire una valutazione adeguata.\nQuesto varrebbe per la suddivisione mostrata nella ﬁgura \nprecedente, ma anche per altre suddivisioni, come ad es.:\nSelezione dei parametri  \nvia Cross Validation \nValidation\n Set\nValidation\n Set",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#37": " \n38\nOssia vale per qualsiasi scelta di un sottoinsieme dei dati \ndisponibili, da utilizzare come validation set.\nQuale di tali sottoinsiemi possiamo utilizzare? Sappiamo che \nciascuno di essi è troppo piccolo per le valutazioni di nostro \ninteresse.\nSelezione dei parametri  \nvia Cross Validation \nLa risposta è la seguente: li utilizziamo tutti, effettuando una \n“averge performance” su tutte le scelte. \nIn tal modo (cross validation) si evita la “sensitivity” dei \nrisultati in base al particolare sottoinsieme scelto, causata \ndelle poche osservazioni che contiene.",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#38": " \n39\nk-fold Cross Validation \nConsideriamo dunque il resto dei dati (N dati), ottenuto \nscorporando un training set di dimensione adeguata dai dati \ndisponibili.\nSuddividiamo tali N dati in K blocchi, assegnando casualmente  \ni dati a ciascuno dei blocchi:\nResto dei dati \nN/K N/K N/K ………….1 2 K ……….",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#39": " \n40\nk-fold Cross Validation \nPer ciascuno dei K blocchi, operiamo considerandolo il \nValidation Set, utilizzando quindi i dati rimanenti come \nTraining Set.\nIn sostanza, dopo aver ﬁssato un valore per λ, operiamo \ncome segue per il primo blocco (ricordiamoci che in verde \nabbiamo il Training Set):\nValidation\n Set\nˆw(1)\n\u0000error 1(\u0000)1",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#4": "Esempio di funzione\n \n5\nApplichiamo rumore gaussiano, campioniamo 30 \nosservazioni e addestriamo vari modelli:y=s i n ( 4 x)",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#40": " \n41\nk-fold Cross Validation \nPer il secondo blocco abbiamo:\nValidation\n Set\nˆw(2)\n\u0000 error 2(\u0000)2",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#41": " \n42\nk-fold Cross Validation \n… e così via ﬁno al blocco K:\nValidation\n Set\nˆw(K)\n\u0000errorK(\u0000)K",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#42": " \n43\nk-fold Cross Validation \nL’algoritmo è il seguente:\nPer ogni scelta del valore di λ:\n for k = 1, 2, …., K \n   1. stima di ŵλ sui blocchi di training \n   2. calcolo dell’errore sul “validation block”:\nCalcolo dell’Average Error: CV(\u0000)=1\nK·KX\nk=1error k(\u0000)\nScelta di λ* che minimizza l’errore CV( λ)error k(\u0000)",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#43": " \n44\nleave-one-out Cross Validation \nFormalmente, la migliore approssimazione si ha per validation \nset di dimensione 1 (K = N).\nIn tal caso si parla di leave-one-out cross validation :\n123 n\n1 2 3 N\n2 3 N\n1 2 3 N\n1 2 3 N\n……………1…\n…\n…\n…\n…1 2 3 N",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#44": "Scelta del valore K \n \n45\nIl Leave-one-out è molto pesante dal punto di vista \ncomputazionale: \nrichiede il calcolo di N “ﬁt” del modello per ogni λ. \nIn genere, tipici valori utilizzati per K sono: \nK = 5 (5-fold CV) \nK = 10 (10-fold CV)",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#45": " \n46\nLa gestione dell’intercetta \nNella discussione su Ridge e Cross Validation non abbiamo \nconsiderato il come gestire l’intercetta, che compare in tanti  \nmodelli.\nyi=w0\u00000(xi)+w1\u00001(xi)+ ···+wD\u0000D(xi)+✏i=\n=DX\nj=0wj\u0000j(xi)+✏i\nRicordiamoci innanzi tutto il modello a “multiple regression”:\nSappiamo che spesso la feature ɸ0 è posta uguale a 1. In tal \ncaso il coefﬁciente w 0 rappresenta per l’appunto l’intercetta \ndel modello.",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#46": " \n47\nLa gestione dell’intercetta \nConosciamo bene la funzione di costo per la Ridge:\nMinimizzando tale funzione, anche l’intercetta w 0 (così \ncome gli altri coefﬁcienti) tende ad assumere piccoli valori.RSS(w)+\u0000·kwk2\n2\nIn realtà ciò non sarebbe necessario. L’intercetta non è \nindicativa dell’overﬁtting.",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#47": " \n48\nLa gestione dell’intercetta \nPossiamo dunque considerare una versione modiﬁcata della \nfunzione di costo per la Ridge:\nIn tal modo, la parte relativa alla L 2-norm (penalty) non \nconsidera w 0.\nVediamo come possiamo implementare ciò nel caso in cui si \nusi l’algoritmo di Gradient Descent.RSS(w0,wresto)+\u0000·kwrestok2\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#48": " \n49\nL’algoritmo viene modiﬁcato come segue:\nAlgoritmo di Gradient Descent \n[caso della Ridge Regression senza penalizzazione dell’intercetta] \nw(1)= 0 (oppure lo inizializziamo in modo casuale)\nt=1\nwhilekrcosto ridge( w(t))k2>✏\nfor j=0,1,. . . ,D\nderivata parziale RSS[ j]=\u00002NX\ni=1\u0000j(xi)[yi\u0000ˆyi(w(t))]\nifj=0\nw(t+1)\nj w(t)\nj\u0000↵⇤derivata parziale RSS[ j]\nelse\nw(t+1)\nj (1\u00002↵\u0000)w(t)\nj\u0000↵⇤derivata parziale RSS[ j]\nt t+1",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#49": "Riferimenti\n \n50\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , 3a edizione, \nApogeo, 2015. \nMachine Learning: Regression\n , University of Washington - Coursera, 2015. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. ",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#5": "Polinomio di grado 2\n \n6\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#6": "Polinomio di grado 4\n \n7\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#7": "Polinomio di grado 16\n \n8\n",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#8": "Overﬁtting con molte feature\n \n9\nQuesto fenomeno accade non solo nella polynomial \nregression, ma anche: \n quando è elevato il numero degli input d (e.g., per il \ncaso degli appartamenti, oltre alla metratura abbiamo \nil #bagni, ecc.); \ne, più in generale, quando è elevato il numero delle \nfeature ( D elevato):\nyi=DX\nj=0wj\u0000j(xi)+✏i",
    "data_test\\rootfolder\\università\\MachineLearning\\5-Ridge Regression - Cross Validation-sbloccato.pdf#9": "Overﬁtting e #osservazioni\n \n10\nIl problema dell’overﬁtting, che in genere aumenta \nall’aumentare della complessità del modello, dipende \nanche dal numero delle osservazioni di cui disponiamo: \nPoche osservazioni (N piccolo) → è facile avere overﬁt al crescere \ndella complessità del modello. \nTante osservazioni (N molto grande)  → è più  difﬁcile avere overﬁt.Prezzo\nPrezzo\nArea Area",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nFeature Selection e LASSO\nMachine Learning \n \n1",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#1": "Sommario\nLa Selezione delle Feature nella Regression \nAlgoritmo All Subsets \nApproccio Greedy per Feature Selection (Forward Stepsize \nAlgorithm) \nAlgoritmo Coordinate Descent \nLASSO \nConfronto tra Ridge e LASSO\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#10": " \n11\nLa scelta del Modello \nCi sono varie possibilità: \nValutazione delle prestazioni sul validation set (se abbiamo dati sufﬁcienti) \nCross Validation \nAltre metriche proposte in letteratura che penalizzano la “model \ncomplexity”\nLa domanda ora è la seguente: quale conﬁgurazione di \nfeature scegliamo? \nCome sappiamo, non conviene scegliere il modello con RSS \npiù basso, che diminuisce all’aumentare della complessità del \nmodello.",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#11": "Complessità di “All Subsets”\n \n12\nLa complessità computazionale dell’algoritmo è elevata: basti \nconsiderare il numero di modelli da valutare! E’ esponenziale \nrispetto al numero delle feature:\n[000 ···0]\n[100 ···0]\n[010 ···0]\n···\n[110 ···0]\n···\n[111 ···1]yi= ✏i\nyi=w0\u00000(xi)+✏i\nyi=w1\u00001(xi)+✏i\n··· ··· ···\nyi=w0\u00000(xi)+w1\u00001(xi)+✏i\n··· ··· ···\nyi=w0\u00000(xi)+w1\u00001(xi)+ ···+wD\u0000D(xi)+✏i\n2D+1feature vector",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#12": "Algoritmi Greedy\n \n13\nUn approccio alternativo è quello di utilizzare algoritmi \ngreedy che ci consentono di ottenere soluzioni sub-ottime, \nma con complessità computazionale molto più bassa.\nL’algoritmo che ora vedremo si chiama Forward Stepsize \nAlgorithm. Esso si distingue dal precedente perché, \nall’aumentare del numero di feature, sceglie solo una nuova \nfeature conservando quelle scelte nei passi precedenti.",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#13": "Forward Stepsize Algorithm\n \n14\nPartiamo dalla ﬁgura che rappresenta la fase ﬁnale di All Subsets:\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#14": " \n15\nVediamo come il Forward Stepsize si differenzia:\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nForward Stepsize Algorithm",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#15": " \n16\nPer #features = 1 sceglie la migliore:\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nForward Stepsize Algorithm",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#16": " \n17\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nForward Stepsize Algorithm\nderiva dal passo  \nprecedente\nPer #features = 2 aggiunge alla 1\n a\n già scelta la 2\n a\n migliore:",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#17": " \n18\nPer #features = 2 aggiunge alla 1\n a\n già scelta la 2\n a\n migliore:\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nForward Stepsize Algorithm\nseconda feature \nselezionata",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#18": " \n19\nE’ evidente la differenza rispetto all’algoritmo All Subset:\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nForward Stepsize Algorithm\nfeature selezionate da \n“all subset algorithm”",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#19": " \n20\n… e così via …\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nForward Stepsize Algorithm",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#2": "Selezione delle Feature\n \n3\nLa selezione delle feature nella regression è una fase molto \nimportante per due motivi: \n1. Efﬁcienza di elaborazione : \n•Se la dimensione di w è elevata (e.g., 100B) la predizione è \nmolto pesante computazionalmente. \n•Del resto, se ŵ è sparso, il calcolo dipende solo dai valori \nnon nulli. \n2. Interpretabilità : \n• Quali feature sono rilevanti per la nostra  predizione? ",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#20": " \n21\n… ﬁno al caso che considera tutte le feature:\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nForward Stepsize Algorithm",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#21": " \n22\nConsideriamo il dizionario delle feature {\n ɸ\n0\n, \nɸ\n1\n, … , \nɸ\nD\n} \nImpostiamo l’insieme delle feature F come segue: F\n 0\n = \n∅\n   (\no impostiamo \n ɸ\n0\n a 1\n); \naddestriamo e calcoliamo l’errore. \nt = 0 \n•\nPer ogni j (≠ dalle feature correnti), addestriamo il modello usando le \nfeature correnti F\n t\n + {\nɸ\nj\n(x)} per ottenere il vettore dei pesi \n ŵ\n per j. \n•\nSelezioniamo la best feature \n ɸ\nj*\n(x) (e.g., quella che dà luogo al training \nerror più basso quando addestriamo con F\n t\n + {\nɸ\nj*\n(x)}) \n•\nSet  F\n t+1\n <— F\n t \n+ {\nɸ\nj*\n(x)};  \n•\nt = t + 1 \n•\nRipetere il ciclo \nForward Stepsize Algorithm\nL’algoritmo in sintesi è il seguente:",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#22": " \n23\nLa complessità computazionale di questo algoritmo è \nsensibilmente minore di quella dell’All Stepsize Algorithm. \nInfatti, il numero di modelli valutati (con D feature) è il \nseguente: \n1° step: D modelli \n2° step: D-1 modelli (si aggiunge 1 feature tra le D-1 possibili) \n3° step: D-2 modelli (si aggiunge 1 feature tra le D-2 possibili) \necc.\nForward Stepsize Algorithm\nIl numero massimo di step è uguale a D. Abbiamo dunque: \nO(D2)",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#23": " \n24\nQuesto metodo, proposto da Robert Tibshirani nel 1996, consente \ndi effettuare una feature selection, oltre a limitare i valori assoluti \ndei coefﬁcienti w. \nLasso Regression usa la somma dei valori assoluti dei pesi ( L1 \nRegularization ). \nLa funzione che rappresenta il costo totale nel Lasso è dunque la \nseguente:    \ndove il parametro λ (tuning parameter ) serve per bilanciare i due \ntermini.                                              costo lasso(w)=R S S ( w)+\u0000·kwk1\nLASSO \n(\nLeast Absolute Shrinkage and Selection Operator\n )",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#24": " \n25Vediamo cosa accade a fronte di diversi valori del parametro λ: \nSe λ = 0: \nci riconduciamo alla vecchia soluzione, ossia minimizzazione \ndell’ RSS( w) → ŵLS \nSe λ → ∞: \nper soluzioni dove ŵ ≠ 0, il costo totale → ∞ \nl’unica soluzione per minimizzare il costo è: ŵlasso = 0 \nSe 0 < λ < ∞: \nSoluzioni Lasso \nper diversi valori \n λ\n0kˆwlassok1kˆwLSk1",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#25": "Andamento Coefﬁcienti \nRidge e Lasso\n \n26\nRidge Lasso\nλ λ ",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#26": "Visualizzazione costo Ridge\n \n27\nellissicosto ridge(w)=R S S ( w)+\u0000·kwk2\n2\nRSS(w)=NX\ni=1[yi\u0000w0\u00000(xi)\u0000w1\u00001(xi)]2",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#27": " \n28\ncirconferenzecosto ridge(w)=R S S ( w)+\u0000·kwk2\n2\nkwk2\n2=w2\n0+w2\n1\nVisualizzazione costo Ridge",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#28": "Simulazione Ridge\n \n29\nλ = 0\nλ→∞ˆwridge\nˆwridgeˆwridge\nˆwridge ˆwridge ˆwridge",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#29": " \n30\ncosto lasso(w)=R S S ( w)+\u0000·kwk1\nRSS(w)=NX\ni=1[yi\u0000w0\u00000(xi)\u0000w1\u00001(xi)]2ellissi\nVisualizzazione costo Lasso",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#3": "Selezione delle Feature\n \n4\nUn approccio che possiamo adottare per selezionare le \nmigliori feature consiste nel considerare ogni possibile \ncombinazione delle feature che abbiamo disponibili, \nveriﬁcando le prestazioni di ciascuna scelta. \nQuesto è esattamente ciò che fa l’ All Subset Algorithm  che \nora vediamo. \nEsso comincia considerando 0 feature, poi tutte le \npossibilità per 1 feature, poi tutte le possibilità per 2 \nfeature, ecc., scegliendo ogni volta la migliore \ncombinazione.",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#30": " \n31\nkwk1= |w0|+|w1|costo lasso(w)=R S S ( w)+\u0000·kwk1\ndiamonds \nVisualizzazione costo Lasso",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#31": "Simulazione Lasso\n \n32\nλ→∞λ = 0\nˆwlasso\nˆwlassoˆwlasso\nˆwlasso ˆwlasso ˆwlasso",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#32": "Confronto tra Ridge e Lasso\n \n33\nPer ogni valore di \n λ\n, per la Ridge esiste un certo valore \n s\n tale \nche le seguenti due equazioni forniscono le stesse stime dei \ncoefﬁcienti w\n ridge\n:\nargmin\nwRSS(w)\nsotto la condizione:DX\nj=0w2\njsargmin\nw[RSS(w)+\u0000·kwk2\n2]",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#33": "Confronto tra Ridge e Lasso\n \n34w1\nw0\nw2\n0+w2\n1s\nargmin\nwRSS(w)\nsotto la condizione:DX\nj=0w2\njs\nConsideriamo per semplicità il caso a 2 dimensioni. L’equazione: \nindica che i coefﬁcienti ŵridge stimati sono quelli che hanno il più \npiccolo RSS tra i punti appartenenti al cerchio deﬁnito da: w2\n0+w2\n1s",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#34": "Confronto tra Ridge e Lasso\n \n35\nAnalogamente, per ogni valore di \n λ\n, per il Lasso esiste un \ncerto valore \n s\n tale che le seguenti due equazioni forniscono \nle stesse stime dei coefﬁcienti w\n lasso\n:\nargmin\nwRSS(w)\nsotto la condizione:DX\nj=0|wj|sargmin\nw[RSS(w)+\u0000·kwk1]",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#35": "Confronto tra Ridge e Lasso\n \n36|w0|+|w1|sw0w1\n|w0|+|w1|s\nAnalogamente, la seguente equazione:  \nindica che i coefﬁcienti ŵlasso stimati sono quelli che hanno il più \npiccolo RSS tra i punti appartenenti al “diamante”: argmin\nwRSS(w)\nsotto la condizione:DX\nj=0|wj|s",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#36": "Confronto tra Ridge e Lasso\n \n37\nLa ﬁgura seguente ci mostra i punti di minimo per i due casi, e il \nperché Lasso spesso consente di eliminare alcune feature. In rosso \nsono indicate le curve di livello per RSS.\n|w0|+|w1|s w2\n0+w2\n1s\nw0w1\nw0w1\nˆwLSˆwLS\nˆwlasso ˆwridge",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#37": "Confronto tra Ridge e Lasso\n \n38\nSe \ns\n è sufﬁcientemente grande, le regioni in verde conterranno la \nsoluzione Least Squares. Pertanto le stime della Ridge regression e \ndel Lasso saranno le stesse della Least Squares (tale grande valore per \ns\n corrisponde a \n λ\n = 0) \nSe invece la soluzione Least Squares sta al di fuori delle regioni in \nverde, essa non può essere la soluzione perché non rispetta i vincoli \ncitati in precedenza. \nI punti di minimo sono dunque quelli che corrispondono alla curva \ndi livello più “stretta” che passa per l’area in verde (ricordiamoci che \nle curve di livello per RSS corrispondono a valori sempre più alti \nmano a mano che si “allargano” rispetto alla soluzione LS)",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#38": "Confronto tra Ridge e Lasso\n \n39\nNella ﬁgura abbiamo considerato il caso di 2 dimensioni per il \nvettore \n w\n. \nNel caso di 3 dimensioni la “constraint region” in verde diventa una \nsfera per la Ridge e un poliedro per il Lasso. \nNel caso di dimensione > 3, essa diventa una ipersfera per la Ridge e \nun politopo per il Lasso (politopo è un termine coniato da Alicia \nBoole, ﬁglia di George Boole)",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#39": "Minimizzazione della  \nfunzione di costo Lasso\nIn precedenza abbiamo visto come poter minimizzare la \nfunzione di costo (per LS e per Ridge) mediante: \nLa forma chiusa (uguagliando a zero il gradiente della \nfunzione) \nGradient Descent \nPer il Lasso ci sono delle difﬁcoltà per il calcolo del gradiente.\n \n40",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#4": "Ricerca delle migliori feature \nSize: 0\n \n5# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\n",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#40": "Minimizzazione della  \nfunzione di costo Lasso\nLa funzione da minimizzare è la seguente: \n \n41\nwj\n|wj|\nderivata = +1 derivata = -1\nnon derivabile\ncosto lasso(w)=R S S ( w)+\u0000·kwk1\ncome calcolare \nla derivata?",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#41": "Minimizzazione della  \nfunzione di costo\nNon possiamo quindi calcolare il gradiente della funzione. \nSuccessivamente vedremo come utilizzare il concetto di \nsubgradient\n  per superare la difﬁcoltà appena vista. \nOra cogliamo l’occasione per vedere un nuovo algoritmo per \nminimizzare una funzione di costo, chiamato \n Coordinate \nDescent\n . \nPresenteremo l’algoritmo in generale, per poi vedere come esso \npossa essere usato convenientemente per minimizzare la \nfunzione di costo per il Lasso.\n \n42",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#42": "Il nostro scopo è quello di minimizzare una certa funzione:\nAlgoritmo Coordinate Descent\ng(w)=g(w0,w1,···,wD)\nLa caratteristica distintiva del Coordinate Descent è che la \nminimizzazione avviene lungo una singola dimensione per \nvolta, come illustrato qui di seguito nel semplice caso di \nfunzione di due variabili w\n 0\n e w\n 1\n. \n \n43",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#43": "Algoritmo Coordinate Descent\nCurve di livello di una funzione g(\n w\n) da minimizzare:\n \n44\nw0w1\nscegliamo il \npunto inizialevalori di g( w) maggiori per  \ncurve di livello più esterne ",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#44": "Facciamo variare una sola coordinata:\n \n45\nw0w1\nw0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#45": "Troviamo il minimo e spostiamoci su tale punto (axis-alined move):\n \n46\nw0w1\nw0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#46": "Facciamo variare una sola altra coordinata:\n \n47\nw0w1\nw0\n1\nw0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#47": "Troviamo il minimo e passiamo su tale punto:\n \n48\nw0w1\nw0\n1\nw0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#48": "……. e così via …….\n \n49\nw0w1\nw0\n1\nw00\n0w0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#49": "………\n \n50\nw0w1\nw0\n1\nw00\n0w0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#5": " \n6# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nRicerca delle migliori feature \nSize: 1",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#50": "………\n \n51\nw0w1\nw0\n1w00\n1\nw00\n0w0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#51": "………\n \n52\nw0w1\nw0\n1w00\n1\nw00\n0w0\n0\nAlgoritmo Coordinate Descent",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#52": "…. ﬁno ad arrivare al minimo globale:\n \n53\nw0w1\nw0\n1w00\n1\nw00\n0w0\n0\n……\nAlgoritmo Coordinate Descent\nper problemi convessi, \nstep sempre più piccoli \nman mano che ci  \navviciniamo alla soluzione",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#53": "Algoritmo Coordinate Descent\n \n54\nInizializza ŵ = 0 (o in modo “smart”) \nwhile  not converged: \n   scegli una coordinata j\nsi minimizza solo sulla j-esima coordinataˆwj argmin\n!g(ˆw0,ˆw1,···,ˆwj\u00001,!,ˆwj+1,···,ˆwD)",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#54": "Algoritmo Coordinate Descent\n \n55\nCome scegliere nell’algoritmo la coordinata successiva? \nIn modo casuale (“random” o “stochastic\" coordinate descent) \nIn modo “round robin” \necc. \nSi noti che in questo algoritmo non è necessario scegliere lo step size! \nTale approccio è utilissimo per numerosi problemi \nConverge ad un ottimo in vari altri casi (e.g., funzione “strongly \nconvex”) \nConverge per la funzione obiettivo Lasso ",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#55": "Normalizzazione delle Feature\n \n56\nL’applicazione dell’algoritmo Coordinate Descent per il Lasso è \nsempliﬁcata se operiamo una normalizzazione delle feature. \nPer far questo dobbiamo prendere in considerazione la matrice \ndelle feature   usata in precedenza, nella quale ogni colonna \ncorrisponde ad una feature (0, 1, … D) applicata ai vari ingressi xi \ndei training data. \u0000",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#56": "Normalizzazione Feature\n \n57\u0000=2\n664\u00000(x1)\u00001(x1) ... \u0000j(x1) ... \u0000D(x1)\n\u00000(x2)\u00001(x2) ... \u0000j(x2) ... \u0000D(x2)\n... ... ... ... ... ...\n\u00000(xN)\u00001(xN) ... \u0000j(xN) ... \u0000D(xN)3\n775\n\u0000j(xk)=\u0000j(xk)qPN\ni=1\u00002\nj(xi)feature generica normalizzata:",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#57": "Normalizzazione delle Feature\n \n58Zj=vuutNX\ni=1\u00002\nj(xi)\nPer normalizzare abbiamo usato a denominatore il seguente \n“normalizer”: \nRicordiamoci che per i test data dovremo applicare lo stesso \nnormalizer.  ",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#58": "Coordinate Descent per \nUnregularized Regression\n \n59\nVediamo ora come sia possibile applicare l’algoritmo Coordinate \nDescent nel caso di regressione senza regularization, ossia nel \ncaso Least Squares. \nIl passo successivo sarà la sua applicazione al Lasso. \nPer l’applicazione al caso Least Squares (con feature \nnormalizzate) dobbiamo calcolare le derivare parziali della \nfunzione di costo RSS (necessarie per minimizzare sulla singola \ncoordinata): \nRSS(w)=NX\ni=1[yi\u0000DX\nj=0wj\u0000j(xi)]2",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#59": "Derivate di RSS \ncon feature normalizzate\n \n60@RSS(w)\n@wj=\u00002NX\ni=1\u0000j(xi)[yi\u0000ˆyi(w)] =\u00002NX\ni=1\u0000j(xi)[yi\u0000DX\nj=0wj\u0000j(xi)] =\n=\u00002NX\ni=1\u0000j(xi)[yi\u0000X\nk 6=jwk\u0000k(xi)\u0000wj\u0000j(xi)] =\n=\u00002⇢jz }| {\nNX\ni=1\u0000j(xi)[yi\u0000X\nk 6=jwk\u0000k(xi)]\n| {z }\nprediz. senza \u0000j+2wj,1z}|{\nNX\ni=1\u00002\nj(xi)=\n=\u00002⇢j+2wj",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#6": " \n7# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nmigliore feature\nRicerca delle migliori feature \nSize: 1",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#60": "Ricerca del minimo per una \ncoordinata\n \n61⇢j=NX\ni=1\u0000j(xi)[yi\u0000X\nk6=jwk\u0000k(xi)] =NX\ni=1\u0000j(xi)[yi\u0000ˆyi(ˆw\u0000j)]@RSS(w)\n@wj=\u00002⇢j+2wj=0\nˆwj=⇢j\ndove:",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#61": "Algoritmo Coordinate Descent \nper Least Squares\n \n62set:calcola:\nInizializza ŵ = 0 (o in altro modo) \nwhile  not converged: \n   for j = 0, 1, …, D:\n⇢j=NX\ni=1\u0000j(xi)[z}| {\nyi\u0000ˆyi(ˆw\u0000j)|{z}]residual senza la feature j\npredizione senza \nla feature j\nˆwj=⇢j",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#62": " \n63ˆwj=8\n<\n:⇢j+\u0000\n2se⇢j<\u0000\u0000\n2\n0 se ⇢j2[\u0000\u0000\n2,\u0000\n2]\n⇢j\u0000\u0000\n2se⇢j>\u0000\n2\nAlgoritmo Coordinate Descent \nper Lasso\nVediamo ora la versione dell’algoritmo per il caso del Lasso. \nIn questo caso l’impostazione per il peso \n ŵ\nj\n dipende dal valore \nassunto dal parametro \n λ\n: \nUna dimostrazione formale è mostrata nella prossima lezione. ",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#63": "Coefﬁcienti per LS, Ridge e Lasso\n \n64ˆwj=8\n<\n:⇢j+\u0000\n2se⇢j<\u0000\u0000\n2\n0 se ⇢j2[\u0000\u0000\n2,\u0000\n2]\n⇢j\u0000\u0000\n2se⇢j>\u0000\n2\nsoft thresholding\n+\u0000\n2\u0000\u0000\n2⇢i 0 0 ⇢i0 0ˆwj ˆwj",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#64": "Algoritmo Coordinate Descent \nper Lasso  \n[versione con feature normalizzate]\n \n65ˆwj=8\n<\n:⇢j+\u0000\n2se⇢j<\u0000\u0000\n2\n0 se ⇢j2[\u0000\u0000\n2,\u0000\n2]\n⇢j\u0000\u0000\n2se⇢j>\u0000\n2set:calcola:\nInizializza ŵ = 0 (o in altro modo) \nwhile  not converged: \n   for j = 0, 1, …, D:\n⇢j=NX\ni=1\u0000j(xi)[yi\u0000ˆyi(ˆw\u0000j)]",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#65": "Algoritmo Coordinate Descent \nper Lasso  \n[versione con feature non normalizzate]\n \n66\ncalcola:  \nInizializza ŵ = 0 (o in altro modo) \nwhile  not converged: \n   for j = 0, 1, …, D:\nset:calcola:\nˆwj=8\n><\n>:⇢j+\u0000\n2\nzjse⇢j<\u0000\u0000\n2\n0 se ⇢j2[\u0000\u0000\n2,\u0000\n2]\n⇢j\u0000\u0000\n2\nzjse⇢j>\u0000\n2zj=NX\ni=1\u0000j(xi)2\n⇢j=NX\ni=1\u0000j(xi)[yi\u0000ˆyi(ˆw\u0000j)]",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#66": " \n67\nUn aspetto importante è il criterio di convergenza. \nPer problemi convessi (in particolare strongly convex) gli step \nsono sempre più piccoli mano a mano che ci si avvicina al \npunto di ottimo:\nAlgoritmo Coordinate Descent \nper Lasso\nUn criterio che possiamo utilizzare per la convergenza è quello \ndi considerare una misura degli step fatti in un ciclo completo su \ntutte le feature e fermarsi quando: \n max step < \n ε",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#67": "Scelta del parametro \n λ\nper approfondimenti: \nMurphy, K. “\n Machine Learning: A Probabilistic Perspective\n ”. The MIT Press, \n2012.\n \n68\nIl parametro \n λ\n può essere scelto avvalendosi dell’approccio che \nusa il validation set, a patto di avere un numero sufﬁciente di \nosservazioni. \nAltrimenti possiamo usare la k-fold cross validation. \nQuest’ultima tende a scegliere il parametro che ottiene la \nmigliore “predictive accuracy”. Essa tende a favorire soluzioni \nmeno “sparse”, ossia con piccoli valori di \n λ\n, anziché soluzioni \ncon maggiore feature selection.",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#68": "Riferimenti\n \n69\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , Apogeo, 2015. \nMachine Learning: Regression\n , University of Washington - Coursera, 2015. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. \nMurphy, K.P. \n Machine Learning - A Probabilistic Approach\n , The MIT Press, 2012.",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#7": " \n8# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nmigliori 2 feature\nRicerca delle migliori feature \nSize: 2",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#8": " \n9\nRicerca delle migliori feature \nSize: 8\n# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront",
    "data_test\\rootfolder\\università\\MachineLearning\\6-Regression - FS  Lasso-sbloccato.pdf#9": " \n10# di featuresRSS(ŵ)\n0 1 2 3 4 5 6 7 8 •# bedrooms \n•# bathrooms \n•sq.ft. living \n•sq.ft. lot \n•ﬂoors \n•year built \n•year renovated \n•waterfront\nRicerca delle migliori feature  \nandamento in base al numero di feature",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nDimostrazioni Formali Lasso\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#1": "Sommario\n \n2Dimostrazione della formula di aggiornamento dei \ncoefﬁcienti nell’algoritmo coordinate descent per \nLASSO\n",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#10": "Differential set della  \nfunzione di costo Lasso\n \n11\nIl differential set rispetto al generico peso w\n j\n è pertanto il seguente:\n@wj[costo lasso] = 2 zjwj\u00002⇢j+\u0000·@wj|wj|da RSS da L 1 penalty\n@wj[costo lasso] = 2 zjwj\u00002⇢j+8\n<\n:\u0000\u0000 sewj<0\n[\u0000\u0000,\u0000]s e wj=0\n\u0000 sewj>0\n",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#11": "Differential Set della  \nfunzione di costo Lasso  \n \n12\nAbbiamo pertanto la seguente espressione ﬁnale:\n@wj[costo lasso] =8\n<\n:2zjwj\u00002⇢j\u0000\u0000 sewj<0\n[\u00002⇢j\u0000\u0000,\u00002⇢j+\u0000]s e wj=0\n2zjwj\u00002⇢j+\u0000 sewj>0",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#12": "Soluzione ottima \n \n13\nSe uguagliamo a zero la precedente espressione, abbiamo tre casi:\ncaso 1 (\n w\nj  \n< 0\n):2zjˆwj\u00002⇢j\u0000\u0000=0\nˆwj=2⇢j+\u0000\n2zj=⇢j+\u0000\n2\nzj\nˆwj=⇢j+\u0000\n2\nzj<0 ⇢j+\u0000\n2<0 ⇢j<\u0000\u0000\n2\nda cui otteniamo:\nPoiché \n ŵ\nj  \n< 0, abbiamo:",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#13": "Soluzione ottima \n \n14\nAbbiamo dunque:\nIn deﬁnitiva:\ncaso 2 ( wj  = 0): l’intervallo                                            deve contenere 0 [\u00002⇢j\u0000\u0000,\u00002⇢j+\u0000]\n\u0000\u0000\n2⇢j\u0000\n2\u00002⇢j+\u0000\u00000\u00002⇢j\u0000\u00000\n⇢j\u0000\n2⇢j\u0000\u0000\u0000\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#14": "Soluzione ottima \n \n15\ncaso 3 (\n w\nj  \n> 0\n):\nda cui otteniamo:\nPoiché \n ŵ\nj  \n> 0, abbiamo:2zjˆwj\u00002⇢j+\u0000=0\nˆwj=2⇢j\u0000\u0000\n2zj=⇢j\u0000\u0000\n2\nzj\nˆwj=⇢j\u0000\u0000\n2\nzj>0 ⇢j\u0000\u0000\n2>0 ⇢j>\u0000\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#15": " \n16\nIn conclusione:\nˆwj=8\n><\n>:⇢j+\u0000\n2\nzjse⇢j<\u0000\u0000\n2\n0 se ⇢j2[\u0000\u0000\n2,\u0000\n2]\n⇢j\u0000\u0000\n2\nzjse⇢j>\u0000\n2@wj[costo lasso] =8\n<\n:2zjwj\u00002⇢j\u0000\u0000 sewj<0\n[\u00002⇢j\u0000\u0000,\u00002⇢j+\u0000]s e wj=0\n2zjwj\u00002⇢j+\u0000 sewj>0\nSoluzione ottima ",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#16": "Algoritmo Coordinate Descent \nper Lasso  \n[versione con feature non normalizzate]\n \n17\ncalcola:  \nInizializza ŵ = 0 (o in altro modo) \nwhile  not converged: \n   for j = 0, 1, …, D:\nset:calcola:\nˆwj=8\n><\n>:⇢j+\u0000\n2\nzjse⇢j<\u0000\u0000\n2\n0 se ⇢j2[\u0000\u0000\n2,\u0000\n2]\n⇢j\u0000\u0000\n2\nzjse⇢j>\u0000\n2zj=NX\ni=1\u0000j(xi)2\n⇢j=NX\ni=1\u0000j(xi)[yi\u0000ˆyi(ˆw\u0000j)]",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#17": "Coefﬁcienti per LS, Ridge e Lasso\n \n18\nsoft thresholding\n+\u0000\n2\u0000\u0000\n2⇢i 0 0 ⇢i0 0ˆwj ˆwjˆwj=8\n><\n>:⇢j+\u0000\n2\nzjse⇢j<\u0000\u0000\n2\n0 se ⇢j2[\u0000\u0000\n2,\u0000\n2]\n⇢j\u0000\u0000\n2\nzjse⇢j>\u0000\n2",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#18": "Riferimenti\n \n19\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , Apogeo, 2015. \nMachine Learning: Regression\n , University of Washington - Coursera, 2015. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. \nMurphy, K.P. \n Machine Learning - A Probabilistic Approach\n , The MIT Press, 2012.",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#2": "Ottimizzazione Lasso\n \n3\nCome sappiamo, la Funzione Obiettivo per il Lasso da ottimizzare \nmediante Coordinate Descent è la seguente:\nRSS(w)+\u0000·kwk1=NX\ni=1[yi\u0000DX\nj=0wj\u0000j(xi)]2+\u0000DX\nj=0|wj|\nVediamo come calcolare le derivate parziali dei due termini \npresenti nell’espressione rispetto ai pesi w j.",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#3": "Derivazione del termine RSS\n \n4RSS(w)+\u0000·kwk1=NX\ni=1[yi\u0000DX\nj=0wj\u0000j(xi)]2+\u0000DX\nj=0|wj|\n@RSS(w)\n@wj=\u00002NX\ni=1\u0000j(xi)[yi\u0000ˆyi(w)] =\u00002NX\ni=1\u0000j(xi)[yi\u0000DX\nj=0wj\u0000j(xi)] =\n=\u00002NX\ni=1\u0000j(xi)[yi\u0000X\nk 6=jwk\u0000k(xi)\u0000wj\u0000j(xi)] =\n=\u00002⇢jz }| {\nNX\ni=1\u0000j(xi)[yi\u0000X\nk 6=jwk\u0000k(xi)]\n| {z }\nprediz. senza \u0000j+2wj,zjz}|{\nNX\ni=1\u00002\nj(xi)=\n=\u00002⇢j+2wjzj",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#4": " \n5\nIn questo caso c’è il problema del calcolo della derivata parziale:\nRSS(w)+\u0000·kwk1=NX\ni=1[yi\u0000DX\nj=0wj\u0000j(xi)]2+\u0000DX\nj=0|wj|\n\u0000·@|wj|\n@wj=?\nDerivazione del termine L\n 1\n penalty\nderivata = +1 derivata = -1\nnon derivabilewj|wj|\n",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#5": "Subgradiente di Funzioni Convesse \n 6\nI metodi che conosciamo (e.g., Gradient Descent, Coordinate \nDescent) richiedono che la funzione da ottimizzare sia \ndifferenziabile. \nE’ possibile però generalizzare la discussione andando al di là \ndelle funzioni differenziabili. \nE’ possibile ad esempio mostrare come i precedenti algoritmi \npossano essere applicati anche per funzioni non differenziabili, \nutilizzando il subgradiente anziché il gradiente.",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#6": "Subgradiente di Funzioni Convesse\n \n7Un vettore S che soddisfa la: \nè detto subgradiente di g in v.g(w)\u0000g(v)+ST(w\u0000v)\n.wg(w)\nvS1pendenza \nS2 pendenza \n. \n. \n.g:R!R\nL’insieme dei subgradienti di g in v è chiamato “differential set” e \nindicato: @g(v)",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#7": "Subgradiente della funzione  \nValore Assoluto\n \n8\nNel punto non derivabile della funzione “valore assoluto” i \nsubgradienti variano da -1 a +1:\nderivata = +1 derivata = -1\nwj|wj|",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#8": "Subgradiente della funzione  \nValore Assoluto\n \n9\nIl “differential set” è dunque il seguente per i vari punti:\n@wj|wj|=8\n<\n:{\u00001} sewj<0\n[\u00001,1] se wj=0\n{1} sewj>0",
    "data_test\\rootfolder\\università\\MachineLearning\\7-Dimostrazioni-Lasso-sbloccato.pdf#9": "Subgradiente di L\n 1\n term\n \n10\nNel caso del Lasso abbiamo:\n\u0000·@wj|wj|=8\n<\n:\u0000\u0000 sewj<0\n[\u0000\u0000,\u0000]s e wj=0\n\u0000 sewj>0",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nRegressione:  \nFonti di Errore \nExpected Prediction Error\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#1": "Sommario\nLe tre fonti di errore: Noise, Bias, Variance \nDeﬁnizione e derivazione formale delle tre fonti di \nerrore \nExpected Prediction Error \n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#10": "Noise: \nVarianza dell’Errore del modello\n \n11\nEPE( xt)=\u00002+ [bias( fˆw(xt))]2+ var[ fˆw(xt)]\ny=fw(true)(x)+ ✏\n\u00002= varianza di ✏",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#11": "Bias della funzione stimata \n \n12bias( fˆw(xt)) = fw(true) (xt)\u0000f¯w(xt)EPE( xt)=\u00002+ [bias( fˆw(xt))]2+ var[ fˆw(xt)]\nx\nt(true) (true)\n(true)average \nestimated \nfunction:\nf¯w(xt),Etrain [fˆw(xt)]",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#12": "Varianza della funzione stimata \n \n13EPE( xt)=\u00002+ [bias( fˆw(xt))]2+ var[ fˆw(xt)]\nvar(fˆw(xt)) =Etrain [(fˆw(xt)\u0000f¯w(xt))2]",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#13": "Dimostrazione per  \nl’Expected Prediction Error (EPE)\nVediamo ora come dimostrare la formula dell’EPE, in cui \nentrano in gioco i tre termini che abbiamo deﬁniti \nformalmente in precedenza:\n \n14EPE( xt)=\u00002+ [bias( fˆw(xt))]2+ var[ fˆw(xt)]",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#14": "Dimostrazione per  \nl’Expected Prediction Error (EPE)\nA tal ﬁne, ricordiamo innanzi tutto la deﬁnizione di tale \nerrore:\n \n15EPE = Etrain[Generalization Error per ˆw(train)] =\n=Etrain[Ex,y[L(y,f ˆw(train) (x))]]\n1.\n Consideriamo: \n2.\n Riferiamoci ad uno speciﬁco \n x\ntL[y, f ˆw(x)] = ( y\u0000ˆy)2=[y\u0000fˆw(x)]2\nFacciamo poi le seguenti due assunzioni (già citate prima):",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#15": "Dimostrazione per  \nl’Expected Prediction Error (EPE)\nCon le due assunzioni precedenti l’espressione per l’EPE \nsi sempliﬁca come segue:\n \n16EPE( xt)=Etrain ,yt[(yt\u0000ˆyt)2]=Etrain ,yt[(yt\u0000fˆw(train) (xt))2]\ndove non abbiamo più l’Expectation su \n x\n, avendo ﬁssato \nuno speciﬁco \n x\nt\n, e dove l’Expectation su y è diventata \nl’Expectation su y\n t\n perché dobbiamo considerare solo le \nosservazioni che abbiamo a fronte dell’input \n x\nt\n.",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#16": "Expected Prediction Error\nPartendo da tale espressione, vediamo come dimostrare la \nformula dell’EPE:\n \n17EPE( xt)= Etrain ,yt[(yt\u0000ˆyt)2]=Etrain ,yt[(yt\u0000fˆw(train) (xt))2]=\n=Etrain ,yt[(yt\u0000fz }| {\nfw(true) (xt)+fz }| {\nfw(true) (xt)\u0000ˆfz }| {\nfˆw(train) (xt))2]=\n=Etrain ,yt[((yt\u0000f)+(f\u0000ˆf))2]=\n=Etrain ,yt[(yt\u0000f)2+ 2(yt\u0000f)·(f\u0000ˆf)+(f\u0000ˆf)2]=\n=Etrain ,yt[(yt\u0000f)2]+2 ·Etrain ,yt[(yt\u0000f)·(f\u0000ˆf)] +Etrain ,yt[(f\u0000ˆf)2]",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#17": "Expected Prediction Error\n1° termine: Si sempliﬁca come segue, poiché  y\n t\n e f non \ndipendono dal training set:\n \n18\nSi noti che l’Expectation del quadrato dell’errore \n ε\n è la \nvarianza di \n ε\n, avendo esso media nulla.Etrain ,yt[(yt\u0000f)2]=Eyt[(yt\u0000f)2]=Eyt[✏2],\u00002",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#18": "Expected Prediction Error\n2° termine:\n \n192·Etrain ,yt[(yt\u0000f)·(f\u0000ˆf)] = 2 ·Etrain ,yt[(✏)·(f\u0000ˆf)] =\n=2 ·Etrain ,yt[✏]·Etrain ,yt[(f\u0000ˆf)] =\n=2 ·0·Etrain ,yt[(f\u0000ˆf)] = 0\nSi noti che ε è indipendente da  e da , e quindi è \nindipendente da           .  \nSi ricordi, inoltre, che l’Expectation dell’errore ε è uguale \na zero.ˆf f\n(f\u0000ˆf)",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#19": "Expected Prediction Error\n3° termine:\n \n20\nL’espressione per l’errore EPE può essere dunque scritta \ncosì:\nAbbiamo: Etrain ,yt[(f\u0000ˆf)2]=Etrain [(f\u0000ˆf)2]\nEPE( xt)=\u00002+M S E ( ˆf)MSE( ˆf),Etrain[(f\u0000ˆf)2]\nConsideriamo ora il Means Squared Error (MSE), deﬁnito \ncome segue:",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#2": "Deﬁnizione e derivazione \nformale\nOccupiamoci della deﬁnizione e derivazione formale delle \ntre sorgenti di errore: \nnoise \nbias \nvariance \nA tal ﬁne introduciamo innanzi tutto l’Expected Prediction \nError, le cui componenti sono le suddette sorgenti di errore.\n \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#20": "Means Squared Error\nDobbiamo ora dimostrare che:\n \n21MSE[ fˆw(train) (xt)] = Etrain[(fw(true) (xt)\u0000fˆw(train) (xt))2]=\n=Etrain[(fz }| {\nfw(true) (xt)\u0000¯fz}|{\nf¯w(xt)+¯fz}|{\nf¯w(xt)\u0000ˆfz }| {\nfˆw(train) (xt))2]=\n=Etrain[((f\u0000¯f)+( ¯f\u0000ˆf))2]=\n=Etrain[(f\u0000¯f)2+ 2(f\u0000¯f)·(¯f\u0000ˆf)+( ¯f\u0000ˆf)2]=\n=Etrain[(f\u0000¯f)2]+2 ·Etrain[(f\u0000¯f)·(¯f\u0000ˆf)] +Etrain[(¯f\u0000ˆf)2]MSE( ˆf) = [bias( ˆf)]2+ var( ˆf)",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#21": "1° termine: ricordiamo che\n \n22\nNe consegue che:\npoiché   e    non dipendono dal training set. f¯f\nMeans Squared Error\n¯f,Etrain [ˆf]\nEtrain [(f\u0000¯f)2]=(f\u0000¯f)2,[bias( ˆf)]2",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#22": "2° termine:\n \n232·Etrain [(f\u0000¯f)·(¯f\u0000ˆf)] = 2 ·(f\u0000¯f)·Etrain [(¯f\u0000ˆf)] =\n=2 ·(f\u0000¯f)·(¯f\u0000Etrain [ˆf]) =\n=2 ·(f\u0000¯f)·(¯f\u0000¯f)=\n=2 ·(f\u0000¯f)·0=0\nMeans Squared Error",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#23": "Means Squared Error\n3° termine:\n \n24Etrain [(¯f\u0000ˆf)2]=Etrain [(ˆf\u0000¯f)2],var( ˆf)",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#24": "E’ in tal modo dimostrato che:\n \n25MSE( ˆf) = [bias( ˆf)]2+ var( ˆf)\nMeans Squared Error",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#25": "In conclusione:\n \n26EPE( xt)=\u00002+M S E [ fˆw(xt)] =\u00002+ [bias( fˆw(xt))]2+ var[ fˆw(xt)]\nle 3 sorgenti di errore\nEspressione per \nExpected Prediction Error",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#26": "Riferimenti\n \n27\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , 3a edizione, \nApogeo, 2015. \nMachine Learning: Regression\n , University of Washington - Coursera, 2015. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. ",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#3": "Training Set Randomness\nUn Training Set è un campione di N osservazioni (e.g., N \nappartamenti venduti di cui conosciamo le features e il \nprezzo).  \nCosa accade se il Training Set è costituito da altre N \nosservazioni diverse dalle precedenti?  \nCome cambiano le prestazioni del sistema?\n \n4",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#4": "Training Set Randomness\nAd esempio, nella ﬁgura sono mostrate due situazioni \nrelative a due diverse scelte del training set:\n \n5Test Set\nPer valutare la prestazione dei due “ﬁt” dobbiamo \nprendere in considerazione il Generalization Error.",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#5": "Training Set Randomness\nNei due casi otterremo due diversi valori del \nGeneralization Error:\n \n6Test Set\n",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#6": "Expected Prediction Error\nIdealmente, vorremmo poter ottenere una misura delle \nprestazioni del sistema, mediata su tutti i possibili training \nset. \nPossiamo deﬁnire formalmente tale quantità, che \nchiamiamo Expected Prediction Error (EPE), come segue: \n \n7EPE = Etrain[Generalization Error per ˆw(train)]\n“averaging” su tutti i possibili Training Set \n(pesati in base alle loro probabilità)parametri calcolati su \nuno speciﬁco Training Set",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#7": "Expected Prediction Error \nsu un target input\nPer analizzare questo tipo di errore, cominciamo a \nprendere in considerazione uno speciﬁco punto \n x\nt\n: \n \n8Test \nx\nt\nSupponiamo  inoltre che la Loss function sia la seguente: \nL[y, f ˆw(x)] = ( y\u0000ˆy)2=[y\u0000fˆw(x)]2",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#8": "Expected Prediction Error \nsu un target input\nE’ possibile dimostrare che l’errore EPE in \n x\nt\n è uguale alla \nsomma di tre termini:\n \n9\nVediamo ora di illustrare adeguatamente tali tre termini.EPE( xt)=\u00002+ [bias( fˆw(xt))]2+ var[ fˆw(xt)]",
    "data_test\\rootfolder\\università\\MachineLearning\\8-Expected Prediction Error-sbloccato.pdf#9": "Noise: \nVarianza dell’Errore del modello\n \n10EPE( xt)=\u00002+ [bias( fˆw(xt))]2+ var[ fˆw(xt)]\nfw(true) (x)\n",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#0": "Università Roma Tre \nDipartimento di Ingegneria \nAnno Accademico 2021 - 2022 \nIntroduzione  \nalla  \nClassiﬁcazione\nMachine Learning ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#1": "Sommario\nIntroduzione alla Classiﬁcazione \nEsempi di applicazione della Classiﬁcazione \nDecision Boundary \nLogistic Regression \nMaximum Likelihood Estimation \nTraining mediante Gradient Ascent\n \n2",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#10": "Introduzione alla Classiﬁcazione\n \n11\n•Come si vede in ﬁgura, tutte le immagini del test set, tranne una, \nsono classiﬁcate correttamente. \n•L’errore per il Boston terrier  è dovuto completamente alla nostra \nscelta delle features, scelta fatta basandoci sul training set \ndisponibile (un po’ troppo piccolo). \n•Per migliorare dobbiamo perciò ricominciare, collezionando più \ndati e individuare più features.",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#11": "Learning Pipeline\n \n12\n•In ﬁgura è rappresentata la learning pipeline del problema di \nclassiﬁcazione che stiamo considerando. \n•Lo stesso processo è usato essenzialmente per tutti i task di \nMachine Learning, non solo per la classiﬁcazione. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#12": "Learning Pipeline\n \n13•Deﬁnire il problema . Qual è il task che vogliamo sia appreso dal \ncomputer? \n•Collezionare i dati . Raccogliere i dati per il training set e il test set. Più \ni dati sono numerosi e diversiﬁcati, meglio è per il successo del \nsistema da realizzare. \n•Individuare le features . Quali sono le features migliori per descrivere i \ndati? \n•Addestrare il modello . Scegliere il modello e calibrare i suoi parametri \nsul training set mediante metodi di ottimizzazione. \n•Testare il modello . Valutare sul test set le prestazioni del modello \naddestrato. Se i risultati non sono soddisfacenti, ripensare la scelta \ndelle features utilizzate e collezionare, se possibile, più dati. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#13": "Minimizzazione di una funzione di costo\n \n14\nCaso della regressione:",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#14": "Minimizzazione di una funzione di costo\n \n15\nCaso della classiﬁcazione:",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#15": "Esempi di applicazione \nObject Detection\n \n16\n",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#16": "Esempi di applicazione \nSpam Filtering\n \n17(Testo della email, \nmittente, IP, ecc.)\nInput: x Output: ŷ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#17": "Esempi di applicazione \nImage Classiﬁcation\n \n18\nInput: x Output: ŷ\n(pixel dell’immagine) (categoria predetta)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#18": "Esempi di applicazione \nDiagnosi Mediche Personalizzate \n \n19Input: x Output: ŷ\n",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#19": "Esempi di applicazione \nReading Your Mind\n \n20\nOutput: ŷ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#2": "Introduzione alla Classiﬁcazione\nIl task della \n Classiﬁcazione\n  è simile in linea di principio a \nquello della \n Regressione  \nLa vera differenza tra i due è che, anziché predire un valore di \noutput continuo, nella classiﬁcazione cerchiamo di prevedere \nvalori discreti o \n classi \n \n3",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#20": "Esempi di applicazione \n \n21Sentiment Analysis\nEsempio: classiﬁcatore di reviews di ristorantiInput: xOutput: ŷ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#21": "Esempi di applicazione \nSentiment Analysis\n \n22\nInput x:  In questo ristorante preparano i migliori  \n“spaghetti alla carbonara” di Roma\nŷ = +1",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#22": "Punteggio di una frase \n(\nScore\n ) \n \n23Termine Peso w\nmigliore 1.5\nbuono 1.0\ncattivo -1.0\nmagniﬁco 2.0\nterribile -2.1\neccezionale 2.7\nil, noi, dove, ecc. 0.0Un modo che possiamo adottare per classiﬁcare una review come \npositiva o negativa consiste nel considerare alcuni “termini” che \nriteniamo rilevanti ai ﬁni della classiﬁcazione, calcolando per ciascuno \ndi essi il numero di occorrenze con cui compare nella review e un \n“valore di rilevanza” (peso) da utilizzare per calcolare un “punteggio”. \nAd esempio:",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#23": " \n24x1 = #eccezionalex2 = #terribile\n+\n++ ++-\n--\n-\n+Score( x) = 2.7 x1 - 2.1 x2Termine Peso w\neccezionale 2.7\nterribile -2.1\nPunteggio di una frase \n(\nScore\n ) \n-Score( x) < 0\nScore( x) > 0r:  2.7 x1 - 2.1 x2 = 0  \n      (decision boundary)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#24": " \n25+\n++ ++-\n--\n-\n+Termine Peso w\neccezionale 2.7\nterribile -2.1\nPunteggio di una frase \n(\nScore\n ) \n+Score( x) < 0\nScore( x) > 0\nx1 = #eccezionalex2 = #terribiler:  1.0 + 2.7 x1 - 2.1 x2 = 0 \n    (decision boundary)Score( x) = 1.0 + 2.7 x1 - 2.1 x2",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#25": " 26\nˆyi= sign[Score( xi)]Nel caso in cui i termini siano d possiamo calcolare il punteggio \ncome segue:\ne classiﬁcare la review in questo modo:\nPunteggio di una frase \n(\nScore\n ) \ndove:\nsign(Score) =⇢+1 se Score >0\n\u00001 se Score <0",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#26": " 27ˆyi= sign[Score( xi)]Score( xi)= w0\u00000(xi)+w1\u00001(xi)+ ···+wD\u0000D(xi)=\n=DX\nj=0wj\u0000j(xi)=wT·\u0000(xi)Nel caso generale di classiﬁcazione binaria abbiamo:\nPunteggio di una frase \n(\nScore\n ) ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#27": "Afﬁdabilità della Previsione \n 28•La funzione sign vista in precedenza ci fornisce una \nclassiﬁcazione binaria del sentiment della revisione. \n•Potremmo però essere interessati anche ad avere un grado di \nconﬁdenza della previsione. \n•Ad esempio, potremmo voler distinguere il caso di uno \nScore di poco superiore allo zero (e.g., 0.1) dal caso di uno \nScore ben più elevato (e.g., 4.0), punteggi che in entrambi i \ncasi danno luogo a review positive. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#28": "Probabilità come “degree of belief” \n 290 1P(yi = +1)\nAssolutamente certo \nreview negative0.5\nAssolutamente certo \nreview positive\nNon so se le review  \nsono positive o negative•A tale scopo possiamo avvalerci del calcolo delle probabilità. \n•Se diciamo che la probabilità di avere y i = +1 è di 0.7, vogliamo \ndire che ci aspettiamo di avere nell’insieme delle review \ndisponibili il 70% di review positive. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#29": "Probabilità come “degree of belief” \nProbabilità Condizionate \n 30•E’ molto utile in tale contesto considerare le probabilità \ncondizionate. \n•Se diciamo ad esempio che la probabilità di avere una review \npositiva, condizionata al fatto di avere nella review 3 occorrenze \ndi “eccezionale” ed 1 di “terribile”, è di 0.9, vogliamo dire che ci \naspettiamo il 90% delle review positive nella lista delle review \ndisponibili, considerando però solo quelle che hanno 3 \n“eccezionale” e 1 “terribile” (in verde nella ﬁgura che segue). ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#3": "Introduzione alla Classiﬁcazione\nIn buona sostanza, un classiﬁcatore realizza un mapping: \ndove  \n    è l’\n instance space\n , ossia l’insieme di tutte le possibili \nistanze del problema. Se esse sono descritte da un numero \nprecisato di features, abbiamo: \n \n    è un ﬁnito e in genere piccolo insieme di \n class labels\n : \n \n4C\nC=[C1,C2,...,C k]X!C\nX=[F1⇥F2⇥···⇥FD]X",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#30": " \n31 xi : testo della review yi: sentiment\nIl cacio e pepe era delizioso. +1\nLa carbonara era eccezionale. L’ambiente terribile. Il servizio eccezionale. \nComplessivamente un ristorante eccezionale.+1\nMia moglie ha preso i carcioﬁ alla romana, che erano pessimi. -1\n………… -1\n………… +1\n……. eccezionale ……… terribile …… eccezionale …… eccezionale -1\n………… +1\n……. eccezionale ……… terribile …… eccezionale …… eccezionale +1P(y i = +1| 3 eccezionale & 1 terribile) = 0.9Probabilità come “degree of belief” \nProbabilità Condizionate ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#31": " 320 1P(yi = +1| xi)\nAssolutamente certo \nche xi è negativa0.5\nAssolutamente certo \nche xi è positiva\nNon sono sicuro se la xi  \nè positiva o negativaIn generale, dato un input xi, (e.g., una review) abbiamo la \nseguente situazione: Probabilità come “degree of belief” \nProbabilità Condizionate ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#32": "Link Function \n 33Score( xi, w)- ∞ + ∞ •Il problema che dobbiamo risolvere, se vogliamo avvalerci delle \nprobabilità condizionate, è capire come passare dai valori dello \nScore a quelli delle probabilità. \n•La funzione Score ha un range che va da -∞ a +∞: \n•La probabilità, come sappiamo, può variare da 0 a 1:  \n0 1P(yi = +1| xi)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#33": " 34Score( xi, w)\n- ∞ + ∞ \n0 10\n0.5\nP(yi = +1| xi, w) = g[Score( xi, w)]Link Function \n•Dobbiamo pertanto deﬁnire una “link function” g (generalized \nlinear model ) che realizzi un mapping tra i due intervalli: ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#34": " \n35\n•Una funzione tipicamente usata in questi casi è la funzione \nlogistica , o sigmoide , così deﬁnita:Link Function \n•Essa, come si vede, ha l’insieme di deﬁnizione costituito \ndall’intervallo (-∞, +∞) e come codominio l’intervallo [0, 1].  ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#35": " 36Score( xi, w)\n- ∞ + ∞ \n0 10\n0.5\nP(yi = +1| xi, w) = sigmoid [Score( xi, w)]Logistic Regression Model \n•Il nostro modello diventa dunque il seguente: ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#36": " 37Logistic Regression Model \n•L’espressione per la probabilità, dato un ingresso xi ed un vettore \ndei pesi calcolato ŵ, è dunque la seguente: \nˆP(yi=+ 1 |xi,ˆw)=1\n1+e\u0000ˆwT·\u0000(xi)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#37": " 38Il Processo di Training \n•Il processo di training consiste nel deﬁnire una funzione di costo, \no una funzione che misura la “qualità” della previsione, e nel \ndeterminare la conﬁgurazione dei pesi (vettore w) che ottimizza \nla funzione per gli esempi di training. \n•Nella ﬁgura che segue sono mostrati i passi relativi a tale \nprocesso. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#38": "Il Processo di Training \n[caso della Classiﬁcazione]\nDati di \nTraining\nEstrazione \ndelle \nFeatures  \nModello \ndi ML\nmetrica \nvalutazione \nqualità\nCalcolo vettore \ndei pesi ŵyi osservato(xi)\n 39xi\nFunzione  \nvalutazione \nqualitàŵɸ\n(N esempi)\nAlgoritmo di \nApprendimentoˆP(yi=+ 1 |xi,ˆw)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#39": "Funzione per Valutazione Qualità \n \n40•Dobbiamo ora deﬁnire una funzione che possiamo usare per la \nvalutazione della qualità delle prestazioni del sistema. \n•A tal ﬁne possiamo prendere in considerazione le probabilità \ncondizionate deﬁnite in precedenza. \n•in particolare, per ciascuno degli esempi di training ( xi, yi) che \nabbiamo disponibili, possiamo calcolare la probabilità di avere in \nuscita un valore y i dato un vettore di pesi w (vedi ﬁgura seguente). ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#4": "Introduzione alla Classiﬁcazione\n \n5\nVediamo un semplice esempio di classiﬁcazione: \nSupponiamo di voler addestrare un computer a distinguere \nimmagini di gatti da immagini di cani (\n task\n). \n•\nDobbiamo innanzi tutto procurarci un certo numero di \nimmagini (\n training set\n ) in modo da poter addestrare il \ncomputer: \n",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#40": "Maximum Likelihood Estimation \n(\nMLE\n)\n \n41Data Point xi,1 xi,2 yi Scegliere w che massimizza:\nx1, y1 2 1 +1 P(y 1=+1| x1, w)\nx2, y2 0 2 -1 P(y 2=-1| x2, w)\nx3, y3 3 3 -1 P(y 3=-1| x3, w)\nx4, y4 4 1 +1 P(y 4=+1| x4, w)\nL(w)=P(y1|x1,w)·P(y2|x2,w)·P(y3|x3,w)·P(y4|x4,w)La funzione che possiamo usare per la valutazione della qualità è: ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#41": " \n42L(w)=NY\ni=1P(yi|xi,w)La forma generale della funzione Likelihood  è:\nL’obiettivo è dunque quello di massimizzare tale funzione, ad \nesempio mediante Hill Climbing (o Gradient Ascent ), visto che \nnon si ha una forma chiusa:\nmax\nwL(w) = max\nwNY\ni=1P(yi|xi,w)\nMaximum Likelihood Estimation \n(\nMLE\n)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#42": " \n43•Per il calcolo del gradiente dobbiamo calcolare le varie derivate parziali \ndella funzione. \n•Possiamo sempliﬁcare tale calcolo trasformando la funzione,  \nconsiderando il logaritmo naturale del Likelihood: \n•In tal modo trasformiamo i prodotti in somme, pur non cambiando il \npunto di massimo assoluto. Infatti si ha:lnL(w)=l nNY\ni=1P(yi|xi,w)=NX\ni=1lnP(yi|xi,w)\nˆw= argmax\nwL(w) ˆwln= argmax\nwlnL(w) ˆw=ˆwln\nLog-Likelihood \n[facilita l'operazione di derivazione]",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#43": "Log-Likelihood \n[facilita l'operazione di derivazione]\n \n44\ndove è stata utilizzata la Indicator Function :Per facilitare i calcoli possiamo riscrivere la funzione come segue:\nlnL(w)=NX\ni=1lnP(yi|xi,w)=\n=NX\ni=1{I[yi= +1] ·lnP(yi=+ 1 |xi,w)+I[yi=\u00001]·lnP(yi=\u00001|xi,w)}",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#44": " \n45\nLog-Likelihood \n[facilita l'operazione di derivazione]\nSostituendo alle probabilità P le seguenti espressioni:\notteniamo, per un solo punto  i, la forma che segue:P(yi=+ 1 |xi,w)=1\n1+e\u0000wT·\u0000(xi)\nP(yi=\u00001|xi,w)=1 \u0000P(yi=+ 1 |xi,w)=1\u00001\n1+e\u0000wT·\u0000(xi)=\n=1+e\u0000wT·\u0000(xi)\u00001\n1+e\u0000wT·\u0000(xi)=e\u0000wT·\u0000(xi)\n1+e\u0000wT·\u0000(xi)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#45": "Forma della Funzione da Derivare \n[per un punto \n i\n] \n \n46lnL(w)= I[yi= +1] ·lnP(yi=+ 1 |xi,w)+I[yi=\u00001]·lnP(yi=\u00001|xi,w)=\n= I[yi= +1] ·ln1\n1+e\u0000wT·\u0000(xi)+( 1\u0000I[yi= +1]) ·lne\u0000wT·\u0000(xi)\n1+e\u0000wT·\u0000(xi)=\n=\u0000I[yi= +1] ·ln(1 + e\u0000wT·\u0000(xi))+\n+(1\u0000I[yi= +1]) ·[\u0000wT·\u0000(xi)\u0000ln(1 + e\u0000wT·\u0000(xi))] =\n=\u0000(1\u0000I[yi= +1]) wT·\u0000(xi)\u0000ln(1 + e\u0000wT·\u0000(xi))",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#46": " \n47\nRegole applicate:\nlne\u0000wT·\u0000(xi)\n1+e\u0000wT·\u0000(xi)=l n ( e\u0000wT·\u0000(xi))\u0000ln(1 + e\u0000wT·\u0000(xi))=\n=\u0000wT·\u0000(xi)\u0000ln(1 + e\u0000wT·\u0000(xi))ln1\n1+e\u0000wT·\u0000(xi)=\u0000ln(1 + e\u0000wT·\u0000(xi))\nForma della Funzione da Derivare \n[per un punto \n i\n] ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#47": " \n48lnL(w)=\u0000(1\u0000I[yi= +1]) wT·\u0000(xi)\u0000ln(1 + e\u0000wT·\u0000(xi))\nForma della Funzione da Derivare \n[per un punto \n i\n] ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#48": "Derivata Parziale per un punto\n \n49dove:\ne:Per uno solo punto i abbiamo:\n@(wT·\u0000(xi))\n@wj=\u0000j(xi)@lnL(w)\n@wj=\u0000(1\u0000I[yi= +1]) ·@(wT·\u0000(xi))\n@wj\u0000@\n@wjln(1 + e\u0000wT·\u0000(xi))=\n=\u0000(1\u0000I[yi= +1]) ·\u0000j(xi)+\u0000j(xi)·P(yi=\u00001|xi,w)=\n=\u0000j(xi){I[yi= +1]\u0000P(yi=+ 1 |xi,w)}\n@\n@wjln(1 + e\u0000wT·\u0000(xi))=\u0000\u0000j(xi)·e\u0000wT·\u0000(xi)\n1+e\u0000wT·\u0000(xi)=\u0000\u0000j(xi)·P(yi=\u00001|xi,w)",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#49": "Derivata parziale su tutti i punti\n \n50Sommando su tutti i punti i otteniamo:\nQuesta è la forma della derivata parziale che possiamo usare \nnell’algoritmo di Gradient Ascent  per trovare il vettore ŵ che \nottimizza la funzione:@lnL(w)\n@wj=NX\ni=1\u0000j(xi){\u0000tra valore vero e predettoz }| {\nI[yi= +1]\u0000P(yi=+ 1 |xi,w)}",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#5": "Introduzione alla Classiﬁcazione\n \n6\n•\nDobbiamo poi identiﬁcare le caratteristiche distintive \n(\nfeatures\n ) che ci possano consentire di distinguere le due \ntipologie di immagini. Nel nostro caso potremmo ad esempio \nscegliere le due seguenti: \n•\nDimensione del naso (da piccolo a grande) \n•\nForma delle orecchie (da arrotondate ad appuntite) \nsupponendo di essere in grado di estrarle dalle immagini. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#50": "Algoritmo di Gradient Ascent\n \n51w(1)= 0 (oppure lo inizializziamo in modo casuale)\nt=1\nwhilekrlnL(w(t))k2>✏\nfor j=0,1,. . . ,D\nderivata parziale[ j]=NX\ni=1\u0000j(xi){I[yi= +1]\u0000P(yi=+ 1 |xi,w(t))}\nw(t+1)\nj w(t)\nj+↵⇤derivata parziale[ j]\nt t+1",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#51": "Riferimenti\n \n52\nWatt, J., Borhani, R., Katsaggelos, A.K. \n Machine Learning Reﬁned\n , 2nd edition, \nCambridge University Press, 2020. \nJames, G., Witten, D., Hastie, T., Tibishirani, R. \n An Introduction to Statistical \nLearning\n , Springer, 2013. \nRoss, S.M. \n Probabilità e Statistica per l’Ingegneria e le Scienze\n , 3a edizione, \nApogeo, 2015. \nMachine Learning: Classiﬁcation\n , University of Washington - Coursera, 2017. \nFlach, P. \n Machine Learning - The Art and Science of Algorithms that Make Sense of \nData\n, Cambridge University Press, 2012. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#6": "Introduzione alla Classiﬁcazione\n \n7\n•Se rappresentiamo le immagini del training set nello spazio \ndelle features , abbiamo le seguente situazione, in cui le varie \nimmagini appaiono ben aggregate:",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#7": "Introduzione alla Classiﬁcazione\n \n8•Ora che abbiamo una buona rappresentazione dei dati di \ntraining nello spazio delle features, l’ultimo passo per \naddestrare il computer a distinguere le immagini dei gatti da \nquelle dei cani è un problema geometrico: \n•identiﬁcare un modello (ad esempio un linear model ) che \nsepari chiaramente i gatti dai cani nello spazio delle \nfeatures.",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#8": "Introduzione alla Classiﬁcazione\n \n9\n•La ﬁgura che segue mostra un modello lineare (la retta in nero) \n“addestrato” ( trained linear model ), che divide lo spazio delle \nfeatures in due regioni. \n•Una volta determinata questa linea, una nuova immagine la cui \nrappresentazione sta al di sopra della linea (regione blu) sarà \nconsiderata dal computer relativa ad un gatto. Se invece sta \nsotto la linea sarà considerata relativa ad un cane. ",
    "data_test\\rootfolder\\università\\MachineLearning\\9-Classification - IntroduzioneLR-sbloccato.pdf#9": "Introduzione alla Classiﬁcazione\n \n10\n•Per veriﬁcare l’efﬁcacia del sistema dobbiamo valutare le sue \nprestazioni su un insieme di immagini ( test set ) distinte da \nquelle usate per l’addestramento: ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#0": "Programmazione\nOrientata agli Oggetti\nIntroduzione al Linguaggio di \nProgrammazione Java",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#1": "Programmazione orientata agli oggetti\nSommario\nBreve riassunto della storia del linguaggio Java\n–Motivazioni del linguaggio e scelte progettuali\n–Versione di Riferimento\nMacchina virtuale JVM\nLa portabilità e la piattaforma Java\nLibrerie Standard Java\nJava vs C\nJava vs C++\nControllo del Flusso\nTipi di dato primitivi\n–Un tipo molto particolare : String\nErrori a tempo di compilazione e di esecuzione\nDiagnostica in Java vs in C\nLimiti del Paradigma Procedurale",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#10": "Programmazione orientata agli oggetti\nCompilazione Nativa vs JVM\nCodice C\n001010101\n0101011111\n00010101Codice\nJava\nJava\nBytecodeJVM\nCompilazione Compilazione.java\n.class",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#11": "Programmazione orientata agli oggetti\nJava e Portabilità\n●Java è supportato praticamente da tutti i sistemi \noperativi più diffusi (Mac OS, Windows, Linux)\n●E da altrettante piattaforme hardware\n—Lo stesso programma Java può essere eseguito su \ntutte le piattaforme che dispongono di una JVM\n—Senza bisogno di ricompilarlo!\n●Basta spostare i file .class\n●Java permette di scrivere programmi altamente portabili\n—Senza preoccuparsi di quali operazioni siano \nsupportate da quali piattaforme\n—N.B. Tuttavia le piattaforme sottostanti sono diverse, \ne talvolta rimane possibile osservarne le differenze \nanche programmando in Java\n●Ad es. nomi dei file (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#12": "Programmazione orientata agli oggetti\nJava e Portabilità: Dov’Eravamo \nRimasti? Il Linguaggio C...\n●Ad esempio, il seguente codice in C ha un comportamento \nindefinito\nint i = 127;\ni = i++;\n●In C la rappresentazione dei tipi predefiniti non possiede \nuna dimensione fissata da uno standard\n✔Un int è grande quanto la parola del microprocessore \nusato. Rappresentazione degli interi a  8, 16 o 32 bit? \nDipende dalla piattaforma: possibile overflow!\n—Se ++ viene eseguito prima dell'assegnazione allora il valore \nin i sarà 1 (incrementa i a 2 ma il risultato, assegnato a i, è \n1)\n—Se ++ è eseguito dopo l'assegnazione allora il valore in i \nsarà 2.\n●Il comportamento dipende anche dalla precedenza data dal \ncompilatore alle diverse operazioni",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#13": "Programmazione orientata agli oggetti\nPortabilità del Codice Java\n●In Java la sintassi e la semantica del linguaggio \nè ben definita e coerente su tutte le \nimplementazioni della JVM\n●Sono descritte in un documento noto come\nJava Language Specification\n—Eventuali ambiguità residuali sono progressiva-\nmente identificate, quindi rimosse e/o accettate\n●Le istruzioni non dipendono da una particolare \npiattaforma\n—rimangono le stesse per ogni piattaforma hardware",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#14": "Programmazione orientata agli oggetti\nLibrerie di Java\n●Altro principale motivo del successo di Java: \ndistribuizione congiunta linguaggio e librerie\nPiattaforma Java = Linguaggio + Compilatore + Librerie +...\n●In C la libreria standard permette di gestire\n—la memoria ( malloc, free , ...)\n—l'I/O (FILE, printf, scanf , ...)\n—le stringhe ( strcpy, strcat , ...)\n—ecc. ecc....\n●Ma non esiste il supporto per liste, insiemi, \narray associativi ecc. ecc.\n●Tutto si può aggiungere ma...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#15": "Programmazione orientata agli oggetti\nLibrerie Standard di Java\n●In Java quasi tutto quello di cui si ha bisogno è già \npresente nella libreria del linguaggio distribuita \nassieme al compilatore ed all’ambiente di \nesecuzione \n—Le collezioni sono presenti in Java da anni\n—Librerie spesso sviluppate da esperti del settore\n—Usate e testate da migliaia di utenti\n●Rarissimo che si trovino ancora bug di significativo \nimpatto pratico\n●Queste librerie sono > sempre< , invariabilmente,  \nmeglio delle nostre implementazioni “artigianali”",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#16": "Programmazione orientata agli oggetti\nFilosofia Java vs C (1)\n●Al giorno d’oggi alcune differenze tra il linguaggio C ed il \nlinguaggio Java (che pure deriva dal primo), si possono \ninterpretare correttamente solo considerando le differenze \nnella filosofia di fondo da cui scaturirono i due linguaggi\n●A sua volta queste derivano dalle differenze negli obiettivi \n●A sua volta queste differenze negli obiettivi nascono dalle \ndifferenze nel contesto in cui i due linguaggi sono nati\n—C: nato in cui contesto in cui le risorse di calcolo erano scarse\n—Java: era già chiaro che le risolse di calcolo sarebbero state \nsempre meno un problema per gli applicativi ordinari\n—Il vero problema (per l’industria) erano i programmatori!\n●C: linguaggio di medio livello di astrazione\n—Ideato per scrivere parti dei S.O. \n●Java: linguaggio di alto livello di astrazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#17": "Programmazione orientata agli oggetti\nFilosofia Java vs C (2)\n●Java nasce cercando di correggere in tal senso i più \nevidenti difetti dei linguaggi C/C++ rispetto agli  \nobiettivi all’epoca sentiti come più nuovi ed urgenti\n●C: prestazioni e controllo anzitutto \n—meglio veloce/compatto che semplice e portabile\n—non fare nulla se non è richiesto esplicitamente dal \nprogrammatore a cui si lascia il controllo diretto e quasi \ntotale dell’esecuzione\n●Java: semplice e portabile anzitutto\n—evitiamo gli errori più comuni anche se farlo costa, in \ngenerale, e non serve sempre: semplicità piuttosto che \nprestazioni e portabilità piuttosto che controllo totale\n—togliamo al programmatore alcune responsabilità, \nsoprattutto quelle più faticose da gestire correttamente \n(es. gestione memoria)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#18": "Programmazione orientata agli oggetti\nJava vs C++\n●Alcune volte Java sembra veramente limitarsi a \nrimuovere  alcune possibilità aggiuntive offerte dal \nlinguaggio di programmazione C++ \n—Il vero competitor iniziale di Java, più del linguaggio C…\n●Perché nasce per supportare lo stesso paradigma (>>)\n—Java rimuove (e non rimpiazza) i costrutti del linguaggio C++ più \ndiscutibili rispetto gli obiettivi della piattaforma Java stessa \n●I costrutti rimossi non sono strettamente indispensabili e rendono il \nlinguaggio più utile solo per pochissimi esperti\n●Contemporaneamente, però, finiscono  per renderlo più difficile da \nutilizzare correttamente da parte di tutti gli altri\n●Ad es. in Java non esiste\n—Ereditarietà multipla delle implementazioni (sino Java 7)\n—Operator overloading\n—Costruttore di copia\n—Passaggio di parametri per variabile/riferimento\n—Puntatori/Aritmetica dei puntatori...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#19": "Programmazione orientata agli oggetti\nUn Programma Java\n●La sintassi è simile al C perché ideata per esserlo\n—con lo scopo di attrarre il parco programmatori del \nlinguaggio Java, che non furono “sorpresi” dalla \nnuova sintassi\n—Java è il più famoso linguaggio C-like ma ne esistono \naltri (C++)\n●Ogni programma inizia con l’esecuzione del \ncosidetto metodo  statico main() esattamente come \nin C l’esecuzione comincia dalla funzione statica \nmain()\n●Curiosamente i nuovi linguaggi (tra tutti Scala) che \ncercano di “scalzare” Java, usano tattiche simili, se \nnon addirittura più efficaci\n—Compatibilità con la JVM",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#2": "Programmazione orientata agli oggetti\nBreve Storia (1)\n●Java è un linguaggio di programmazione \nsviluppato dalla Sun Microsystems e pubblicato \nnella sua versione 1.0 nel 1995\n●Lo sviluppo di Java iniziò nei primi anni '90 sotto \nla guida di James Gosling\n—Inizialmente il nome del linguaggio era Oak \n(quercia)\n—ed era pensato per sistemi embedded \n(televisori, sportelli bancomat, palmari, ecc...)\n●Verso il 1993 l'interesse verso i dispositivi \nembedded, per cui Java era stato pensato, crollò\n—Java aveva (ben presto!) perso il suo originale e \nprincipale dominio di applicazione!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#20": "Programmazione orientata agli oggetti\nProgramma in C vs Java (1)\n●Un programma che stampa tutti gli argomenti \npassati da linea di comando\n●In C:\n#include<stdio.h>\nint main(int argc, char *argv[]) {\nint i = 0;\nfor (i=0; i<argc; i++)\nprintf(“%s\\n”, argv[i]);\nreturn 0;\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#21": "Programmazione orientata agli oggetti\nProgramma in C vs Java (2)\n●In Java (scriviamo un file JavaApp.java ):\npublic class JavaApp {\npublic static void main(String[] args) {\nfor (int i=0; i<args.length; i++)\nSystem.out.println(args[i]);\n}\n}\nPer eseguirlo (da linea di comando):\n$ javac JavaApp.java\n$ java JavaApp ciao mondo\n$ ciao mondoCompilazione \nin ByteCode\nEsecuzione \ntramite la JVM",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#22": "Programmazione orientata agli oggetti\nMetodo main()\n●Ogni programma Java inizia con l'esecuzione del main()\n—La dichiarazione del metodo deve essere preceduta \ndalla parola chiave static\n—N.B. usiamola per questo scopo ma dimentichiamoci \ndella sua esistenza per le lezioni a venire (>>)\n●Il main()\n—Non restituisce alcun valore\n●Il suo tipo di ritorno è void\n—Prende in ingresso un array di stringhe\n●String[] args\n—argomento passato da linea di comando a sostituire la \ncoppia di parametri in C\n●char *argv[]\n●int argc",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#23": "Programmazione orientata agli oggetti\nJava: Produrre Stampe\n●Per stampare sullo schermo è possibile usare\nSystem.out.println( <valore> )\n●println  stampa <valore>  e si posiziona su \nuna nuova riga\n●Il tipo del parametro attuale di println()  può \nessere qualsiasi:\n—int, boolean, float, double, String , ...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#24": "Programmazione orientata agli oggetti\nTipi di Dato Primitivi Java\n●N.B. a differenza che nel linguaggio C, la dimensione della \nrappresentazione di questi tipi di dato primitivi non dipendono \ndalla piattaforma, ma sono fissati (per sempre) nella JLS\n●Java supporta  i seguenti tipi primitivi\n—int\ninteri a 32 bit (da  -231 a 231 – 1)\n—long\ninteri a 64 bit (da -263 a 263 – 1)\n—char\ncaratteri in codifica UTF-16 (alfabeto latino,\narabo, greco, cinese, giapponese, ecc....)\n—float\nnumeri in virgola mobile a 32 bit\n—double\nnumeri in virgola mobile a 64 bit\n—Altri tipi di dato primitivi meno usati ...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#25": "Programmazione orientata agli oggetti\nDichiarazione di Variabili\n●Per dichiarare una variabile si usa la seguente \nsintassi: <tipo> <nome>;\nint eta = 18;\n●Java è case sensitive quindi eta != Eta != \neTa != ETA != ... \n●Per evitare ambiguità (differentemente dal \nlinguaggio C), ad ogni variabile deve essere \nassegnato un valore prima del suo utilizzo\n✔In caso contrario la compilazione non andrà a \nbuon fine",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#26": "Programmazione orientata agli oggetti\nTipo Booleano in Java\n●A differenza di quanto accade nel linguaggio C, \nJava ha un tipo dedicato per le variabili \nbooleane:\n—boolean\nvariabili di tale tipo possono avere solo due \nvalori: true e false\n●Java offre i gli operatori logici and e or con la \nseguente  sintassi:\n—And: &&\n—Or: ||",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#27": "Programmazione orientata agli oggetti\nSistema dei Tipi Java\n●Ogni variabile ha un certo tipo e può assumere solo \nvalori appartenenti al dominio  di quel tipo\n—Il tipo di una variabile deve essere specificato al \nmomento della sua dichiarazione\nint eta = 18;\n—Se ad una variabile si assegna un valore non \nappartenente al dominio del suo tipo si genera \nun errore durante la compilazione\neta = false;\n●Il comportamento di Java e C sono simili\n●Sono entrambi linguaggi staticamente  tipati\n✔già durante la compilazione deve essere noto il \ntipo di ogni dato utilizzato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#28": "Programmazione orientata agli oggetti\nIl Tipo di Dato String\n●Java introduce il tipo di dato String per \nrappresentare stringhe\n—String  non è un tipo di dato primitivo (come ad es. \nint, double , …)\n—Le stringhe, per diffusione ed importanza, hanno da \nsubito meritato un supporto molto particolare dal \nlinguaggio (e dal compilatore)\n—si volevano attirare programmatori non esperti \nfacilitando l’utilizzo delle stringhe\n–È possibile dichiarare una variabile di tipo String ed \nassegnargli un valore come segue:\nString nome = \"Bob\";\nSystem.out.println(nome); // stampa Bob",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#29": "Programmazione orientata agli oggetti\nString\n●I letterali di tipo stringa sono compresi tra doppi \napici\n—\"una stringa \"\n●A differenza di C, è possibile assegnare più volte \nad una variabile di tipo String  un letterale di \nstringa\nString nome = \"Bob\";\nnome = \"Alice\"; // ok\nSystem.out.println(nome); // Stampa Alice\n●Molte operazioni sulle stringhe offerte direttamente \ndalle librerie Java distribuite con la piattaforma (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#3": "Programmazione orientata agli oggetti\nBreve Storia (2)\n●Tuttavia, sempre  nel 1993, la NCSA rilasciò  Mosaic\n—Uno dei primi browser con interfaccia grafica\n—Rese più accessibile Internet\n●Il team di Java decise allora di dedicarsi allo sviluppo di \nun sistema embedded per i browser: le applet\n—Le applet sono/erano piccole applicazioni che \npotevano essere eseguite all'interno del browser\n●Lo scopo era quello di arricchire le pagine web \ntramite l'uso di contenuto interattivo\n●Perché il concetto era rivoluzionario? il codice veniva \nscaricato da remoto ed eseguito (in maniera sicura) \nlocalmente. Javascript  non c’era ancora...\n●Ci fu un significativo effetto traino. Ma oggi, quanti \nsanno cosa sia un’applet?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#30": "Programmazione orientata agli oggetti\nControllo del Flusso\n●Le istruzioni per il controllo del flusso sono le \nstesse del C\n—La sintassi è esattamente  la stessa\n●Sono quindi presenti\n—if e else\n—switch , break\n—for, while , do while\n●break e continue",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#31": "Programmazione orientata agli oggetti\nControllo del Flusso\n●Essendo presente il tipo boolean , le condizioni sono \nsempre espresse su espressioni di tipo boolean\nint i = 0;\nif (i) { … } \n●In Java NON compila\n●In C è prassi comune eseguire controlli direttamente \nsu espressioni di tipo int\nchar stringa[] = “null terminated”;\nint c = 0;\nwhile (stringa[c]) {\n printf(“%c\\n”, stringa[c]);\n c++;\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#32": "Programmazione orientata agli oggetti\nErrori: Tempo di Compilazione vs \nTempo di Esecuzione\n●Due momenti ben distinti in cui si può \nriscontrare un errore in un programma\n—A tempo di compilazione\n—A tempo di esecuzione\n●La rapida risoluzione degli errori in tempi brevi, è \nimportante per la produttività\n●La diagnostica dovrebbe sempre riportare il \nmotivo dell'errore e la posizione esatta in cui si è \nverificato\n●La diagnostica del C talvolta risulta “vaga”\n—Soprattutto per gli errori a tempo di esecuzione\nSegmentation fault",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#33": "Programmazione orientata agli oggetti\nDiagnostica\n●La diagnostica è decisamente uno dei principali \npunti forti della piattaforma Java\n●Permette di trovare gli errori di entrambi i tipi per risolverli \ntempestivamente, così riducendo:\n—i tempi di ricerca degli errori (debugging)\n—i costi\n●Buona parte di questi vantaggi (soprattutto a tempo \ndinamico) derivano dall’adozione di un processore \nvirtuale (JVM)\n✔Ottenere le stesse funzionalità con un processore \nfisico è decisamente più ostico \n●Consiglio:  \nAbituarsi fin da subito ad un uso ossessivo della diagnostica",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#34": "Programmazione orientata agli oggetti\nErrori a Tempo di Compilazione\npublic class Prova {\npublic static void main(String[] args) {\nSystem.out.println(\"hi\")\n}\n}\nException in thread \"main\" java.lang.Error: \nUnresolved compilation problem: \nSyntax error, insert \";\" to complete \nBlockStatements\nat Prova.main(Prova.java:3)Il problemaDove si è \nverificato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#35": "Programmazione orientata agli oggetti\nErrori a Tempo di Esecuzione (1)\n●Gli errori a tempo di esecuzione sono \ndecisamente più temibili\n—Si verificano durante l'esecuzione del codice, \nnon sono né riconoscibili né prevedibili dal \ncompilatore\n—In Java, gli errori a tempo di esecuzione sono \nspesso anche detti runtime- exception\n●prendendo in prestito il  nome del costrutto del \nlinguaggio pensato per gestirli (>>) \n●Anche in questo caso la diagnostica è molto precisa",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#36": "Programmazione orientata agli oggetti\nErrori a Tempo di Esecuzione (2)\npublic class Prova {\npublic static void main(String[] args) {\nint infinito = 1/0;\nSystem.out.println(infinito);\n}\n}\nLa compilazione va a buon fine ma durante \nl'esecuzione si ha\nException in thread \"main\" \njava.lang.ArithmeticException: / by zero\nat Prova.main(Prova.java:3)Il problemaDove si è \nverificato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#37": "Programmazione orientata agli oggetti\nParadigma di Programmazione \n●Il paradigma di programmazione  influenza \ndecisamente il modo di programmare\n●Non esiste una definizione comunemente \naccettata di paradigma di programmazione\n(e forse neanche serve...)\n●Con un pò di esperienza inter-paradigma risulta \nfacile apprezzare come un linguaggio di \nprogrammazione rende un paradigma più \nsemplice da esprimere rispetto ad altri linguaggi\n●Il vostro corso di studi ne tiene conto!\n—Procedurale: FdI  -  I anno (Linguaggio C)\n—Oggetti: POO  - II anno (Linguaggio Java)\n—Funzionale: PF  - III anno (Linguaggio OCaml)\n—Paradigma Ibrido Oggetti/Funzionale (Scala )",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#38": "Programmazione orientata agli oggetti\nIl Paradigma Procedurale\n●Il paradigma procedurale è stato il primo a diffondersi \n●La divisione tra dati ed operazioni è tipica \ndell'hardware moderno (memoria/CPU) e \ndell’architettura di Von Neumann\n—T ale divisione impone che i dati siano sempre separati \ndalle operazioni\n—La correlazione tra dati ed il codice che li lavora non è \nesplicita  \n●Nel paradigma procedurale un programma è \nspecificato come una sequenza di comandi che \ncambiano lo stato dell’esecuzione, passo dopo passo\n●Il C è un linguaggio di programmazione che rende \nnaturale esprimere il paradigma procedurale\n●In effetti è un linguaggio “vicino alla macchina”",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#39": "Programmazione orientata agli oggetti\nLimiti del Paradigma Procedurale (1)\n●Uno dei maggiori limiti del paradigma procedurale \nè proprio la netta separazione tra dati ed \noperazioni sugli stessi\n—lo stato è ben distinto e separato dalle operazioni\n●Dati e funzioni che li lavorano, sono comunque \nseparati nel codice\nstruct Persona {\nconst char* nome;\nint eta;\n};\nint isMaggiorenne(struct Persona persona) {\nreturn ( persona.eta >= 18 );\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#4": "Programmazione orientata agli oggetti\nBreve Storia (3)\n●Le applet sono ormai obsolete\n–Ma cavalcando gli anni della diffusione di massa di internet, \nhanno sicuramente contribuito a rendere Java uno dei \nlinguaggi più popolari tra gli sviluppatori dell’epoca\n●La popolarità e diffusione di Java sono dovuti a molti fattori \n●Tra i principali sicuramente l’incremento di produttività dei \nprogrammatori che lo adottarono\n●Java semplifica alcuni aspetti rispetto al C/C++\n—possiede un gran numero di strumenti facenti parte della \npiattaforma  che semplificano grandemente lo sviluppo\n—riduce significativamente il livello di expertise  necessario \nad un programmatore per essere produttivo\n—In sintesi, Java ampliò la disponibilità di manodopera  (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#40": "Programmazione orientata agli oggetti\nLimiti del Paradigma Procedurale (2)\n●È del tutto legittimo avere operazioni sugli \nstessi dati in luoghi anche molto distanti del \ncodice\n●Addirittura incentivata(!) la ripartizione dei due \naspetti in file distinti\n●La definizione di un tipo di dato in C specifica \nsolo i dati componenti ma non le operazioni \nche vi si possono applicare\n—queste sono definite altrove e non fanno parte \ndella definizione del tipo di dato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#41": "Programmazione orientata agli oggetti\nLimiti del Paradigma Procedurale (3)\nstruct Persona {\nconst char* nome;\nint eta;\n};void invecchia(struct Persona *p) {\np->eta = p->eta + 1;\n}\nint isMaggiorenne(struct Persona p) \n{\nreturn ( p.eta >= 18 );\n} File: Persona.h File: Persona.c\nDati                        Operazioni\n●Il tipo di dato Persona  non definisce le operazioni che \npossono essere eseguite sul tipo stesso",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#5": "Programmazione orientata agli oggetti\nBreve Storia (4)\n●Oltre 20 anni dopo la nascita e la sua diffusione di massa, \nrimane molto adottato sia nell’industria che \nnell’accademia\n●T alvolta forse addirittura con eccessivo entusiasmo \n(RM3!)\n●Ha conosciuto nuovi ed importanti ambiti di utilizzo\n–Applicazioni su Web\n–Sistemi embedded\n–App Android\n–E molti altri ancora (“ 3 Billion Devices Run Java ”)\n●Vari motivi dietro la durata di questo successo (>>)\n—pur avendo accumulato molti difetti, ha saputo innovarsi \nsenza rompere la retro-compatibilità, che rimane un \nobiettivo dogmatico nonostante il succedersi di \nnumerose nuove versioni, talune anche molto innovative",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#6": "Programmazione orientata agli oggetti\nQuanto è Popolare Java?\nhttps://www.tiobe.com/tiobe-index/Java 8\n",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#7": "Programmazione orientata agli oggetti\nDa Java 1 a Java 17 in ~25 Anni\n●Java 1.0: 1995, la prima versione di Java\n●Java 1.1: 1997, Inner Classes e Reflection\n●Java 1.2: 1998, Just In Time Compiler introdotto nella JVM di Sun, \n  Java Collections Framework\n●Java 1.3: 2000\n●Java 1.4: 2002\n●Java 1.5 / 5: 2004, Generics, Auto boxing/unboxing, Enum, Varargs, for each loop\n●Java 1.6 / 6: 2006\n●Java 1.7 / 7: 2011\n●Java 1.8 / 8: 2014, “Deriva funzionale”, lambda expressions\n●Java 9: 2017, “Moduli”\n●Java 10: 2018\n●Java 11: 2019\n●…\n●Java 17!: 2021\n●… e presto seguiranno altre: rilasci semestrali(Cambio politica dei rilasci, ora accellerati)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#8": "Programmazione orientata agli oggetti\nLa Portabilità di Java\n●Java ha rivoluzionato il concetto di portabilità\n●Uno dei principi fondamentali di Java\n—Write Once Run Anywhere\n—Un programma Java può essere eseguito su tutte \nle piattaforme che supportano Java senza dover \nricompilare il codice  per quella specifica \npiattaforma\n●Java è infatti nato come un linguaggio \ninterpretato\n—Il risultato della compilazione è il cosiddetto \nBytecode : codice oggetto che può essere eseguito \nda un processore virtuale noto come\nJava Virtual Machine (JVM)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-01-introduzione-java-da-C.pdf#9": "Programmazione orientata agli oggetti\nJVM vs Codice Nativo\n●Una JVM è, banalizzando, un processore “software” \n●Programma in grado di interpretare ed eseguire codice \noggetto Java (bytecode) letto da file di \nestensione .class\n—I dispositivi che supportano Java forniscono una \nimplementazione della JVM per il sottostante hardware\n●Il Bytecode può essere eseguito su qualunque computer \nche disponga di una implementazione della JVM\n–Al contrario, i compilatori C generano codice nativo \neseguibile solo su un particolare hardware\n–Un programma C deve essere ricompilato  per ogni \npiattaforma su cui si desidera eseguirlo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#0": "Programmazione\nOrientata agli Oggetti\nIntroduzione al Paradigma di \nProgrammazione Orientato \nagli Oggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#1": "Programmazione orientata agli oggetti\nSommario\n●Paradigmi di Programmazione\n–Il Paradigma Orientato agli Oggetti\n–Classi\n–Oggetti\n–Esercizio con Eclipse\n–Gli oggetti in Rete\n●La notazione puntata\n●Stato degli oggetti\n–Variabili di istanza\n–Inizializzazione\n–Campo d’Azione (Scope)\n●Comportamento degli oggetti\n–Metodi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#10": "Programmazione orientata agli oggetti\nOggetti: Stato + Comportamento (1)\n●Secondo il paradigma OO l’esecuzione di un \nprogramma avviene con la creazione di un rete di \noggetti che si scambiano messaggi, aggiornano ed \ninterrogano il proprio stato durante l’esecuzione\n●Ogni oggetto\n—possiede  uno stato interno\n—offre operazioni agli altri oggetti\n●Gli oggetti sono dotati di:\n—Stato: informazioni memorizzate in variabili di istanza  \n(o campi o attributi)\n—Comportamento : metodi  che si possono invocare \nsull'oggetto\n—Identità : un oggetto può essere distinto dagli altri (anche \ne soprattutto da esemplari  della stessa tipologia)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#11": "Programmazione orientata agli oggetti\nOggetti: Stato + Comportamento (2)\n●Ad esempio l'oggetto Giocatore  possiede come \nstato il suo nome e la stanza in cui si trova\n●È possibile chiedere all’oggetto Giocatore  di \ncompiere delle azioni\n—ad es. getStanzaCorrente()  per  ottenere \nl’oggetto che rappresenta la stanza corrente\n—ad es. setStanzaCorrente()  per spostarsi \nnella prossima stanza\n●Può servire quindi interrogare l’oggetto Stanza\n—ad es. getStanzaAdiacente()  per ottenere \nl’oggetto che rappresenta la stanza \nadiacente a quella considerata corrente",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#12": "Programmazione orientata agli oggetti\nClasse (1)\n●Possono esistere vari oggetti dello stesso tipo\n—diverse stanze; attrezzi; giocatori\n●Tutti gli oggetti di un certo tipo possiedono le  \ninformazioni dello stesso tipo\n—tutte le stanze hanno un nome ed una o più \nstanzaAdiacente\n●Tutti gli oggetti di un certo tipo offrono le \nstesse operazioni\n—A tutti i giocatori è possibile chiedere di spostarsi \nnella prossima stanza o di raccogliere un oggetto \nAttrezzo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#13": "Programmazione orientata agli oggetti\nClasse (2)\n●È necessario un meccanismo per costruire oggetti:\n1)Possiedono una propria identità ed autonomia \n2)Possiedono informazioni e dati specifici...\n3)...ma sembrano chiaramente rispondere anche ad altri tratti \ncomuni a tutti gli oggetti della stessa tipologia\n●Serve una sorta di fabbrica specializzata  che definisca \ncome tutti gli oggetti di un certo tipo siano fatti \nlasciando la libertà di variare alcuni aspetti\n●Si pensi ad un fabbrica di Orologi  tutti uguali\n1)Ciascun esemplare è distinto dagli altri\n2)Ciascun esemplare ha un numero di serie ed un colore \ndiverso...\n3)…ma tutti offrono la possibilità di leggere l’ora\n●Nella programmazione orientata agli oggetti queste \nfabbriche sono dette classi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#14": "Programmazione orientata agli oggetti\nClasse (3)\n●Per ogni tipo di oggetto che si vuole rappresentare \nesiste una classe\n—Una per Giocatore , una per Stanza , ecc...\n●Tutti gli oggetti sono costruiti  a partire dalla \ndefinizione di una classe\n●La classe astrae e definisce\n—lo stato di un oggetto di un certo tipo\n—il comportamento degli oggetti di un certo tipo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#15": "Programmazione orientata agli oggetti\nClasse, un Esempio\n●Tralasciamo per ora le stanze, gli attrezzi e i \ngiocatori… T orneranno più tardi (>>)\n●Più semplice utilizzare forme geometriche in un \npiano cartesiano con coordinate intere per un \nprimo esempio di classe\n—per rappresentare una forma è necessario \nconoscerne la posizione\n—servono le coordinate di un punto sul piano \ncartesiano: x, y\n●si definisce una classe Punto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#16": "Programmazione orientata agli oggetti\nLa Classe Punto (1)\npublic class Punto {\nprivate int x;\nprivate int y;\npublic void setX(int posX) {\nx = posX;\n}\npublic void setY(int posY) {\ny = posY;\n}\npublic int getX() {\nreturn x;\n}\npublic int getY() {\nreturn y;\n}\n}Stato\nOperazioni, per \nimpostare e \nleggere lo stato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#17": "Programmazione orientata agli oggetti\nLa Classe Punto (2)\npublic class Punto {\nprivate int x;\nprivate int y;\npublic void setX(int posX) {\nx = posX;\n}\npublic void setY(int posY) {\ny = posY;\n}\npublic int getX() {\nreturn x;\n}\npublic int getY() {\nreturn y;\n}\n}Ogni oggetto di tipo \nPunto ha una x e una y\nvariabili d’istanza\nInoltre, dispone di una \nserie di operazioni per \nleggere e impostare il \nsuo stato\nmetodi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#18": "Programmazione orientata agli oggetti\nCreazione di Oggetti\n● La creazione di un oggetto a partire da una classe \navviene utilizzando l’operatore new:\nPunto origine = new Punto();\n●Restituisce un riferimento  ad un oggetto appena \ncreato  \n●Ora è presente un oggetto in memoria ed è  \npossibile chiedergli di svolgere delle operazioni per \ntramite del riferimento, qui conservato nella \nvariabile locale origine  \n●Ad esempio per impostare la ascissa ed ordinata\norigine.setX(0);\norigine.setY(0);",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#19": "Programmazione orientata agli oggetti\nConcetti Correlati ma Ben Distinti!\nQuesta semplice istruzione \nPunto origine = new Punto();\ngià nasconde almeno tre concetti \nchiaramente distinti sebbene correlati\nI.Variabile locale\nII.Riferimento\nIII.Oggetto:Punto\nx1\n0:Punto\nx0\ny0\norigineriferimentooggetto\nVariabile locale\nCi torneremo più volte sopra!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#2": "Programmazione orientata agli oggetti\nIl Paradigma Orientato agli Oggetti (1)\n●La divisione tra codice e operazioni è tipica \ndell'hardware moderno (memoria/CPU) secondo \nl’architettura di Von Neumann\n●Ma gli esseri umani ragionano allo stesso modo?\n●Ovvero, quando pensiamo ad un oggetto  pensiamo \nsolo al suo stato od anche alle operazioni che vi si \npossono compiere\n●Ad esempio… un televisore ...!?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#20": "Programmazione orientata agli oggetti\nLa Notazione Puntata\n●Per richiedere un'operazione ad un oggetto si \nusa la cosidetta notazione puntata\n<riferimento-oggetto>.<metodo>(<parametri-attuali>);\n●Ad esempio:\n—origine.setX(0);\n●Anche solo questa notazione mostra come i dati \nsiano stati  avvicinati  alle operazioni sugli stessi\ndati.operazione(parametri)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#21": "Programmazione orientata agli oggetti\nClassi e Oggetti\n●A partire da una classe possiamo creare molte \nistanze,  ognuna dotata di uno stato autonomo\nPunto unoZero = new Punto();\nunoZero.setX(1);\nunoZero.setY(0);\n●L'oggetto  creato in precedenza (ovvero quello il cui \nriferimento è conservato dentro origine ) risulta invariato \ndopo tali operazioni e non viene modificato \ndall’invocazione di metodi tramite il riferimento conservato \ndentro unoZero  perché le operazioni sono svolte su un \noggetto distintoUn nuovo oggetto viene creato in \nmemoria; è possibile chiedergli di \nsvolgere operazioni attraverso il \nriferimento conservato nella \nvariabile locale unoZero",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#22": "Programmazione orientata agli oggetti\nTerminologia\n●\nNella programmazione orientata agli \noggetti si utilizza la seguente terminologia:\n–Oggetti o istanze : istanze di una certa classe, \npresenti in memoria, mantengono lo stato di un \noggetto\n– Metodi : specificano le operazioni che determinano il \ncomportamento degli oggetti\n•Es.: setX(), getY(), …\n–Variabili di Istanza : consentono di memorizzare le \ninformazioni di ciascun oggetto (lo stato dell’oggetto)\n–Riferimento ad oggetto: un riferimento ad un \noggetto in memoria (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#23": "Programmazione orientata agli oggetti\nClassi e File\n●In Java, una classe di nome XYZ DEVE essere \ncontenuta all'interno di un file di nome XYZ.java\n—esistono alcune eccezioni a questa regola che \ntralasciamo per il momento\nclassi nidificate  (>>)\n●Da qualsiasi altra classe del nostro programma \nsarà possibile far riferimento ad una classe \nspecificandone il suo nome\n—In Java non è necessario importare un file esplicitamente \nper poter usare il codice in esso contenuto\n—Ma bisogna mettere il compilatore (e la JVM durante \nl’esecuzione) in condizione di trovarlo a partire dal suo \nnome (>> )",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#24": "Programmazione orientata agli oggetti\nEsercizio: Eclipse (0)\n•JDK (Standard Edition)\n–Scaricare ed installare dal sito della ORACLE \nl'ultima versione disponibile di Java 8\nhttps://www.java.com/download/java8_update.jsp\n–Recommended Version 8 Update 321\n–Oppure Java 11\n•Documentazione JDK\n–Scaricare ed installare dal sito della ORACLE\n•Strumenti di sviluppo \n–Un IDE professionale: Eclipse (4.X)\n•Scaricare  Eclipse IDE for Java Developers da \nhttps://www.eclipse.org/downloads/eclipse-packages/\n(è la versione usata anche in sede d’esame)\n•Configurare Eclipse per compilare Java 7\n25",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#25": "Programmazione orientata agli oggetti\nEsercizio: Eclipse (1)\n●Realizzare la classe Punto usando l'IDE eclipse\n—File > New > Java Project\n●Project Name: Forme > click su 'Finish'\n—Selezionare il progetto appena creato\n—File > New > Class\n●Name: Punto > click su 'Finish'\n●Oltre al codice mostrato prima, aggiungere il metodo\npublic void trasla(int dx, int dy)  che sposta il \npunto di dx sull'asse x e dy sull'asse y",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#26": "Programmazione orientata agli oggetti\nEsercizio: Eclipse (2)\nRealizzare la classe MainForme  usando l'IDE eclipse\n—Creare una classe di nome MainForme\n—Aggiungere il metodo main()\npublic class MainForme {\npublic static void main(String[] args) {\nPunto origine = new Punto();\norigine.setX(0);\norigine.setY(0);\nSystem.out.println(origine.getX());\nSystem.out.println(origine.getY());\norigine.trasla(1, 1);\nSystem.out.println(origine.getX());\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#27": "Programmazione orientata agli oggetti\nGli Oggetti in Rete  (1)\n●La definizione  di un programma “orientato agli oggetti” \nconsiste nella definizione di diverse classi di oggetti\n●L’esecuzione  di un programma orientato agli oggetti avviene \norchestrando lo scambio di messaggi tra un plurarità di \noggetti istanza delle classi definite nel programma\n–gli oggetti devono “ conoscersi”\n•un oggetto può possedere riferimenti  verso gli altri oggetti\n•gli oggetti possono inviare messaggi ad altri oggetti dei \nquali possiedono un riferimento\n●Si vuole realizzare la classe Rettangolo\n—Stato composito:\n●base\n●altezza\n●posizione del vertice in alto a sx",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#28": "Programmazione orientata agli oggetti\nGli Oggetti in Rete  (2)\n●Il vertice in alto a sinistra è un oggetto istanza \ndella classe Punto\n—di coordinate (x, y)\n●Ogni oggetto della classe Rettangolo  deve \nconoscere un oggetto della classe Punto che \nrappresenta il suo vertice in alto a sinistra\nbasealtezza\nvertice\nDi tipo Punto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#29": "Programmazione orientata agli oggetti\nGli Oggetti in Rete  (3)\n:Rettangolo\n base 8\n altezza 3\n vertice\n:Punto\n x 0\n y 0riferimento ad oggettooggetto\noggetto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#3": "Programmazione orientata agli oggetti\nIl Paradigma Orientato agli Oggetti (2)\n●Un problema è più naturalmente modellabile se \npensato come popolato da una pluralità di \noggetti che possiedono uno stato ( spento , \nacceso, sintonizzato  su un certo canale)\nTelevisore , Persona , ecc…\ne che interagiscono ciascuno secondo i messaggi \nche sanno naturalmente interpretare:\nfabio.accende(tvInSalone)\n●Gli oggetti conoscono ed interagiscono con altri \nesemplari (anche dello stesso tipo)\n—Un oggetto n11 di tipo Stanza  conosce altri \noggetti Stanza , ad esempio n12, nelle immediate \nadiacenze a formare un oggetto Labirinto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#30": "Programmazione orientata agli oggetti\n(Con Eclipse)  La Classe Rettangolo\n●Nello stesso progetto della classe Punto\npublic class Rettangolo {\nprivate int base;\nprivate int altezza;\nprivate Punto vertice;\npublic void setBase(int b) { base = b; }\npublic void setAltezza(int a) { altezza = a; }\npublic void setVertice(Punto v) { vertice = v; }\npublic int getBase() { return base; }\npublic int getAltezza() { return altezza; }\npublic Punto getVertice() { return vertice; }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#31": "Programmazione orientata agli oggetti\nLa Classe Rettangolo (2) \n●Il main() può essere modificato come segue\npublic class MainForme {\npublic static void main(String[] args) {\nPunto origine = new Punto();\norigine.setX(0);\norigine.setY(0);\nRettangolo rect = new Rettangolo();\nrect.setVertice(origine);\nrect.setBase(8);\nrect.setAltezza(3);\n// …\n// Seguono stampe per verificarne il funzionamento\n}\n}Dopo questa \nistruzione l'oggetto \nistanza di \nRettangolo  \nconosce l'oggetto  \nistanza di Punto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#32": "Programmazione orientata agli oggetti\nMessaggi tra Oggetti (1)\n●Si vuole implementare il metodo\nsposta(int deltaX, int deltaY)\nnella classe  Rettangolo\n●Per traslare il suo vertice il rettangolo può chiedere al suo \nstesso vertice di spostarsi: scambio di messaggi tra oggetti  \npublic class Rettangolo {\n// … Come prima … \npublic void sposta(int deltaX, int deltaY) {\nvertice.trasla(deltaX, deltaY);\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#33": "Programmazione orientata agli oggetti\nMessaggi tra Oggetti (2)\n●E se la classe Punto non disponesse del metodo \ntrasla() ?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#34": "Programmazione orientata agli oggetti\nMessaggi tra Oggetti (3)\npublic class Rettangolo {\n// … come prima … \npublic void sposta(int deltaX, int deltaY) {\nint xVertice = vertice.getX();\nint yVertice = vertice.getY();\nvertice.setX(xVertice + deltaX);\nvertice.setY(yVertice + deltaY);\n}\n}\n●Il comportamento desiderato è comunque ottenibile \nutilizzando i metodi setX() e setY() ma il  codice risulta \n“meno pulito” rispetto alla soluzione basata sulla disponibilità \ndel metodo  trasla()  già all’interno della classe Punto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#35": "Programmazione orientata agli oggetti\nVariabili di Istanza e Metodi\n●Una classe definisce sia delle variabili di \nistanza sia dei metodi; rappresentano, rispett.\n—lo stato degli oggetti istanza di quella classe\n—il comportamento  di tali oggetti \n●La definizione di una classe segue questa \nsintassi difatti:\npublic class <NomeClasse> {\n <definizione di variabili di istanza>\n <definizione di metodi>\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#36": "Programmazione orientata agli oggetti\nVariabili di Istanza (1)\n●\nLe variabili di istanza memorizzano \ninformazioni che rappresentano lo stato \ndi un oggetto\npublic class Rettangolo {\nprivate int base;\nprivate int altezza;\nprivate Punto vertice;\n…\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#37": "Programmazione orientata agli oggetti\nVariabili di Istanza (2)\n●Nella definizione di una variabile di istanza:\n—Modificatore di visibilità (>>)\n—Tipo\n—Nome della variabile\nprivate int base;\nModificatore di visibilitàTipoNome della variabile\n●Il modificatore di visibilità specifica se una certa variabile è \nvisibile dall'esterno\n—Per il momento si usa private : la variabile è visibile solo \nall'interno della classe in cui è dichiarata\n—Per accedervi dall'esterno si usano i metodi getter e setter\n ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#38": "Programmazione orientata agli oggetti\nVariabili Istanza: Inizializzazione\n●Ci sono diversi modi per inizializzare lo stato di un oggetto\n●Le variabili di qualsiasi genere (ovvero di istanza, locali ed \naltro >>) vengono sempre inizializzate, esplicitamente od \nimplicitamente\n●In Java non esiste il problema delle variabili accidentalmente \nrimaste non inizializzate tipico del linguaggio C\n●Per le variabili di istanza: s e non viene specificato alcun \nvalore iniziale, assumono un valore di default:\n●per le variabili di un tipo numerico ( int, float…) è 0\nRettangolo rect = new Rettangolo();\nSystem.out.println(rect.getBase()); // Stampa 0\n●Stampa (sempre e prevedibilmente) 0 nonostante \nnon sia stato invocato il metodo setBase(0);",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#39": "Programmazione orientata agli oggetti\nVariabili di Istanza:\nCampo d’Azione (o Scope )\n●Le variabili di istanza sono visibili all'interno \ndella sola classe in cui sono dichiarate\n—Ogni metodo può referenziarle semplicemente per nome\npublic class Rettangolo {\nprivate int base;\n// ...\npublic int getBase() {\nreturn base;\n}\n// …\n}Qui ‘base’ fa \nriferimento alla \nvariabile di \nistanza base",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#4": "Programmazione orientata agli oggetti\nIl Paradigma Orientato agli Oggetti (3)\n●Ognuno di questi oggetti ha delle proprietà\n—Peso, Nome, Altezza , …\n●Ognuno di questi oggetti può svolgere delle \nazioni\n—Un oggetto Macchina  può accendersi()\n—Un oggetto Persona  può salutare()\n●Risulta più naturale programmare se \nprogrammiamo in maniera più vicina a come \nnaturalmente pensiamo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#40": "Programmazione orientata agli oggetti\nVariabili di Istanza\n●Una variabile di istanza ha un valore come parte dello \nstato di uno specifico oggetto istanza della sua classe\n●Si usa dire che una variabile di istanza “appartiene ad \nun oggetto”  anche se è definita  nella sua classe\n●Un oggetto, tramite le proprie variabili di istanza, \npossiede un proprio stato autonomamente  rispetto a \ntutti gli altri oggetti istanza della sua stessa classe\nRettangolo rect1 = new Rettangolo();\nrect1.setBase(10);\nrect1.base; // NON COMPILA\nRettangolo rect2 = new Rettangolo();\nrect2.setBase(20); /* N.B. la base del secondo oggetto cambia;\n                      Quella del primo rimane invariata */",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#41": "Programmazione orientata agli oggetti\nUn Parallelismo con il Linguaggio C (1)\n●Pare abbastanza naturale associare una variabile di \nistanza di una classe Java ad un campo di una \nstruct  di C\ntypedef struct {\n int base;\n...\n} Rettangolo;\nRettangolo *r = malloc(sizeof(Rettangolo));\nr->base = 15;\nRettangolo *r2 = malloc(sizeof(Rettangolo));\nr2->base = 30;\nfree(r1);\nfree(r2);",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#42": "Programmazione orientata agli oggetti\nUn Parallelismo con il Linguaggio C (2)\n●I metodi definiscono le operazioni che si possono \nsvolgere su un oggetto di una certa classe\n●Viene naturale associare un metodo di una classe \nJava ad una funzione C che opera su una struct\ntypedef struct {\nint base;\n…\n} Rettangolo;\nvoid setBase(Rettangolo * this, int base) {\nthis->base = base; \n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#43": "Programmazione orientata agli oggetti\nInvocazione dei Metodi\n●L'invocazione dei metodi è alla base della \nprogrammazione orientata agli oggetti come \nmeccanismo per lo scambio di messaggi tra oggetti\n●I metodi mettono in comunicazione diretta l’oggetto che \ninvoca il metodo con quello su cui il metodo viene \ninvocato\n●Ad esempio:\n—Per impostare od ottenere la base di un rettangolo \nabbiamo invocato dei metodi della classe Rettangolo\n—Per spostare un oggetto istanza della classe Rettangolo  il \nsuo metodo sposta()  ha invocato dei metodi della classe \nPunto una cui istanza ne rappresenta il vertice\n ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#44": "Programmazione orientata agli oggetti\nMetodo main()\n●L'esecuzione di un programma inizia sempre \ncon l'invocazione di un particolare e specifico \nmetodo \n●Per convenzione (eredità dal linguaggio C) tale \nmetodo si chiama main()\n—Questo metodo “scatena” l’esecuzione \ninvocando a sua volta altri metodi\n●Tranne che per il metodo main() da cui comincia \nl’esecuzione, per ogni invocazione di metodo \nesiste sempre un metodo invocante  ed un \nmetodo invocato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#45": "Programmazione orientata agli oggetti\nMetodo Invocante e Invocato\npublic class Main {\npublic static void main(String args[]) {\nRettangolo rect = new Rettangolo();\nrect.setBase (22);\n}\n}Metodo \ninvocante\nMetodo \ninvocatoMetodo \ninvocatoInvocazione di metodo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#46": "Programmazione orientata agli oggetti\nDefinizione di Metodo\n●I metodi sono dichiarati all'interno della \ndefinizione di una classe e definiscono il \ncomportamento di tutti gli oggetti appartenenti a \nquella classe\n●La dichiarazione di un metodo comprende due parti:\n—Intestazione\n●Modificatore di accesso/visibilità\n●Tipo valore restituito\n●Nome del metodo\n●Lista dei parametri formali\n—Corpo\n●Definizioni di variabili locali\n●Istruzioni\npublic void setX(int x) {   …   }Corpo Intestazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#47": "Programmazione orientata agli oggetti\nMetodi: Valore Restituito\n●I metodi possono comunicare verso l'esterno \nrestituendo un valore\n—Il metodo invocato  comunica con il metodo invocante\n—esattamente come per le funzioni in C\n●Se un metodo non ritorna nessun valore al \nmomento della dichiarazione del tipo di ritorno \nsi utilizza la parola chiave  void",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#48": "Programmazione orientata agli oggetti\nMetodi e Aggiornamenti di Stato\n●Conviene, per diversi motivi (>>), distinguere sempre i \nmetodi che\n—interrogano  (solamente) lo stato dell'oggetto su cui sono \ninvocati\n●Solo lettura dello stato\n—aggiornano  lo stato dell'oggetto su cui sono invocati\n●Anche scrittura dello stato\n●Ad esempio (nella classe Punto)\npublic int getX() { \nreturn x;       // interroga lo stato\n}\npublic void trasla(int dx, int dy) {\nx += dx;    // aggiorna lo stato\ny += dy;        // aggiorna lo stato\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#49": "Programmazione orientata agli oggetti\n(Esercizio con Eclipse) \nVariabili di Istanza e Metodi\n●Realizzare la classe Attrezzo\n—Con le variabili di istanza\n●nome di tipo String\n●peso di tipo int\n—aggiungere i relativi metodi getter & setter\n●Realizzare la classe Stanza\n—Con le variabili di istanza\n●nome di tipo String\n●stanzaAdiacente  di tipo Stanza\n●attrezzoContenuto di tipo Attrezzo\n—aggiungere i relativi metodi getter & setter",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#5": "Programmazione orientata agli oggetti\nIl Paradigma Orientato agli Oggetti (4)\n●E’ infatti possibile definire classi di oggetti che\n—mantengono uno stato\n—offrono operazioni che lo modificano/interrogano\nin maniera logicamente coesa\n●La realizzazione di un programma consiste nella \ndefinizione di opportune classi di oggetti che si \nscambiano messaggi che sanno come interpretare\n●Java è solo uno dei tanti linguaggi ideati per \nsupportare la programmazione secondo il \nparadigma orientato agli oggetti\n●Anzi… a ben vedere esistono fonti che spiegano \ncome programmare in maniera orientata agli \noggetti anche in linguaggio C (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#50": "Programmazione orientata agli oggetti\nModificare lo Stato di un Oggetto (1)\n●Lo stato di un oggetto può essere cambiato\n:Attrezzo\nnome“spada”\npeso7public class MainStanzeAttrezzi {\n  public static void main(String[] args) {\n    Attrezzo spada = new Attrezzo();\n spada.setNome(“spada”);\n spada.setPeso(7);\n Attrezzo osso = new Attrezzo();\n Osso.setNome(“osso”);\n Osso.setPeso(1);\n Stanza n11 = new Stanza();\n n11.setNome(“N11”);\n n11.setAttrezzo(spada);\n n11.setAttrezzo(osso);\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#51": "Programmazione orientata agli oggetti\nModificare lo Stato di un Oggetto (2)\n●Lo stato di un oggetto può essere cambiato\n:Attrezzo\nnome“spada”\npeso7public class MainStanzeAttrezzi {\n  public static void main(String[] args) {\n    Attrezzo spada = new Attrezzo();\n spada.setNome(“spada”);\n spada.setPeso(7);\n Attrezzo osso = new Attrezzo();\n osso.setNome(“osso”);\n osso.setPeso(1);\n Stanza n11 = new Stanza();\n n11.setNome(“N11”);\n n11.setAttrezzo(spada);\n n11.setAttrezzo(osso);\n}\n}:Attrezzo\nnome“osso”\npeso0",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#52": "Programmazione orientata agli oggetti\nModificare lo Stato di un Oggetto (3)\npublic class MainStanzeAttrezzi {\n  public static void main(String[] args) {\n    Attrezzo spada = new Attrezzo();\n spada.setNome(“spada”);\n spada.setPeso(7);\n Attrezzo osso = new Attrezzo();\n osso.setNome(“osso”);\n osso.setPeso(1);\n Stanza n11 = new Stanza();\n n11.setNome(“N11”);\n n11.setAttrezzo(spada);\n n11.setAttrezzo(osso);\n}\n}:Attrezzo\nnome“osso”\npeso1:Attrezzo\nnome“spada”\npeso7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#53": "Programmazione orientata agli oggetti\nModificare lo Stato di un Oggetto (4)\npublic class MainStanzeAttrezzi {\n  public static void main(String[] args) {\n    Attrezzo spada = new Attrezzo();\n spada.setNome(“spada”);\n spada.setPeso(7);\n Attrezzo osso = new Attrezzo();\n Osso.setNome(“osso”);\n Osso.setPeso(1);\n Stanza n11 = new Stanza();\n n11.setNome(“N11”);\n n11.setAttrezzo(spada);\n n11.setAttrezzo(osso);\n}\n}:Stanza\nnome“N11”\nattrezzoContenuto\n...:Attrezzo\nnome“osso”\npeso1:Attrezzo\nnome“spada”\npeso7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#54": "Programmazione orientata agli oggetti\nModificare lo Stato di un Oggetto (5)\npublic class MainStanzeAttrezzi {\n  public static void main(String[] args) {\n    Attrezzo spada = new Attrezzo();\n spada.setNome(“spada”);\n spada.setPeso(7);\n Attrezzo osso = new Attrezzo();\n Osso.setNome(“osso”);\n Osso.setPeso(1);\n Stanza n11 = new Stanza();\n n11.setNome(“N11”);\n n11.setAttrezzo(spada);\n n11.setAttrezzo(osso);\n}\n}:Stanza\nnome“N11”\nattrezzoContenuto\n...:Attrezzo\nnome“osso”\npeso1:Attrezzo\nnome“spada”\npeso7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#55": "Programmazione orientata agli oggetti\nModificare lo Stato di un Oggetto (6)\npublic class MainStanzeAttrezzi {\n  public static void main(String[] args) {\n    Attrezzo spada = new Attrezzo();\n spada.setNome(“spada”);\n spada.setPeso(7);\n Attrezzo osso = new Attrezzo();\n Osso.setNome(“osso”);\n Osso.setPeso(1);\n Stanza n11 = new Stanza();\n n11.setNome(“N11”);\n n11.setAttrezzo(spada);\n n11.setAttrezzo(osso);\n}\n}:Attrezzo\nnome“osso”\npeso1:Attrezzo\nnome“Spada”\npeso7\n:Stanza\nnome“N11”\nattrezzoContenuto\n...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#56": "Programmazione orientata agli oggetti\nRiferimenti ad Oggetti\n(continua...)\n●La creazione di un nuovo oggetto in memoria \navviene tramite l'operatore new\n●L'operatore new restituisce un \nriferimento ad un oggetto \nappena creato \n●Ad esempio: Stanza n11 = new Stanza();\n●La variabile locale n11 NON contiene l'oggetto \ncreato, ma bensì un riferimento  ad esso (>>)\nn11:Stanza\nnome\nattrezzoContenuto\n...\nn11Questa freccia entrante \nin un oggetto è la nostra \nrappresentazione grafica \ndei riferimenti!Questa rettangolo è \nla nostra \nrappresentazione \ngrafica degli oggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#6": "Programmazione orientata agli oggetti\nLa Rete di Oggetti (1)\n●L’esecuzione di un programma Java si risolve \nnello scambio di messaggi tra oggetti che \naggiornano ed interrogano il proprio stato\n●Ad esempio, se si volesse rappresentare la \ntopologia delle aule della nostra università, \nuseremmo degli oggetti Stanza (n10, n11, \ncampusOne , ...)\n—Per potersi scambiare messaggi, questi oggetti \ndevono conoscersi tramite dei riferimenti\n—si crea una Rete di Oggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#7": "Programmazione orientata agli oggetti\nLa Rete di Oggetti (2)\n:Stanza\nnome “Aula N11”\nstanzaAdiacente:Stanza\nnome “Aula N12”\nstanzaAdiacente\n:Stanza\nnome “CampusOne”\nstanzaAdiacenteUna “rete” di oggetti\nriferimento\nriferimentoriferimentooggetto\noggettooggetto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#8": "Programmazione orientata agli oggetti\nLa Rete di Oggetti (3)\n●Grazie ad un riferimento è possibile rappresentare \nl’oggetto Stanza  corrente di un ipotetico oggetto \nGiocatore\n●Grazie ai collegamenti tra oggetti è possibile \nspostarsi all’interno di un oggetto Labirinto  che \nconosce  certamente le stanze\n●Un oggetto Stanza  può “contenere” un oggetto \nAttrezzo\n●L’oggetto Giocatore  potrebbe “chiedere” \nall’oggetto Stanza  corrente quale oggetto Attrezzo  \npossiede, usando operazioni come \nprendiAttrezzo()  e rimuoviAttrezzo()\n●Allo scopo deve scambiare messaggi con l'oggetto \nStanza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-02-paradigma-OO.pdf#9": "Programmazione orientata agli oggetti\nLa Rete di Oggetti (4)\n:Stanza\nnome “Aula N11”\nstanzaAdiacente\nattrezzo\n:Attrezzo\nnome “Spada”\npeso 7:Giocatore\nnome “Alice”\nstanzaCorrente\nAltre\nstanze...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#0": "Programmazione\nOrientata agli Oggetti\nOggetti e Riferimenti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#1": "Programmazione orientata agli oggetti\nSommario\n●Riferimenti ad Oggetti\n●Molteplici riferimenti verso lo stesso oggetto\n●Riferimenti ed Effetti Collaterali\n●Riferimenti e passaggio dei parametri per valore\n●Metodi che restituiscono riferimenti\n●Riferimento nullo e NullPointerException\n●Campo d’Azione delle variabili e Shadowing\n●La parola chiave this\n●Convenzioni di Stile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#10": "Programmazione orientata agli oggetti\nMolteplici Riferimenti verso \nlo Stesso Oggetto (4)\npublic class MainStanzeRiferimenti {\npublic static void main(String[] args) {\nStanza n12 = new Stanza();\nn12.setNome(“aula n12”);\nStanza n11 = new Stanza();\nn11.setNome(“aula n11”);\nn11.setStanzaAdiacente(n12);\nStanza n11Alias = n11;\n}\n}\nn11:Stanza\nnome“aula n11”\nstanzaAdiacente\n...n11\nn11Aliasn12:Stanza\nnome“aula n12”\nstanzaAdiacente\n...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#11": "Programmazione orientata agli oggetti\nPiù Riferimenti & Side-Effect (1)\n●\nQual è l'output del seguente programma?\npublic static MainRiferimentiSideEffect {\npublic static void main(String[] args) {\nStanza n11 = new Stanza();\nStanza n11Alias  = n11;\nn11.setNome(“N11”);\nn11Alias .setNome(“ aula N11”);\nSystem.out.println(n11.getNome());\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#12": "Programmazione orientata agli oggetti\nPiù Riferimenti & Side-Effect (2)\npublic static MainRiferimentiSideEffect {\npublic static void main(String[] args) {\nStanza n11 = new Stanza();\nStanza n11Alias = n11;\nn11.setNome(“N11”);\nn11Alias.setNome(“ aula N11”);\nSystem.out.println(n11.getNome());\n}\n}\nn11:Stanza\nnome\nstanzaAdiacente\n...n11",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#13": "Programmazione orientata agli oggetti\nPiù Riferimenti & Side-Effect (3)\npublic static MainRiferimentiSideEffect {\npublic static void main(String[] args) {\nStanza n11 = new Stanza();\nStanza n11Alias = n11;\nn11.setNome(“N11”);\nn11Alias.setNome(“ aula N11”);\nSystem.out.println(n11.getNome());\n}\n}\nn11:Stanza\nnome\nstanzaAdiacente\n...n11\nn11Alias",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#14": "Programmazione orientata agli oggetti\nPiù Riferimenti & Side-Effect (4)\npublic static MainRiferimentiSideEffect {\npublic static void main(String[] args) {\nStanza n11 = new Stanza();\nStanza n11Alias = n11;\nn11.setNome(“N11”);\nn11Alias.setNome(“ aula N11”);\nSystem.out.println(n11.getNome());\n}\n}\nn11:Stanza\nnome“N11”\nstanzaAdiacente\n...n11\nn11Alias",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#15": "Programmazione orientata agli oggetti\nPiù Riferimenti & Side-Effect (5)\npublic static MainRiferimentiSideEffect {\npublic static void main(String[] args) {\nStanza n11 = new Stanza();\nStanza n11Alias = n11;\nn11.setNome(“N11”);\nn11Alias.setNome(“ aula N11”);\nSystem.out.println(n11.getNome());\n}\n}\nn11:Stanza\nnome“aula N11”\nstanzaAdiacente\n...n11\nn11Alias",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#16": "Programmazione orientata agli oggetti\nPiù Riferimenti & Side-Effect (6)\n●L'output è “ aula N11 ”\n●Sorprendente per chi aveva creato l’oggetto e lo \naveva chiamato semplicemente “ n11”?\n●Questo tipo di comportamenti spesso vengono \nindicati con il nome di \nEffetti Collaterali  (Side-Effect )\n—un’azione genera effetti visibili ben al di fuori \ndell’ambito in cui è avvenuta\n●Sia la variabile n11 che n11Alias  fanno \nriferimento allo stesso oggetto\n—una modifica effettuata a tale oggetto tramite uno dei \ndue riferimenti e visibile anche usando l’altro",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#17": "Programmazione orientata agli oggetti\nRiferimenti per Valore (1)\n●Quando una variabile contenente un riferimento è passata \ncome argomento ad un metodo\n—il passaggio è per valore\n—viene copiato il riferimento  contenuto nell’argomento\n—l'oggetto a cui fa riferimento NON viene copiato\n●Tramite due copie distinte ma identiche dello stesso \nriferimento si finisce per accedere (e modificare) lo stesso \noggetto\n●Il cambio di stato operato ad un oggetto all’interno del \nmetodo invocato  è visibile (come effetto collaterale) anche al \nlivello metodo invocante  che effettuato la chiamata passando \nper valore un riferimento all’oggetto\n—Passando per valore riferimenti si possono quindi ottenere  \neffetti simili a quelli che in altri linguaggi si ottengono mediante \nil cosidetto passaggio dei parametri per variabile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#18": "Programmazione orientata agli oggetti\nRiferimenti per Valore (2)\ntypedef struct { \n int base; … \n} Rettangolo;\nRettangolo *r = malloc(sizeof(Rettangolo));\nsetBase(r, 15);\n●Similarmente a quanto avviene in C, passando (per valore) \nil puntatore  ad un’area di memoria  allocata con  malloc\n—è possibile cambiare il contenuto della memoria  il cui \nindirizzo  è fornito come argomento\n—non è possibile cambiare il contenuto della variabile che \nospita tale indirizzo  al momento dell’invocazione\nanche in Java, passando un riferimento  ad un oggetto\n—è possibile cambiare lo stato dell’ oggetto il cui riferimento  è \nfornito come argomento\n—non è possibile cambiare il contenuto della variabile che \nospita tale riferimento  al momento dell’invocazionevoid setBase(struct Rettangolo * this, int b) {\nthis->base = b;      \n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#19": "Programmazione orientata agli oggetti\nPassaggio di Riferimenti (1)\npublic class MainPassRef {\n  public static void main(String[] args) {\n    Stanza n11 = new Stanza();\n    Stanza n12 = new Stanza();\n    n11.setStanzaAdiacente(n12);\n  }\n}\npublic class Stanza {\n  // ...\n  public void setStanzaAdiacente(Stanza stanza) {\n    this.stanzaAdiacente = stanza;\n  }\n}n11:Stanza\nnome\nstanzaAdiacente\n...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#2": "Programmazione orientata agli oggetti\nRiferimenti ad Oggetti (1)\n●La creazione di un nuovo oggetto in memoria \navviene tramite l'operatore new\n●L'operatore new restituisce un \nriferimento ad un oggetto \nappena creato \n●Ad esempio: Stanza n11 = new Stanza();\n●La variabile locale n11 NON contiene l'oggetto \ncreato, ma bensì un riferimento  ad esso (>>)\nn11:Stanza\nnome\nattrezzoContenuto\n...\nn11Questa freccia entrante \nin un oggetto è la nostra \nrappresentazione grafica \ndei riferimenti!Questa rettangolo è \nla nostra \nrappresentazione \ngrafica degli oggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#20": "Programmazione orientata agli oggetti\nPassaggio di Riferimenti (2)\npublic class MainPassRef {\n  public static void main(String[] args) {\n    Stanza n11 = new Stanza();\n    Stanza n12 = new Stanza();\n    n11.setStanzaAdiacente(n12);\n  }\n}\npublic class Stanza {\n  // ...\n  public void setStanzaAdiacente(Stanza stanza) {\n    this.stanzaAdiacente = stanza;\n  }\n}n11:Stanza\nnome\nstanzaAdiacente\n...\nn12:Stanza\nnome\nstanzaAdiacente\n...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#21": "Programmazione orientata agli oggetti\nPassaggio di Riferimenti (3)\npublic class MainPassRef {\n  public static void main(String[] args) {\n    Stanza n11 = new Stanza();\n    Stanza n12 = new Stanza();\n    n11.setStanzaAdiacente(n12);\n  }\n}\npublic class Stanza {\n  // ...\n  public void setStanzaAdiacente(Stanza stanza) {\n    this.stanzaAdiacente = stanza;\n  }\n}n11:Stanza\nnome\nstanzaAdiacente\n...\nn12:Stanza\nnome\nstanzaAdiacente\n...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#22": "Programmazione orientata agli oggetti\nPassaggio di Riferimenti (4)\npublic class MainPassRef {\n  public static void main(String[] args) {\n    Stanza n11 = new Stanza();\n    Stanza n12 = new Stanza();\n    n11.setStanzaAdiacente(n12);\n  }\n}\npublic class Stanza {\n  // ...\n  public void setStanzaAdiacente(Stanza stanza) {\n    this.stanzaAdiacente = stanza;\n  }\n}n11:Stanza\nnome\nstanzaAdiacente\n...\nn12:Stanza\nnome\nstanzaAdiacente\n...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#23": "Programmazione orientata agli oggetti\nRiferimenti & Valore Restituito\n●Quando un riferimento viene restituito  da un \nmetodo\n—Viene restituita una copia del riferimento\n—L'oggetto a cui si riferisce NON viene copiato\npublic class Stanza {\n  // ...\n  public Stanza getStanzaAdiacente() {\n     return stanzaAdiacente;\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#24": "Programmazione orientata agli oggetti\nEsercizio ( con Eclipse )\n●Assumiamo che:\n—la classe Rettangolo  non disponga del metodo \nsposta()\n—la classe Punto invece disponga del metodo trasla()\n●Trovare un modo alternativo per spostare gli \noggetti Rettangolo\n●Vediamo due soluzioni:\nN.B. nessuna delle due è raccomandabile (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#25": "Programmazione orientata agli oggetti\nEsercizio (2)\n●Prima soluzione\npublic static void main(String[] args) {\nPunto origine = new Punto();\norigine.setX(0);\norigine.setY(0);\nRettangolo rect = new Rettangolo();\nrect.setVertice(origine);\norigine.trasla(1, 1);\n}\n●Se spostiamo l’oggetto istanza della classe Punto che \nutilizziamo come vertice dell’oggetto istanza della classe \nRettangolo , spostiamo, come effetto collaterale , il \nrettangolo stesso",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#26": "Programmazione orientata agli oggetti\nEsercizio (3)\n●Seconda soluzione\npublic static void main(String[] args) {\nPunto origine = new Punto();\norigine.setX(0);\norigine.setY(0);\nRettangolo rect = new Rettangolo();\nrect.setVertice(origine);\nPunto verticeRect = rect.getVertice();\nverticeRect.trasla(1, 1);\n}\n●Si ottiene una copia del riferimento  all’oggetto istanza della \nclasse Punto che figura come vertice dell’oggetto istanza \ndella classe Rettangolo  che spostato produce, ancora come \neffetto collaterale , lo spostamento del rettangolo stesso",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#27": "Programmazione orientata agli oggetti\nEsercizio (4)\n●Entrambe le soluzioni sono poco raccomandabili, \nperché non rendono affatto evidente la reale \nintenzione di spostare il rettangolo\n●Nessuna invocazione diretta  di un metodo della classe \nRettangolo  induce a pensare che lo si sta spostando\n●Il rettangolo viene spostato solamente come risultato \ndell’effetto collaterale dello spostamento di un punto \n(il vertice) di cui conservava un riferimento\n●E’ preferibile dotare la classe Rettangolo  di un  \napposito metodo trasla() la sua implementazione \npuò fare affidamento ad un metodo (anche con lo \nstesso nome) di Punto\n●Gli effetti collaterali risultano difficili da tracciare",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#28": "Programmazione orientata agli oggetti\nRiferimento Nullo\n●In Java esiste un solo letterale di tipo \nriferimento ad oggetto: null\n●Un valore speciale e distinto da tutti gli altri \nvalori, il riferimento  nullo\n●Indica l’assenza di un reale riferimento ad un \noggetto esistente\n—N.B. In Java non esiste alcuna relazione particolare \ntra il valore null e 0 (letterale di tipo int)\n—In C, la macro NULL è invece un alias per il valore 0\n●Un po’ come già accadeva per i booleani, la \ntipizzazione Java è più stringente",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#29": "Programmazione orientata agli oggetti\nUtilizzo del Riferimento Nullo\n●Il riferimento nullo è utile\n—come valore speciale restituito da un metodo \nper segnalare un caso speciale. Ad es.:\nPersona cercata = rubrica.trova(“Alice”);\nrestituisce null se non esiste alcuna persona \ndi nome “Alice”  nella rubrica\n—per fornire un valore di default a variabili che \ncontengono riferimenti ad oggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#3": "Programmazione orientata agli oggetti\nRiferimenti ad Oggetti (2)\n●Proviamo a stampare il valore di variabili che \ncontengono riferimenti\npublic class MainRiferimenti {\npublic static void main(String[] args) {\nStanza n11 = new Stanza();\nSystem.out.println(n11);\n}\n}\n●Stampa:  Stanza@ 15db9742\n—Ma ovviamente dipende dalla particolare esecuzione\n—Possiamo per il momento semplificare il significato di questa \nstampa: è [ un numero che dipende dal ]l’indirizzo in memoria  \ndell’oggetto referenziato\n—In realtà non è esattamente così, ma per i nostri presenti scopi \nquesta semplificazione fa molto comodo (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#30": "Programmazione orientata agli oggetti\nUso di null: Esempio \npublic class MainNull {\npublic static void main(String[] args) {\nStanza n12 = new Stanza();\n     n12.setNome(“aula n12”);\nn12.setStanzaAdiacente( null );\n}\n}\n:Stanza\nnome“aula n12”\nstanzaAdiacente\n...Si intende \nrappresentare che \nla stanza n12 NON \npossiede stanze \nadiacenti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#31": "Programmazione orientata agli oggetti\nNullPointerException (1)\npublic class MainNullPointerException {\npublic static void main(String[] args) {\nStanza n12 = new Stanza();\n     n12.setNome(“aula n12”);\nn12.setStanzaAdiacente( null );\nStanza adiacenteN12 = n12.getStanzaAdiacente();\nSystem.out.println( adiacenteN12.getNome()) ;\n}\n}●Cosa succede se si invoca un metodo su un \nriferimento nullo?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#32": "Programmazione orientata agli oggetti\nNullPointerException (2)\n●null rappresenta l’assenza di un riferimento \nad un oggetto\n—Si genera un errore a tempo di esecuzione, \nun’eccezione  ( runtime-exception >> )\nNullPointerException\n$ java MainNullPointerException\nException in thread \"main\" \njava.lang.NullPointerException\nat MainNullPointerException.main(MainNullPointerException.java: 10)Tipologia\neccezione\nNumero di linea del codice in cui si è verificata",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#33": "Programmazione orientata agli oggetti\nNullPointerException: Diagnostica\n●Java, ancora una volta, fornisce una \ndiagnostica efficace \n●Cosa accadrebbe utilizzando il linguaggio di \nprogrammazione C?\nstruct Punto {\nint x;\nint y;\n}; \nint main() {\nstruct Punto *origine = NULL;\norigine->x = 0;\n}\nSegmentation fault !  ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#34": "Programmazione orientata agli oggetti\nInizializzazione delle \nVariabili di Istanza e null\n●Il compilatore forza l’inizializzazione di tutte le \nvariabili di istanza \n●Quelle dichiarate come contenenti un riferimento \nad oggetto sono inizializzate a null\nRettangolo rect = new Rettangolo();\nSystem.out.println(rect.getBase()); // 0\nSystem.out.println(rect.getVertice()); // null\nSystem.out.println( rect.getVertice() .getX());\n✔Il valore restituito da getVertice()  è null, \ninvocando un metodo sul suo risultato si \ngenera una NullPointerException",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#35": "Programmazione orientata agli oggetti\nEvitare NullPointerException\n●Se è noto che una funzione può ritornare \nnull come valore speciale è necessario \npredisporre un controllo sul valore restituito\n…\nPersona cercata = rubrica.trova(“Alice”);\nif (cercata!=null)\nSystem.out.println(cercata.getEta());\nelse\nSystem.out.println(“non trovato”);\n●In C: if (cercata!=0)\n—In Java non compilerebbe",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#36": "Programmazione orientata agli oggetti\nCampo d’Azione \ndelle Variabili e dei Parametri\n●Se il metodo setX()  venisse così dichiarato?\npublic class Punto {\nprivate  int x;\nprivate int y;\npublic void setX( int x) {\nx = x;\n}\n…\n}\nPunto unoUno = new Punto();\nunoUno.setX(1);\nSystem.out.println(unoUno.getX()); // Stampa 0",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#37": "Programmazione orientata agli oggetti\nShadowing  \n●Si è verificato il cosiddetto shadowing:\n—Il parametro formale x ha lo stesso nome della \nvariabile di istanza x\n—Il parametro formale ha però uno scope (>>) più \nristretto e quindi ha precedenza\n—Nel contesto del corpo del metodo, l’identificatore \n’x’ viene considerato un riferimento al \nparametro formale (e non alla var. di istanza)\n●Si dice anche che il parametro formale offusca (“ fa \nombra” ) la variabile di istanza\n●x = x; è un'espressione che assegna al parametro \nformale x il suo stesso valore (inutile!)\n✔Alcuni IDE moderni (come Eclipse) possono essere \nconfigurati per segnalare il problema",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#38": "Programmazione orientata agli oggetti\nLa Parola Chiave this (1)\n●All'interno di ogni metodo è possibile \nottenere un riferimento all’oggetto corrente  \n●La parola chiave this\n—Conserva un riferimento all'oggetto sul quale il \nmetodo in corso di esecuzione è stato invocato\n—Tramite questo riferimento è quindi possibile:\n●modificare le variabili di istanza dell’oggetto\n●fare invocazioni di metodo nidificate sullo stesso \noggetto\n●passare un riferimento all’oggetto corrente  come \nargomento di altre invocazioni di metodo…",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#39": "Programmazione orientata agli oggetti\nLa Parola Chiave this (2)\n●Si risolve anche il problema dello shadowing\n… \npublic void setX(int x) {\nthis.x = x;\n}\n… \nPunto unoUno = new Punto();\nunoUno.setX(1);\nSystem.out.println(unoUno.getX());// Stampa  1\n●All'interno del corpo del metodo setX() , this è \nun riferimento allo stesso oggetto a cui si riferisce \nanche unoUno  Parametro\nVariabile \ndi istanza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#4": "Programmazione orientata agli oggetti\nRiferimenti ad Oggetti (3)\n●Una stessa variabile può contenere, in \nmomenti diversi, riferimenti ad oggetti \ndistinti dello stesso tipo. Ad esempio:\npublic class MainRiferimenti {\n    public static void main(String[] args) {\n        Stanza n11 = new Stanza();\n        System.out.println(n11);  // Stampa Stanza@15db9742\n   n11 = new Stanza();\n      System.out.println(n11); // Stampa Stanza@ 6d06d69c \n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#40": "Programmazione orientata agli oggetti\nthis in C?\n●T alvolta può far comodo pensare a this come ad un \nparametro aggiuntivo passato automaticamente (ed \nimplicitamente)  ad ogni metodo\n●Ad esempio il metodo  setX()  verrebbe tradotto in C \ncon la seguente funzione\nvoid setX(struct Punto *this, int x) {\nthis -> x = x;      // Codice C\n}\n●Quindi l’invocazione di  unoUno.setX(1); diverebbe:\nstruct Punto unoUno;   // Codice C\nsetX(&unoUno, 1);      // Codice C",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#41": "Programmazione orientata agli oggetti\nAccedere \nle Variabili di Istanza con this\n●this può essere usato per accedere alle variabili di istanza \nma può anche essere omesso in assenza di ambiguità\n… \npublic int getX() {\nreturn this.x;\n  }\n… \n●Equivale a\n… \npublic int getX() {\nreturn x;\n  }\n… ●Il compilatore risolve \nl’identificatore x come \nvariabile di istanza \ndell'oggetto su cui il \nmetodo è stato invocato\n●In un certo senso, è \ncome se aggiungesse \nthis. automaticamente",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#42": "Programmazione orientata agli oggetti\nthis : Convenzione di Stile\n●Adottiamo comunque la convenzione di \nusare sempre e comunque  this per \nreferenziare variabili di istanza\n●Con le seguenti motivazioni:\n—si evita lo shadowing\n—si favorisce la leggibilità del codice\n✔si favorisce l’apprendimento di questi concetti \ndi base",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#43": "Programmazione orientata agli oggetti\nInvocare Metodi Mediante this (1)\n●La parola chiave this può essere usata per \ninvocare metodi sullo stesso oggetto su cui \nil metodo corrente è stato invocato\n—Se this viene omesso il compilatore lo \nconsidera comunque presente\n●Ad esempio è possibile scrivere il metodo \nsetXY()  usando gli altri due metodi della \nclasse Punto : setX()  e setY()",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#44": "Programmazione orientata agli oggetti\nInvocare Metodi Mediante this (2)\npublic void setXY(int x, int y) {\n  this.setX(x);\n  this.setY(y);\n}\n●Anche per alcune  invocazioni di metodi è \nconsigliabile usare this per aumentare la \nleggibilità\n●Ad esempio quando si vuole evidenziare l’utilizzo \ndi altri metodi della stessa classe nella scrittura di \nun primo metodo  (passo top-down )",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#45": "Programmazione orientata agli oggetti\nEsercizio\nFare le verifiche disponibili sul sito del \ncorso:\n•Studente.java\n•Tesi.java\n•Sommatore.java",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#5": "Programmazione orientata agli oggetti\nRiferimenti ad Oggetti (4)\npublic class MainRiferimenti {\n  public static void main(String[] args) {\n    Stanza n11 = new Stanza();\n    System.out.println(n11); // stampa Stanza@ 15db9742\n    n11 = new Stanza();\n    System.out.println(n11); // stampa Stanza@ 6d06d69c  \n  }\n}\nnome\nattrezzoContenuto\n...0x15db9742 :Stanza\nn11",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#6": "Programmazione orientata agli oggetti\nRiferimenti ad Oggetti (5)\npublic class MainRiferimenti {\n  public static void main(String[] args) {\n    Stanza n11 = new Stanza();\n    System.out.println(n11); // stampa Stanza@ 15db9742\nn11 = new Stanza();\nSystem.out.println(n11); // stampa Stanza@ 6d06d69c  \n  }\n}0x15db9742 :Stanza\nnome\nattrezzoContenuto\n...6d06d69c  :Stanza\nnome\nattrezzoContenuto\n...n11",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#7": "Programmazione orientata agli oggetti\nMolteplici Riferimenti verso \nlo Stesso Oggetto (1)\nn11 :Stanza\nnome“aula n11”\nstanzaAdiacente\n...\nn10:Stanza\nnome“aula n10”\nstanzaAdiacente\n...n12:Stanza\nnome“aula n12”\nstanzaAdiacente\n...●In alcuni casi più variabili contengono un \nriferimento allo stesso oggetto\n●Ad esempio due stanze adiacenti la \nmedesima:",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#8": "Programmazione orientata agli oggetti\nMolteplici Riferimenti verso \nlo Stesso Oggetto (2)\npublic class MainStanzeRiferimenti {\npublic static void main(String[] args) {\nStanza n12 = new Stanza();\nn12.setNome(“aula n12”);\nStanza n11 = new Stanza();\nn11.setNome(“aula n11”);\nn11.setStanzaAdiacente(n12);\nStanza n10 = new Stanza();\nn10.setNome(“aula n10”);\nn10.setStanzaAdiacente(n12);\n}\n}●La configurazione appena vista si può \nottenere con il seguente codice",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-03-oggetti-riferimenti.pdf#9": "Programmazione orientata agli oggetti\nMolteplici Riferimenti verso \nlo Stesso Oggetto (3)\npublic class MainStanzeRiferimenti {\npublic static void main(String[] args) {\nStanza n12 = new Stanza();\nn12.setNome(“aula n12”);\nStanza n11 = new Stanza();\nn11.setNome(“aula n11”);\nn11.setStanzaAdiacente(n12);\nStanza n11Alias = n11;\n}\n}\n●Ora sia n11 sia n11Alias  fanno riferimento allo stesso oggetto●Un altro esempio:",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#0": "Programmazione\nOrientata agli Oggetti\nGestione della Memoria",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#1": "Programmazione orientata agli oggetti\nSommario\nAllocazione di Oggetti\n–L’operatore new\n–Costruttori\n–Costruttore di default\nStack e Heap\n–Stack e Record di Attivazione\n–Stack-Overflow\n–Heap\nEquivalenza di oggetti ed identicità dei riferimenti\nRiferimenti in Java vs Puntatori in C\nGestione della Memoria\n–Garbage Collection",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#10": "Programmazione orientata agli oggetti\nCostruttore di Default (1)\n●Tutte le classi devono avere almeno un \ncostruttore, sempre\n●Se non viene esplicitamente dichiarato, ne \nviene aggiunto uno implicitamente\n●E’ un costruttore senza parametri\n(costruttore no-args )\n●Anche senza dichiarare esplicitamente il \ncostruttore senza argomenti di Punto :\nPunto punto = new Punto(); // COMPILA",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#11": "Programmazione orientata agli oggetti\nCostruttore di Default (2)\n●Appena viene definito esplicitamente un costruttore, \nil costruttore no-args  non viene più generato \nautomaticamente e non è più possibile invocarlo\n●Ad esempio:\n—Dopo aver dichiarato un primo costruttore (con parametri) \nnella classe Rettangolo :\npublic class MainNoArgs {\n    public static void main(String[] args) {\n        Rettangolo rect = new Rettangolo(); //NON COMPILA\n    }\n}\nException in thread \"main\" java.lang.Error: Unresolved compilation \nproblem: \nThe constructor Rettangolo() is undefined\nat MainNoArgs.main( MainNoArgs.java:4 )",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#12": "Programmazione orientata agli oggetti\nVariabili di Istanza: \nValore di default e null\n●È fortemente consigliato l'uso di un costruttore \nche inizializzi le variabili di istanza di tipo \nriferimento\n—Onde evitare NullPointerException\n—Se non specificato altrimenti, le variabili di istanza \ndi tipo riferimento sono inizializzate a null\n●Altro possibile costruttore per la classe  Rettangolo :\n   …\npublic Rettangolo(int base, int altezza) {\n  this.vertice = new Punto(0, 0);\n  this.base = base;\n  this.altezza = altezza;\n}\n… ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#13": "Programmazione orientata agli oggetti\nCostruttori Alternativi\n●Se il vertice non viene specificato, questo \ncostruttore suppone che sia nell’origine\n—è poi possibile usare la variabile di istanza \nvertice  senza sollevare NullPointerException\n●Spesso si definiscono molteplici costruttori con \ndiversi parametri\n—Utili per costruire oggetti\n●a partire da informazioni diverse. Ad es. per i rettangoli\n●base, altezza, vertice alto a sx\n●vertice alto a sx, vertice basso a dx\n●senza essere costretti a specificare tutti i parametri  \n—Per ora: unico costruttore, il più generico possibile \n(>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#14": "Programmazione orientata agli oggetti\nGestione della Memoria:\nStack e Heap\n●Durante l’esecuzione, un programma ha accesso \nad almeno due distinte aree di memoria\n●Stack\nPer la memorizzazione delle informazioni necessarie \nper l’esecuzione dei metodi, anche nidificati\n—Conserva:\n●Stato dell’esecuzione\n●Variabili locali e loro valore\n●Heap\nPer la memorizzazione di oggetti creati tramite \nl’operatore new \n—Conserva:\n●Oggetti e loro stato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#15": "Programmazione orientata agli oggetti\nStack\n●Area di memoria assegnata dalla JVM all’inizio \ndell’esecuzione \n—dimensione massima prefissata (qualche mb)\n●Adibita a conservare quanto serve per mantere lo \nstato dell’esecuzione (tranne lo stato degli oggetti \n>>)\n—È una struttura dati gestita secondo una disciplina LIFO\n●Last In Fist Out\n●Per gestire le invocazioni di metodo, comunque \nnidificate, a cominciare dal main()\n●Contiene i Record di Attivazione (RDA)\n—Struttura dati che contiene tutte le informazioni \nnecessarie all'invocazione ed all’esecuzione dei metodi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#16": "Programmazione orientata agli oggetti\nRecord di Attivazione\n●Ogni volta che un metodo viene invocato, il \ncorrispondente RDA viene creato ed inserito \nin cima allo stack\n●Quando il metodo termina, il suo RDA viene \nrimosso\n●La gestione dello stack avviene in modo \nautomatico e trasparente da parte della JVM ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#17": "Programmazione orientata agli oggetti\nMetodi e Record di Attivazione (1)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\noggetto2.metodo2();\n}\n…\nStackRDA main",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#18": "Programmazione orientata agli oggetti\nMetodi e Record di Attivazione (2)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\noggetto2.metodo2();\n}\n…\nStackRDA mainRDA metodo1",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#19": "Programmazione orientata agli oggetti\nMetodi e Record di Attivazione (3)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\noggetto2.metodo2();\n}\n…\nStackRDA mainRDA metodo1RDA metodo2",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#2": "Programmazione orientata agli oggetti\nOperatore new (1)\n●La creazione di oggetti in Java è permessa dall’operatore \nnew\n—richiede la specifica della classe di cui si vuole creare una nuova \nistanza\n—ovvero, più precisamente, di uno dei costruttori  di tale classe\n—restituisce un riferimento  all’oggetto appena creato\n●Punto origine = new Punto();\npublic class Punto {\n  private int x;\n  private int y;\n  public Punto() {\n    this.x = 0;\n    this.y = 0;\n  }\n  …\n}\n●Attenzione: origine contiene un riferimento;  NON contiene l’oggetto \nappena creatoDefinizione del Costruttore\n(sintassi: stesso nome della classe)\nFile Punto.javaCorpo del costruttore {…}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#20": "Programmazione orientata agli oggetti\nMetodi e Record di Attivazione (4)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\noggetto2.metodo2();\n}\n…\nStackRDA mainRDA metodo1",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#21": "Programmazione orientata agli oggetti\nMetodi e Record di Attivazione (5)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\noggetto2.metodo2();\n}\n…\nStackRDA main",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#22": "Programmazione orientata agli oggetti\nMetodi e Record di Attivazione (6)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\noggetto2.metodo2();\n}\n…\nStack",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#23": "Programmazione orientata agli oggetti\nContenuto di un RDA\n●Un Record di Attivazione  contiene:\n—parametri attuali\n—eventuale riferimento all'oggetto corrente per \ninvocazione  ( this)\n—variabili locali\n—valore di ritorno del metodo (se non è void)\n—il punto di ritorno dell’invocazione di metodo: \nl’istruzione successiva all’invocazione nel \nmetodo chiamante",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#24": "Programmazione orientata agli oggetti\nVariabili Locali dentro il RDA\n●Il record di attivazione ospita anche le variabili \nlocali di ogni metodo\n●Lo scope delle variabili locali è il metodo in cui \nsono definite, ed il ciclo di vita è chiaramente \nquello dell’invocazione di metodo\n●L’identificatore  di una variabile locale è un alias \n(gestito dal compilatore e dall’ambiente di \nesecuzione) di un indirizzo di memoria relativo \n(sullo stack) in cui è conservato il suo valore\n—Attenzione: Nulla a che fare con i riferimenti \n—Gli identificatori (ad es. origine ) sono decisamente \npiù facili da ricordare rispetto ad un indirizzo di \nmemoria!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#25": "Programmazione orientata agli oggetti\nStack & Variabili Locali (1)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStackRDA main",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#26": "Programmazione orientata agli oggetti\nStack & Variabili Locali (2)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStackRDA mainRDA metodo1\ncontat: 0x34fa09b1\ncontat è in realtà un alias per un indirizzo di \nmemoria relativo nel record di attivazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#27": "Programmazione orientata agli oggetti\nStack & Variabili Locali (3)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStackRDA mainRDA metodo1\ncontat:3",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#28": "Programmazione orientata agli oggetti\nStack & Variabili Locali (4)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStackRDA mainRDA metodo1\ncontat:3RDA metodo2",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#29": "Programmazione orientata agli oggetti\nStack & Variabili Locali (5)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStackRDA mainRDA metodo1\ncontat:5",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#3": "Programmazione orientata agli oggetti\nOperatore new (2)\n●Solo la JVM può creare oggetti\n—l’operatore new consente di chiederne i servizi\n●Per inizializzare nuovi oggetti, sinora:\n—invocazione del cosidetto costruttore senza \nargomenti\nnew Punto();\n—invocazione dei vari metodi setter\n●Ad esempio:\nPunto zeroUno = new Punto();\nzeroUno.setX(0);\nzeroUno.setY(1);\n●Si possono definire costruttori  che ricevono parametri ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#30": "Programmazione orientata agli oggetti\nStack & Variabili Locali (6)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStackRDA main",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#31": "Programmazione orientata agli oggetti\nStack & Variabili Locali (7)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStackRDA main",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#32": "Programmazione orientata agli oggetti\nStack & Variabili Locali (8)\npublic static void main(String[] args) {\n…\noggetto1.metodo1();\n}\n// … (nella classe di “oggetto1”)\npublic void metodo1() {\nint contat = 3;\noggetto2.metodo2();\ncontat = 5;\n}\n…\nStack",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#33": "Programmazione orientata agli oggetti\nScope delle Variabili Locali\n●Il primo record di attivazione è sempre \nquello del metodo main()\n●Quando  un RDA viene rimosso dallo stack le \nvariabili locali e i parametri non sono più \nutilizzabili\n—Lo scope di queste informazioni è limitato al \nsolo metodo di appartenenza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#34": "Programmazione orientata agli oggetti\nStack Overflow (1)\n●Lo stack ha una dimensione limitata\n➢non può contenere troppi RDA \n●Ad esempio\npublic class SommatoreRicorsivo {\npublic int sommaDaZeroA(int n) {\nreturn n + sommaDaZeroA(n-1);\n}\n}\n// ERRORE! manca il caso base!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#35": "Programmazione orientata agli oggetti\nStack Overflow (2)\n●L'esecuzione del metodo sommaDaZeroA(1) \ngenera il seguente errore\n(indipendentemente dal suo argomento)\nException in thread \"main\" \njava.lang. StackOverflowError\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\n...\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\nat SommatoreRicorsivo.sommaDaZeroA( SommatoreRicorsivo.java:3 )\n...N.B. La diagnostica \nriporta nomi delle \nclassi e dei metodi i cui \nRDA sono nello stack al \nmomento del trabocco",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#36": "Programmazione orientata agli oggetti\nHeap\n●Tutti gli oggetti creati con l'operatore new \npossiedono uno stato che viene conservato \nin un’area di memoria denominata Heap\n—Letteralmente “mucchio”,  si tratta di un’area di \nmemoria assegnata dalla JVM\n—NON è gestito come lo stack (LIFO)\n●la sua dimensione può crescere e decrescere \ndinamicamente anche durante l'esecuzione\n●gli oggetti sono creati e deallocati in ordine sparso\n●spesso e volentieri l’occupazione dell’heap risulta molto \nframmentata",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#37": "Programmazione orientata agli oggetti\nnew vs malloc\n●new sta ad una classe (quasi) come malloc  \nsta ad una struct\n—malloc\n●alloca memoria nello heap\n●restituisce l’indirizzo di tale area di \nmemoria\n—new\n●alloca memoria nello heap\n●restituisce il riferimento all’oggetto \nappena allocato\n●invoca un costruttore",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#38": "Programmazione orientata agli oggetti\nHeap e Stack (1)\n●Il valore restituito dall'operatore new è il \nriferimento all'oggetto appena creato\nPunto unitario = new Punto(1, 1)\n●La variabile locale unitario  contiene il \nriferimento all'oggetto creato, NON l'oggetto \nstesso\n●unitario  è un alias per\nl’indirizzo 0xef11d34f  \nStack0x15db9742 0xef11d34f:\nHeap0x15db9742 :Punto\n    x\n1     y1\nVariabile \nunitario",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#39": "Programmazione orientata agli oggetti\nHeap e Stack (2)\n●È possibile stampare anche il contenuto di \nuna variabile che contiene un riferimento ad \noggetto usando il metodo println()\nSystem.out.println(unitario);\n●println()  stampa una stringa che dipende \nsolamente dall'indirizzo di memoria e dalla \nclasse di appartenenza\nStampa: Punto@15db9742\n(è possibile alterare questo comportamento>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#4": "Programmazione orientata agli oggetti\nCostruttori\n●Ogni classe ha sempre  (almeno) un \ncostruttore che viene eseguito ogni volta \nche un oggetto di tale classe viene creato\n✔altrimenti non sarebbe possibile creare oggetti\n●Il corpo del costruttore costruisce lo stato \niniziale di un nuovo oggetto\n—riceve informazioni sullo stato iniziale da \ncostruire tramite i parametri\n—fissa i valori iniziali delle variabili di istanza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#40": "Programmazione orientata agli oggetti\nAssegnazione di Riferimenti (1)\n●L'assegnazione di una variabile che contiene \nun riferimento copia il riferimento\n—NON si crea un nuovo oggetto \nPunto unitario = new Punto(1, 1);\nPunto copia = unitario;\nStack0xef11d34f:\nHeap0x15db9742 :Punto\n    x\n1     y1\n0xef11d381:0x15db9742\n0x15db9742unitario\ncopia",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#41": "Programmazione orientata agli oggetti\nAssegnazione di Riferimenti (2)\n●unitario  e copia sono due variabili locali distinte\n●Possiedono valori in posizioni distinte sullo S tack…\n—0xef11d34f  e 0xef11d381\n...ma fanno riferimento allo stesso oggetto dentro \nl’heap, quello di indirizzo\n—0x15db9742\nHeap0x15db9742 :Punto\n    x\n1     y1\n0xef11d34f:\n0xef11d381:0x15db9742\n0x15db9742unitario\ncopia\nStack",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#42": "Programmazione orientata agli oggetti\nMolteplici Riferimenti Stesso Oggetto\n●Se due variabili contengono un riferimento \nal medesimo oggetto, ogni modifica operata \nad un oggetto tramite un riferimento è \nanche visibile tramite l’altro riferimento\n●Ad esempio:\nPunto unitario = new Punto(1, 1);\nPunto copia = unitario;\nSystem.out.println( copia.getX()); // Stampa 1\ncopia.setX( 2);\nSystem.out.println( unitario .getX()); // Stampa 2",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#43": "Programmazione orientata agli oggetti\nSide-Effect e Metodi  \n●I metodi che aggiornano lo stato di oggetti di \ncui ricevono un riferimento (tra i parametri) \nproducono effetti collaterali ( side-effect ) \npublic class ModificatoreDiPunti {\npublic void azzera(Punto p) {\np.setX( 0); p.setY(0);\n}\n}\n…\npublic static void main(String[] args) {\n  ModificatoreDiPunti m = new ModificatoreDiPunti();\n  Punto unitario = new Punto( 1, 1);\n  m.azzera(unitario);\n  System.out.println(unitario.getX()); // Stampa 0\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#44": "Programmazione orientata agli oggetti\nEquivalenza tra Oggetti  vs \nIdenticità dei Riferimenti\nPunto origine = new Punto(0, 0);\nPunto zeroZero = new Punto(0, 0);\nif (origine == zeroZero)\nSystem.out.println(“uguali”);\nelse\nSystem.out.println(“diversi”);\n●Stamperà “ uguali ” o “diversi ”?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#45": "Programmazione orientata agli oggetti\nIdenticità dei Riferimenti (1)\n●L'operatore == verifica se il contenuto delle \ndue variabili è identico\n—origine  e zeroZero  contengono però \nriferimenti diversi verso due oggetti distinti \n(con diversi indirizzi di memoria)\n—Sono quindi considerati diversi\nStack0x15db76fa origine:\nHeap0x15db76fa :Punto\n    x\n0     y0\nzeroZero: 0x15dbb7c2 :Punto\n    x\n0     y00x15dbb7c2",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#46": "Programmazione orientata agli oggetti\nIdenticità dei Riferimenti (2)\n●L'operatore ==, quando applicato a variabili che contengono \nriferimenti ad oggetti, verifica se i riferimenti raggiungono lo \nstesso oggetto\nPunto origine = new Punto(0, 0);\nPunto copiaDelRif = origine;\nif (origine == copiaDelRif)\nSystem.out.println(“uguali”);\nelse\nSystem.out.println(“diversi”);\n●Può essere applicato anche ad espressioni di tutti i tipi \nprimitivi, con la semantica più naturale: identicità di valori\n●Invece, per verificare se due oggetti distinti sono equivalenti \nin base al loro stato ( non in base ai riferimenti) è necessario \nimplementare un criterio di equivalenza  tra oggetti di un \ncerto tipo (>> )",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#47": "Programmazione orientata agli oggetti\nEquivalenza fra Stringhe\n●Molte classi offrono un metodo equals()  che definisce un \ncriterio di equivalenza basato sullo stato\n—verifica l'uguaglianza con un altro oggetto passato come \nparametro \n—segnatura niente affatto scontata (>>)\n●Per il momento conviene vedere esempi di criteri di equivalenza \ngià definiti nelle librerie standard, come per la classe String\n●In Java le stringhe sono oggetti a tutti gli effetti\n—è possibile confrontarle con il loro metodo equals()\n—equivalenza carattere-per-carattere\nString nome1 = new String(“alice”);\nString nome2 = new String(“alice”);\nSystem.out.println(nome1 == nome2);      // Stampa: false\nSystem.out.println(nome1. equals(nome2)); // Stampa: true",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#48": "Programmazione orientata agli oggetti\nMetodo equals()\n●Il metodo equals()  si usa quando si vuole \nverificare l’ equivalenza  tra due oggetti distinti \nsecondo un criterio definito dal programmatore \n(e dipendente dalla classe)\n—In effetti vedremo che il metodo equals()  è \nofferto, sempre,  da tutte le classi (>>)\n—Ma se non viene esplicitamente implementato \n(>>) ha la stessa semantica dell’operatore == \nper il confronto tra riferimenti\n●Se lo scopo è quello di controllare se due \nvariabili fanno riferimento allo stesso oggetto \nmeglio usare, sempre e comunque, il più \nesplicito operatore ==",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#49": "Programmazione orientata agli oggetti\nUguaglianza con null\n●L’uguaglianza di un riferimento ad oggetti con il \nvalore null si verifica tramite l’operatore ==\n—null è un valore speciale che può essere utilizzato \nper rapprensentare l’assenza di un oggetto (di un \nqualsiasi tipo )\n●Tipica istruzione condizionale:\nif (varRif!=null) {\n  <operazioni su varRif non null>\n  }\n●Attenzione: valutando varRif.equals(null)  si \nottiene\n—false se varRif!=null\n—NullPointerException  se varRif==null",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#5": "Programmazione orientata agli oggetti\nSintassi dei Costruttori\n●I costruttori si denotano con lo stesso nome della \nclasse in cui compaiono\n●A differenza dei metodi, NON possono né devono \nspecificare il tipo del valore restituito\n—N.B. NON è nemmeno possibile usare void\npublic class Punto {\nprivate int x;\nprivate int y;\npublic Punto(int x, int y)  {\n  this.x = x;\n  this.y = y;\n}\n}\n✔N.B. I costruttori NON sono niente affatto metodi\n—Le differenze non sono limitate al solo tipo restituito",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#50": "Programmazione orientata agli oggetti\nNull Simmetria dell’Equivalenza\n✔  Quindi: a.equals(b)  non è, in generale, pari a \nb.equals(a)\n●Comportamento atteso se a e b possono anche assumere \nvalori null\n●Se invece sia a sia b sono\n●non nulle\n●riferimenti ad oggetti dello stesso tipo\n●È necessario che valgano le proprietà tipiche delle \nrelazioni di equivalenza, come: \n–a.equals(a)                   ( riflessiva )  \n–( a.equals(b) ) == ( b.equals(a) )  ( simmetrica )\n–(altre seguiranno >>) ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#51": "Programmazione orientata agli oggetti\nRiferimenti  in Java vs Puntatori  C\n●Nel linguaggio C il concetto più simile a quello di \nriferimento Java è sicuramente quello di puntatore \nad una cella di memoria\n●N.B. sono e rimangono comunque concetti molto \ndiversi, i cui costrutti, nei due linguaggi, \nsupportano un insieme di operazioni ben diverse\n—In C tale insieme di operazioni è molto più ampio\n—In Java non esiste l'aritmetica dei puntatori come in \nC o C++\n—In Java non è possibile referenziare nulla che non \nsia un oggetto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#52": "Programmazione orientata agli oggetti\nDiagnostica: Riferimenti  vs Puntatori\n●Per colpa di un errore di programmazione, spesso si \nfinisce per produrre puntatori “impazziti” in C\n—Questi a loro voltano, causano errori a tempo di esec.\n—In buona parte i puntatori sono alla base della \npessima diagnostica a tempo di esecuzione: non si \npuò prevedere a cosa si finisce per accedere\n✔risulta difficile risalire alla vera causa di un problema se \ngran parte degli errori viene diagnosticato con un \ngenerico Segmentation fault !",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#53": "Programmazione orientata agli oggetti\nRiferimenti  vs Puntatori\n●In Java i riferimenti sono stati progettati proprio per \nsuperare i limiti ed i difetti tipici dei puntatori in C\n●In Java è stato deciso di evitare alla radice i rischi \ntipici dei puntatori\n—Non è possibile referenziare nulla che non sia un \noggetto\n●Si “nasconde” al programmatore l'esistenza stessa \ndel concetto di puntatore sostituendolo con il ben \npiù “sicuro” riferimento ad oggetto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#54": "Programmazione orientata agli oggetti\nGestione della Memoria\n●In Java l’operatore new (che serve ad allocare \noggetti) è il costrutto più simile alla funzione (C) \nmalloc\n—entrambi allocano un’area di memoria nell’heap\n●Ad ogni chiamata della funzione malloc  deve però \ncorrispondere una chiamata ad una funzione free \nper deallocare l’area di memoria occupata \nnell’heap e restituirla ai successivi utilizzi\n●In Java NON è necessario\n—finora non è stata scritta neanche una operazione \nper liberare la memoria occupata con le new\n●La deallocazione della memoria a carico della JVM\n—specificatamente di un componente, il  Garbage Collector",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#55": "Programmazione orientata agli oggetti\nDeallocazione della Memoria in C\n●Deallocazione della memoria a carico del \nprogrammatore\n●Possibili errori che ne conseguono, ad es.:\n—malloc senza corrispondente free\nMemory leak : perdita di memoria utilizzabile\n—free senza porre il puntatore a NULL\nDangling Pointer : rimane disponibile un puntatore verso \nun’area di memoria non più disponibile\n●N.B. è possibile avere la stessa problematica ancor più \nsemplicemente (senza malloc) anche creando puntatori ad  \naree di memoria sullo stack non più in uso!\nstruct Punto *origine;\norigine = malloc(sizeof(struct Punto));\nfree(origine);\norigine->x = 0; // Comportamento indefinito ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#56": "Programmazione orientata agli oggetti\nDeallocazione della Memoria in Java\n●La piattaforma Java solleva il \nprogrammatore dalla responsabilità di \ndeallocare esplicitamente la memoria\n—La JVM si occupa, a tempo di esecuzione, di \ntrovare gli oggetti inutilizzati e non più \nutilizzabili e recupera la loro memoria \n—Il componente della JVM che si occupa di \nquesto compito si chiama Garbage Collector\n●Alcuni esperti ritengono che questa sia la \ndifferenza tra i linguaggi C/Java che da sola, ed in \nassoluto, contribuisce in maggiore misura \nall’incremento di produttività dei programmatori \nJava",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#57": "Programmazione orientata agli oggetti\nAttenzione!\n●Per prevenire fraintendimenti, meglio precisare \nsubito che la gestione della memoria, ed un suo \nutilizzo appropriato, rimane comunque tra le più \nimportanti responsabilità anche di un \nprogrammatore Java\n●Il fatto che non è necessario fare esplicitamente le \nchiamate alla free non significa affatto che non si \nstia occupando memoria!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#58": "Programmazione orientata agli oggetti\nIl Garbage Collector (1)\n●Il ruolo del garbage collector è quello di identificare \ngli oggetti non utilizzati e non più utilizzabili\n—Questi oggetti devono essere marcati come \nreclamabili\n—la loro memoria recuperata per fare spazio \nall’allocazione di nuovi oggetti\n●Come può succedere che un oggetto diventa \nreclamabile ? Semplice:\nPunto origine = new Punto(0,0); // occupa mem.\norigine = null;\n// da qui in poi la memoria dell’oggetto può essere \nrecuperata perché l’oggetto appena creato non è più \nraggiungibile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#59": "Programmazione orientata agli oggetti\n●Dal punto di vista del Garbage Collector, un \noggetto può essere:\n—IN_USO : raggiungibile tramite una catena di \nriferimenti  \n—RECLAMABILE : oggetto non più raggiungibile, non \nesiste alcuna catena di riferimenti  che vi arrivi \n●Catena di riferimenti: sequenza di riferimenti che \nparte dallo stato dell’esecuzione corrente e \nconduce ad un oggetto\n—eventualmente passando attraverso diversi \noggetti intermedi \n●Come di creano queste catene?\n●Da dove iniziano? Il Garbage Collector (2)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#6": "Programmazione orientata agli oggetti\nInvocazione dei Costruttori\n●Attraverso l’operatore new è possibile invocare \nil costruttore e specificare degli argomenti\n●Ad esempio:\nPunto origine = new Punto(0, 0) ;\nSystem.out.println(origine.getX()); // Stampa 0Parametri attuali del costruttore",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#60": "Programmazione orientata agli oggetti\nIl Garbage Collector: \nCatene di Riferimenti (1)\n●I riferimenti costituiscono un grafo orientato\n—Si pensi ad una possibile classe Persona\n:Persona\nnome\nfiglio“Adamo”\npadre:Persona\nnome\nfiglio“Caino”\npadre\n:Persona\nnome\nfiglio“Enoch”\npadre",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#61": "Programmazione orientata agli oggetti\nHeapIl Garbage Collector: \nCatene di Riferimenti (2)\n●Se la prima persona ( Adamo ) è raggiungibile \ntutte le altre mostrate nello schema sono allora \nraggiungibili (indirettamente tramite la prima)\n●Riferimento iniziale  (>>):\n●Se la prima persona non fosse raggiungibile,\nnessuna delle altre lo sarebbe \n:Persona\nnome\nfiglio“Adamo”\npadre:Persona\nnome\nfiglio“Caino”\npadre\n:Persona\nnome\nfiglio“Enoch”\npadreadamo\nStack",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#62": "Programmazione orientata agli oggetti\nIl Garbage Collector: \nCatene di Riferimenti (3)\n●Per interrompere una catena di riferimenti è  \nsufficiente porre a null una variabile (di istanza o \nlocale) che contiene il riferimento ad un oggetto\nadamo.setFiglio(null);\n:Persona\nnome\nfiglio“Adamo”\npadre:Persona\nnome\nfiglio“Caino”\npadre\n:Persona\nnome\nfiglio“Enoch”\npadreX\n●Adesso, Caino e \nEnoch non saranno\npiù raggiungibili e\nquindi la loro memoria\npuò essere recuperataadamo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#63": "Programmazione orientata agli oggetti\nIl Garbage Collector: \nCatene di Riferimenti (4)\n●In questo modo il Garbage Collector rileva che \ngli oggetti Caino ed Enoch  non saranno più \nutilizzati\n—verranno marcati come reclamabili\n●Mettendo a null il riferimento verso Caino non \nè più possibile raggiungerli\n:Persona\nnome\nfigli\np“Adamo”\npadre:Persona\nnome\nfigli\no“Caino”\npadre\n:Persona\nnome\nfigli\no“Enoch”\npadreX\n—Quando il Garbage\nCollector entrerà in\nazione marcherà la loro\nmemoria come reclamabile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#64": "Programmazione orientata agli oggetti\nIl Garbage Collector: \nCatene di Riferimenti (5)\n●Un oggetto può diventare irraggiungibile anche \nsovrascrivendo  un riferimento\nPersona abele = new Persona(“Abele”);\nadamo.setFiglio(abele);\n:Persona\nnome\nfiglio“Adamo”\npadre:Persona\nnome\nfiglio“Caino”\npadre\n:Persona\nnome\nfiglio“Enoch”\npadreX\n:Persona\nnome\nfiglio“Abele”\npadre●Enoch e Caino \nnon sono più \nraggiungibili, la \nloro memoria \nverrà marcata \ncome reclamabile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#65": "Programmazione orientata agli oggetti\nInizio delle Catene di Riferimenti\n●Sinora si supponeva che il primo oggetto \nAdamo  fosse raggiungibile\n—è necessario avere un primo riferimento per \niniziare le catene\n—sia gli oggetti sia le variabili di istanza \ncontenenti i riferimenti si trovano nell’heap\n●Quali oggetti sono però raggiungibili da fuori \ndell’heap, e con quali riferimenti iniziali?\n—Quelli referenziati dalle variabili locali o dai \nparametri dei metodi i cui RDA sono nello \nStack",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#66": "Programmazione orientata agli oggetti\nGarbage Collection (1)\n●L’algoritmo lavora inseguendo i riferimenti a \ncominciare da quelli contenuti dentro i RDA (var. \nlocali e parametri attuali) nello Stack \n—esplora il grafo dei riferimenti dentro l’Heap\n—continua con quelli conservati nelle var. di istanza \ndegli oggetti conservati nell’ Heap\n—se un oggetto è \n●Raggiungibile : allora viene mantenuto in memoria\n●Non raggiungibile : la sua memoria viene marcata \nreclamabile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#67": "Programmazione orientata agli oggetti\nGarbage Collection (2)\n●Successivamente  (quando serve memoria e non \nnecessariamente subito) la memoria reclamabile \nviene recuperata per poter allocare nuovi oggetti\n●Abbiamo discusso solo i problemi ed i concetti \nbasilari di una semplice implementazione \ndell’Algoritmo di Garbage Collection\n—Enorme interesse per questi algoritmi\n—Quelli utilizzati sono il risultato di decadi di ricerca \n(accademica ed industriale)\n—Una delle classi di algoritmi più studiate in assoluto: \nsempre più veloci ed ottimizzati",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#68": "Programmazione orientata agli oggetti\nGarbage Collection:\nVantaggi e Svantaggi\n●Facilitano lo sviluppo rapido di applicazioni\n●Tuttavia esistono alcuni importanti svantaggi:\n—La visita del grafo dei riferimenti è un’operazione \nonerosa che limita le prestazioni\n—Il garbage collector è fuori dal controllo diretto del \nprogrammatore\n●può intervenire in momenti non facilmente \nprevedibili e talvolta inopportuni\n●non adatto in tutte le situazioni in cui i tempi di \nrisposta devono essere prevedibili con certezza\n●Un programmatore esperto può sicuramente ottenere \nprestazioni migliori gestendo la memoria \nmanualmente ma specificatamente per la sua \napplicazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#7": "Programmazione orientata agli oggetti\nOggetti in Corso di Costruzione\n●Il corpo del costruttore è adibito alla costruzione \ndello stato iniziale dell’oggetto creato\n●Dal momento dell’invocazione dell’operatore  per \nla creazione  di un nuovo oggetto\n—new Punto()\nal momento in cui l’esecuzione del corpo del \ncostruttore è completamente terminata, \nl’inizializzazione dello stato non è completa\n●Durante la costruzione l'oggetto transita per una \nserie di stati intermedi, ovvero è inconsistente\n—è buona regola  non farsi mai “sfuggire” riferimenti ad \noggetti in uno stato potenzialmente inconsistente (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#8": "Programmazione orientata agli oggetti\nEsempio (con Eclipse)\n●Implementare il costruttore della classe \nRettangolo\n—I parametri servono a specificare\n●vertice\n●base\n●altezza\ndell’oggetto appena creato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-04-gestione-memoria.pdf#9": "Programmazione orientata agli oggetti\nEsempio  (2)\npublic class Rettangolo {\n private Punto vertice;\n private int base;\n private int altezza;\n public Rettangolo(Punto v, int base, int altezza) {\n  this.vertice = v;\n  this.base = base;\n  this.altezza = altezza;\n }\n// soliti getter\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#0": "Programmazione\nOrientata agli Oggetti\nOverloading, Costruttori, \nStringhe e Array",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#1": "Programmazione orientata agli oggetti\nSommario\nOverloading\nCostruttore primario e costruttori secondari\nLa classe String\n–Il metodo  toString()\nDiagramma degli oggetti\nArray\nCostanti\nVariabili e metodi di classe",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#10": "Programmazione orientata agli oggetti\nEsempio (2)\npublic class Rettangolo {\n  private int base;\n  private int altezza;\n  // …\n  public void scala(float fattore) {\n     this.scala(fattore, fattore);\n  } \n   public void scala(float fattoreBase,\n                     float fattoreAltezza) {\n      this.base *= fattoreBase;\n      this.altezza *= fattoreAltezza;\n   }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#11": "Programmazione orientata agli oggetti\nEsempio (3)\n●Notare come il metodo\nscala(float fattore)\nfaccia uso del metodo\nscala(float fattoreBase, float fattoreAltezza)\n...per evitare la duplicazione del codice (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#12": "Programmazione orientata agli oggetti\nRisoluzione Metodi Sovraccarichi (1)\n●In assenza di una corrispondenza perfetta tra\n—lista dei parametri attuali di una invocazione\n—segnatura di un metodo\nil compilatore applica un algoritmo di risoluzione che \nprima di fermare la compilazione cerca di capire se \nsemplici conversioni di tipo permettono la chiamata\n—Ad es. int → float\n●L’algoritmo è intriso di dettagli per coprire l’ampia \ncasistica; in pratica basta quasi sempre limitarsi a \nricordare che:\n—sono ammesse semplici promozioni di tipo se necessarie \na rendere un’invocazione confacente con una delle \nsegnature disponibili\n—sono preferite le promozioni più “conservative”, ovvero \nquelle al tipo immediatamente più grande",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#13": "Programmazione orientata agli oggetti\nRisoluzione Metodi Sovraccarichi (2)\n•Più nel dettaglio:\n–Argomento di tipo intero\n•se esiste un metodo che prende come parametro \nformale int, allora viene usato quel metodo\n•altrimenti viene promosso al tipo di dato più piccolo tra \nquelli disponibili (ma “capaci di contenere” un int): \nlong, float, double\n–Argomento in virgola mobile\n•Si cerca il metodo che ha come parametro formale un \ndouble\n–Argomento di tipo carattere\n•Se non trova una corrispondenza con char, si prova la \npromozione a int",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#14": "Programmazione orientata agli oggetti\nRisoluzione Metodi: Esempio\nMotivo di confusione è la promozione implicita  di tipo \nper i tipi primitivi. Ad esempio:\npublic class Prova {\n   void f(long i)   { System.out.println(\"long\");   }\n   void f(float i)  { System.out.println(\"float\");  }\n   void f(double i) { System.out.println(\"double\"); }\n   public static void main(String[] args) {\n     f(5); // ???\n   }\n}\npublic class Prova {\n   void f(long i)   { System.out.println(\"long\");   }\n   void f(float f)   { System.out.println(\"float\");   }\n   void f(double i) { System.out.println(\"double\"); }\n   public static void main(String[] args) {\n     f(5);  // ???\n     f(5f); // ???\n   }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#15": "Programmazione orientata agli oggetti\nCon Riferimenti: Cosa Stampa?\npublic class Boo {\n  void f(String n) {\n    System.out.println(\"stringa\");\n  }\n  void f(int n) {\n    System.out.println(\"intero\");\n  }\n  public static void main(String[] args) {\n    Boo b = new Boo();\n    String s = new String(\"pppp\");\n    int i = 0;\n    b.f(s);\n    b.f(i);\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#16": "Programmazione orientata agli oggetti\nOverloading di Operatori\n●L'overloading è una caratteristica comoda ma usata in \nmaniera decisamente controllata in Java\n●Anche l'operatore + è sovraccarico: così come già accade \nin C, risulta più agevole la manipolazione di tipi numerici\n—Somma interi ( int, long)\n—Somma numeri in virgola mobile ( float, double)\n●Invece specifica del linguaggio Java è la sua applicazione \nad una classe con un supporto particolare: String \n—Concatenazione di stringhe; Ad es.:\n—System.out.println(\"Ciao\"+\" \"+\"Mondo \");\n●In C++ (ed in Scala) c’è anche la possibilità di \nsovraccaricare tutti gli altri operatori, in Java \n(fortunatamente) NO\n—per il pessimo rapporto costi/benefici",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#17": "Programmazione orientata agli oggetti\nCostruttori ed Overloading\n●Anche i costruttori possono essere sovraccarichi\n●Ad esempio la classe Rettangolo  potrebbe \navere i seguenti costruttori\n—Un costruttore no-args . Quindi\n●base = 0; altezza = 0, vertice = (0, 0)\n—Un costruttore che ha come parametri base e \naltezza. Quindi:\n●vertice = (0, 0)\n—Un costruttore generico\n●vertice\n●base\n●altezza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#18": "Programmazione orientata agli oggetti\nEsempio ( con Eclipse )\n●Realizzare i costruttori della classe Rettangolo  \nappena descritti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#19": "Programmazione orientata agli oggetti\nEsempio (2)\npublic class Rettangolo {\n  private int altezza;\n  private int base;\n  private Punto vertice;\n  public Rettangolo(Punto vert, int base, int altezza) {\n    this.vertice = vert;\n    this.base = base;\n    this.altezza = altezza;\n  }\n  public Rettangolo(int base, int altezza) {\n    this.vertice = new Punto(0, 0);\n    this.base = base;\n    this.altezza = altezza;\n  }\n  public Rettangolo() {\n    this.vertice = new Punto(0, 0);\n    this.base = 0;\n    this.altezza = 0;\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#2": "Programmazione orientata agli oggetti\nOverloading (1)\n●Caratteristica che permette ad un linguaggio di \nprogrammazione di definire molteplici \nmetodi/funzioni/procedure con lo stesso nome \n●Una classe Java può ospitare due o più versioni  \ndello stesso metodo (ovvero, con lo stesso nome)\n●T ale metodo si dice sovraccarico\n●Le versioni sovraccariche dello stesso metodo \ndevono comunque risultare distinguibili\n●Devono differire per la lista di parametri formali\n—per numero di parametri, e/o\n—per il tipo di uno o più parametri, e/o\n—per l’ordine dei parametri",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#20": "Programmazione orientata agli oggetti\nEsempio (3)\npublic class MainOverloading {\n  public static void main(String[] args) {\n   // base 0, altezza 0, vertice in (0, 0) \nRettangolo r1 = new Rettangolo();\n// base 3, altezza 5, vertice in (0, 0)\nRettangolo r2 = new Rettangolo(3, 5);\nPunto vertice = new Punto(4, 9);\nRettangolo r3 = new Rettangolo(vertice, 3, 5);\n  }\n}\n ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#21": "Programmazione orientata agli oggetti\nCostruttore “Primario”\n●Costruttori definiti ripetendo molto codice,  come \nappena fatto sono... sconsigliabili!\n●Le ripetizioni nel codice causano problemi (>>)\n●Meglio eleggere un costruttore al ruolo di \n“primario” , il più generico possibile\n●Eccolo per la classe Rettangolo :\npublic Rettangolo(Punto vertice, int base, int altezza) {\nthis.vertice = vertice;\nthis.base = base;\nthis.altezza = altezza;\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#22": "Programmazione orientata agli oggetti\nCostruttori “Secondari”\n●A questo punto, gli altri costruttori ( “secondari” ) \npossono affidarsi a quello più generico\n●Per invocare un costruttore da un costruttore:\nthis(<lista di argomenti>)\n—L'invocazione di un altro costruttore deve essere la \nprima istruzione  nel corpo del costruttore secondario\n●Altrimenti: errore di compilazione\n●I costruttori secondari finiscono per fissare dei \nfissare dei valori predefiniti per tutti i parametri \nche non ricevono esplicitamente\n●In Scala esiste una sintassi apposita per \ndistinguere costruttori primari  dai secondari , in \nJava NO",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#23": "Programmazione orientata agli oggetti\nCostruttori Chiamati dai Costruttori\npublic class Rettangolo {\n  private int altezza;\n  private int base;\n  private Punto vertice;\n  public Rettangolo(Punto vert, int base, int altezza) {\n    this.vertice = vert;\n    this.base = base;\n    this.altezza = altezza;\n  }\n  public Rettangolo(int base, int altezza) {\n    this(new Punto(0, 0), base, altezza);\n  }\n  public Rettangolo() {\n    this(new Punto(0, 0), 0, 0);\n  }\n}Crea un nuovo \noggetto Punto ed \ninvoca un altro \ncostruttore entro \nun’unica (la prima) \nriga di codice",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#24": "Programmazione orientata agli oggetti\nLa Classe String (1) \n●In Java esiste la classe String  per rappresentare \nsequenze di caratteri immutabili\n—Le stringhe sono oggetti, String è la classe a cui \nappartengono le sue istanze\n●Una variabile dichiarata di tipo String  contiene \nquindi un riferimento  ad un oggetto istanza della \nclasse String\n●String favorita = new String( \"Sono la favorita \"!);\n●Pur essendo una classe come tutte le altre, spesso \nsi ha la tentazione di pensare che non lo sia \naffatto…\n●Il motivo è che ha un supporto molto particolare sia \nnel linguaggio che da parte del compilatore",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#25": "Programmazione orientata agli oggetti\nLa Classe String (2)\n●Per rendere il linguaggio più semplice ed attraente per nuovi \nsviluppatori, all’epoca si decisero trattamenti decisamente \n“di favore” per questa classe\n●Prima di Java 5 era (>>)\n—l’unica classe che possiede dei letterali appositi\n—l’unico tipo di oggetto che si può creare senza fare una new \nesplicita\n●String favorita = \"Sono la favorita! \";\nequivale a \n●String favorita = new String( \"Sono la favorita! \");\n●L’inserimento dell’invocazione della new è operata \ndirettamente dal compilatore\n●Anche il fatto che l’operatore + sia sovraccarico per gestire \nla concatenazione conferma il trattamento di favore…\n●Sicuramente tutto questo “opacizza” il codice (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#26": "Programmazione orientata agli oggetti\nEquivalenza di Stringhe (1)\n●Dato che le stringhe sono oggetti veri e propri, \nl’equivalenza deve essere valutata mediante il  \nmetodo appositamente previsto dalla classe equals()\nInfatti:\n—String nome = new String( \"Alice\");\n—String omonimo = new String( \"Alice\");\nSystem.out.println(nome == omonimo);\n// Stampa false",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#27": "Programmazione orientata agli oggetti\nEquivalenza di Stringhe (2)\nString nome = new String(…);\n:String oggetto\nString omonimo = new String(…);\n:String●Si sta in realtà verificando che i riferimenti  risultino \nidentici\n—falso: nome e omonimo  contengono riferimenti diversi, \novvero restituiti da due distinte invocazioni della new\n“Alice”“Alice”\noggetto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#28": "Programmazione orientata agli oggetti\nEquivalenza di Stringhe (3)\n●Invece, utilizzando:\nString nome = new String(“Alice”);\nString omonimo = new String(“Alice”);\nSystem.out.println(nome.equals(omonimo)); \n// stampa true\nil metodo equals()  controllerà l’equivalenza della \nsequenza di caratteri che compone le stringhe, \ncarattere per carattere ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#29": "Programmazione orientata agli oggetti\nImmutabilità della Classe String\n●Gli oggetti String  usati per rappresentare \nstringhe in Java sono immutabili\n—Non è possibile modificare i caratteri all’interno di \nuna stringa una volta creata\n—Non si possono aggiornare: bisogna sempre crearne \ndi nuove per memorizzare la modifica\n●Ad esempio per concatenare due stringhe:\nString s1 = “ciao ”; // = new String(“ciao ”);\nString s2 = “mondo”; // = new String (“mondo”);\ns1 = s1 + s2; // → Si sta creando un nuovo oggetto  \n  // String  e si sta sovrascrivendo il vecchio \nriferimento\nSystem.out.println(s1); // ciao mondo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#3": "Programmazione orientata agli oggetti\nOverloading (2)\n●La classe Sommatore  può avere più metodi di stesso nome \nper sommare due interi o tre interi\npublic class Sommatore {\npublic int add(int a, int b)  { return a + b; }\npublic int add(int a, int b, int c)  {\nreturn a + b + c;\n}\n}\n●Il tipo e l’ordine dei parametri è significativo:\npublic double add(int a, double b)  {\nreturn a + b;\n}\n… è un metodo distinto da:\npublic double add(double a, int b)  {\nreturn a + b;\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#30": "Programmazione orientata agli oggetti\nMetodi della Classe String (1)\n●String s = \"POO \";\n—Lunghezza della stringa\ns.length();  restituisce 3\n—Ottenere un carattere in una certa posizione\ns.charAt(0);  restituisce ‘ P’ \n●Indicizzazione base 0\n●come per gli array il primo carattere ha indice 0",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#31": "Programmazione orientata agli oggetti\nMetodi della Classe String (2)\n●String s = \"una stringa \";\n—Indice di un carattere\ns.indexOf(‘s’);  restituisce 4\n—Indice di una stringa\n—s.indexOf( \"ring\"); restituisce 6\nIl metodo indexOf()  è sovraccarico\n—Restituisce -1 se non trova il carattere o la stringa \ncercata\ns.indexOf(‘z’);  restituisce -1",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#32": "Programmazione orientata agli oggetti\nMetodi della Classe String (3)\n●String s = \"una stringa \";\n—Rimpiazzare caratteri\n●String s2 = s.replace( \"stringa \", \"stringa lunga \");\nSystem.out.println(s);  // Stampa ‘una stringa’\nSystem.out.println(s2); // Stampa ‘una stringa lunga’\n—Le stringhe sono immutabili: in realtà si crea un \nnuovo oggetto String  il cui riferimento finisce in s2\n—molti e molti altri metodi ancora….\nhttps://docs.oracle.com/javase/7/docs/api/java/lang/String.html",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#33": "Programmazione orientata agli oggetti\nStampare Descrizioni di Oggetti\n●Se si esegue  println() su un riferimento ad un \noggetto verrà stampato il valore di tale riferimento\n// un attrezzo di nome spada e peso 7 kg\n●Attrezzo spada = new Attrezzo( \"spada\", 7);\nSystem.out.println(spada);\n●Stampa, ad es.:  Attrezzo@70dea4e\n—Non descrive affatto il contenuto e lo stato dell’oggetto, \nma il riferimento",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#34": "Programmazione orientata agli oggetti\nIl Metodo toString()  (1)\n●È possibile cambiare questo comportamento \nimplementando di segnatura:\n   public String toString()\nper tutte le classi in cui si vuole specificare come \ntrasformare gli oggetti in stringhe\n●Il metodo toString()  restituisce la \nrappresentazione dell’oggetto sotto forma di \nstringa\n●Addirittura fondamentale per rendere agevole il \ndebugging  ed il tracing",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#35": "Programmazione orientata agli oggetti\nIl Metodo toString()  (2)\n●Ad esempio in Attrezzo\npublic class Attrezzo {\n  private String nome;\n  private int peso;\n  public Attrezzo(String nome, int peso) {\nthis.nome = nome; this.peso = peso;\n  }\n  // getter\n  public String toString()  {\nreturn \"Attrezzo di nome \"+ this.getNome() + \n       \". Peso: \"+ this.getPeso();\n  }\n}  ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#36": "Programmazione orientata agli oggetti\nIl Metodo toString()  (3)\npublic class MainToString {\npublic static void main(String[] args) {\n●Attrezzo spada = new Attrezzo(\"spada \", 7);\nSystem.out.println( spada);\n}\n}\n●Stampa\nAttrezzo di nome spada. Peso: 7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#37": "Programmazione orientata agli oggetti\nIl Metodo toString()  (4)\n●Stesso comportamento concatenando un \nriferimento ad un oggetto con un  letterale \nstringa:\npublic class MainToString {\n  public static void main(String[] args) {\n    Attrezzo spada = new Attrezzo( \"spada\", 7);\n    String descr = \"attrezzo posseduto: \" + spada;\nSystem.out.println(descr);\n}\n}\nStampa:\n  attrezzo posseduto: Attrezzo di nome spada. Peso: 7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#38": "Programmazione orientata agli oggetti\nprintln() & toString()\n●Il metodo toString()  (come equals() ), anche \nè  presente in tutti gli oggetti che creiamo (>>)\n●Tuttavia:\n—Se non esplicitamente implementato stampa [un \nnumero che dipende dal]l’indirizzo di memoria \ndell’oggetto su cui è invocato\n—L’invocazione del metodo  toString() , è inserita \ndirettamente dal compilatore, nelle istruzioni di \nstampa (anche se non esplicitamente richiesta!):\n●System.out.println(oggRef)  \nstampa il risultato di oggRef.toString()  ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#39": "Programmazione orientata agli oggetti\nDiagramma delle Classi\n•Diagrammi UML\n–Ne esistono diversi tipi\n•Sono un utile e comodo strumento a supporto (e non in \nsostituzione) della comunicazione\n–Tempo dinamico: abbiamo già utilizzato \nrappresentazioni diagrammatiche degli oggetti\n–Tempo statico: diagrammi delle classi\n•Il diagramma delle classi illustra le caratteristiche \nprincipali (variabili di istanza, costruttori e metodi) delle \nclassi che compongono la applicazione\n•L’enfasi è sulla relazione tra le classi, quindi sugli \naspetti “statici”",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#4": "Programmazione orientata agli oggetti\nOverloading: in C?\n●Il linguaggio C non supporta l’overloading\n—Non si possono definire due funzioni con lo stesso nome\n●A conferma che questa possibilità offerta dal linguaggio \nJava è avvertita come una viva esigenza da molti \nprogrammatori, basta osservare la convenzione adottata da \nmolti programmatori C per gestire situazioni simili, ovvero:\n—double add_id(int a, double b)\n—double add_di(double a, int b)\n✔I nomi sono resi sintatticamente diversi, con un prefisso \ncomune che rende evidente l’esistenza di una radice \ncomune",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#40": "Programmazione orientata agli oggetti\nEsempio\n•Supponiamo\n   public class Stanza {\n   private String nome;\n   private Attrezzo attrezzo;\n   public Stanza(String nome) {\n    this.nome = nome;\n   }   \n   \n   public void setAttrezzo(Attrezzo attrezzo) {\n   this.attrezzo = attrezzo;\n   }\n   … altri metodi\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#41": "Programmazione orientata agli oggetti\nDiagramma delle Classi: Esempio\nStanza\nString \nAttrezzo\n \nPer modellare le adiacenze",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#42": "Programmazione orientata agli oggetti\nDiagramma degli Oggetti (1)\n•Per avere un’idea della evoluzione di un \nprogramma è utile rappresentare lo stato delle \nistanze: a tal fine usiamo una rappresentazione \ngrafica, chiamata diagramma degli oggetti\n•Il diagramma degli oggetti mostra gli oggetti \nistanziati in memoria durante l’esecuzione \ndell’applicazione \n•L’enfasi è sullo stato interno degli oggetti e sugli \naspetti dinamici\n–ogni oggetto ha un indirizzo di memoria\n–le variabili di tipo riferimento ad oggetto \nmemorizzano l'indirizzo dell'oggetto referenziato\n–una rappresentazione grafica efficace basata sull’uso \ndi frecce...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#43": "Programmazione orientata agli oggetti\nEsempio\n…\n       public static void main(String[] args) {\n       Attrezzo spada = new Attrezzo(\"spada\", 10);\n       Stanza n10 = new Stanza(\"Aula N10\");\n       n10.addAttrezzo(spada);\n       // < ---\n       /* disegnare il diagramma degli oggetti \n          in questo punto dell’esecuzione */\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#44": "Programmazione orientata agli oggetti\nDiagramma degli Oggetti: Esempio (1)\n@01:Stanza @21:String\n\"Aula N10\"\n@32:Attrezzo@21nome\nattrezzo\n…@32\n@67:String\n\"spada\"nome\npeso     10  @67",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#45": "Programmazione orientata agli oggetti\nDiagramma degli Oggetti (2)\n•Una rappresentazione grafica efficace dei \nvalori memorizzati nelle variabili riferimento \nprevede l'uso di frecce che collegano \n–la variabile riferimento \n–all'oggetto referenziato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#46": "Programmazione orientata agli oggetti\nDiagramma degli Oggetti: Esempio (2)\n:Stanza :String\n\"Aula N10\"\nnome\nattrezzo\n:Attrezzo\nnome\npeso     10  \n:String\n\"Spada\"",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#47": "Programmazione orientata agli oggetti\nTipi Primitivi e Oggetti\n•Dal diagramma degli oggetti notiamo che le \nvariabili (di istanza) possono memorizzare \n–tipi primitivi\n–riferimenti ad oggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#48": "Programmazione orientata agli oggetti\nTipi Primitivi in Java\nboolean  vero (true) o falso ( false)\nchar caratteri Unicode 2.1 (16-bit) \nbyte  interi a 8 bit (con segno e in C2)\nshort interi a 16 bit (con segno in C2)\nint interi a 32 bit (con segno in C2)\nlong interi a 64 bit (con segno in C2)\nfloat numeri in virgola mobile a 32-bit\ndouble  numeri in virgola mobile a 64-bit",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#49": "Programmazione orientata agli oggetti\nTipi Primitivi e Oggetti (1)\n32\nOggettoTipo primitivo\nStanza s;int i;\n:StanzaRiferimento ad Oggetto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#5": "Programmazione orientata agli oggetti\nRisoluzione Chiamate in Overloading\n●Per ciascuna invocazione di metodo (sovraccarico) \nsi confrontano:\n—tutte le segnature  del metodo, ed in particolare\n●tipo e numero dei parametri formali\n—con la lista dei parametri attuali\n●In presenza di una lista di parametri formali in \nperfetto accordo con la segnatura di un metodo \n(per numero e tipo di parametri), la scelta è \nsemplice:\n—Si utilizza la versione di un metodo sovraccarico \nche segue fedelmente il numero ed il tipo dei \nparametri attuali",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#50": "Programmazione orientata agli oggetti\nTipi Primitivi e Oggetti (2)\n•La variabile s non  memorizza un oggetto \n(nell'esempio, una istanza della classe \nStanza ), ma un riferimento all’oggettooggettoStanza s = new Stanza(…);\n:Stanza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#51": "Programmazione orientata agli oggetti\nTipi Primitivi e Oggetti (3)\n•Nel caso dei tipi primitivi, il valore è \nmemorizzato direttamente nella variabile32Tipo primitivoint i;",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#52": "Programmazione orientata agli oggetti\nTipi Primitivi e Oggetti (4)\n32Tipi primitiviMiaClasse a;\nint a;\nMiaClasse b;\n32int b;a = b;Oggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#53": "Programmazione orientata agli oggetti\nTipi Primitivi e Oggetti (5)\nint i1 = 0;\nint i2 = 5;\ni1 = i2;\ni1\ni20\n55",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#54": "Programmazione orientata agli oggetti\nString  nei Diagrammi ad Oggetti\n●Come il compilatore Java, anche noi \nriserviamo un trattamento di favore agli \noggetti istanza della classe String\n:Stanza\nnome\n:String\n\"Atrio\"\n:Stanza\nnome\"Atrio\"",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#55": "Programmazione orientata agli oggetti\nTipi Primitivi e Oggetti (6)\nStanza s1;\ns1 = new Stanza(\"atrio\");\nStanza s2;\ns2 = new Stanza(\"bar\");\ns1 = s2;\ns1\ns2:Stanza\nnome\n:Stanza\nnome\"Bar\"\"Atrio\"",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#56": "Programmazione orientata agli oggetti\nEsercizio\n•Disegnare il diagramma degli oggetti che \nrappresenta lo stato degli oggetti \nreferenziati dalle variabili a, b, c del \nseguente programma al termine della \nesecuzione della istruzione in linea 3\n1.Attrezzo a = new Attrezzo(\"spada\", 40);\n2.Attrezzo b = new Attrezzo(\"scudo\", 30);\n3.Attrezzo c = new Attrezzo(\"lancia\", 10);\n4.a = b;",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#57": "Programmazione orientata agli oggetti\nEsercizio\n•Disegnare il diagramma degli oggetti che \nrappresenta lo stato degli oggetti \nreferenziati dalle variabili a, b, c del \nseguente programma al termine della \nesecuzione della istruzione in linea 4\n1.Attrezzo a = new Attrezzo(\"spada\", 40);\n2.Attrezzo b = new Attrezzo(\"scudo\", 30);\n3.Attrezzo c = new Attrezzo(\"lancia\", 10);\n4.a = b;",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#58": "Programmazione orientata agli oggetti\nEsercizio\n•Disegnare il diagramma degli oggetti che \nrappresenta lo stato del seguente programma al \ntermine della esecuzione della istruzione in linea 5\n•Quale valore ha il peso dell'attrezzo che si trova \nnella stanza referenziata dalla variabile s al termine \ndella istruzione 4? E al termine della istruzione 5? \n1.Attrezzo a = new Attrezzo(\"spada\", 40);\n2.Attrezzo b = new Attrezzo(\"lancia\", 10);\n3.Stanza s = new Stanza(\"N10\");\n4.s.setAttrezzo(a);\n5.a = b;\n6.System.out.println(s.toString()); //quale attrezzo è \n7.                            //presente nella stanza s?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#59": "Programmazione orientata agli oggetti\nArray (1)\n•Un array definisce una struttura di dati che \nmemorizza un insieme di valori dello stesso tipo\n•Dichiarazione di una variabile array:\nint[] a;\n•L'oggetto array va creato, specificando il numero di \nelementi\na = new int[10];\n•Un array può essere inizializzato esplicitamente al \nmomento della creazione:\nint[] a = {21,12,23,34,15,21,7,80,1,-21};\n•In ogni caso verrà inizializzato con dei valori di \ndefault",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#6": "Programmazione orientata agli oggetti\nChi Risolve le Chiamate \nSovraccariche?\n•Quando viene operata la scelta?\n–a tempo statico (durante la compilazione ) \noppure \n–a tempo dinamico (durante l’ esecuzione ) \n•E’ il compilatore  a decidere  quale versione di un \nmetodo sovraccarico invocare\n–Già durante la compilazione (a tempo statico)\n–La scelta è definitiva, scritta nei . class generati\n–Operata esclusivamente sulla base dell’analisi dei \ntipi della lista di parametri attuali (nell’invocazione \ndi metodo) effettuata durante la compilazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#60": "Programmazione orientata agli oggetti\nArray (2)\n•Le sintassi \nint[] array  e int array[]\nsono equivalenti.\n•int[] array  forse più esplicita:\n–già dal prefisso della dichiarazione si capisce che la \nvariabile seguente è un array",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#61": "Programmazione orientata agli oggetti\nArray (3)\n•È possibile accedere a ciascun valore dell'array \nmediante un indice\nint[] a;\nint i;\na = new int[10];\ni = a[4];\na[6] = 3*i;\n•Attenzione: gli array in Java (come in C) usano base-0: \nl'indice del primo elemento è 0\n•Per avere la dimensione di un array: .length\n•Scansione degli elementi di un array:\nfor (int i = 0; i< a.length; i++)\nSystem.out.println(a[i]);",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#62": "Programmazione orientata agli oggetti\nArray (4)\n•A partire da Java 5 è stata introdotta una variante \ndel costrutto for (chiamata for-each ) che consente \ndi scorrere gli elementi di un array (e, come \nvedremo, anche di altre tipologie di collezioni) \nsenza gestire esplicitamente l'indice di iterazione:\nint a[];\na = new int[100];\nfor (int elemento : a)\nSystem.out.println(elemento);\nequivale a:\nfor (int i=0; i<a.length; i++)  {\nint elemento = a[i];\nSystem.out.println(elemento);\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#63": "Programmazione orientata agli oggetti\nArray: Diagramma degli Oggetti\nint[] a;\na = new int[10];\nint[]\na [0]\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9] 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0 0",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#64": "Programmazione orientata agli oggetti\nArray: Diagramma degli Oggetti\nint[] a;\na = new int[10];\na[0] = 5;\n:int[]\na [0]        5\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9] 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#65": "Programmazione orientata agli oggetti\nArray: Diagramma degli Oggetti\nAttrezzo attrezzi[];\nattrezzi = new Attrezzo[10];\n:Attrezzo[] attrezzi\n[0]\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#66": "Programmazione orientata agli oggetti\nArray: Diagramma degli Oggetti\nAttrezzo attrezzi[];\nattrezzi = new Attrezzo[10];\nattrezzi[0] = new Attrezzo(\"vite\",1);\nattrezzi[5] = new Attrezzo(\"dado\",2);\n:Attrezzo[]\n[0]\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n:Attrezzo\nnome\"vite\"\npeso  1  \n:Attrezzo\nnome\"dado\"\npeso  2  attrezzi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#67": "Programmazione orientata agli oggetti\nArray: Diagramma degli Oggetti\nAttrezzo[] attrezzi;\nattrezzi = new Attrezzo[10];\nattrezzi[0] = new Attrezzo(\"vite\",1);\nattrezzi[5] = new Attrezzo(\"dado\",2);\nattrezzi[3] = attrezzi[0];\n:Attrezzo[]\na[0]\n[1]\n[2]\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n:Attrezzo\nnome\"vite\"\npeso  1  \n:Attrezzo\nnome\"dado\"\npeso  2  ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#68": "Programmazione orientata agli oggetti\nEsercizio\n•Disegnare il diagramma degli oggetti che \nrappresenta lo stato del seguente programma al \ntermine della esecuzione della istruzione in linea 7\n•Quale valore ha il peso dell'attrezzo referenziato \ndalla variabile con indice 0 dell'array al termine \ndella istruzione 6? E al termine dell’istruzione 7? \n1.Attrezzo[] attrezzi;\n2.attrezzi = new Attrezzo[5];\n3.Attrezzo a = new Attrezzo(\"spada\", 40);\n4.Attrezzo b = new Attrezzo(\"scudo\", 30);\n5.attrezzi[0] = a;\n6.attrezzi[1] = b;\n7.a = b;",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#69": "Programmazione orientata agli oggetti\nEsercizio\n•Disegnare il diagramma degli oggetti che \nrappresenta lo stato del seguente programma al \ntermine della esecuzione della istruzione in linea 7\n•Quale valore ha il peso dell'attrezzo referenziato \ndalla variabile con indice 0 dell'array al termine \ndella istruzione 6? E al termine della istruzione 7? \n1.Attrezzo[] attrezzi;\n2.attrezzi = new Attrezzo[5];\n3.Attrezzo a = new Attrezzo(\"spada\", 40);\n4.Attrezzo b = new Attrezzo(\"scudo\", 30);\n5.attrezzi[0] = a;\n6.attrezzi[1] = b;\n7.attrezzi[0] = attrezzi[1];",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#7": "Programmazione orientata agli oggetti\nOverloading\npublic class ProvaOverloading {\n public void metodo(int a) {\n  System.out.println(“parametro int”);\n }\n public void metodo(double a) {\n  System.out.println(“parametro double”);\n }\n}\nProvaOverloading prova = new ProvaOverloading();\nprova.metodo(3);    // Stampa “parametro intero”\nprova.metodo(3.0d); // Stampa “parametro double”",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#70": "Programmazione orientata agli oggetti\nEsercizio ( con Eclipse )\n●Riscrivere la classe Stanza  in modo che \ncontenga un array di Attrezzi\n—Implementare quindi il metodo:\nboolean  addAttrezzo(Attrezzo attrezzo)\naggiunge un attrezzo nella stanza. \n—Se c’è spazio restituisce true; altrimenti \nrestituisce false\n—quindi aggiungere il metodo \nboolean hasAttrezzo(String nomeAttrezzo)\nControlla che nella stanza ci sia un attrezzo di nome \nnomeAttrezzo:\n—Se presente restituisce  true; altrimenti  false",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#71": "Programmazione orientata agli oggetti\nEsercizio\npublic class Stanza {\nprivate Attrezzo[] attrezzi;\nprivate int numeroAttrezzi;\nprivate String nome;\npublic Stanza(String nome) {\nthis.nome = nome;\nthis.attrezzi = new Attrezzo[10]; // per ora solo 10 attrezzi\nthis.numeroAttrezzi = 0;\n}\npublic boolean addAttrezzo(Attrezzo attrezzo) {\nif (this.numeroAttrezzi < 10) { // massimo 10 attrezzi\n this.attrezzi[numeroAttrezzi] = attrezzo;\n this.numeroAttrezzi++;\n return true;\n} else {\n return false;\n}\n}\n      // … continua …",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#72": "Programmazione orientata agli oggetti\nEsercizio\n…\npublic boolean hasAttrezzo(String nomeAttrezzo) {\nboolean trovato;\ntrovato = false;\nfor (Attrezzo attrezzo : this.attrezzi) {\nif (attrezzo.getNome().equals(nomeAttrezzo))\ntrovato = true;\n}\nreturn trovato;\n    }\n} // fine classe Stanza\n●L’equivalenza di stringhe viene controllata con il \nmetodo  equals()",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#73": "Programmazione orientata agli oggetti\nEsercizio\n●Scrivere il metodo\nAttrezzo getAttrezzo(String nomeAttrezzo)\nche restituisce l’attrezzo di nome nomeAttrezzo  se \nesiste, null altrimenti\n●Scrivere il metodo\nString toString()\nche restituisca una descrizione della stanza compresa \nuna lista di tutti gli oggetti in essa contenuti\n●Conviene scrivere un metodo toString()  anche nella \nclasse Attrezzo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#74": "Programmazione orientata agli oggetti\nCostanti\n•Talvolta, vogliamo imporre che i valori di alcune \nvariabili di istanza non possano essere cambiati\n–definiamo valori costanti\n•A tal fine si usa la parola chiave final \nfinal double NUMERO_MASSIMO_DIREZIONI = 4;\nNUMERO_MASSIMO_DIREZIONI = 20; // ERR. COMPILAZIONE\n•Convenzione di stile: gli identificatori delle costanti \nsi scrivono in maiuscolo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#75": "Programmazione orientata agli oggetti\nVariabili di Classe\n•Talvolta è utile avere variabili che devono essere \ncondivise da tutti gli oggetti della stessa classe: \nvariabili di classe\n•A tal fine si usa la parola chiave static\nprivate static int perTutti;\n•ATTENZIONE: Esistono importanti motivazioni \ndidattiche per limitarne il più possibile l’uso\n•Per il momento limitiamoci ad usare questa parola \nchiave solo ed esclusivamente per\n–dichiarare il metodo main()\n–definire costanti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#76": "Programmazione orientata agli oggetti\nCostanti e Variabili di Classe\n•È ragionevole che una costante sia anche un \nvariabile di classe. Perché?\n•Per questo le dichiarazioni delle costanti di \nsolito sono come segue:\nfinal static double NUMERO_MASSIMO_DIREZIONI = 4;",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#77": "Programmazione orientata agli oggetti\nMetodi di Classe (Statici)\n•Attenzione: nella POO i metodi di classe sono una  \nvera e propria anomalia\n–Durante l’apprendimento del paradigma OO sono \nquasi “pericolose”!\n•Un metodo di classe corrisponde ad una operazione \nche può essere svolta senza utilizzare lo stato \ndell'oggetto (oppure usando solo variabili di classe, \ncondivise da tutti gli oggetti)\n•Tipicamente si usano per realizzare funzioni pure \n(es. i metodi di java.lang.Math )\n•Sono come il freno a mano di un'automobile: \n–Molto utile da fermi...\n–...ma non usatelo mai per frenare in corsa!!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#78": "Programmazione orientata agli oggetti\nAncora su final  (1)\n•Attenzione:\nfinal int a = 10;\na = 3; // ERRORE DI COMPILAZIONE\nfinal int a = 10;\nint b = 4;\na = b; // ERRORE DI COMPILAZIONE",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#79": "Programmazione orientata agli oggetti\nAncora su final  (2)\n•Attenzione:\nfinal Stanza ds1 = new Stanza(\"Aula DS1\");\nStanza n7 = new Stanza(\"Aula N7\");\nds1 = n7; // ERRORE DI COMPILAZIONE\nfinal Stanza ds1 = new Stanza(\"Aula DS1\");\nStanza n7 = new Stanza(\"Aula N7\");\nAttrezzo v = new Attrezzo(\"vite\",1);\nds1.setAttrezzo(v);  // LECITO! PERCHE'?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#8": "Programmazione orientata agli oggetti\nOverloading e Tipo Restituito\n●Versioni sovraccariche dello stesso metodo possono \ndifferire anche per il tipo del valore restituito:\nint add(int a, int b);\ndouble add(double a, int b);\n●NON è però possibile distinguere due metodi con gli \nstessi parametri formali (stesso numero, tipo e ordine \ndei parametri) usando solamente il tipo di ritorno\nint f(int a, int b);    // Non Compila\ndouble f(int a, int b); // Non Compila\n✔Non compila: il compilatore non sarebbe in grado di \ncapire quale versione si dovrebbe invocare (in generale)…\nSi usa anche dire che \nil tipo restituito non fa parte della segnatura di un metodo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#80": "Programmazione orientata agli oggetti\nAncora su final  (3)\n•In sostanza, quando final  si usa su\n–primitivi: rende costante la variabile\n–riferimenti: rende costante il riferimento, ma non \nil contenuto dell’oggetto referenziato, che rimane \nlibero di cambiare",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#81": "Programmazione orientata agli oggetti\nStudio di Caso (1)\n•Il gioco di ruolo diadia\n–In questa versione iniziale ci concentriamo su \npoche classi\n–La prima versione, oltre ad essere molto \nsemplice, ha un codice scritto piuttosto male:\n•Ci sono errori (a tempo di esecuzione)\n•È di difficile manutenzione \n•È poco riutilizzabile\n–Il primo aspetto (errori) lo verificherete \nimmediatamente eseguendo il programma\n–Capire come ovviare agli altri due aspetti è uno \ndegli obiettivi formativi del corso",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#82": "Programmazione orientata agli oggetti\nStudio di Caso (2)\n•Scaricare  lo studio di caso versione base:\n–https://sites.google.com/view/rm3-poo/materiale-didattico\n•Eseguire il metodo main() nella classe DiaDia\n•Digitare alcuni comandi\n–aiuto per avere un elenco dei comandi\n–vai nord|sud|est|ovest  per cambiare stanza\n•Spostandoci da una stanza all'altra noteremo presto \nerrori (uno porta alla terminazione del programma)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#83": "Programmazione orientata agli oggetti\nEsercizio\n•Studiare a fondo il codice della classe Stanza\n–a che cosa serve la variabile numeroDirezioni ?\n–che cosa fanno i metodi \nimpostaStanzaAdiacente(String, Stanza)  e \ngetStanzaAdiacente(String)?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#84": "Programmazione orientata agli oggetti\nEsercizio\n•Scrivere una classe StanzaTest1  con il \nmetodo main()  con le istruzioni per:\n–Definire due oggetti Stanza : bar e mensa\n–Impostare le uscite dei due oggetti affinché:\n•L'uscita nord del bar porti nella mensa\n•L'uscita sud della mensa porti nel bar\n–Stampare la descrizione della stanza dietro la \nporta nord del bar\n–Stampare la descrizione della stanza dietro la \nporta sud della mensa\n√Controllare che le stampe siano quelle attese",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#85": "Programmazione orientata agli oggetti\nEsercizio\n•Scrivere una classe StanzaTest2  con un metodo \nmain() :\n–Definire due oggetti Stanza: bar e mensa\n–Definire due oggetti Attrezzo : tazzina e piatto\n–Impostare le uscite dei due oggetti Stanza affinché:\n•L'uscita nord del bar porti nella mensa\n•L'uscita sud della mensa porti nel bar\n–Aggiungere nel bar l'oggetto Attrezzo  tazzina\n–Aggiungere nella mensa l'oggetto Attrezzo  piatto\n–Stampare il nome e il peso dell'attrezzo presente nella \nstanza dietro la porta nord del bar\n–Stampare il nome e il peso dell'attrezzo presente nella \nstanza dietro la porta sud della mensa\nControllare che le stampe siano quelle attese",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-05-costruttori-string-array.pdf#9": "Programmazione orientata agli oggetti\nEsempio (con Eclipse)\n●All’interno della classe Rettangolo  scrivere \ni metodi\n—void scala(int fattore)\ndeve modificare l’oggetto Rettangolo  di modo che\nbase = base * fattore\naltezza = altezza * fattore\n—void scala(int fattoreBase, fattoreAltezza)\ndeve modifica l’oggetto Rettangolo  di modo che\nbase = base * fattoreBase\naltezza = altezza * fattoreAltezza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#0": "Programmazione \nOrientata agli Oggetti\nQualità del codice:\nJava Base Library\nDocumentazione\nPackage",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#1": "Programmazione orientata agli oggetti\nSommario\n•Java Base Libraries\n•Consultare la documentazione\n•Produrre la documentazione delle \nproprie classi\n•Package",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#10": "Programmazione orientata agli oggetti\nProdurre Documentazione (2)\n•Un’idea tanto semplice quanto efficace\n–Documentazione in formato ipertestuale\n•pagine web\n–La documentazione viene generata dai \ncommenti immersi direttamente nel codice\n–Basta usare una semplice sintassi pensata \nallo scopo\n–e l’utility javadoc",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#11": "Programmazione orientata agli oggetti\nCommenti di Documentazione\n•Tutti i comandi javadoc si trovano solo \nentro commenti /** … */\n•Speciali marcatori (immersi nei \ncommenti) permettono di definire \naspetti specifici della documentazione\n•Per l’elenco completo dei marcatori \njavadoc si consulti la documentazione:\nhttp://docs.oracle.com/javase/1.5.0/docs/tooldocs/solaris/javadoc.html#javadoctags",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#12": "Programmazione orientata agli oggetti\nDocumentazione delle Classi\n•Forma generale\n/**\n * Nome-classe : commento che descrive \n * scopo e caratteristiche generali della classe\n *\n * @author nome-autore\n * @see riferimento ad altra classe\n * @see riferimento ad altra classe\n * @version  versione\n */\npublic class Nome-classe {",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#13": "Programmazione orientata agli oggetti\nEsempio\n/**\n * Una semplice classe che modella un attrezzo.\n * Gli attrezzi possono trovarsi all'interno delle stanze\n * del labirinto.\n * Ogni attrezzo ha un nome ed un peso.\n *\n * @author   docente di POO\n * @see Stanza\n * @version 0.9\n *\n */\n public class Attrezzo {\n  …\n }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#14": "Programmazione orientata agli oggetti\nDocumentazione dei Costruttori\n•Forma generale\n/**\n * Commento che descrive scopo e caratteristiche \n * generali del costruttore\n *\n * @param nome-parametro breve descrizione\n */\nNome-classe(…) {",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#15": "Programmazione orientata agli oggetti\nEsempio\n /**\n   * Crea un attrezzo\n   * @param nome il nome che identifica l'attrezzo \n   * @param peso il peso dell'attrezzo \n   */\n   public Attrezzo(String nome, int peso) {\n      this.peso = peso;\n      this.nome = nome;\n   }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#16": "Programmazione orientata agli oggetti\nDocumentazione dei Metodi\n•Forma generale\n/**\n * Commento che descrive scopo e caratteristiche \n * generali del metodo\n *\n * @param nome-parametro breve descrizione\n * @return valore di ritorno, breve descrizione\n */\npublic type nome-metodo(…) {",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#17": "Programmazione orientata agli oggetti\nEsempio\n /**\n   * restituisce il nome identificatore dell'attrezzo\n   * @return identificatore dell'attrezzo\n   */\n   public String getNome() {\n      return this.nome;\n   }\n /**\n   * restituisce il peso dell'attrezzo\n   * @return peso dell'attrezzo\n   */\n   public int getPeso() {\n      return this.peso;\n   }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#18": "Programmazione orientata agli oggetti\nGenerare Documentazione\n•Si usa il tool javadoc\n•Per dettagli \njavadoc -h\n•Per generare la documentazione\njavadoc *.java",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#19": "Programmazione orientata agli oggetti\nPackage\n•Le classi sono raggruppate in package\n•Questo raggruppamento consente di:\n–Mantenere assieme classi concettualmente \ne logicamente correlate \n–Creare spazi di nomi che evitino conflitti\n–Definire un dominio di protezione\n(cfr modificatori di accesso)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#2": "Programmazione orientata agli oggetti\nJava Base Libraries\n•Migliaia di classi\n•Decine di migliaia di metodi\n•Molte classi utili che ci semplificano \ndrasticamente la vita\n•Un programmatore competente deve \nessere in grado di lavorare con le librerie",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#20": "Programmazione orientata agli oggetti\nImport\n•Una classe può usare tutte le classi dello stesso \npackage e tutte le classi pubbliche di altri package\n•Si può accedere alle classi pubbliche di un altro \npackage in due modi\n–Usando il nome completamente qualificato di una classe, cioè \nanteponendo il nome del pacchetto alla classe:\njava.util.Scanner  s = \nnew java.util.Scanner (input);\n–Importando la classe e scrivendone direttamente il nome\nimport java.util.Scanner;\n…\nScanner s = new Scanner(input);",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#21": "Programmazione orientata agli oggetti\nPackage\n•È bene organizzare il proprio codice \norganizzando le classi in package\n•Le classi che appartengono ad un package \ndevono dichiarare la propria appartenenza al \npackage tramite la dichiarazione\npackage nome-package ;\n•La dichiarazione di appartenenza ad un \npackage deve comparire all’inizio del file\n•Una classe può appartenere al più ad un \npackage",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#22": "Programmazione orientata agli oggetti\nPackage, Convenzioni sui Nomi\n•Il nome di un package deve essere univoco.\n•A tal fine di solito il nome del package \ncomprende il nome del dominio Internet \ndell’organizzazione, scritto in ordine inverso\npackage it.uniroma3.diadia;",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#23": "Programmazione orientata agli oggetti\nPackage e Classi Pubbliche\n•Una classe può essere usata al di fuori del \npackage solo se è dichiarata pubblica\n•Esempio:\npackage it.uniroma3.diadia;\npublic class Stanza {\n  …\n}\nla classe Stanza può essere usata al di fuori del \npackage it.uniroma3.diadia (importandola)\n•Se invece scrivessimo:\npackage it.uniroma3.diadia;\nclass Stanza {\n  …\n}\nla classe Stanza non potrebbe essere usata fuori dal \npackage it.uniroma3.diadia",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#24": "Programmazione orientata agli oggetti\nPackage\n•Il nome di un package possiede una struttura \ngerarchica\n•Tale struttura deve trovare corrispondenza \ndiretta nel file system\n•Ad esempio le classi del package \nit.uniroma3.diadia\ndevono  essere memorizzate nella cartella\nit/uniroma3/diadia",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#25": "Programmazione orientata agli oggetti\nCompilazione ed Esecuzione \ndi Classi nei Package\n•Per compilare le classi di un package si deve far riferimento alla \ngerarchia fisica\n•Con un buon IDE (ad es. Eclipse: cfr. source folders ) il processo è quasi \ncompletamente trasparente\n•Da riga di comando questo può essere un po' articolato:\n–Supponiamo di mettere tutto il nostro codice nella directory C:\\src\n–La versione base di diadia è nel package:\n it.uniroma3.diadia\n      quindi le classi Java sono nella directory\n src/it/uniroma3/diadia\n–Per compilare una classe (ad esempio la classe Stanza.java)  del package: \n•dalla radice del package, cioè dalla directory che contiene la directory it (supponiamo C:\\\nsrc)\njavac it/uniroma3/diadia/Stanza.java\n•oppure, dalla directory in cui si trovano le classi da compilare\njavac –classpath \"C:\\src\\\" Stanza.java\n(supponendo che la directory it sia nella directory src del volume C:)\n–Per eseguire una classe di un package si deve far riferimento alla gerarchia logica\n•dalla radice del package (cioè dalla directory che contiene la directory it) \njava it.uniroma3.diadia.DiaDia\n•oppure da una qualunque directory \njava –classpath \"C:\\src\" it.uniroma3.diadia.DiaDia",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#26": "Programmazione orientata agli oggetti\nRicapitolazione\n•Java offre un insieme estremamente vasto e ricco di \nlibrerie\n•Le librerie sono documentate in un formato standard\n•Nella definizione delle nostre classi è possibile creare \nautomaticamente documentazione standard\n–Usando opportunamente i commenti javadoc\n•Le librerie (e le applicazioni) sono organizzati in \npackage\n–Creazione di uno spazio univoco dei nomi\n–Raggruppamento delle classi\n–Definizione di un nuovo livello di visibilità",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#27": "Programmazione orientata agli oggetti\nEsercizio\n•Mettere tutte le classi dello studio di \ncaso (versione base) in un package \ndiadia\n•Compilare ed eseguire il programma",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#3": "Programmazione orientata agli oggetti\nJava Base Libraries\n•Un programmatore competente dovrebbe saper \ncostruire le proprie classi, ma anche sapere \nquando è inutile scriverne di nuove\n•Per poter usare efficacemente una libreria, \nbisogna:\n–Conoscere alcune sue importanti classi per nome\n–Sapere come trovare e usare altre classi\n•Importante:\n–Serve conoscere solo l’interfaccia, non \nl’implementazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#4": "Programmazione orientata agli oggetti\nDocumentazione di Librerie (1)\n•La documentazione delle librerie Java è \nin formato HTML\n–javadoc\n•Si può leggere agevolmente con un \nbrowser\n•Class API: Application Programmers’ \nInterface\n–Descrizione delle interfacce per tutte le \nclassi della libreria",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#5": "Programmazione orientata agli oggetti\nDocumentazione di Librerie (2)\n•La documentazione include\n–Il nome della classe\n–Una descrizione generale della classe\n–Una lista dei costruttori e dei metodi\n–Valori di ritorno e parametri per costruttori e metodi\n–Una descrizione dello scopo di ciascun costruttore e \ndi ciascun metodo\n•Come si usa la classe: tutto quello che \nserve al \nprogrammatore-utilizzatore",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#6": "Programmazione orientata agli oggetti\nDocumentazione di Librerie (3)\n•La documentazione non include\n–Campi privati \n(tutti i campi dovrebbero essere privati)\n–Metodi privati\n–Il corpo dei metodi e dei costruttori\n•Dettagli sull'implementazione: non servono al \nprogrammatore-utilizzatore\n•Anzi: potrebbe risultare controproducente \ndoverli necessariamente conoscere",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#7": "Programmazione orientata agli oggetti\nProdurre Documentazione (1)\n•Le classi che progettiamo dovrebbero essere \ndocumentate come le classi della libreria\n•Altri programmatori devono essere in grado di \nusare le nostre classi senza conoscere \nl’implementazione di dettaglio",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#8": "Programmazione orientata agli oggetti\nElementi della Documentazione\n•La documentazione di una classe dovrebbe \nincludere\n–Il nome della classe\n–Un commento che descriva lo scopo e caratteristiche \ngenerali della classe\n–Un numero di versione\n–Il nome degli autori\n–Riferimenti ad altre classi\n–Documentazione per ciascun costruttore e per ciascun \nmetodo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-06-documentazione-package.pdf#9": "Programmazione orientata agli oggetti\nDocumentare Costruttori e Metodi\n•La documentazione di ciascun costruttore / \nmetodo dovrebbe includere\n–Nome e tipo di ciascun parametro\n–Una breve descrizione di ciascun parametro\n–Una descrizione dello scopo e della funzione del \ncostruttore/metodo\n–Il nome del metodo\n–Il tipo di ritorno\n–Una descrizione del valore ritornato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#0": "Programmazione \nOrientata agli Oggetti\nQualità del codice:\nCoesione e Accoppiamento",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#1": "Programmazione orientata agli oggetti\nContenuti \n•Accoppiamento\n•Coesione \n•Introduzione a: \n–Testing\n–Refactoring",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#10": "Programmazione orientata agli oggetti\nCoesione ed Accoppiamento\n•Sono le due facce della stessa medaglia\n–L’alta coesione di una classe si ottiene \nperseguendo lo scarso accoppiamento di \nquella classe verso altre classi\n–Lo scarso accoppiamento di una classe \nverso le altre classi si ottiene perseguendo \nla sua alta coesione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#11": "Programmazione orientata agli oggetti\nLocalizzare le Modifiche\n•Basso accoppiamento e alta coesione \nportano ad una localizzazione delle \nmodifiche\n•Quando è necessario operare una \nmodifica, il minor numero possibile di \nclassi dovrebbero essere coinvolte\n•La qualità del proprio codice si osserva \nproprio quando sorge l’esigenza di fare \nmodifiche",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#12": "Programmazione orientata agli oggetti\nPensare in Avanti\n•Quando progettiamo una classe dovremmo \nsforzarci di pensare a \n–quali cambiamenti potranno essere richiesti in \nfuturo\n–come verrà usata la nostra classe dal \nprogrammatore-utilizzatore\n•Le nostre scelte iniziali potrebbero facilitare \nl’evoluzione futura (su questo aspetto faremo \nesperienza nello studio di caso ed in altri \nesercizi)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#13": "Programmazione orientata agli oggetti\nQualità Interna vs Qualità Esterna\ndel Codice\n•La qualità del codice può essere osservata da \nmolti punti di vista ben distinti\n•Due particolarmenti interessanti per noi\n–Quello degli utilizzatori finali:   qualità esterna\n–Quello degli sviluppatori:     qualità interna\n•Gli utilizzatori finali possono fornire un giudizio \ndi merito sulla capacità di un applicativo di \nrispondere ai requisiti ed alle proprie esigenze\n•Solo gli sviluppatori possono valutare la \nmanutenibilità del codice nel momento in cui \nnasce la necessità di modificarlo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#14": "Programmazione orientata agli oggetti\n“Refactoring”\n•La manutenzione del software spesso richiede \nl’aggiunta di nuovo codice\n•Le classi e i metodi tendono così a diventare più \nlunghi, a perdere in coesione, ad aumentare \nl’accoppiamento versi altre porzioni di codice\n•A seguito delle modifiche, per mantenere un’alta \ncoesione ed un basso accoppiamento, classi e metodi \ndovranno essere riorganizzate\n•Questo processo di riorganizzazione del codice viene \ndefinito “refactoring”\nE’ il principale strumento che uno sviluppatore \npossiede per controllare la qualità interna  del codice\n–Modifiche del codice che non alterano il funzionamento\n–Ma preparano il codice ad accogliere le modifiche future",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#15": "Programmazione orientata agli oggetti\nRefactoring e Testing\n•Quando modifichiamo il codice, è necessario \nisolare le conseguenze del refactoring da altri \nfattori\n•Quindi la riorganizzazione delle classi e dei metodi \n(il refactoring) deve essere effettuata prima di \nintrodurre nuove funzionalità\n•Come contrastare la naturale paura di fare \nmodifiche su un base di codice che si considerava \nfunzionante?\n•Per assicurarci di non aver introdotto nuovi errori, \neseguiamo i test prima e dopo ogni azione di \nrefactoring\n–Vedremo come organizzare metodicamente refactoring \ne testing",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#16": "Programmazione orientata agli oggetti\nLinee Guida (1)\n•Domande comuni\n–Quanto dovrebbe essere lunga una classe?\n–Quanto dovrebbe essere lungo un metodo?\n•Possiamo rispondere in termini di \ncoesione e accoppiamento",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#17": "Programmazione orientata agli oggetti\nLinee Guida (2)\n•Un metodo è troppo lungo se è responsabile di \npiù di un compito logico\n•Una classe è troppo complessa se rappresenta \npiù di un concetto, se ha più di una \nresponsabilità\n●Nota: queste sono linee guida – solo attraverso \nl’esperienza si riescono a concretizzare",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#18": "Programmazione orientata agli oggetti\nCaso di Studio: \nle classi Diadia  e Partita\n•Le classi Diadia  e Partita  sono \nparticolarmente lunghe\n•Se guardiamo il loro codice ci accorgiamo che \nhanno diverse (troppe!) responsabilità\n–Diadia implementa la logica di tutti i possibili \ncomandi\n–Partita  gestisce lo stato del gioco e crea il \nlabirinto\n•Questa miriade di responsabilità è indice di \npoca coesione\n19",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#19": "Programmazione orientata agli oggetti\nAumentiamo la Coesione\n•Possiamo iniziare a migliorare la coesione \ndella classe Partita  togliendole qualche \nresponsabilità \n•Iniziamo a togliere la responsabilità di creare e \ngestire il labirinto\n•Affidiamo questa responsabilità ad una nuova \nclasse appositamente introdotta: Labirinto\n20",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#2": "Programmazione orientata agli oggetti\nIl Software Evolve\n•Il software evolve in continuazione\n•…inevitabilmente…\n•Viene esteso, corretto, mantenuto, \nportato su altre piattaforme, adattato, …\n•Molte persone, in tempi diversi \npartecipano a questo processo\n•Se il costo dell’evoluzione è troppo alto, \nil software viene gettato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#20": "Programmazione orientata agli oggetti\nEsercizio\n•Creare la classe Labirinto  e modificare la \nclasse Partita  affinché non abbia la \nresponsabilità della creazione del labirinto\n–Un labirinto ha una entrata (stanza di ingresso) \ned una uscita (stanza vincente)\n–La classe Labirinto  ha un metodo privato \ninit()  che inizializza il labirinto\n•Provare ad eseguire il codice del gioco prima e \ndopo le modifiche e verificare che il \ncomportamento sia rimasto invariato\n21",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#21": "Programmazione orientata agli oggetti\nRefactoring …\n•Un programmatore OO deve avere forte senso \ncritico, evidenziare i limiti delle proprie \nsoluzioni sia in termini di funzionalità che di \nqualità del codice per risolverli attraverso \ndisciplinati passi di refactoring \n–N.B. con un pizzico di attenzione al pericolo della \nsovra-ingegnerizzazione\nbisogna risolvere i problemi di oggi del progetto! \nPoi, se c’è la possibilità, anche quelli che si \nprevedono\nmai compromettere le soluzioni di oggi per \nbisogni che forse avremo in futuro\n22",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#22": "Programmazione orientata agli oggetti\nRefactoring …\n•Concludiamo citando alcune problematiche risolvibili \nefficacemente solo dopo aver studiato il \npolimorfismo\n–Forte accoppiamento tra l’insieme dei comandi disponibili e la \nclasse Diadia: risolvendo il primo homework è evidente che \nogni volta che aggiungiamo/modifichiamo un comando \ndobbiamo agire su questa classe\n•Anche dopo i passi di refactoring precedenti la classe Diadia continua ad \naddossarsi diverse (troppe!) responsabilità\n–Implementa la logica del gioco \n–Implementa la logica di tutti i possibili comandi\n–Ma è possibile che la struttura del labirinto sia cablata \nall’interno della classe Labirinto ? E pensando ad un gioco a \npiù livelli di difficoltà con diversi labirinti?\n–Se volessi cambiare labirinto o comunque riutilizzare un \noggetto istanza della classe Partita con diversi labirinti, \npotrei farlo?\n23",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#23": "Programmazione orientata agli oggetti\nRicapitoliamo\n•I programmi SW sono in continua evoluzione\n•E’ importante facilitare e prevedere questa \nevoluzione\n•La qualità del codice richiede molto più del \ncorretto funzionamento di un programma in un \npreciso momento \n–“funziona!”… condizione necessaria, ma non sufficiente\n–in altri termini: \n•se funziona non è detto che vada bene\n•se non funziona certamente non va bene\n•Il codice deve essere comprensibile e \nmanutenibile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#24": "Programmazione orientata agli oggetti\nRicapitoliamo\n•Codice di buona qualità: “no copia e incolla”, alta \ncoesione, basso accoppiamento\n•Anche lo stile di codifica (identificatori, \nindentazione, spaziatura) è fondamentale (il \nprogramma è uno strumento di comunicazione)\n–codice ben scritto non ha bisogno di commenti!\n•Il costo di manutenzione del software dipende \nfortemente dalla qualità del codice ed è ormai da \ntempo noto essere la voce di costo preponderante \nsul medio/lungo termine",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#3": "Programmazione orientata agli oggetti\nQualità del Codice\n•La qualità del codice dipende da \ndue fattori importanti:\n–Accoppiamento (coupling)\n–Coesione (cohesion)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#4": "Programmazione orientata agli oggetti\nAccoppiamento\n•Due o più unità di un programma si dicono \naccoppiate quando è impossibile modificare \nuna senza dover modificare anche le altre\n•L’accoppiamento si riferisce ai legami tra unità \nseparate e distinte di un programma\n•Se due classi dipendono strettamente e per \nmolti dettagli l’una dall’altra, diciamo che sono \nstrettamente accoppiate\n•Per un codice di qualità dobbiamo puntare ad \nun basso accoppiamento",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#5": "Programmazione orientata agli oggetti\nDuplicazione del Codice\n•“Codice Copia e Incolla”\n•La duplicazione del codice \n–è sintomo di un cattivo progetto\n–porta facilmente alla propagazione di errori \ndurante lo sviluppo\n–rende difficile la manutenzione\n–porta inevitabilmente alla introduzione di errori \nnelle attività di manutenzione\n•E’ una forma elementare di accoppiamento",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#6": "Programmazione orientata agli oggetti\nBasso Accoppiamento\n•Un basso accoppiamento permette di:\n–Capire il codice di una classe senza leggere i \ndettagli delle altre\n–Modificare una classe senza che le \nmodifiche comportino conseguenze sulle \naltre classi\n•Quindi un basso accoppiamento migliora \nla manutenibilità del software",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#7": "Programmazione orientata agli oggetti\nCoesione\n•La coesione fa riferimento al numero e alla \neterogeneità dei compiti di cui una singola \nunità è responsabile\n•Se ciascuna unità è responsabile di un singolo \ncompito, diciamo che tale unità possiede una \nalta coesione\n•La coesione si applica alle classi e ai metodi \n(ed anche ai package!)\n•Perseguiamo l’alta coesione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#8": "Programmazione orientata agli oggetti\nAlta Coesione\n•Un’alta coesione favorisce:\n–La comprensione dei compiti di una classe o \ndi un metodo\n–L’utilizzo di nomi appropriati, efficaci, \ncomunicativi\n–Il riuso delle classi e dei metodi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-07-coesione-accoppiamento.pdf#9": "Programmazione orientata agli oggetti\nCoesione\n•Coesione dei metodi\n–Un metodo dovrebbe essere responsabile di \nun solo compito ben definito\n•Coesione delle classi\n–Ogni classe dovrebbe rappresentare un \nsingolo concetto ben definito",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#0": "Programmazione\nOrientata agli Oggetti\nQualità del Codice:\nIntroduzione Unit-Testing",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#1": "Programmazione orientata agli oggetti\n 2\n",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#10": "Programmazione orientata agli oggetti\nCosto di un Bug e “Località”\nC h is s à\nQ u a n d oD o p o  T a n t oD o p o  U n\nP o 'S u b it oP o c h eAlc u n eT a n t eT r o p p e\n11TempoLineeCosto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#11": "Programmazione orientata agli oggetti\nTipologie di Test\n12\n",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#12": "Programmazione orientata agli oggetti\nLe Tre Fasi di un Test\n•Tutte le tipologie di test di un qualsiasi \nsistema prevedono la costruzione di uno \nscenario di testing che si articola sempre in tre \nfasi strettamente sequenziali\n1.mettere il sistema in un stato iniziale ben noto\n2.inviare una serie di sollecitazioni\n3.controllare che alla fine il sistema si trovi nello \nstato atteso\n•I test possono fallire od avere successo\n13",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#13": "Programmazione orientata agli oggetti\nObiettivi del Testing\n•Se ben progettati e mantenuti, i test \naiutano a confinare i bug nella “zona \nverde”, ovvero con spiccata località\n–i bug si manifestano immediatamente e \npalesemente\n–la ricerca del bug è confinabile in poche \nlinee facilmente localizzabili\n•Le esecuzioni che palesano un bug non \nsono mai troppo lunghe e complesse\n14",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#14": "Programmazione orientata agli oggetti\nIl Valore Aggiunto dal Testing\n•Se il test ha successo: \n–si possiede una garanzia sul comportamento \ndinamico del codice (assenza di bug)\n•Se il test non ha successo:\n–il bug dovrebbe risultare facilmente \nlocalizzabile nell’unità di codice sollecitata \ndal test\n•In entrambi  i casi c’è un significativo \nvalore aggiunto\n15",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#15": "Programmazione orientata agli oggetti\nTesting vs Regressione\n•Se il test \n–funzionava subito prima di effettuare un \nmodifica\n–ma smette di funzionare subito dopo aver \neffettuato la modifica\n•E’ altamente probabile che l’errore è stato \nappena introdotto con la modifica\n–Ricerca «locale» e quindi economica\n•E’ possibile prevenire la regressione\n16",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#16": "Programmazione orientata agli oggetti\nEsercizio\n•Supponiamo di voler testare il metodo max() \ndella classe Sequenza  (quiz di benvenuto al corso)\n–Scriviamo in un documento di testo (.txt) diverse istanze \ndell'array di interi, per ogni sequenza scriviamo il \nmassimo atteso\n–facciamo girare il programma su ciascuna sequenza e \nverifichiamo che il risultato sia quello atteso\n•Osservazione:\n–possiamo scrivere i test senza preoccuparci \ndell'algoritmo per il calcolo del massimo (ovviamente è \nnecessario scegliere con cura gli array di test) \n•Conseguenza: \n–possiamo scrivere i test prima di scrivere il programma!\n17",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#17": "Programmazione orientata agli oggetti\nTesto QUIZ\n•Scrivere il codice del metodo massimo() che \ndeve restituire il più grande valore presente \nnella variabile di istanza sequenza , un array:\n18public class Sequenza {\n    private int[] sequenza;\n    \n    public Sequenza(int n){\n        sequenza = new int[n];\n    }\n    \n    public int massimo(){\n// scrivere il codice di questo metodo:\n// deve restituire il valore piu' grande \n// presente nell'array sequenza\n    }    \n    public void setElemento(int indice, int valore) {\n        sequenza[indice] = valore;\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#18": "Programmazione orientata agli oggetti\nCodice di Test\n•Nella pratica, accanto al codice di produzione \nsi sviluppa sempre del codice di test  \n•Unico motivo di esistere del codice di test è \nquello di verificare la correttezza a tempo di \nesecuzione del codice principale\n•Il codice di test accompagna e supporta lo \nsviluppo del codice di produzione ma non fa \nparte del codice consegnato a fine progetto\n19",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#19": "Programmazione orientata agli oggetti\nTest Unitari Automatici\n•Esistono diverse tipologie di test\n•Nostra attenzione è limitata ad una in \nparticolare: unit-testing automatico\n–test che si focalizzano su frammenti (unità)  del \nsistema \n–senza alcun intervento umano (tranne la richiesta di \nesecuzione)\n•Praticamente i test unitari si codificano nel \nmedesimo linguaggio di programmazione \nutilizzato per lo sviluppo (Java)\n20",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#2": "Programmazione orientata agli oggetti\nSommario\n•Errori nel software\n–errori di compilazione vs bug\n–località dei bug\n•Testing\n–motivazioni\n–le tre fasi di un test\n•Unit-Testing\n•Introduzione a JUnit\n•Qualità dei test\n•Testing Continuo\n•TDD\n3",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#20": "Programmazione orientata agli oggetti\nTest Unitari – Unit Testing\n•Test su frammenti  di un sistema piuttosto che \nsull’intero sistema\n•Concettualmente un test unitario si articola in \nquesti passi\n1)mettere uno o più oggetti da testare in un \nstato iniziale ben noto\n2)invocare i metodi degli oggetti\n3)controllare che alla fine gli oggetti si trovino \nnello stato atteso\n21",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#21": "Programmazione orientata agli oggetti\nAutomazione dei Test\n•Abbiamo già eseguito test manuali \n(cfr. Esercizi precedenti)\n–basta riportare i valori di ingresso ed i valori di \noutput attesi in un documento di testo e \nverificare che ogni esecuzione produca quanto \natteso\n•Chiaramente o gni esecuzione manuale \nrichiede uno sforzo sia per inserire l’input che \nper ispezionare visivamente i risultati\n✔Difficilmente si è disposti a ripetere l’operazione troppe volte\n•L’automazione dei test è fondamentale\n–altrimenti viene meno la loro economicità\n22",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#22": "Programmazione orientata agli oggetti\nAutomazione Artigianale\n•Una possibile soluzione \n–molto artigianale\n–automatica\n   chiarisce il funzionamento dei test unitari\n•Scriviamo un programma in cui\n–inizializziamo un certo numero di oggetti con \nsequenze di interi su cui basare il test ( fixture )\n–invochiamo il metodo da testare e verifichiamo che \nil risultato restituito sia uguale a quello atteso\n•Successivamente  vedremo un framework (JUnit) \nche rende l’automazione ancora più spedita>>\n23",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#23": "Programmazione orientata agli oggetti\nEsempio Soluzione Artigianale (1)\n24public static void main(String[] args){\nSequenza positivi;\nSequenza negativi;\nSequenza negEpos;\nSequenza negEzero;\nSequenza inPrimaPos;\nSequenza inUltimaPos;\npositivi = new Sequenza(5);\npositivi.setElemento(0,1);\npositivi.setElemento(1,5);\npositivi.setElemento(2,8);  // MAX!\npositivi.setElemento(3,3);\npositivi.setElemento(4,4);\nnegativi = new Sequenza(5);\nnegativi.setElemento(0,-6);\nnegativi.setElemento(1,-1); // MAX!\nnegativi.setElemento(2,-8);\nnegativi.setElemento(3,-13);\nnegativi.setElemento(4,-10);",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#24": "Programmazione orientata agli oggetti\nEsempio Soluzione Artigianale (2)\n25negEpos = new Sequenza(5);\nnegEpos.setElemento(0,100);\nnegEpos.setElemento(1,-5);\nnegEpos.setElemento(2,-80);\nnegEpos.setElemento(3,1000); // MAX!\nnegEpos.setElemento(4,10);\nnegEzero = new Sequenza(5);\nnegEzero.setElemento(0,-1);\nnegEzero.setElemento(1,0); // MAX!\nnegEzero.setElemento(2,-80);\nnegEzero.setElemento(3,-10);\nnegEzero.setElemento(4,-10);\ninPrimaPos = new Sequenza(5);\ninPrimaPos.setElemento(0, 1000); // MAX!\ninPrimaPos.setElemento(1, 0);\ninPrimaPos.setElemento(2, 80);\ninPrimaPos.setElemento(3,-10);\ninPrimaPos.setElemento(4,-10);\ninUltimaPos = new Sequenza(5);\ninUltimaPos.setElemento(0, 1);\ninUltimaPos.setElemento(1, 0);\ninUltimaPos.setElemento(2, 80);\ninUltimaPos.setElemento(3,-10);\ninUltimaPos.setElemento(4, 1000);  // MAX! ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#25": "Programmazione orientata agli oggetti\nEsempio Soluzione Artigianale (3)\n26    boolean esito = true;\n    esito &= (positivi.massimo() == 8);\n    System.out.println(positivi.massimo() == 8);\n    esito &= (negativi.massimo() == -1);\n    System.out.println(negativi.massimo() == -1);\n    esito &= (negEpos.massimo() == 1000);\n    System.out.println(negEpos.massimo() == 1000);\n    esito &= (negEzero.massimo() == 0);\n    System.out.println(negEzero.massimo() == 0);\n    esito &= (inPrimaPos.massimo() == 1000);\n    System.out.println(inPrimaPos.massimo() == 1000);\n    esito &= (inUltimaPos.massimo() == 1000);\n    System.out.println(inUltimaPos.massimo() == 1000);\n    System.out.println(esito);\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#26": "Programmazione orientata agli oggetti\nSoluzione Artigianale\n•La soluzione presentata, benché chiaramente \nartigianale , è completamente automatica\n–dopo ogni modifica al metodo sotto test possiamo \nvelocemente far rigirare il programma di test e \nverificare se ci sono cambiamenti (regressioni) nei \nrisultati\n–in presenza di fallimenti la ricerca degli errori risulta \nfortemente semplificata dalle informazioni \ndesumibili già dal test fallito stesso\n27",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#27": "Programmazione orientata agli oggetti\nAutomazione del Testing\n•I test devono essere:\n–automatici  (per mantenere rapido il ciclo di \nfeedback)\n•devono essere eseguiti molte volte al giorno\n–efficienti\n•devono essere convenienti rispetto alle ispezioni manuali\n–isolati e che garantiscano la località degli errori\n•dal fallimento di un test alla rimozione del bug deve \ntrascorre poco tempo grazie alle caratteristiche di forte \nlocalità del test per gli errori che rilevano\n–ed inoltre:\n•separati dal codice applicativo\n•eseguibili e verificabili separatamente\n28",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#28": "Programmazione orientata agli oggetti\nAutomazione del Testing: JUnit\n•Esistono vari strumenti per assistere il \nprogrammatore nel testing, ed in particolare \nnello unit-testing\n•JUnit: http://www.junit.org\n•Il più noto ed utilizzato framework (insieme di \nclassi e convenzioni d'uso) per la scrittura di \ntest unitari\n•Fortemente integrato con gli ambienti di \nsviluppo più diffusi come Eclipse>>\n29",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#29": "Programmazione orientata agli oggetti\nJUnit: Test del Metodo massimo()\nimport static org.junit.jupiter.api.Assertions..*;\nimport org.junit.jupiter.api.Test;\npublic class SequenzaTest  {\n  @Test\n  public void testMassimoPositivi() {\n      Sequenza seq = new Sequenza(5);\n      seq.setElemento(0,1);\n      seq.setElemento(1,5);\n      seq.setElemento(2,8);\n      seq.setElemento(3,3);\n      seq.setElemento(4,4);\n      assertEquals(8, seq.massimo());\n  }\n  @Test\n  public void testMassimoegativi() {\n    …\n    …\n  }…}\n30test-caseimport di classi ed\nannotazioni Junit 5 \nAnnotazione di metodo come test-case\nAsserzionenome\ntest-case",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#3": "Programmazione orientata agli oggetti\nSoftware ed Errori \n•I primi errori con i quali ci scontriamo di solito \nsono errori di sintassi\n–Ci vengono indicati dal compilatore \n•Successivamente incorriamo in errori logici\n–Il compilatore non ci può aiutare \n–Sono noti anche come “bug” (bachi)\n•Alcuni errori logici non si manifestano \nimmediatamente\n–Il software è estremamente complesso\n–Anche il software commerciale non è affatto privo di \nerrori\n4",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#30": "Programmazione orientata agli oggetti\nJUnit: Struttura Classi di Test (1)\n•Tutte le classi di test che scriveremo avranno \nquesta struttura\n•Ovviamente le classi di test vanno progettate \nsulla base delle peculiarità della classe testata\n•Collochiamo la classe di test nello stesso \npackage della classe che si sta testando\n•Convenzione sui nomi basata sul suffisso: \n   Classe \n   Sequenza → SequenzaTest  \n  Classe di Test\n31",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#31": "Programmazione orientata agli oggetti\nJUnit: Struttura Classi di Test (2)\n•import static org.junit.jupiter.api.Assertions.*;\nServe per importare vari metodi statici del \nframework che permettono di fare asserzioni >> \n•import org.junit.jupiter.api.Test;\nServe per importare l’ annotazione del framework\n@Test utile a marcare i metodi i test-case\n•Non è (più) necessario ma è (tuttora) buona \nnorma usare ‘ test’ come prefisso del nome dei \ntest-case\n @Test\npublic void testCostruzioneComandiInvalidi() {\n…\n}\n32",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#32": "Programmazione orientata agli oggetti\nJUnit: Asserzioni\n•Asserzione: \naffermazione che può essere vera o falsa\n•I risultati attesi sono documentati con delle \nasserzioni  esplicite, non mediante stampe \n–richiederebbero dispendiose ispezioni visuali\n•Se l’asserzione è\n–vera : il test ha avuto successo, è andato a buon fine\n–falsa : il test è fallito ed il codice testato non si comporta \ncome atteso, quindi c’è un errore a tempo dinamico\n33",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#33": "Programmazione orientata agli oggetti\nJUnit: Metodi assert XYZ()  \n•Una asserzione non vera fa fallire il test-case\n–assertEquals(Object expected ,Object actual): \nafferma l’«uguaglianza» degli argomenti (>>)\n–assertNull(Object object) : afferma che il suo \nargomento è nullo (fallisce se non lo è)\n–molte altre varianti\n•assertNotNull()\n•assertTrue()\n•assertFalse()\n•assertSame()…  \ntutte sovraccariche … e talvolta facilmente \nintercambiabili...\n•Usare sempre la versione più pertinente! \n–meglio assertFalse(b ) di assertTrue(b==false), \n–meglio assertNotNull(o ) di assertTrue(o!=null)\n34",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#34": "Programmazione orientata agli oggetti\nJUnit: assertEquals()\n•assertEquals(Object expected, Object actual)\n«expected»  è il valore atteso, che ci si aspetta normalmente\n«actual»  è il valore effettivo, reale, quello ottenuto\n–afferma che il suo secondo argomento è uguale al primo \nargomento\n–va a buon fine se e solo se expected.equals(actual ) \nrestituisce true\n•Una variante, spesso preferibile\n  assertEquals( String message , Object expected, Object actual)\n–un messaggio diagnostico da stampare solo in caso di fallimento\n–se ben ideato, dovrebbe permettere di comprendere il motivo del \nfallimento senza nemmeno aprire il debugger\n35",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#35": "Programmazione orientata agli oggetti\nLe “Tre Fasi” in Pratica\n @Test\n  public void testMassimoPositivi() {\n      Sequenza seq = new Sequenza(5);\n      seq.setElemento(0,1);\n      seq.setElemento(1,5);\n      seq.setElemento(2,8);\n      seq.setElemento(3,3);\n      seq.setElemento(4,4);\n      assertEquals(8, seq.massimo());\n  }\n1) mettere un “frammento” del sistema in un stato \niniziale noto\n•il frammento comprende un solo oggetto Sequenza \nopportunemente popolato di valori (nell’es. si tratta di interi \ntutti positivi)\n2) inviare una serie di sollecitazioni ( seq.massimo() )\n3) controllare  tramite asserzioni che si raggiunga lo stato \nfinale atteso ( assertEquals(...) )\n36",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#36": "Programmazione orientata agli oggetti\nJUnit: Compilare i Test\nPer compilare ed eseguire i test:\n•Nel classpath devono essere presenti le \nlibrerie di JUnit  \nEclipse snellisce molti di questi passaggi sino \na renderli quasi trasparenti\n–ed arriva a suggerire di aggiungere la libreria \nnel build path del vostro progetto non appena \nne nota l’utilizzo\n37",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#37": "Programmazione orientata agli oggetti\nJUnit ed Eclipse\n•JUnit è talmente popolare da venire fornito già  \nintegrato e fortemente supportato negli IDE\n•Nel caso di Eclipse:\n–per creare una classe di test \nclick con tasto destro del mouse sulla classe\nnew-> JUnit Test Case  (spuntare new JUnit 5)\n–per eseguire una classe di test\nclick con tasto destro del mouse sulla classe di test\nrun as -> JUnit test\n•barra verde: il test è andato a buon fine\n•barra blue: il test è fallito violando una asserzione\n•barra rossa: il test è fallito causando un errore\n38",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#38": "Programmazione orientata agli oggetti\nJUnit ed Eclipse\n•Le classi di test devono essere nello stesso \npackage delle classi da testare. Scomodo!\n•Si pensi ad una consegna del solo codice di produzione \n•Ma in Eclipse possiamo collocare uno stesso \npackage anche in directory ( source folder) diverse\n–In ogni progetto, \n•nella source folder src mettiamo il codice di produzione\n•nella source folder test mettiamo le classi di test (organizzate \ncon gli stessi package delle classi di produzione)\n–per creare una source folder: tasto destro del mouse sul \nprogetto, quindi new->Source Folder\n•dentro la source folder test creiamo una copia “parallela” dei \npackage del codice di produzione (new->package)\n39",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#39": "Programmazione orientata agli oggetti\n 40Source folder src (qui vanno le classi del codice di produzione)\nSource folder test (qui vanno le classi del codice di test)src vs test  Cartella/Folder",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#4": "Programmazione orientata agli oggetti\nErrori di Compilazione\n•Il compilatore ci dà indicazioni precise e \nmolto utili a correggere l'errore\n•Il messaggio di errore del compilatore \nVA LETTO E CAPITO\nDiadia.java:27:invalid method declaration; \nreturn type required \nprivate creaStanze() {\n                       ^\n1 error\n5LINEA DI CODICE  IN CUI E' STATO RISCONTRATO L'ERRORE\nERRORE RISCONTRATO",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#40": "Programmazione orientata agli oggetti\nJUnit: Fixture\n•Per facilitare la scrittura dei test-case, è \nspesso comodo creare degli oggetti in uno \nstato iniziale noto e «pronto» per l’utilizzo da \nparte di tutti i test-case\n•Può convenire fattorizzare il codice di \ncreazione di questi oggetti. Ad es.\n•utilizzando metodi setUp()\n•mediante i cosidetti factory methods \n•Le fixture  sono oggetti in uno stato iniziale \nnoto ed ospitati in variabili d’istanza che le \nclassi di test predispongono allo scopo\n41",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#41": "Programmazione orientata agli oggetti\nFixture e JUnit\n•Attraverso l’annotazione @BeforeEach  è possibile \nindicare quali metodi eseguire prima di ciascuna \ninvocazione di un test-case\n•Tipicamente questi metodi inizializzano le fixture\n•Spesso, ma non più obbligatoriamente,  questi \nmetodi vengono tuttora denominati setUp() \n•perché con le versioni di JUnit pre-annotazioni Java (JUnit \n3.x) era un nome imposto per convenzione\n42",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#42": "Programmazione orientata agli oggetti\nFixture e Metodo setUp() (1)\nimport static org.junit.jupiter.api.Assertions.*;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\npublic class SequenzaTest {\nprivate Sequenza positivi;\nprivate Sequenza negativi;\n@BeforeEach\npublic void setUp() {\nthis.positivi = new Sequenza(5);\nthis.positivi.setElemento(0,1);\nthis.positivi.setElemento(1,5);\nthis.positivi.setElemento(2,8);\nthis.positivi.setElemento(3,3);\nthis.positivi.setElemento(4,4);\nthis.negativi = new Sequenza(5);\nthis.negativi.setElemento(0,-6);\n…\n   }\n   @Test\npublic void testMassimoPositivi() {…}\n   @Test\npublic void testMassimoNegativi() {…}\n}\n43Metodo eseguito prima di ogni \ninvocazione di test-caseFixture",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#43": "Programmazione orientata agli oggetti\nFixture e Metodo setUp() (2)\n…\npublic class SequenzaTest {\n…\n@Test\npublic void testMassimoPositivi() {\nassertEquals(8, this.positivi.massimo());\n}\n@Test\npublic void testMassimoNegativi() {\nassertEquals(-1, this.negativi.massimo());\n}\n…\n}\n44",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#44": "Programmazione orientata agli oggetti\nsetUp() e l’Importanza dei Nomi\n•Dopo diverso tempo dalla prima scrittura , a meno \nche il nome della fixture this.positivi  sia \nperfettamente indicativo di “sequenza non vuota \ndi interi tutti positivi” si finirà per dover leggere il \ncorpo del metodo setUp() per poterne \ncomprendere appieno il significato\n•Una situazione migliore:\n…\npublic class SequenzaTest {\n…\n@Test\npublic void \ntestMassimoDi SequenzaNonVuotaDiInteriTuttiPositivi () {\nassertEquals(8, this.positivi.massimo());\n}\n…\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#45": "Programmazione orientata agli oggetti\nsetUp(): Controindicazioni\n•Se nel setUp()  si accumulano le fixture di diversi \ntest-case (seq. positive, negative ecc. ecc. …)  si \ncreano tanti oggetti che non hanno nulla a che \nvedere con il singolo test appena fallito, ad es. \ntestMassimoPositivi()\n•Si è costretti a leggere un lungo setUp() solo \nper comprendere un breve test-case\n•Mettendo a fattor comune tutte le fixture \nutilizzate una sola volta si sta ledendo la \nleggibilità dei test-case rendendoli meno \nautocontenuti\n•Tramite il setUp()  si finisce per creare impliciti \nma sottili accoppiamenti tra test-case",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#46": "Programmazione orientata agli oggetti\nsetUp(): Indicazioni\n•In genere, nel setUp() ha senso fattorizzare solo \nla creazione di oggetti che vengano utilizzati da \nalmeno due test-case distinti\n•In tutti gli altri casi meglio non distribuire il \ncodice di uno scenario di test in due distinti \nmetodi test() + setUp() . Altrimenti:\n•Il test non è isolato: per comprenderne uno si \nfinisce per dover capire (almeno una parte) di tutti\n•Il test non è autocontenuto : per ricostruire lo \nscenario di test bisogna cercare ben oltre il corpo \ndel test-case stesso\n•In definitiva, la località del test potrebbe risentirne",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#47": "Programmazione orientata agli oggetti\nQualità dei Test-Case (1)\n•ATTENZIONE: la qualità di un test si avverte in \nparticolare quando il test smette di funzionare e \nbisogna trovare l’errore all’origine del fallimento\n–può capitare anche dopo molto tempo  dalla scrittura \niniziale del codice\n–quando oramai lo stesso non risulta affatto “familiare”\n–magari subito dopo avere effettuato un refactoring...\nLo sforzo necessario per rimuovere un errore \nappena introdotto è uno dei più importanti \nindicatori della qualità dei test che smettono di \nfunzionare\nQuesto sforzo, abbiamo già visto, dipende \nlargamente dalla località  di un test",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#48": "Programmazione orientata agli oggetti\nQualità dei Test-Case (2)\nQual’è la lunghezza ottimale di un test-case?\n–1 (dicesi UNA) – linea di codice totale!\n•E’ possibile perseguire questo obiettivo \nutilizzando alcuni accorgimenti\n–fixtures\n–factory methods\n–minimalità >>\nN.B. per la località dei test non è solo utile conseguire \nil risultato di aver test monolinea, ma è forse ancora \npiù importante ricordarsi di perseguirlo\n–meno linee possibili per test-case\n–molto meglio 10 test-case con 1 asserzione ciascuno che \n1 test-case con 10 asserzioni!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#49": "Programmazione orientata agli oggetti\nFactory Methods\n•Per favorire la semplicità dei test si può \npensare di fattorizzare il codice di creazione \ndella fixture…\npublic class SequenzaTest {\n…\n   private Sequenza sequenza(int... array) {\n       Sequenza risultato = new Sequenza(array.length);\n       for(int i=0; i<array.length; i++) {\n           risultato.setElemento(i,array[i]);\n    }\n    return risultato;\n   }\n@Test\npublic void testMassimoDiSequenzaNonVuotaDiInteriTuttiPositivi() {\nassertEquals(8, sequenza(1,5,8,3,4 ).massimo());\n}\n…\n}Equivale a \nprivate Sequenza sequenza(int[] array)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#5": "Programmazione orientata agli oggetti\nErrori a Tempo di Esecuzione\n•Anche in questo caso abbiamo \ninformazioni molto precise (dalla \nmacchina virtuale)\nException in thread \"main\" \njava.lang.NullPointerException\n        at Diadia.vaiNellaStanza(Gioco.java:176)\n        at Diadia.processaComando(Gioco.java:117)\n        at Diadia.gioca(Gioco.java:71)\n        at Diadia.main(Gioco.java:209)\n6",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#50": "Programmazione orientata agli oggetti\nTesting - Punto di Vista di un \nProgrammatore-Utilizzatore (1)\n•Il factory method sequenza()  ha reso evidente \nquanto sia “faticoso” creare una sequenza per gli \nutilizzatori della classe\n•la classe di test è solo una delle possibili classi \nutilizzatrici/clienti\n•A ben vedere anche altri utilizzatori della classe \npossono convidere la stessa esigenza\n•Se cambiamo Sequenza  per facilitare il testing, \nrendiamo più semplice l’uso della classe da parte \nanche di tutti gli altri utilizzatori\n•Basta aggiungere il costruttore Sequenza(int[] e) ?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#51": "Programmazione orientata agli oggetti\nAutocontenimento dei Test\n…\npublic class SequenzaTest {\n…\n@Test\npublic void testMassimoDi SequenzaNonVuotaDiInteriTuttiPositivi () {\nassertEquals(8, new Sequenza(1,5,8,3,4).massimo());\n}\n…\n@Test\npublic void testMassimoDi SequenzaNonVuotaDiInteriTuttiNegativi () {\nassertEquals(-1, new Sequenza(-6,-1,-8,-13,-10).massimo());\n}\n…\n@Test\npublic void testMassimo InPrimaPosizione () {\nassertEquals(1000, new Sequenza(1000,0,80,-10,-10).massimo());\n}\n…\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#52": "Programmazione orientata agli oggetti\nMinimalità (1)\n•I test visti sinora non sono minimali\n•E’ possibile esprimere lo stesso scenario di \ntesting con test-case più brevi, e che fanno \nuso di oggetti di stato meno complesso\n•Quale di questi test-case preferire? perché?\n@Test\npublic void testMassimoDiSequenzaNonVuotaDiInteriTuttiPositivi () {\n    assertEquals(8, new Sequenza(1,5,8,3,4) .massimo());\n}\n…oppure…\n@Test\npublic void testMassimoDiSequenzaNonVuotaDiInteriTuttiPositivi () {\n    assertEquals(2, new Sequenza(1,2) .massimo());\n}\n???",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#53": "Programmazione orientata agli oggetti\nMinimalità (2)\n•L’uso di test minimali rende molto più \nsemplice la ricerca degli errori\n•Nulla è più minimale di una sequenza vuota!\n   \n@Test\n   public void testMassimoDiSequenzaVuota () {\n   assertEquals( ???, new Sequenza().massimo());\n   }\n•Qual’è il massimo di una sequenza vuota?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#54": "Programmazione orientata agli oggetti\nTesting - Punto di Vista di un \nProgrammatore-Utilizzatore (2)\n•Di nuovo il testing ha evidenziato un \nproblema nel contratto di utilizzo che il \nmetodo massimo()  espone agli utilizzatori\n   @Test(expected = java.util.NoSuchElementException )\n   public void testMassimoDiSequenzaVuota () {\n   new Sequenza().massimo();\n   }public class Sequenza {\n…\n   public int massimo() {\n     if (this.sequenza.length==0)\n        throw new java.util.NoSuchElementException();\n     // …\n}    \n}\n@Test(expected = java.util.NoSuchElementException.class)\n   public void testMassimoDiSequenzaVuota() {\n   new Sequenza().massimo();\n   }JUnit 4",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#55": "Programmazione orientata agli oggetti\n“Si nasce minimali, non ci si diventa”\nConviene sempre partire dai test più semplici, \nperché sono quelli che permetteranno di \nrimuovere la maggior  parte dei bug il minor \nsforzo possibile\nE’ poi naturale aumentare, via via, la \ncomplessità degli scenari di testing\nper aumentare la propria confidenza sulla \ncorrettezza del proprio codice\nAd esempio, se ipotizziamo di ordinare tutti i \ntest-case secondo la complessità dello \nscenario di testing trattato…",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#56": "Programmazione orientata agli oggetti\nScenari di Testing di Complessità Crescente\n…\npublic class SequenzaTest {\n@Test(expected = java.util.NoSuchElementException.class)\n   public void testMassimoDiSequenzaVuota() {\n   new Sequenza().massimo();\n   }\n   @Test\npublic void testMassimoDiSequenzaSingleton() {\n  assertEquals(1, new Sequenza(1).massimo());\n}\n@Test\npublic void testMassimoInPrimaPosizione() {\n  assertEquals(2, new Sequenza(2,1).massimo());\n}\n@Test\npublic void testMassimoInSecondaPosizione() {\n  assertEquals(2, new Sequenza(1,2).massimo());\n}\n   @Test\npublic void testMassimoDiSequenzaNonVuotaDiInteriTuttiNegativi() {\n  assertEquals(-1, new Sequenza(-2,-1).massimo());\n}\n…\n} Fixture di complessità \ncrescente",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#57": "Programmazione orientata agli oggetti\nQuando Scrivere i Test?\n•Uno dei più gravi e purtroppo frequenti errori di \nchi viene introdotto allo unit-testing è aspettare \nla fine della scrittura di tutto il codice principale \nper cominciare a scrivere il codice di test\n•E’ la scelta peggiore! si massimizzano i costi di \nscrittura dei test e si minimizzano i benefici\npotranno esistere contemporaneamente molteplici errori, \nanche correlati, e se tanti test falliscono, non è più chiaro da \ndove cercarli… in due parole: scarsa località\nRisulta più conveniente scriverli \ncontinuativamente, durante  o addirittura prima \ndella scrittura del codice principale (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#58": "Programmazione orientata agli oggetti\nTesting Continuo\n•Il testing deve essere una attività associata ed \naffiancata all’ordinario sviluppo del codice \nprincipale: avviene progressivamente e \ncontinuativamente\n•Principali motivazioni legate ai costi\n–la rimozione precoce degli errori riduce i costi di sviluppo\n–si costruisce contestualmente al codice principale un \nambiente di test\n–si accumulano batterie di test   molto utili per lo sviluppo \ne la manutenzione efficace del codice principale\n–i test possono essere riutilizzati durante la manutenzione \ndel software ad esempio per evitare regressioni\n59",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#59": "Programmazione orientata agli oggetti\n•Chi scrive test si costringe nel ruolo del \nProgrammatore-Utilizzatore  e si focalizza sulla \nsemplicità di utilizzo del proprio codice\n•Per questo motivo il testing aiuta a cambiare \nla prospettiva di visione sul proprio codice, a \nconcentrarsi sulle interfacce delle proprie \nclassi e sulla distribuzione delle responsabilità\n•Tipicamente il codice di qualità è più testabile  \ne viceversa\n•Esistono metodologie di sviluppo che portano \nall’estremo questa attitudine: T.D.D.\n60Testing - Punto di Vista di un \nProgrammatore-Utilizzatore (3)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#6": "Programmazione orientata agli oggetti\nMotivazioni del Testing\n•I programmi sono descrizioni “statiche” a cui possono \ncorrispondere molteplici esecuzioni “dinamiche”\n•I compilatori moderni sono in grado di indicare esattamente \nposizione e motivo degli errori di compilazione\n–addirittura già mentre si scrive! (compilazione \nincrementale )\n•Al contrario i compilatori NON possono prevedere come \nevolverà l’esecuzione di un programma e non sono in grado \ndi individuare gli errori dei programmatori (né possono \nsapere cosa intendessero esprimere con il proprio codice)\n•In sintesi:\n–il compilatore ci aiuta sugli aspetti statici (ad es. analizzando i tipi)\n–il compilatore non dice nulla di nuovo sugli aspetti dinamici (più di \nquanto non sia già implicato dagli aspetti statici)\n7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#60": "Programmazione orientata agli oggetti\nTest Driven Development\n•Promuove l’uso dei test anche come \nstrumento di progettazione \n–i test guidano lo sviluppo verso codice che sia \nsemplice, facilmente testabile e di qualità\n•Predica la scrittura dei test-case prima  della \nscrittura del codice testato\n•anticipa nel tempo ed evidenzia il punto di \nvista del Programmatore-Utilizzatore\n•predilige micro-iterazioni\ntesting-coding-testing-coding ... \nche incentivano la minimalità dei test\n61",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#61": "Programmazione orientata agli oggetti\nSviluppo Guidato dai Test\n•Se scriviamo il codice di test prima del \ncodice stesso siamo incentivati a:\n–precisare i metodi visibili all’esterno in \nquanto il codice di test è codice cliente\n•esterno alla classe alla stregua di tutte le \naltri classi clienti del codice testato\n–chiarire la semantica dei metodi\n–cercare di semplificare al massimo \nl’utilizzo del codice\n–Individuare i casi limite e chiarire la \ngestione delle situazioni anomale\n62",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#62": "Programmazione orientata agli oggetti\nMotivazioni del Testing: Conclusioni\n•La rimozione precoce degli errori riduce i costi \ndi sviluppo e migliora la qualità del codice\n•I test inducono ad assumere anticipatamente \nil punto di vista del Programmatore-\nUtilizzatore e spingono gli sviluppatori verso \nsoluzioni più semplici per gli utilizzatori\n•Si documenta in maniera formale e precisa il \nfunzionamento del codice\n63",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#63": "Programmazione orientata agli oggetti\nJupiter - JUnit 5 vs JUnit 4\n●Molti miglioramenti, nessuna rivoluzione, perché già JUnit 4 funzionava benissimo\n●Alcune rendono il testing più facile in particolari scenari\n─test parametrici (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#64": "Programmazione orientata agli oggetti\nJUnit 5 vs JUnit 4: In Pratica\n●Nella pratica serve conoscere entrambe le versioni\n─“Greenfield” project: usare Jupiter – JUnit 5\n─“Brownfield” project: continuare ad usare JUnit 4\n●Conviene cambiare?\n─Soprattutto per un uso basico, JUnit 4 già funzionava benissimo\n─Per utilizzi più avanzati, le soluzioni Junit 4 risultano, in genere, sensibilmente \npiù “macchinose” delle equivalenti in JUnit 5\n●Un project “brownfield” su tutti: il SISTEMA QUIZ\n─Si basa su JUnit 4 e non vale la pena di aggiornarlo, al momento…\n●Se di decide di aggiornare un progetto, come prima cosa ricordarsi di cambiare gli \nimport…\n─Da JUnit 4:\n•import static org.junit.Assert.*;  // Asserzioni  assertXYZ… \n•import org.junit.*;                // Annotazioni @… \n─A JUnit 5:\n•import static org.junit.jupiter.api.Assertions.*; // assertXYZ…\n•import org.junit.jupiter.api.*;    // Annotazioni @… ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#65": "Programmazione orientata agli oggetti\nEsercizi\n•Scrivere (con Eclipse) una classe di test JUnit per la classe \nPersone (dal Quiz di preparazione alla prima verifica)  \n•In particolare testare il metodo int contaOmonimiDi(String nome)\n•Scrivere il codice del metodo int contaOmonimiDi(String nome)\n•Eseguire la classe di test JUnit (se il test fallisce, correggere il \nmetodo sotto test e far girare nuovamente la classe di test)\n66public class Persone {\n    private String[] nomi;\n    \n    public Persone(int n) {\n        this.nomi = new String[n];\n    }\n    \n    public int contaOmonimiDi(String nome) {\n        // metodo da scrivere\n    }\n    \n    public void aggiungiNome(int indice, String nome){\n        this.nomi[indice] = nome;\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#7": "Programmazione orientata agli oggetti\nI Bug\n•I bug sono errori nell’evoluzione \ndinamica di un programma su cui il \ncompilatore non ha potuto prevedere e \ndire nulla\n•Il debugging è completamente a carico \ndel programmatore\n•Il costo di debugging è ritenuto di gran \nlunga la componente principale nel costo \ndei moderni progetti software\n8",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#8": "Programmazione orientata agli oggetti\nCiclo di Debugging\n•Come si effettua il debugging di un \nprogramma che compila? Con estenuanti \ncicli:\n \n     \n        esecuzione\n        controllo manuale dei risultati\n       \n      ricerca del bug\n      modifiche al codice\n      compilazione\n      \n          rimozione errori di compilazione\n          compilazione\n      \n9A sua volta può richiedere:\nsessioni di tracing/logging\nsessioni con il debugger",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-08-testing.pdf#9": "Programmazione orientata agli oggetti\nCosto del Debugging\n•Debugging del codice: operazione molto \ncostosa (nonostante gli ausili dell'IDE)\n•E’ noto che il costo della correzione di bug \ndipende da almeno due grandezze che ne \ndeterminano la località :\n–le “dimensioni” del contesto\nnumero di linee di codice in cui il bug può annidarsi\n–il “tempo” che il bug impiega  per manifestarsi\nmisura temporale di quanto dista la causa del bug ( durante \nun’esecuzione del codice ) ed il rilevamento dei suoi effetti \n10",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#0": "Programmazione \nOrientata agli Oggetti\nInterfacce e Polimorfismo \nUpcasting e Downcasting",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#1": "Programmazione orientata agli oggetti\nContenuti \n•Riferimenti tipati\n•Java Interface\n•Principio di sostituzione\n•Polimorfismo e late binding\n•Tipo statico e tipo dinamico\n•Interfacce come ruolo\n2",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#10": "Programmazione orientata agli oggetti\nJava Interface (3)\n•Nelle interface specifichiamo solo le segnature \n(e il tipo restituito) dei metodi che un tipo può \noffrire\n•In una interface  non c'è nessun dettaglio \nrelativo alla implementazione \n–Niente variabili\n–Niente costruttori\n–Niente corpo dei metodi\n•Le interface non si possono istanziare\n•Ma una classe può implementare una (o più) \ninterface\n•Una classe che implementa una interface  \ngarantisce che le sue istanze rispettino il tipo \nspecificato nella interface  11",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#11": "Programmazione orientata agli oggetti\nClassi ed Interface (1)\n•La parola chiave implements  serve a \nspecificare che la classe Tamburo  \nimplementa l'interfaccia Strumento\n•Questo significa che gli oggetti Tamburo  \nsono in grado di offrire i metodi del tipo \nStrumento\n12public class Tamburo implements Strumento  {\n  public void produciSuono() {\n    System.out.println(\"bum-bum-bum\");   \n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#12": "Programmazione orientata agli oggetti\nClassi ed Interface (2)\n•Una classe che implementa una interface  \npuò avere altri metodi (oltre a quelli della \ninterface ) specifici della classe\n13public class Chitarra implements Strumento  {\n  private int[] corde;\n  public Chitarra(){\n    corde = new int[6];\n  }\n  public void produciSuono() {\n    System.out.println(\"dlen-dlen-dlen\");   \n  }\n  public int accorda(int corda, int val) {\n    return corde[corda] += val;\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#13": "Programmazione orientata agli oggetti\nEsempio\n14public class Tamburo implements Strumento  {\n  public void produciSuono()  {\n    System.out.println(\"bum-bum-bum\");   \n  }\n}\npublic class Chitarra implements Strumento  {\n  private int[] corde;\n  public Chitarra(){\n    corde = new int[6];\n  }\n  public void produciSuono()  {\n    System.out.println(\"dlen-dlen-dlen\");   \n  }\n  public int accorda(int corda, int val) {\n    return corde[corda] += val;\n  }\n}public class Tromba implements Strumento  {\n  public void produciSuono()  {\n    System.out.println(\"pe-pe-re-pe-pe\");\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#14": "Programmazione orientata agli oggetti\nDiagramma delle Classi\n15\n<<interface>>\nStrumento\nvoid produciSuono()<<interface>>\nStrumento\nvoid produciSuono()\nTromba\nTromba\nTamburo Tamburo<<implements>><<implements>>\nChitarra\ncorde int[]\nChitarra\ncorde int[]<<implements>>",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#15": "Programmazione orientata agli oggetti\nTipi, Sottotipi & Supertipi\n•Abbiamo detto che una interface  definisce \nun tipo\n•Se la classe C implementa una interface I  \ndiciamo che: \n–C è un sottotipo di I\ne che \n–I è un supertipo  di C\n•Ad esempio\n•Tamburo  è un sottotipo di Strumento\n•Strumento  è un supertipo di Tamburo\n16",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#16": "Programmazione orientata agli oggetti\nContenuti \n•Riferimenti tipati\n•Java Interface\n•Principio di sostituzione\n•Polimorfismo e late binding \n•Tipo statico e tipo dinamico\n•Interfacce come ruolo\n17",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#17": "Programmazione orientata agli oggetti\nPrincipio di Sostituzione (1)\n•In Java vale il principio di sostituzione  (di Liskov) : \nun sottotipo può essere usato al posto di un suo supertipo\n•Rivediamo il metodo suona(Strumento s)  della classe \nMusicista:\n•Se invochiamo il metodo suona(Strumento s), per il \nprincipio di sostituzione, possiamo passargli anche un \nriferimento ad un oggetto istanza di una qualunque \nclasse che implementi l'interfaccia Strumento\n18public void suona( Strumento s ){\ns.produciSuono();\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#18": "Programmazione orientata agli oggetti\nPrincipio di Sostituzione (2)\n•Esempio:\n•Nelle chiamate al metodo suona(Strumento s) \nabbiamo usato un riferimento ad un oggetto \nChitarra  (e poi un riferimento ad un oggetto \nTamburo ) al posto di un riferimento a Strumento\n19public static void main(String[] args){\n  Chitarra c = new Chitarra();\n  Strumento t = new Tamburo();\n  Musicista ludovico = new Musicista(\"Ludovico\");\n  ludovico.suona(c);\n  ludovico.suona(t);\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#19": "Programmazione orientata agli oggetti\nPrincipio di Sostituzione (3)\n•Per il principio di sostituzione, un riferimento \nad un sottotipo può essere assegnato ad un \nriferimento ad un suo supertipo\n•Esempio:\n    Strumento s;\n    Chitarra c;\n    c = new Chitarra();\n    s = c;\n20",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#2": "Programmazione orientata agli oggetti\nContenuti \n•Riferimenti tipati\n•Java Interface\n•Principio di sostituzione\n•Polimorfismo e late binding \n•Tipo statico e tipo dinamico\n•Interfacce come ruolo\n3",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#20": "Programmazione orientata agli oggetti\nPrincipio di Sostituzione (4)\n•Commentiamo le precedenti istruzioni\nStrumento s;\n–Abbiamo definito una variabile s: contiene un riferimento ad \nun oggetto che rispetta il tipo Strumento\nChitarra c;\n–Abbiamo definito una variabile c: contiene un riferimento ad \nun oggetto che rispetta il tipo Chitarra\nc = new Chitarra();\n–Abbiamo creato un oggetto Chitarra  e ne abbiamo \nassegnato il riferimento alla variabile c\ns = c;\n–Abbiamo assegnato il riferimento all'oggetto c alla variabile s\n•È tutto lecito perché l'oggetto c è istanza della classe \nChitarra  che implementa il supertipo Strumento\n21",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#21": "Programmazione orientata agli oggetti\nUpcasting\n•La promozione da un tipo ad un suo supertipo \nviene chiamata upcasting\n•upcasting  : un riferimento ad un oggetto è \n“promosso” in un riferimento ad un suo \nsupertipo\n•Il termine (“Up”=”verso l’alto”) è \ntradizionalmente legato al modo in cui vengono \nespresse graficamente le dipendenze \nsupertipo/sottotipo \n(vedi diagramma delle classi)\n22\n<<interface>>Strumentovoid produciSuono()<<interface>>Strumento\nvoid produciSuono()\nTromba\nTromba\nTamburo Tamburo<<implements>><<implements>>\nChitarra\ncorde int[]\nChitarra\ncorde int[]<<implements>>",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#22": "Programmazione orientata agli oggetti\nContenuti \n•Riferimenti tipati\n•Java Interface\n•Principio di sostituzione\n•Polimorfismo e late binding\n•Tipo statico e tipo dinamico\n•Interfacce come ruolo\n23",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#23": "Programmazione orientata agli oggetti\nPolimorfismo e Late Binding (1)\n•Consideriamo la classe Musicista\n24public class Musicista {\nprivate String nome;\npublic Musicista(String nome){\nthis.nome = nome;\n} \npublic void suona(Strumento s){\ns.produciSuono();\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#24": "Programmazione orientata agli oggetti\nPolimorfismo e Late Binding (2)\n•Cosa succede a tempo di esecuzione, quando al \nparametro s è legato un oggetto?\n•Sappiamo che il metodo produciSuono()  viene \ninvocato da un oggetto la cui classe implementa \nl'interfaccia Strumento  \n•Ma il codice da eseguire non è noto se non a tempo di \nesecuzione\n•Il collegamento tra segnatura e corpo del codice da \neseguire per produciSuono()  viene stabilito solo a \ntempo di esecuzione ( late binding )\n•C'è un comportamento polimorfo  del parametro \nformale Strumento s  \n–può assumere forme/comportamenti diversi: tutti quelli dei \nsuoi sottotipi\n25",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#25": "Programmazione orientata agli oggetti\nContenuti \n•Riferimenti tipati\n•Java Interface\n•Principio di sostituzione\n•Polimorfismo e late binding\n•Tipo statico e tipo dinamico\n•Interfacce come ruolo\n26",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#26": "Programmazione orientata agli oggetti\nTipo Statico e Tipo Dinamico\n•Consideriamo la seguente istruzione:\nStrumento s = new Chitarra();\n•È lecita, per il principio di sostituzione\n•Qual è il tipo della variabile s?\n•Dobbiamo distinguere tra\n–Tipo statico\n–Tipo dinamico\n27",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#27": "Programmazione orientata agli oggetti\nTipo Statico\n•Il tipo statico è quello che viene usato nella \ndichiarazione della variabile\n•Ad esempio, nella istruzione:\nStrumento s = new Chitarra();\nil tipo statico di s è Strumento  \n•Il tipo statico è determinato a tempo di compilazione\n•Il compilatore permette di applicare i metodi del tipo \nstatico (ovvero verifica che su una variabile siano \ninvocati i metodi del suo tipo statico)\n•Nel nostro esempio possiamo invocare su s solo i \nmetodi di Strumento  \n    Strumento s = new Chitarra();\n    s.produciSuono();// CORRETTO\n    s.accorda(2,1);  // ERRATO: il tipo Strumento non \n// possiede il metodo accorda(int, int)\n28",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#28": "Programmazione orientata agli oggetti\nTipo Dinamico\n•Il tipo dinamico è quello dell'oggetto realmente \nistanziato e quindi referenziato in memoria\n•Ad esempio, nella istruzione:\nStrumento s = new Chitarra();\nil tipo dinamico di s è Chitarra\n•Il tipo dinamico stabilisce quale sarà \nl'implementazione usata \n•Nel nostro esempio:\nStrumento s = new Chitarra();\ns.produciSuono();\n•A tempo di esecuzione il codice del metodo \nproduciSuono()  che viene usato è quello definito \nnella classe Chitarra\n29",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#29": "Programmazione orientata agli oggetti\nTipo Statico e Tipo Dinamico\n•Capire la differenza tra tipo statico e tipo \ndinamico è fondamentale\n•Il tipo statico viene assegnato dal compilatore \ne determina l'insieme dei metodi che possono \nessere invocati\n•Il tipo dinamico interviene a tempo di \nesecuzione e determina l'implementazione che \nviene eseguita \n30",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#3": "Programmazione orientata agli oggetti\nRiferimenti Tipati (1)\n•In Java i riferimenti sono tipati, ovvero \nspecificano il tipo dell'oggetto referenziato\n•La definizione:\nStrumento s;\nafferma  che s è un riferimento ad un oggetto \ndi tipo Strumento\n•Questo significa che attraverso s possiamo \ninvocare i servizi del tipo Strumento\n–ovvero che è possibile chiedere di eseguire i \nmetodi offerti dal tipo Strumento all’oggetto \nreferenziato da s\n4",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#30": "Programmazione orientata agli oggetti\nTipo Statico vs Tipo Dinamico (1)\n31Chitarra c = new Chitarra();Qual'è il tipo di c?\nStrumento s = new Chitarra();Qual'è il tipo di s?Tipo Dinamico Tipo Statico\nTipo Dinamico Tipo Statico",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#31": "Programmazione orientata agli oggetti\nTipo Statico vs Tipo Dinamico (2)\n•Il tipo dichiarato di una variabile è il suo tipo \nstatico\n•Il tipo dell'oggetto a cui una variabile si \nriferisce è il suo tipo dinamico\n•Il compilatore si preoccupa di verificare \nviolazioni del tipo statico\n  Strumento strumento = new Chitarra();\n  strumento.accorda(2,1);// ERRORE a tempo di compilazione\n•accorda()  non è tra i metodi di Strumento  \n(tipo statico della var. locale strumento )\n32",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#32": "Programmazione orientata agli oggetti\nTipo Statico vs Tipo Dinamico (3)\n•A tempo di esecuzione viene eseguito il \nmetodo del tipo dinamico\n✔d’altronde i metodi definiti nelle interfacce \nnon possiedono implementazione se non \nquella delle classi che le implementano\n•Nota che il compilatore non solo non \nconosce , ma neanche può prevedere , in \ngenerale , i tipi dinamici >>\n33",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#33": "Programmazione orientata agli oggetti\nImprevedibilità dell’Esecuzione (1)\nimport java.util.Random;\npublic class OrchestraCausale {\npublic static void main(String[] args){\n  Strumento[] orchestra = new Strumento[10];\n  Random r = new Random();\n  for(int i=0; i<orchestra.length; i++) {\n  int numeroAcaso = r.nextInt(3);\n  if (numeroAcaso==0)\n  orchestra[i] = new Chitarra();\n  if (numeroAcaso==1)\n  orchestra[i] = new Tamburo();\n  if (numeroAcaso==2)\n  orchestra[i] = new Tromba();\n   }\n  for(int i=0; i<orchestra.length; i++)\norchestra[i].produciSuono();\n}\n}\n34",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#34": "Programmazione orientata agli oggetti\nImprevedibilità dell’Esecuzione (2)\n•Nell'esempio precedente l'array è riempito \ncasualmente a tempo di esecuzione: non \nsappiamo a priori quali strumenti vengono \nassegnati ai vari elementi dell'array\n•A tempo di esecuzione, ogni elemento \ndell'array produce il suono corrispondente al \ntipo dinamico\n•A tempo di compilazione, ogni elemento \ndell’array possiede tipo statico Strumento\nIl compilatore non può prevedere il tipo dinamico degli \noggetti effettivamente utilizzati a tempo di esecuzione  \n35",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#35": "Programmazione orientata agli oggetti\nTipo Statico e Tipo Dinamico: \nOverloading (1)\n•L'overloading dei metodi viene risolto dal \ncompilatore, quindi staticamente\n•In particolare: \n–se abbiamo un metodo sovraccarico il \ncompilatore guarda il tipo statico dei parametri \nper decidere qual’è il metodo da invocare\n•Vedi esercizio seguente \n36",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#36": "Programmazione orientata agli oggetti\nTipo Statico e Tipo Dinamico: \nOverloading (2)\ninterface  Edificio {\n     public int altezza();\n}\npublic class Palazzo implements Edificio {\nprivate int altezza;\npublic Palazzo(int altezza) {this.altezza = altezza;}\npublic int altezza() {return this.altezza;}\n}\npublic class Coloratore {\n     public void colora(Edificio e) {\nSystem.out.println(\"Colorato Edificio\");\n     }\n     public void colora(Palazzo p) {\nSystem.out.println(\"Colorato Palazzo\");\n     }\n     public static void main(String args[]) {\nPalazzo p = new Palazzo(4);\nEdificio e = new Palazzo(3);\n        Coloratore c = new Coloratore();\n        c.colora(p);\nc.colora(e);\n     }\n}\n37Tipo statico di p è Palazzo\nTipo statico di e è Edificio",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#37": "Programmazione orientata agli oggetti\nTipo Statico e Tipo Dinamico: \nEsempio\ninterface  Veicolo {\n     public void func(Veicolo v);\n     public void func(Autotreno a);\n}\npublic class Autotreno implements Veicolo {\n     public void func(Veicolo v) {\nSystem.out.println(\"Autotreno.func(Veicolo) \");\n     }\n     public void func(Autotreno a) {\nSystem.out.println(\"Autotreno.func(Autotreno) \");\n     }\n     public static void main(String args[]) {\n Veicolo a = new Autotreno();\n        Autotreno b = new Autotreno();\n        a.func(b);\n        a.func(a);\n     }\n}\n38Tipo statico di b è \nAutotreno\nTipo statico di a è Veicolo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#38": "Programmazione orientata agli oggetti\nEsercizi\n•Fare le verifiche\n–L.java\n–Olimpiadi.java\n–Villa.java\n39",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#39": "Programmazione orientata agli oggetti\nContenuti \n•Riferimenti tipati\n•Java Interface\n•Principio di sostituzione\n•Polimorfismo e late binding\n•Tipo statico e tipo dinamico\n•Interfacce come ruolo\n40",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#4": "Programmazione orientata agli oggetti\nRiferimenti Tipati (2)\n•Consideriamo la seguente classe Musicista\n5public class Musicista {\nprivate String nome;\npublic Musicista(String nome) {\nthis.nome = nome;\n} \npublic void suona(Strumento s) {\ns.produciSuono();\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#40": "Programmazione orientata agli oggetti\nInterface come Ruolo (1)\n•Una classe può implementare più di una \ninterface\n•Potremmo dire che ciascuna interface \nimplementata da una classe rappresenta uno \nspecifico \"ruolo\" che la classe può assumere\n•Ragionare sui ruoli (ed usare le potenzialità \ndel polimorfismo) ci aiuta a produrre codice \naltamente riutilizzabile\n41",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#41": "Programmazione orientata agli oggetti\nInterface, Ruoli e Riuso (1)\n•Consideriamo un problema noto che si presta \nnaturalmente ad un comportamento polimorfo \ndegli oggetti interessati: l'ordinamento\n•Supponiamo di avere una classe che modella \nun \"orario\", espresso in ore e minuti\n•Supponiamo di avere una collezione (per \nsemplicità un array) di oggetti orario\n•Supponiamo di voler ordinare questa \ncollezione\n42",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#42": "Programmazione orientata agli oggetti\nEsempio: La Classe Orario\npublic class Orario {\n   private int ore;\n   private int minuti;\n   public Orario(int ore, int minuti) {\n       this.ore = ore;\n       this.minuti = minuti;\n   }\n   public int getOre() {\n       return this.ore;\n   }\n   public int getMinuti() {\n       return this.minuti;\n   }\npublic boolean minoreDi(Orario o) {\n    if (this.getOre() > o.getOre()) \n           return false;\n       if (this.getOre() == o.getOre()) \n           return (this.getMinuti() < o.getMinuti());\n    return true;\n}\n   public String toString() {\n       return this.getOre()+\":\"+this.getMinuti();\n   }\n}\n43",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#43": "Programmazione orientata agli oggetti\nInterface, Ruoli e Riuso (2)\n•Per ordinare la collezione creiamo una opportuna \nclasse che offre questa funzionalità attraverso il \nmetodo ordina(Orario[])\n•Scriviamo il codice \n(usiamo un qualsiasi algoritmo di ordinamento, \ncfr. corso Fondamenti, ad esempio il «selection \nsort»)\n•vedi classe OrdinatoreOrari\n44",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#44": "Programmazione orientata agli oggetti\nLa Classe OrdinatoreOrari\npublic class OrdinatoreOrari {\n  public static void ordina(Orario[] lista) {\n   int imin;\n   for (int ord=0; ord<lista.length-1; ord++) {\n     imin = ord;\n     for (int i=ord+1; i<lista.length; i++)\n       if (lista[i].minoreDi(lista[imin])) {\n          Orario temp=lista[i];\n          lista[i]=lista[imin];\n          lista[imin]=temp;\n       }\n     }\n   }\n }\n45",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#45": "Programmazione orientata agli oggetti\nInterface, Ruoli e Riuso (3)\n•Osserviamo bene il codice di \nOrdinatoreOrari\n•Affinché gli oggetti dell'array possano essere \nordinati, l'unica proprietà che questi oggetti \ndevono avere è quella di possedere un metodo \nminoreDi(Orario)\n•In altri termini l'ordinamento funziona su \noggetti che sappiano interpretare il ruolo di \n«essere confrontati» \n•Questo ruolo lo possiamo esplicitare in una \nopportuna interface\n46",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#46": "Programmazione orientata agli oggetti\n“Confrontabilità”, come Ruolo\n•Creiamo l'interface Comparabile : gli \noggetti delle classi che la implementano \nsono in grado di essere confrontati \ntramite il metodo \nminoreDi(Comparabile)\n47",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#47": "Programmazione orientata agli oggetti\nL'interface Comparabile\npublic interface Comparabile {\npublic boolean minoreDi(Comparabile c);\n}\n48",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#48": "Programmazione orientata agli oggetti\nInterface, Ruoli e Riuso (4)\n•Possiamo ora generalizzare la nostra classe \nOrdinatore (e il relativo algoritmo di \nordinamento) affinché funzioni su tutte le \nclassi che sappiano interpretare il ruolo \nComparabile\n49",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#49": "Programmazione orientata agli oggetti\nLa Classe Ordinatore\npublic class Ordinatore {\n public static void ordina( Comparabile [] lista){\n   int imin;\n   for (int ord=0; ord<lista.length-1; ord++){\n     imin = ord;\n     for (int i=ord+1; i<lista.length; i++)\n       if (lista[i]. minoreDi (lista[imin])){\n          Comparabile temp=lista[i];\n          lista[i]=lista[imin];\n          lista[imin]=temp;\n       }\n     }\n   }\n }50",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#5": "Programmazione orientata agli oggetti\nRiferimenti Tipati (3)\n•Il metodo suona(Strumento s)  prende \ncome parametro un riferimento ad un \noggetto il cui tipo è Strumento\n•Nel corpo del metodo è possibile \ninvocare su s tutti i metodi offerti dal \ntipo Strumento\n–intuiamo che Strumento  offre il metodo \npublic void produciSuono()\n6",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#50": "Programmazione orientata agli oggetti\nLa Classe Orario  (rivisitata)\npublic class Orario implements Comparabile  {\n   private int ore;\n   private int minuti;\n   public Orario(int ore, int minuti) {\n     this.ore = ore;\n     this.minuti = minuti;\n   }\n   public int getOre() {\n     return this.ore;\n   }\n   public int getMinuti() {\n     return this.minuti;\n   }\n   public boolean minoreDi(Comparabile c)  {\n     Orario o;\n     o = (Orario)c;\n     if (this.getOre() > o.getOre()) \n        return false;\n     if (this.getOre() == o.getOre()) \n        return (this.getMinuti() < o.getMinuti());\n     return true;\n   }\n   public String toString() {\n     return this.getOre()+\":\"+this.getMinuti();\n   }\n}\n51 public boolean minoreDi(Comparabile c)  {\n     Orario o;\n     o = (Orario)c;\n     if (this.getOre() > o.getOre()) \n        return false;\n     if (this.getOre() == o.getOre()) \n        return (this.getMinuti() < o.getMinuti());\n     return true;\n   }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#51": "Programmazione orientata agli oggetti\nInterface, Ruoli e Riuso (5)\n•Per rispettare l'interface Comparabile  il \nmetodo minoreDi()  deve prendere come \nparametro un oggetto Comparabile \n   \n   public boolean minoreDi(Comparabile c)\n•Quando però scriviamo il codice, dobbiamo \npoter usare i metodi specifici della classe \nOrario (altrimenti non potremmo \nimplementare il metodo!)\n•Il compilatore non ce lo permette: il tipo \nstatico del parametro è Comparabile\n52",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#52": "Programmazione orientata agli oggetti\nDowncasting (1)\n•È necessaria  allora una “forzatura” sul tipo \ndel parametro\n•In particolare forziamo l’utilizzo (a tempo \nstatico ed anche dinamico>>) del sottotipo\n•Questa operazione viene chiamata \ndowncasting (in opposizione all’ upcasting )\n53\n<<interface>>\nStrumentovoid produciSuono()<<interface>>Strumento\nvoid produciSuono()\nTromba\nTromba\nTamburo Tamburo<<implements>> <<implements>>\nChitarra\ncorde int[]\nChitarra\ncorde int[]<<implements>>",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#53": "Programmazione orientata agli oggetti\nDowncasting (2)\n•Quando si opera il downcasting: \n•Informiamo il compilatore che vogliamo \n“forzarlo” ad usare un certo tipo statico\n•Lo stesso introduce (a tempo statico, durante la \ncompilazione) nel codice oggetto un controllo \nda eseguirsi a tempo dinamico, durante \nl’esecuzione: la macchina virtuale verifica che \nl'operazione sia lecita e possibile\n•Ovvero verifica che l'oggetto sia di un tipo dinamico \neffettivamente sottotipo del tipo statico forzato\n•In caso contrario il programma abortisce \nsollevando una eccezione di tipo \njava.lang.ClassCastException\n54",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#54": "Programmazione orientata agli oggetti\nEsercizio\n1)Scrivere con JUnit test-case minimali  per \nconfermare il corretto funzionamento del \nmetodo minoreDi()  come implementato nella \nclasse Orario\n2)Scrivere con JUnit un test-case per:\n–definire e creare un array di 5 oggetti Orario\n–creare 5 oggetti orario, che rappresentino i seguenti \norari: 12:30, 21:40, 9:20, 4:00, 1:35\n–mettere i riferimenti ai 5 oggetti creati negli elementi \ndell'array\n–Ordinare l'array\n–Verificare che sia correttamente ordinato  \n55",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#55": "Programmazione orientata agli oggetti\nEsercizio\n•Scrivere una classe Studente , che contenga i \ncampi nome (una stringa), età (un intero), un \ncostruttore con due parametri, e i metodi \naccessori\n•La classe Studente deve implementare \nl'interfaccia Comparabile , descritta in precedenza \n(vedi codice di Orario )\n•Scrivere un metodo che crei un array di oggetti \nStudente e lo ordini (per età) usando il metodo \nOrdinatore.ordina () \n•Scrivere con JUnit una classe di test per verificare \nche l’array sia effettivamente ordinato, dopo \nl'invocazione del metodo Ordinatore.ordina ()\n56",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#56": "Programmazione orientata agli oggetti\nEsercizio\n•Introdurre nell'interfaccia Comparabile  un nuovo metodo\n int compara(Comparabile c)  \nche restituisce un valore negativo, pari a 0, positivo, se \nl'oggetto su cui è chiamato il metodo è rispettivamente \nminore, uguale, maggiore del valore del parametro\n•Nella classe Ordinatore , scrivere il codice del metodo:\npublic static int  \n   ricercaBinaria(Comparabile[] v, Comparabile cercato)\nche implementa l'algoritmo di ricerca binaria (cfr. corso di \nFondamenti di Informatica); questo metodo restituisce un \nintero il cui valore corrisponde alla posizione dell'elemento \ncercato  nell'array v oppure a -1 se tale elemento non è \npresente\n•Scrivere, utilizzando JUnit, una classe di test per verificare il \ncorretto funzionamento del metodo\n57",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#6": "Programmazione orientata agli oggetti\nCostrutti Java per la \nDefinizione di Nuovi Tipi\n•Fino ad ora abbiamo visto un solo modo \nper definire nuovi tipi: la definizione di \nnuove classi (mediante il costrutto \nclass )\n•In Java (e in altri moderni linguaggi OO, \ncome ad esempio C#, Scala) esistono \nmolteplici costrutti per definire nuovi tipi \n•È il costrutto interface\n7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#7": "Programmazione orientata agli oggetti\nContenuti \n•Riferimenti tipati\n•Java Interface\n•Principio di sostituzione\n•Polimorfismo e late binding\n•Tipo statico e tipo dinamico\n•Interfacce come ruolo\n8",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#8": "Programmazione orientata agli oggetti\nJava Interface (1)\n•Possiamo dire che una interface  specifica un \ntipo in termini dei servizi, ovvero dei metodi, \nche questi può offrire\n•Una interface  non specifica i dettagli \nimplementativi dei vari servizi, specifica \nsolamente in che modo i servizi possono \nessere invocati (nome, parametri, tipo \nrestituito)\n•In definitiva una interface  consiste in una \nspecifica delle segnature (e dei tipi restituiti) \ndai metodi che il tipo può offrire\n9",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-09-polimorfismo-interfacce.pdf#9": "Programmazione orientata agli oggetti\nJava Interface (2)\n•Esempio:\npublic interface Strumento {\npublic void produciSuono();\n }\n✔L'interface Strumento  definisce il tipo di \noggetti che possono offrire il metodo \nproduciSuono()\n10",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#0": "Programmazione \nOrientata agli Oggetti\nEsercitazione:\nInterfacce, Polimorfismo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#1": "Programmazione orientata agli oggetti\nEsercitazione FormeGeometriche\n(TRATTO DALL'ESAME DEL GIUGNO 2003)\n•Una software house sta sviluppando una libreria per la \ngestione di forme geometriche. Allo stato attuale nella \nlibreria ci sono le classi Punto, Cerchio  e Rettangolo  \n(vedi codice)\n•Sono già date le seguenti classi:\n–Punto \n–Cerchio\n–Rettangolo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#10": "Programmazione orientata agli oggetti\nEsercizio 3: Testing su \nGruppoDiForme\n•Creare una classe di test per GruppoDiForme  e \nscrivere test-case minimali del metodo trasla()  \nche eseguano le seguenti istruzioni\n•Distinguere almeno questi scenari di testing a \ncomplessità crescente, nell’ordine:\n–un gruppo vuoto\n–un gruppo semplice , con una sola forma non \nulteriormente decomponibile\n–un gruppo composito , ovvero di un gruppo contenente \nun gruppo semplice\n–un gruppo complesso, ovvero di un gruppo contenente \nun gruppo composito\n•Scrivere diversi test-case minimali  per ciascun \nscenario",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#11": "Programmazione orientata agli oggetti\nEsercizio 4: Downcasting per \nassertEquals()  nel Metodo equals()\nIl metodo (di JUnit) assertEquals()  si basa \nsull'esecuzione del metodo equals()  sugli oggetti \npassati come argomento\nLa segnatura esatta del metodo equals DEVE essere:\n   @Override\n   public boolean equals( Object o )\nPer accorciare i test-case già prodotti:\n–munire la classe Punto di un metodo equals()\n–Oggetti istanza della classe Punto distinti ma di pari coordinate \neguali sono considerati equivalenti\n–assicurarsi che i test usino assertEquals() passandogli \noggetti istanza di Punto\n–controllare che risulti effettivamente invocato il metodo \nPunto.equals (Object o)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#2": "Programmazione orientata agli oggetti\nLa Classe Punto\npublic class Punto {\nprivate int x,y;\npublic Punto (int x, int y) {\nthis.x = x;\nthis.y = y;\n}\npublic void setX(int x){ \nthis.x = x; \n}\npublic void setY(int y){ \nthis.y = y; \n}\npublic int getX(){ \nreturn this.x; \n}\npublic int getY(){ \nreturn this.y; \n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#3": "Programmazione orientata agli oggetti\nLa Classe Cerchio\npublic class Cerchio {\nprivate int raggio;\nprivate Punto centro;\npublic Cerchio(Punto centro, int raggio) {\nthis.raggio = raggio;\nthis.centro = new Punto(centro.getX(), centro.getY());\n}\npublic void trasla(int deltaX, int deltaY) {\nthis.centro.setX(this.centro.getX() + deltaX);\nthis.centro.setY(this.centro.getY() + deltaY);\n}\npublic Punto getCentro() { return this.centro; }\npublic int getRaggio() { return this.raggio; }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#4": "Programmazione orientata agli oggetti\nLa Classe Rettangolo\npublic class Rettangolo {\nprivate int altezza, base;\nprivate Punto vertice;\npublic Rettangolo(Punto vertice, int altezza, int base) {\n     this.altezza = altezza;\n     this.base = base;\n     this.vertice = new Punto(vertice.getX(), vertice.getY());\n}\npublic void trasla(int deltaX, int deltaY) {\n     this.vertice.setX(this.vertice.getX() + deltaX);\n     this.vertice.setY(this.vertice.getY() + deltaY);\n}\npublic Punto getVertice() { return this.vertice; }\npublic int getBase()      { return this.base;    }\npublic int getAltezza()   { return this.altezza; }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#5": "Programmazione orientata agli oggetti\nEsercizio 1: Polimorfismo\n•Si vuole introdurre una classe GruppoDiForme  che \nrappresenta un raggruppamento di forme. In particolare, le \nforme di un raggruppamento possono essere rettangoli, \ncerchi e altri raggruppamenti.  Un gruppo di forme può \nessere traslato (vengono traslate tutte le forme che lo \ncompongono)\n•La classe GruppoDiForme  deve offrire i metodi:\n–void trasla(int deltaX, int deltaY)\ntrasla tutto il raggruppamento (cioè tutti gli oggetti che \ncompongono il raggruppamento)\n–void aggiungiForma(Forma forma)\n   aggiunge una forma al raggruppamento",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#6": "Programmazione orientata agli oggetti\nEsercizio 1: Polimorfismo (Continua)\n•Suggerimento : astrarre i concetti di forma \ngeometrica (rettangolo, cerchio, gruppo) in una \ninterfaccia Forma. Quindi, nell’ordine:\n1.Scrivere l’interfaccia Forma\n2.Rendere Cerchio  e Rettangolo  specializzazioni di \nForma\n3.Scrivere le classi di test CerchioTest  e \nRettangoloTest\n4.Scrivere la classe GruppoDiForme: Un gruppo di forme \nè composto da un array di riferimenti a oggetti che \nimplementano Forma. Per semplicità si supponga che \nun gruppo di forme possa essere composto al massimo \nda 10 forme.\n5.Scrivere la classe GruppoDiFormeTest",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#7": "Programmazione orientata agli oggetti\nUn GruppoDiForme",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#8": "Programmazione orientata agli oggetti\nUn GruppoDiForme traslato()\n",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-10-polimorfismo-esercitazione.pdf#9": "Programmazione orientata agli oggetti\nEsercizio 2: Testing sulle Forme Semplici\n•Utilizzando JUnit creare classi di test per Cerchio  \ne Rettangolo . \n•Aggiungere una serie di test-case minimali relativi \nal metodo trasla()\n•Ad esempio, un primo test-case testTrasla()  di \nCerchio :\n–istanzia un cerchio unitario (r=1) di centro sull’origine \n(0,0)\n–trasla di (+0, +0)\n–asserisce che dopo la traslazione il cerchio non si è \nspostato\n•Un secondo test-case potrebbe traslare di (+1,+0)\n•Un terzo test-case potrebbe ...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#0": "Programmazione \nOrientata agli Oggetti\nPolimorfismo:\nStudio di caso",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#1": "Programmazione orientata agli oggetti\nRiprendiamo lo Studio di Caso\n•Nelle lezioni precedenti abbiamo già individuato (e \nrimosso) alcuni problemi nel codice dello studio di \ncaso:\n–La responsabilità di gestire il labirinto deve essere assegnata \nad una classe opportuna (la classe Labirinto )\n–Analogamente la responsabilità di gestire le informazioni \nrelative al giocatore (borsa, CFU) devono essere assegnate ad \nuna classe opportuna (la classe Giocatore ): da fare per \nesercizio!\n•La qualità del codice rimane bassa\n–La classe DiaDia implementa tutti(!) i possibili comandi \ndel gioco\n–Analizziamo le conseguenze di quest'ultimo punto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#10": "Programmazione orientata agli oggetti\nLa classe  ComandoVai  (2)\n",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#11": "Programmazione orientata agli oggetti\nOsservazioni (1)\n•Chi ha la responsabilità di creare gli oggetti Comando ?\n•Questa responsabilità è ragionevole sia affidata non ad un \nmetodo della classe DiaDia, ma ad una classe dedicata \n•Una classe che fabbrica comandi \n–FabbricaDiComandi\n–fabbrica un oggetto Comando a partire dall’istruzione digitata\n•Rimangono alcuni problemi (per ora ci accontentiamo…)\n–In particolare:\n•accoppiamento forte con la gestione dell'I/O\n•il codice a fisarmonica non viene definitivamente \neliminato, è stato solamente confinato dentro  \nFabbricaDiComandi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#12": "Programmazione orientata agli oggetti\nOsservazioni (2)\n•I comandi possono avere un parametro\n•Come facciamo ad impostarne il valore?\n–attraverso il costruttore \nEs. ComandoVai(String direzione)\n–oppure, introducendo (nella interface) un metodo setter \nsetParametro(String parametro)\n•La seconda soluzione impone che tutte le classi che \nimplementano Comando  abbiano questo metodo\n–anche quelle che rappresentano comandi senza parametri \n(come «aiuto» o «fine»)\n✔Non necessariamente un problema (il corpo del metodo sarà \nvuoto); sicuramente poco elegante\n•Le due soluzioni sono equivalenti. Preferiamo \ncomunque la seconda \n–(per motivi evidenti solo in seguito >>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#13": "Programmazione orientata agli oggetti\nL'Interface Comando  (rivista)\npublic interface Comando {\n /**\n    * esecuzione del comando\n    */\n    public void esegui(Partita partita);\n /**\n    * set parametro del comando\n    */\n    public void setParametro(String parametro);\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#14": "Programmazione orientata agli oggetti\nLa Classe  ComandoVai  (rivista)\n",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#15": "Programmazione orientata agli oggetti\nCreazione di Oggetti Comando\npublic class FabbricaDiComandi {\npublic Comando costruisciComando(String istruzione) {\n      Scanner scannerDiParole = new Scanner(istruzione);\n  String nomeComando = null;\n  String parametro = null;\n  Comando comando = null;\n  if (scannerDiParole.hasNext())\n    nomeComando = scannerDiParole.next(); // prima parola: nome del comando\n  if (scannerDiParole.hasNext())\n    parametro = scannerDiParole.next();   // seconda parola: eventuale parametro\n  if (nomeComando == null) \n    comando = new ComandoNonValido();\n  else if (nomeComando.equals( \"vai\"))\n    comando = new ComandoVai();\n  else if (nomeComando.equals( \"prendi\"))\n    comando = new ComandoPrendi();\n  else if (nomeComando.equals( \"posa\"))\n    comando = new ComandoPosa();\n  else if (nomeComando.equals( \"aiuto\"))\n    comando = new ComandoAiuto();\n  else if (nomeComando.equals( \"fine\"))\n    comando = new ComandoFine();\n  else if (nomeComando.equals( \"guarda\"))\n    comando = new ComandoGuarda();\n  else comando = new ComandoNonValido();\n  comando.setParametro(parametro);\n  return comando;\n    }            \n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#16": "Programmazione orientata agli oggetti\nOsservazioni\n•Abbiamo migliorato significativamente la qualità \ndel codice della classe DiaDia\n–risulta ora più coesa (con quali responsabilità?)\n–non è più accoppiata ai dettagli dei singoli comandi\n–abbiamo rimosso il codice a fisarmonica\n•ora confinato nella classe FabbricaDiComandi  (anche se in una \nforma molto più semplice e pulita): non deteriora la qualità del \ncodice di DiaDia\n•questa anomalia sarà completamente risolta in seguito (>>)\n–Predisponiamo il codice a questa prevedibile evoluzione:\n•creiamo una interface FabbricaDiComandi , \n•ridenominiamo la classe attuale (che implementa tale \ninterface) FabbricaDiComandiFisarmonica\n–In pratica astraiamo dai dettagli implementativi della fabbrica, \nin attesa di una implementazione alternativa (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#17": "Programmazione orientata agli oggetti\nCreazione di Oggetti Comando\npublic interface FabbricaDiComandi {\n    public Comando costruisciComando(String istruzione);\n}\npublic class FabbricaDiComandiFisarmonica implements FabbricaDiComandi {\n  @Override\n  public Comando costruisciComando(String istruzione) {\n    Scanner scannerDiParole = new Scanner(istruzione);\n    String nomeComando = null;\n    String parametro = null;\n    Comando comando = null;\n    if (scannerDiParole.hasNext())\n       nomeComando = scannerDiParole.next(); // prima parola:   nome del comando\n    if (scannerDiParole.hasNext())\n       parametro = scannerDiParole.next();  // seconda parola: eventuale param.\n    if (nomeComando == null) \n…\n return comando;\n  }\n         \n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#2": "Programmazione orientata agli oggetti\nIntrodurre Nuovi Comandi\n•Per introdurre un nuovo comando dobbiamo: \n–aggiungere un elemento nell'array \nElencoComandi\n–aggiungere un metodo nella classe DiaDia\n–modificare il metodo \nprocessaIstruzione(String)\n•Ma proviamo a ragionare in termini di \nresponsabilità ed a \"pensare in avanti\"",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#3": "Programmazione orientata agli oggetti\nPensiamo in Avanti\n•È ragionevole supporre che in futuro nel \nnostro gioco possano essere introdotti nuovi \ncomandi\n•Se per ogni comando introdotto dobbiamo \nmodificare la classe DiaDia  come abbiamo \nappena descritto, tale classe crescerà a \ndismisura",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#4": "Programmazione orientata agli oggetti\nUna Soluzione (1)\n•Il problema nasce dal fatto che la classe DiaDia \nconosce e realizza i dettagli di tutti i comandi\n–dovrebbe limitarsi a chiamare l'esecuzione di un \ncomando, senza conoscerne i dettagli\n•Le operazioni corrispondenti all'esecuzione di ogni \ncomando dovrebbero essere codificate \ndirettamente da un oggetto Comando\n–ma abbiamo tanti diversi comandi, ognuno con le sue \npeculiarità …\n•Per ovviare al problema un programmatore \nesperto ci suggerisce di sfruttare le potenzialità \ndel polimorfismo, ristrutturando il codice come \nindicato di seguito",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#5": "Programmazione orientata agli oggetti\nUna Soluzione (2)\n•La classe Comando  va trasformata in una interface, che \nrappresenti un generico comando\n•L'interface Comando  deve offrire il metodo \npublic void esegui(Partita partita)\n•Tutti i comandi del gioco saranno realizzati da oggetti \nistanze di classi che implementano l'interface Comando : \n–l'implementazione del metodo esegui(Partita partita)  \ncodifica la semantica del comando specifico\n•(Per ora) la classe DiaDia sulla base delle istruzioni \nlette da tastiera istanzia l'implementazione opportuna \ndel comando\n•Al comando istanziato chiederà quindi di eseguire il \nmetodo esegui(Partita partita) , senza preoccuparsi \ndi come avverrà l'esecuzione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#6": "Programmazione orientata agli oggetti\nLa Classe DiaDia\nprivate boolean processaIstruzione(String istruzione) {\nComando comandoDaEseguire;\n   FabbricaDiComandi factory = new FabbricaDiComandi()\ncomandoDaEseguire = factory.costruisciComando(istruzione);\ncomandoDaEseguire.esegui( this.partita); \nif (this.partita.vinta())\nSystem.out.println (\"Hai vinto!\" );\nif (!this.partita.giocatoreIsVivo())\nSystem.out.println (\"Hai esaurito i CFU...\" );\nreturn this.partita.isFinita();\n}\n•L'oggetto factory (istanza di FabbricaDiComandi ) ha la responsabilità \ndi creare un oggetto Comando. Non ci interessano le specificità di ogni \nsingolo comando disponibile\n–vedremo in seguito (>>) come è fatta la classe FabbricaDiComandi\n•Invochiamo semplicemente il metodo esegui() (che è polimorfo): in \nsostanza lasciamo al comando la responsabilità di eseguire il comando\n•Spariscono dalla classe tutti i metodi che implementano i comandi. \n✔Per disporre di un nuovo comando basta introdurre la sua classe, \nsenza dover modificare la classe DiaDia",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#7": "Programmazione orientata agli oggetti\nL'Interface Comando\npublic interface Comando {\n /**\n    * esecuzione del comando\n    */\n    public void esegui(Partita partita);\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#8": "Programmazione orientata agli oggetti\nLa Classe ComandoVai\n•Proviamo a creare una implementazione (la \nclasse ComandoVai )\n•La classe ComandoVai  implementa il comando \nche permette di cambiare stanza\n•Scriviamone il codice",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-11-polimorfismo-studio-di-caso.pdf#9": "Programmazione orientata agli oggetti\nImplementazione di Comando\npublic class ComandoVai implements Comando {\nprivate String direzione;\npublic ComandoVai(String direzione) {\n  this.direzione = direzione;\n}\n /**\n    * esecuzione del comando\n    */\n  @Override\n  public void esegui(Partita partita) {\n   // qui il codice per cambiare stanza …\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#0": "Programmazione \nOrientata agli Oggetti\nApprofondimenti Interface\nEstensione (prima parte)\nLa classe Object",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#1": "Programmazione orientata agli oggetti\nSommario\n•Estensione di Interfacce\n•Estensione di Classi (prima parte)\n•La classe Object",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#10": "Programmazione orientata agli oggetti\nMeccanismi per la Creazione di Tipi\n•Attraverso l’estensione delle interfacce è possibile \ndefinire nuovi tipi a partire da tipi già esistenti\n•Riassumiamo tutti i meccanismi visti sinora per \nintrodurre nuovi tipi in Java annunciando le linee \nguida per il loro utilizzo \n–Esistono altri meccanismi (classi astratte, tipi \nenumerativi, classi nidificate) che per ragioni di \nnatura prettamente didattica conviene rimandare\n✔Sfruttiamo invece l’occasione per introdurre la \nclasse Object , che al contrario conviene \ncomprendere il prima possibile",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#11": "Programmazione orientata agli oggetti\nCreazione di Tipi Ex-Novo\n•Le Interfacce\n–permettono di definire nuovi tipi senza definire \nl’implementazione dei metodi che formano la \nspecifica di tipo\n•Le Classi\n–permettono di definire nuovi tipi ma richiedono \nl’implementazione di tutti i metodi che formano la \nspecifica di tipo\n•Le Classi Astratte (>>)\n–strumento “intermedio”: permette di lasciare \nqualche metodo astratto, ovvero senza \nimplementazione, pur consentendone la definizione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#12": "Programmazione orientata agli oggetti\nDefinizione di Nuovi Tipi per \nEstensione\n•Estensione di interfacce\n–nuove interfacce definite come estensione di altre\n–nessun metodo nelle due interfacce possiede \nimplementazione \n•Estensione di classi\n–nuove classi definite come estensione di altre già \nesistenti \n–i metodi ereditano anche l’implementazione, che se \nnecessario può essere sovrascritta ( override )\n•Per entrambe:\n–l’insieme dei metodi del tipo esteso comprende \nquelli pubblici del tipo base, più altri di nuova \ndefinizione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#13": "Programmazione orientata agli oggetti\nSommario\n•Estensione di Interfacce\n•Estensione di Classi (prima parte)\n•La classe Object",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#14": "Programmazione orientata agli oggetti\nEstensione di Classi: Introduzione\n•Uno dei meccanismi più caratteristici (e più \ndifficili da usare correttamente ) dei linguaggi \nOO è l'estensione (o ereditarietà)\n•Con l'estensione possiamo definire una nuova \nclasse a partire da una classe esistente\n–aggiungendo  campi (variabili e/o metodi) a \nquelli della classe originale\n–sovrascrivendo metodi della classe originale",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#15": "Programmazione orientata agli oggetti\nEstensione di Classi: Terminologia\n•La classe di partenza viene chiamata superclasse , \no classe base , o classe genitore\n•La classe definita per estensione a partire da una \nclasse base viene chiamata classe estesa , o \nclasse derivata , o sottoclasse , o classe figlia\nnell'ambito del corso preferiamo usare i termini \nclasse base  e classe estesa/derivata\n•Siccome la classe derivata può a sua volta essere \nutilizzata come classe base di una nuova classe, si \ndice anche che le classi sono organizzate in una \ngerarchia",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#16": "Programmazione orientata agli oggetti\nEstensione di Classi: \nCaratteristiche Generali\n•La classe estesa conserva (\"eredita\") tutti i \ncampi della classe base\n•Rispetto alla classe base, la classe estesa di \nsolito:\n–può avere qualche membro (campo e/o metodo) in \naggiunta\n–può ridefinire il comportamento di qualche metodo\n•La classe base viene considerata un supertipo \ndella classe estesa\n–Quindi vale il principio di sostituzione:  un'istanza \ndella classe estesa può essere considerata anche \ncome un'istanza della superclasse",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#17": "Programmazione orientata agli oggetti\nEstensione di Classi: Esempio (1)\npublic class Persona {\nprivate String nome;\n  public Persona(String nome) {\nthis.nome = nome;\n}\npublic void setNome(String nome) {\nthis.nome = nome;\n}\npublic String getNome() {\nreturn this.nome;\n}\n public String toString() {\nreturn this.getNome();\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#18": "Programmazione orientata agli oggetti\nDefinizione di una Classe estesa\n•In Java per indicare la definizione di una nuova \nclasse per estensione di una già esistente si \nusa la parola chiave extends\n class Studente extends Persona  {\n  \n    // metodi e campi\n  \n }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#19": "Programmazione orientata agli oggetti\nEreditarietà: \nCaratteristiche Generali (1)\n•Tutte le variabili e tutte le operazioni definite nella \nclasse base sono «ereditate»  nella classe estesa\n•Rispetto alla classe base, la classe estesa \n–può avere qualche membro (campo e/o metodo) in più\n–può sovrascrivere il comportamento di qualche metodo\n•La classe base viene considerata un supertipo \ndella classe estesa\n–vale il principio di sostituzione: un'istanza della classe \nestesa può essere considerata anche come un'istanza \ndella superclasse",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#2": "Programmazione orientata agli oggetti\nSommario\n•Estensione di Interfacce\n•Estensione di Classi (prima parte)\n•La classe Object",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#20": "Programmazione orientata agli oggetti\nEstensione di Classi: Esempio (2)\npublic class Studente extends Persona {\nprivate String matricola;\npublic Studente(String nome, String matricola) {\n// vediamo dopo\n}\npublic void setMatricola (String matricola) {\nthis.matricola = matricola;\n}\npublic String getMatricola() {\nreturn this.matricola;\n}\n  @Override\npublic String toString() {\n// vediamo dopo\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#21": "Programmazione orientata agli oggetti\nDefinizione di una Classe Estesa (1)\n•La classe Studente rispetto alla classe Persona \npossiede un nuova variabile di istanza: \n–matricola\n... e i corrispondenti due nuovi metodi accessori\n–void setMatricola(String)  \n–String getMatricola()\n•Le variabili di istanza e i metodi vengono ereditati: \n–le istanze della classe estesa hanno le stesse variabili della \nclasse base più quelle eventualmente aggiunte\n–tutti i metodi pubblici della classe base sono disponibili nella \nnuova classe, senza necessità di ridefinirli\n✔N.B.: esattamente come per tutte le altre classi esterne alla \nclasse base, anche la classe estesa può accedere ai membri \n(variabili di istanza o metodi) pubblici della classe base ma \nnon a quelli privati ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#22": "Programmazione orientata agli oggetti\nDefinizione di una Classe Estesa (2)\n•I nuovi membri della classe estesa non hanno \nnulla di particolare\n•Se si dispone di un oggetto Studente è possibile \ninvocare i metodi della classe base Persona\n–esempio: se si dispone di un riferimento ad un oggetto \nStudente è possibile invocare i metodi della classe base\n  Studente anonimo = new Studente(\"\",\"\");\nanonimo.setNome(\"Paolo\");\n•In generale, possiamo usare tutti i metodi pubblici \ndella classe base (ereditati) oltre quelli della \nclasse estesa",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#23": "Programmazione orientata agli oggetti\nEstensione e Polimorfismo (1)\n•Una classe estesa è un sottotipo della classe base \n(la classe base è un supertipo della classe estesa) \n•Infatti la classe estesa offre l'interfaccia \n(e l'implementazione di alcuni) dei metodi della \nclasse base\n•Quindi, in base al principio di sostituzione, la \nclasse estesa può essere usata sempre laddove è \nrichiesto un oggetto della classe base \n✔N.B.: esattamente come nel caso di una classe che \nimplementa un’interfaccia fornendone un sottotipo \nconcreto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#24": "Programmazione orientata agli oggetti\nEstensione e Polimorfismo (2)\n•Studente automaticamente possiede tutti i \nmetodi di Persona , senza bisogno di definirli\n•La classe estesa ha quindi l'interfaccia e  \nl'implementazione  dei metodi della classe \nbase\n•Studente  è un sottotipo di Persona : può \nessere usata al posto di Persona",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#25": "Programmazione orientata agli oggetti\nEstensione di Classi (1)\n•Vale il principio di sostituzione: il sottotipo può \ncertamente essere usato al posto di un \nsupertipo\npublic class ProvaPersona {\n    public static void main(String[] args) {\n        Persona p = new Studente(\"Paolo\", \"123456\");\n        p.setNome(\"Anna\");\n        System.out.println(p.getNome());\n        Studente s = new Studente(\"Luigi\",\"654321\");\n        s.setNome(\"Antonio\");\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#26": "Programmazione orientata agli oggetti\nEstensione di Classi (2)\n•Attenzione: possono essere invocati solo \ni metodi (pubblici) del tipo statico\npublic class RiProvaPersona {\n    public static void main(String[] args) {\n        Persona p = new Studente(\"Paolo\", \"123456\");\n        p.setNome(\"Anna\");\n        p.setMatricola(\"33333\"); // NON COMPILA!\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#27": "Programmazione orientata agli oggetti\nEreditarietà: \nCaratteristiche Generali (2)\n•Tutte le variabili e tutte le operazioni definite nella \nclasse base sono \"ereditate\"  nella classe estesa\n•Rispetto alla classe base, la classe estesa\n–ha qualche membro (campi e/o metodi) in più \n–può sovrascrivere il comportamento di qualche metodo \n•La classe base viene considerata un supertipo \ndella classe estesa\n–vale il principio di sostituzione: un'istanza della classe \nestesa può essere considerata anche come un'istanza \ndella superclasse ed usata al suo posto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#28": "Programmazione orientata agli oggetti\nOverriding (“Riscrittura”)\n•Alcune implementazioni dei metodi offerti nella classe \nbase possono essere non adatte alla classe estesa\n•Tipico esempio il metodo \nString toString() \nla stampa dovrebbe permettere di distinguere le \nistanze della classe base da quelle della classe estesa\nAd es. nella stringa restituita per gli studenti vogliamo che \ncompaia anche la matricola (che non ha senso per tutte le \npersone)\n•Questo comportamento si ottiene facendo l' overriding  \n( sovrascrittura ) del metodo\nAttenzione: non confondere overriding  ed overloading",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#29": "Programmazione orientata agli oggetti\nEstensione di Classi: Esempio (3)\npublic class Studente extends Persona {\nprivate String matricola;\npublic Studente(String nome, String matricola) {\n// vediamo dopo\n}\npublic void setMatricola (String matricola) {\nthis.matricola = matricola;\n}\npublic String getMatricola() {\nreturn this.matricola;\n}\n  @Override\npublic String toString() {\nreturn this.nome + \" \" + this.matricola;\n}\n}  NON COMPILA: si prova ad accedere\n  a campi privati (la variabile di istanza nome)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#3": "Programmazione orientata agli oggetti\nEstensione di Interface (1)\n•Talvolta può essere utile definire una nuova \ninterface a partire da una interface esistente\n•Questo significa definire una nuova interface \nche offre qualche servizio (metodo) \naggiuntivo rispetto ad una interface nota",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#30": "Programmazione orientata agli oggetti\nEstensione: Overriding\n•La soluzione precedente non è praticabile perché i \nmetodi della classe estesa ( Studente ) non \npossono accedere ai campi privati della classe \nbase (Persona ) \n–anche se ogni oggetto Studente  ha ereditato una \nvariabile di istanza in cui viene memorizzata la \nstringa che rappresenta il nome, non è accessibile!\n•Se i metodi della classe estesa vogliono accedere \nai campi della classe base devono  usare \nl'interfaccia pubblica della classe base come tutte \nle altri classe esterne  alla stessa",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#31": "Programmazione orientata agli oggetti\nEstensione di Classi: Esempio (4)\npublic class Studente extends Persona {\nprivate String matricola;\npublic Studente(String nome, String matricola) {\n// vediamo dopo\n}\npublic void setMatricola (String matricola) {\nthis.matricola = matricola;\n}\npublic String getMatricola() {\nreturn this.matricola;\n}\n  @Override\npublic String toString() {\nreturn this.getNome() + \" \" + this.getMatricola();\n}\n}Accesso al metodo pubblicoPreferire sempre e comunque l’utilizzo\ndei metodi accessori rispetto all’uso \ndiretto delle variabili di istanza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#32": "Programmazione orientata agli oggetti\nOverriding e Polimorfismo (1)\n•Un'istanza della classe estesa può essere usata al \nposto di una istanza della classe base\n•Anche qui, si manifesta il polimorfismo (già visto \nper le interfacce) e il legame al codice avviene a \ntempo di esecuzione ( late binding ). Rispetto alle \ninterfacce:\n–se il metodo non è ridefinito, si utilizza (si eredita) \nl'implementazione della superclasse\n–se il metodo è ridefinito (overriding) si utilizza \nl'implementazione della classe estesa\n•In definitiva si sceglie sempre  l'implementazione \ndel tipo dinamico",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#33": "Programmazione orientata agli oggetti\nOverriding e Polimorfismo (2)\npublic class AltraProvaStudente{\n    public static void main(String[] args) {\n \n        Studente studente = new Studente(\"Paolo\", \"123456\");\n        Persona persona = new Studente(\"Anna\", \"654321\");\n     \n        System.out.println(studente.toString()); \n        System.out.println(persona.toString());\n    }\n}\nIl tipo dinamico della variabile locale persona è Studente, \nall'invocazione di toString() viene eseguito il codice del\ncorpo del metodo presente nella classe StudenteIl tipo dinamico della variabile locale studente risulta essere\nStudente, all'invocazione di toString() viene eseguito il \ncodice del corpo del metodo presente nella classe Studente",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#34": "Programmazione orientata agli oggetti\nEstensione: Creazione di Istanze\n•Nella creazione di oggetti istanze di una classe estesa \nbisogna tener presente la relazione esistente con le \nistanze della classe padre\nogni istanza della classe estesa  è  anche una istanza \ndella superclasse\n•Alcuni meccanismi offerti dal linguaggio Java nella \ngestione dei costruttori per classi estese si \ncomprendono meglio tenendo a mente che\nciascuna classe deve essere l’unica responsabile \ndell’inizializzazione delle proprie istanze\nper creare una istanza di una classe estesa bisogna prima \ncreare l’istanza della classe base «che è in lei»\nbisogna concludere la creazione e l’inizializzazione di una \nistanza prima di fare qualsiasi altra cosa con la stessa\nA ben vedere, il servizio di creazione di Object (>> e \nderivati) può essere offerto solo dalla JVM",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#35": "Programmazione orientata agli oggetti\nEstensione: Costruttori (1)\n•Il costruttore di una classe estesa deve inizializzare:\n–direttamente  le proprie variabili di istanza\n–indirettamente  quelle ereditate dalla classe base\n•Per il principio dell'information hiding, la classe estesa \nnon può avere la responsabilità di inizializzare \ndirettamente le variabili di istanza della classe base\n•Per non violarlo, il costruttore della classe estesa deve \npoter delegare l'inizializzazione delle variabili di istanza \ndella classe base ad un costruttore della stessa\n–questa operazione in Java si effettua chiamando dal corpo del \ncostruttore della classe estesa il costruttore della classe base \nmediante la parola chiave super() e specificando i parametri \nattuali del costruttore della classe base tipicamente sulla base \ndei parametri formali ricevuti (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#36": "Programmazione orientata agli oggetti\nEstensione: Costruttori (2)\npublic class Studente extends Persona {\nprivate String matricola;\npublic Studente(String nome, String matricola) {\nsuper(nome);\n     this.matricola = matricola;\n}\npublic void setMatricola (String matricola) {\nthis.matricola = matricola;\n}\npublic String getMatricola() {\nreturn this.matricola;\n}\n  @Override\npublic String toString() {\nreturn this.getNome() + \" \" + this.getMatricola();\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#37": "Programmazione orientata agli oggetti\nEstensione: Costruttori (3)\n•Il corpo dei costruttori di una classe estesa deve \nsempre  avere una chiamata al costruttore della classe \nbase mediante l’uso della parola chiave super come \nprima istruzione\n•In assenza di una chiamata esplicita, il compilatore ne \ninserisce automaticamente  una al costruttore no-arg \ndella superclasse\n–Attenzione: solo  in assenza di tale costruttore nella \nsuperclasse si verifica un errore a tempo di compilazione\n•La chiamata al costruttore della classe base deve \nessere la prima istruzione  nel corpo del costruttore \ndella classe estesa\nCome già per i metodi, ricordiamo che anche per i costruttori è \npossibile definire diverse versioni sovraccariche; valgono le \nregole già viste per l’overloading di metodi",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#38": "Programmazione orientata agli oggetti\nEstensione (Prima Parte): \nRicapitoliamo\n•La classe estesa:\n–ha tutte le proprietà della classe base\n–è in grado di eseguire tutti i metodi (pubblici) della \nclasse base\n–non ha accesso ai membri privati della classe base \n(nessuna eccezione al principio dell'information hiding) \n•Inoltre:\n–può possedere variabili di istanza proprie, oltre a quelle \nereditate dalla classe base\n–può avere metodi propri\n–può specializzare il comportamento di alcuni metodi \ndella classe base\n–può avere versioni sovraccariche del costruttore",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#39": "Programmazione orientata agli oggetti\nSommario\n•Estensione di Interfacce\n•Estensione di Classi (prima parte)\n•La classe Object",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#4": "Programmazione orientata agli oggetti\nEstensione di Interface (2)\n•Esempio: data l'interface A\npublic interface A {\npublic void a1(int i);\npublic String a2();\n}\n•Supponiamo di dover definire l'interface B, che \ndebba offrire gli stessi metodi di A, ed in più il \nmetodo b1()",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#40": "Programmazione orientata agli oggetti\nLa Classe Object\n•Tutte le classi estendono (direttamente od \nindirettamente) la classe Object\n–si usa dire che Object è la radice della gerarchia dei \ntipi Java\n•Object  è una classe predefinita, che viene \nautomaticamente estesa da ogni nuova classe \n(direttamente o indirettamente)\n•La classe Object  ha un insieme di metodi molto \ngenerici, ereditati e (volendo sovrascritti) da \nogni nuova classe\n–tra questi metodi ce ne sono alcuni già noti (ed altri che lo \nsaranno presto)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#41": "Programmazione orientata agli oggetti\n•Tutte le classi estendono automaticamente la \nclasse Object\n•E' una classe predefinita, che viene \nautomaticamente ed implicitamente estesa da \nogni nuova classe (direttamente o indirettamente)\n✔Scrivere:\npublic class MiaClasse {\n}\nè del tutto equivalente a scrivere:\npublic class MiaClasse extends Object  {\n}… extends Object",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#42": "Programmazione orientata agli oggetti\nClasse Object : Alcuni Metodi\n•Vedere documentazione javadoc\n–String toString()\n–boolean equals(Object o)\n–…\n•Di tutti questi metodi le nostre classi ereditano \nl’implementazione, oltre che la segnatura\n•Le nostre classi possono ridefinirne \nl’implementazione (ma a tal fine devono  \nrispettarne la segnatura)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#43": "Programmazione orientata agli oggetti\nMetodo String toString()  (1)\n•Questo è il motivo per il quale non è strettamente \nnecessario definire il metodo toString()  dentro \nle classi di nostra definizione per poterlo usare\n•Se non lo definiamo, verrà comunque ereditata la \ndefinizione del metodo toString()  propria della \nclasse java.lang.Object\n•Questa si preoccupa di stampare un messaggio \ntestuale che dipende dall’indirizzo in memoria \ndell’oggetto sul quale viene invocato\n•Di solito risulta poco esplicativo ed utile, e perciò \nconviene quasi sempre ridefinirlo sulla base delle \nspecificità della classe definita",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#44": "Programmazione orientata agli oggetti\nMetodo String toString()  (2)\npublic class Persona {\nprivate String nome;\n  …\npublic void setNome(String nome){\nthis.nome = nome;\n}\npublic static void main(String[] args) {\nPersona p = new Persona(\"Paolo\");\nSystem.out.println(p.toString());\n  }\n}\nStampa (qualcosa simile a): Persona@10b62c9\nIl metodo toString()  viene ereditato da Object : \nvale l'implementazione di Object",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#45": "Programmazione orientata agli oggetti\nMetodo String toString()  (3)\npublic class Persona {\nprivate String nome;\n…\npublic void setNome(String nome){\nthis.nome = nome;\n}\n   @Override\npublic String toString() {\nreturn this.nome;\n}\npublic static void main(String[] args) {\nPersona p = new Persona();\np.setNome(\"Antonio\");\nSystem.out.println(p.toString());\n  }\n}Stampa : Antonio\nIl metodo toString()  è stato sovrascritto: \nvale l'implementazione riscritta",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#46": "Programmazione orientata agli oggetti\nMetodo boolean equals(Object o)\n•Analogamente non è strettamente necessario \ndefinire il metodo equals()  dentro le classi di \nnostra definizione per poterlo usare\n•Anche in questo caso, se non lo definiamo, verrà \ncomunque ereditata la definizione del metodo \nequals()  propria della classe java.lang.Object\n–questa confronta l’indirizzo in memoria dell’oggetto sul \nquale viene invocato con il riferimento passato come \nparametro (analogamente all’operatore  == )\n–di solito non è questa la semantica desiderata\n–definisce il criterio di equivalenza di oggetti distinti ma \nistanza della stessa classe (specie se le istanze saranno \nutilizzate dentro collezioni >>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#47": "Programmazione orientata agli oggetti\nMetodo equals() & Testing\npublic class TestPersona {\n@Test\npublic void testEquals() {\nPersona p1 = new Persona(\"Paolo\");\nPersona p2 = new Persona(\"Paolo\");\nassertEquals(p1, p2);\n  }\n}\n✔}Il metodo  equals()  viene ereditato da Object : \n✔ vale l'implementazione del metodo nella classe ObjectFALLISCE",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#48": "Programmazione orientata agli oggetti\nMetodo equals() : Esempio\npublic class Persona {\nprivate String nome;\n…\npublic void setNome(String nome){\nthis.nome = nome;\n}\n  @Override // overrides toString() di java.lang.Object\npublic String toString() {\nreturn this.nome;\n}\n  @Override // overrides equals(Object o) di java.lang.Object\npublic boolean equals(Object o) {\nPersona that = (Persona)o; // ← (downcast)\nreturn this.getNome().equals(that.getNome());\n}\n}\nIl metodo equals(Object o)  è stato ridefinito: \n il precedente test ora va a buon fine!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#49": "Programmazione orientata agli oggetti\nMetodo equals() :\nDowncast Necessario\n•Attenzione alla segnatura \nboolean equals( Object o )\n•L’argomento è di tipo Object !\n•Per questo abbiamo dovuto fare un cast \n(tecnicamente un downcast:  abbiamo forzato il \ntipo statico al sottotipo atteso a tempo dinamico) \nPersona that = (Persona)o;\n•Senza questo downcast non potremmo invocare i \nmetodi propri della classe ( Persona nell'esempio) su \ncui si basa il confronto (l’uguaglianza degli oggetti \nString ottenute invocando getNome()  nell’es.)\nN.B. A sua volta String ridefinisce equals() ... ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#5": "Programmazione orientata agli oggetti\nEstensione di Interface (3)\n•Potremmo definire la nuova interface in questo modo\npublic interface B {\npublic void a1(int i);\npublic String a2();\npublic int b1();\n}\n•Ma le due interface non avrebbero alcuna relazione \nesplicita (soprattutto, i tipi che definiscono non la \npossiedono affatto)\n•Di conseguenza non potremmo referenziare con un \noggetto B un riferimento ad A, anche se \nconcettualmente sembrerebbe sensato\nA a = new ClasseCheImplementa_A(); \nB b = new ClasseCheImplementa_B(); \na=b; // ERRORE",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#50": "Programmazione orientata agli oggetti\nMetodo equals() :\nUn Errore Troppo Frequente (1)\n•Un errore è quello di usare come parametro \nformale un riferimento ad un oggetto della classe \nsulla quale si sta ridefinendo il metodo\n    boolean equals( Persona  persona) // ERRORE\n•Questa non è una sovrascrittura  del metodo  \n    boolean equals( Object persona)  \nUtilizzare sempre  l'annotazione @Override per \nribadire al compilatore che si sta eseguendo una \nsovrascrittura\nsegnalerà questo tipo di problemi già in fase di \ncompilazione ed eviterà che si debba ricercarli sulla \nbase degli effetti (non sempre evidenti) in fase di \nesecuzione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#51": "Programmazione orientata agli oggetti\nMetodo equals() :\nUn Errore Troppo Frequente (2)\npublic class Persona {\nprivate String nome;\n…\npublic void setNome(String nome){\nthis.nome = nome;\n}\npublic String toString() {\nreturn this.nome;\n}\n   \npublic boolean equals( Persona that) {\nreturn this.getNome().equals(that.getNome());\n}\n}\npublic class TestPersona {\n@Test\npublic void testEquals() {\nPersona p1 = new Persona(\"Paolo\");\nPersona p2 = new Persona(\"Paolo\");\nassertEquals(p1, p2);\n  }\n}FALLISCE\nperché viene invocato il metodo \nequals(Object  o): non essendo \nstato sovrascritto  per l’errata \nsegnatura, viene invece usato quello \nereditato da Object che confronta \ngli indirizzi in memoria degli \noggetti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#52": "Programmazione orientata agli oggetti\nConclusioni\n•Molti altri dettagli nell’estensione di classi \nmeritano nuovamente uno spazio dedicato più \navanti nel corso\n•E’ tuttavia utile capire già ora come tutte le nostre \nclassi estendono e sono sottotipi di \njava.lang.Object , la radice della gerarchia dei \ntipi in Java, dal quale ereditano alcuni metodi di \npubblica e generale utilità come\n•String toString()\n•boolean equals(Object o)\n✔Questi metodi vengono frequentemente \nsovrascritti nelle nostre classi per adattarli alle \nloro peculiarità",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#6": "Programmazione orientata agli oggetti\nEstensione di Interface (4)\n•Sarebbe utile e sensato riuscire a specificare che \nB è un sottotipo di A\n•In Java (e analogamente in altri linguaggi) questo \nè possibile definendo una interface come una \nestensione di un'altra interface\n•Relativamente al nostro esempio possiamo \nscrivere\npublic interface B extends A  {\npublic int b1();\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#7": "Programmazione orientata agli oggetti\nEstensione di Interface (5)\n•In questo modo stiamo definendo una nuova \ninterface ( B) a partire da una già esistente ( A)\n•In particolare stiamo dicendo che:\n–B è un sottotipo di A\n–B offre tutti i metodi di A più il metodo b1()  \n•Quindi vale il principio di sostituzione\n•In questo caso le istruzioni:\nA a = new ClasseCheImplementa_A(); \nB b = new ClasseCheImplementa_B(); \na = b; // OK B è un sottotipo di A\nsono corrette",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#8": "Programmazione orientata agli oggetti\nEstensione di Interface: \nCorrette Motivazioni\n•L'estensione di interfacce non è una \nscorciatoria per non ripetere la scrittura di \nmetodi\n•E' invece un sofisticato meccanismo per \ndefinire sottotipi con effetti importanti sulla \nmodellazione del dominio (vedi corso APS >>)\n•Usare correttamente l'estensione delle \ninterface richiede esperienza\n–Il legame tra le due interface (tipo/sottotipo) è forte \ne deve essere ben giustificato dal dominio",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-12-polimorfismo-interface-estensione-object.pdf#9": "Programmazione orientata agli oggetti\nUtilizzo dell’Estensione di Interface\n•In questo corso non arriveremo praticamente \nmai a far utilizzo dell'estensione delle interface \ngià in fase di progettazione\n•Però dobbiamo essere in grado di capirne la \nsemantica, perché useremo  spesso e volentieri \nmolte interface definite per estensione di altre\n–Sono infatti frequentemente utilizzate in alcune \ndelle API di Java approfondite in seguito, come ad \nes. nelle collezioni>>",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#0": "Programmazione \nOrientata agli Oggetti\nEstensione (Seconda Parte)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#1": "Programmazione orientata agli oggetti\nEsercizio\npublic class C {\n  public void dim(C c) {\nSystem.out.println(\"C.dim(C) \");\n  }\n  public void dim(L l) {\nSystem.out.println(\"C.dim(L) \");\n  }\n  public void dim(K k) {\nSystem.out.println(\"C.dim(K) \");\n  }\n}\npublic class K extends C {\n  public void dim(C c) {\nSystem.out.println(\"K.dim(C) \");\n  }\n  public void dim(L l) {\nSystem.out.println(\"K.dim(L) \");\n  }\n  public void dim(K k) {\nSystem.out.println(\"K.dim(K) \");\n  }\n}public class L extends C {\n  public void dim(C c) {\n   System.out.println(\"L.dim(C) \");\n  }\n  \n  public void dim(L l) {\n   System.out.println(\"L.dim(L) \");\n  }\n  public void dim(K k) {\n   System.out.println(\"L.dim(K) \");\n  }\n  public static void main(String args[]) {\n      C a = new K();\n      C b = new L();\n      a.dim(b);\n      L a1 = new L();\n      a1.dim(a);\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#10": "Programmazione orientata agli oggetti\nOverriding\n•Alcuni metodi della classe base possono \nessere ridefiniti  (sovrascritti) nella classe \nestesa\n•Per modificare il comportamento \n(l'implementazione) di un metodo si effettua \nun overriding   ( sovrascrittura ) del metodo\n•Nel nostro esempio:\n   @Override\n   public boolean addAttrezzo(Attrezzo attrezzo)\n11",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#11": "Programmazione orientata agli oggetti\nOverriding (Tentativo 1)\n  class StanzaMagica extends Stanza {\n      private int contatoreAttrezziPosati;\n      private int sogliaMagica;\n…\n      @Override\n      public boolean addAttrezzo(Attrezzo attrezzo) {\n         this.contatoreAttrezziPosati++;\n         if (this.contatoreAttrezziPosati>this.sogliaMagica) \n            attrezzo = this.modificaAttrezzo(attrezzo);\n         if (this.numeroAttrezzi <this.attrezzi .length) {   \n            this.attrezzi[this.numeroAttrezzi] = attrezzo;\n            this.numeroAttrezzi++;\n            return true;\n  }\n  else return false;\n      }\n      private Attrezzo modificaAttrezzo(Attrezzo attrezzo) {\n         …\n      }\n  }le variabili attrezzi e  \nnumeroAttrezzi  sono ereditate dalla \nclasse base ma non sono accessibili (in \nquanto private)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#12": "Programmazione orientata agli oggetti\nEstensione: Overriding\n•La soluzione precedente non compila: i metodi della \nclasse estesa StanzaMagica  non possono accedere ai \ncampi privati della classe base Stanza \n–N.B. anche se ogni oggetto StanzaMagica  \nimplicitamente possiede le variabili di istanza per \nrappresentare gli attrezzi contenuti nella stanza\n•La classe estesa può  accedere solo ai membri \npubblici della classe base come tutte le altre classi \nesterne\n✔Coerentemente con il principio dell’ information hiding\n•Nel nostro caso possiamo però pensare di limitarci a \nriutilizzare il metodo pubblico reso disponibile dalla \nsuperclasse Stanza:  \nboolean addAttrezzo(Attrezzo attrezzo)  \n13",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#13": "Programmazione orientata agli oggetti\nOverriding (Tentativo 2)\n \n class StanzaMagica extends Stanza {\n    private int contatoreAttrezziPosati;\n    private int sogliaMagica;\n… \n    @Override\n    public boolean addAttrezzo(Attrezzo attrezzo) {\n       this.contatoreAttrezziPosati++;\n       if (this.contatoreAttrezziPosati > this.sogliaMagica) \n          attrezzo = this.modificaAttrezzo(attrezzo);\n       return this.addAttrezzo( attrezzo);\n    }\n    private Attrezzo modificaAttrezzo(Attrezzo attrezzo) {\n …\n    }\n}  NON FUNZIONA: viene chiamato \n  ricorsivamente (all'infinito) il metodo  \n  addAttrezzo(Attrezzo attrezzo)   !",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#14": "Programmazione orientata agli oggetti\nOverriding (Tentativo 2)\n•La soluzione precedente non funziona perché il \nmetodo addAttrezzo(Attrezzo attrezzo)  chiama \nse stesso!\n–java.lang.StackOverflowError\n✔come implicato dal late-binding: per l’esecuzione si usa \nl’implementazione del tipo dinamico, ovvero il corpo \ndello stesso metodo che si sta definendo\n•E' necessario indicare che vogliamo usare il \nmetodo della superclasse\n•Questo è reso possibile usando nuovamente la \nparola chiave super\n✔ma con un nuovo significato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#15": "Programmazione orientata agli oggetti\nEstensione: Overriding\n class StanzaMagica extends Stanza {\n    private int contatoreAttrezzi Posati;\n    private int sogliaMagica;\n…\n    @Override\n    public boolean addAttrezzo(Attrezzo attrezzo) {\n       this.contatoreAttrezziPosati++;\n       if (this.contatoreAttrezziPosati>this.sogliaMagica) \n          attrezzo = this.modificaAttrezzo(attrezzo);\n       return super.addAttrezzo(attrezzo);\n    }\n    private Attrezzo modificaAttrezzo(Attrezzo attrezzo) {\n        …\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#16": "Programmazione orientata agli oggetti\nEsercizio\n•Scrivere una classe di test \nStanzaMagicaTest per testare la classe \nStanzaMagica che preveda anche test-\ncase per verificarne il comportamento \n“magico”\n•Implementare StanzaMagica  usando \nl'ereditarietà secondo le linee guida \ndiscusse in queste dispense\n17",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#17": "Programmazione orientata agli oggetti\nVisibilità protected (1)\n•I membri private  della classe base non sono \naccessibili dall'esterno nemmeno da una \nsottoclasse\n•In Java esiste un altro modificatore di accesso, \nche consente di definire campi a cui sia \nconsentito l’accesso da sottoclassi\n–È il modificatore di accesso protected\n•Un membro di una superclasse con accesso \nprotetto è visibile a tutte le sottoclassi\n✔Indipendentemente dal package di \nappartenenza\n18",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#18": "Programmazione orientata agli oggetti\nVisibilità protected (2)\nclass Stanza {\n    private String descrizione;\n    protected  Attrezzo[] attrezzi;\n    protected  numeroAttrezzi;  \n…\n}\n•In questo modo, qualsiasi classe che estenda \nStanza può accedere alle sue variabili di istanza \nattrezzi  e numeroAttrezzi\n•Si rende possibile una definizione alternativa del \nmetodo addAttrezzo(Attrezzo attrezzo)  di \nStanzaMagica , con accesso diretto ai campi \nereditati da Stanza\n19",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#19": "Programmazione orientata agli oggetti\nAccesso ai Membri dalla Classe Estesa\n class StanzaMagica extends Stanza {\n    private int contatoreAttrezziPosati;\n    private int numeroPassaggi;\n…\n    @Override\n    public boolean addAttrezzo(Attrezzo attrezzo) {\n       this.contatoreAttrezziPosati++;\n       if (this.contatoreAttrezziPosati > this.sogliaMagica) \n          attrezzo = this.modificaAttrezzo(attrezzo);\n       if (this.numeroAttrezzi <this.attrezzi .length) {   \n          this.attrezzi[this.numeroAttrezzi] = attrezzo;\n          this.nomeroAttrezzi++;\n          return true;\n}\nelse return false;\n    }\n  …\n}\n20La classe estesa StanzaMagica ha la possibilità  di \naccedere ai membri protetti  (attrezzi e numeroAttrezzi )\ndella classe base Stanza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#2": "Programmazione orientata agli oggetti\nSommario\n•Estensione nel caso di studio\n•Chiamate a metodi della superclasse\n•Accesso protetto ai membri\n•Overriding di metodi \n•Ancora sull'ereditarietà multipla\n•La gerarchia dei tipi\n•Considerazioni finali sui tipi\n•Esercizio: Overriding for Overloading",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#20": "Programmazione orientata agli oggetti\nVisibilità protected (3)\n•Più precisamente, è possibile accedere ad un \nmembro protected :\n–da tutte le classi estese\n–da tutte le classi dello stesso package!\n•I membri protetti sono una violazione (seppur \ncontrollata e voluta) dell'information hiding\n–vanno pertanto usati con molta accortezza\n–se possibile, meglio evitare il loro utilizzo\n•L’utilizzo più appropriato è nella progettazione di \nframework, ovvero librerie che consentano \nagevolmente l’estensione da parte di sviluppatori \nesperti\n… ben oltre i nostri obiettivi formativi\n21",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#21": "Programmazione orientata agli oggetti\nLivelli di visibilità in Java\nscontato. comprensibile... da ricordare!\n–Il livello di visibilità ottenuto senza modificatore viene anche \ndenominato «package-private»\n–Il livello di visibilità «protected » è più permissivo del livello \n«package-private» ed è il livello più permissivo in assoluto dopo \n«public»\n–Il livello di visibilità «package-private» è il meno permessivo in \nassoluto dopo «private»\n22",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#22": "Programmazione orientata agli oggetti\nEsercizio\n•Reimplementare StanzaMagica  anche utilizzando \nl'estensione tra classi, impostando il modificatore \ndi accesso delle variabili attrezzi  e \nnumeroAttrezzi della classe base Stanza  a \nprotected  (anziché  private )\n•Le successive evoluzioni del nostro studio di caso \nsaranno un’occasione per vedere quali problemi \nla violazione dell'information hiding può \ncomportare durante la manutenzione del codice…\n23",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#23": "Programmazione orientata agli oggetti\n•Solo i metodi pubblici e protetti della classe base \npossono essere sovrascritti\n•Se proviamo a ridefinire un metodo private della \nclasse base quello che otteniamo è un nuovo metodo \n–N.B. il nuovo metodo «nasconde» l’omonimo metodo \nofferto nella superclasse ma non lo sovrascrive affatto\n•In generale è possibile sovrascrivere un metodo e \ncambiare il modificatore di accesso, ma solo \nmantenendo od ampliandone  la visibilità\n–Perche? Sugg.: pensare al principio di sostituzione\n•Quindi per sovrascrivere un metodo, i livelli di visibi-\nlità permessi sono:\n–sovrascritto  → sovrascrivente  \n–protected → protected\n–protected → public\n–public → public24Overriding ed Information Hiding",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#24": "Programmazione orientata agli oggetti\n•Pertanto il seguente codice è corretto:\npublic class Superclasse {\nprotected void metodo() {}\n}\npublic class Sottoclasse extends Superclasse {\n@Override\npublic void metodo() {}   // da protected  a public OK\n}\n•mentre  invece il seguente non compila:\npublic class Superclasse {\npublic void metodo() {}\n}\npublic class Sottoclasse extends Superclasse {\n@Override\nprotected void metodo() {}   // da public a protected  ERRORE\n}25Visibilità dei Metodi Sovrascritti",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#25": "Programmazione orientata agli oggetti\nCambiare Segnatura in Override\n•Permettere di sovrascrivere un metodo solo \nmantenendone od ampliandone  la visibilità, è la scelta \npiù sensata? \n✔Rispondere applicando il p.d.s.: \n–un metodo che sovrascrive e rimpiazza un altro metodo \ndeve certamente permettere tutti gli utilizzi originali\n–Al più, può consentirne anche di nuovi\n✔Ampliare la visibilità è coerente con il p.d.s.!\n•Finora abbiamo solo visto casi di metodi sovrascritti con \nampliamento della visibilità, ma della stessa identica segnatura. \nAd es. in Stanza e StanzaMagica :\npublic boolean void addAttrezzo(Attrezzo a) ;\n•Applichiamo lo stesso ragionamento per capire se ha senso \ncambiare il tipo restituito e/o quello dei parametri dei metodi \nsovrascritti\n26",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#26": "Programmazione orientata agli oggetti\n  \n•Consideriamo questi due metodi\n–public Studente immatricola()\n–public Persona immatricola ()\ncon Persona  supertipo di Studente\n•Quale di questi due metodi in una classe estesa può \nsovrascrivere l’altro collocato nella classe base? \nPerché?\n–Sugg.: applicare il principio di sostituzione che deve \nrestare valido\n✔Scriviamo due classi Organizzazione  ed Universita,  \ncon la classe Universita  che estende la classe \nOrganizzazione ...Overriding: Metodi di Risultato Polimorfo (1)\n27",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#27": "Programmazione orientata agli oggetti\npublic class Persona {\n}\npublic class Studente extends Persona {\n  public String getMatricola() {…} \n}\npublic class Organizzazione {\n    public Studente immatricola() {\n      return new Studente();\n    }\n}\npublic class Universita extends Organizzazione {\n    @Override\n    public Persona immatricola() {\n      return new Persona();\n    }\n}Overriding: Metodi di Risultato Polimorfo (2)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#28": "Programmazione orientata agli oggetti\nOverriding: Metodi di Risultato Polimorfo (3)\npublic class Persona { \n}\npublic class Studente extends Persona {…\n  public String getMatricola() {…} \n}\npublic class Organizzazione {\n    public Persona immatricola() {\n      return new Persona()\n    }\n}\npublic class Universita extends Organizzazione {\n    @Override\n    public Studente immatricola() {\n      return new Studente();\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#29": "Programmazione orientata agli oggetti\nOverriding: \nMetodi con Parametri Polimorfi (1)\n30•Consideriamo questi due metodi\n–public void immatricola(Studente s)\n–public void immatricola (Persona p)\ncon Persona  supertipo di Studente\n•Quale di questi due metodi in una classe derivata \npuò sovrascrivere l’altro collocato nella classe \nbase e… perché?\n✔Scriviamo due classi Organizzazione  ed \nUniversita , con la classe Universita  che \nestende la classe Organizzazione ...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#3": "Programmazione orientata agli oggetti\nIntroduzione\n•Riprendiamo lo studio di caso diadia\n–supponiamo di voler inserire nel labirinto delle \nstanze particolari, stanze \"magiche\", il cui \ncomportamento differisce da quello usuale\n•Una stanza magica, esattamente come la \nstanza ordinaria, ha una descrizione, possiede \nuna collezione di uscite, e può ospitare una \ncollezione di attrezzi\n4",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#30": "Programmazione orientata agli oggetti\nOverriding: \nMetodi con Parametri Polimorfi (2)\npublic class Persona { \n}\npublic class Studente extends Persona { \n}\npublic class Organizzazione {\n    public void immatricola( Persona p) {\n      // ...\n    }\n}\npublic class Universita extends Organizzazione {\n    @Override\n    public void immatricola( Studente s) {\n      // ...\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#31": "Programmazione orientata agli oggetti\nOverriding: \nMetodi con Parametri Polimorfi (3)\npublic class Persona { \n}\npublic class Studente extends Persona { \n}\npublic class Organizzazione {\n    public void immatricola( Studente s) {\n      // ...\n    }\n}\npublic class Universita extends Organizzazione {\n    @Override\n    public void immatricola( Persona p) {\n      // ...\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#32": "Programmazione orientata agli oggetti\nOverriding: \nMetodi con Parametri Polimorfi (4)\npublic class Persona { \n}\npublic class Studente extends Persona { \n}\npublic class Organizzazione {\n    public void immatricola( Studente s) {\n       System.out.println(\"Organizzazione.immatricola(Studente)\" );\n    }\n}\npublic class Universita extends Organizzazione {\n    \n    public void immatricola( Persona p) {\n       System.out.println(\"Universita.immatricola(Persona)\" );\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#33": "Programmazione orientata agli oggetti\nUn Overriding che Diventa Overloading\nstatic public void main(String args[]) {\n  Persona p = new Persona();\n  Persona ps = new Studente();\n  Studente s = new Studente();\n  Organizzazione org = new Organizzazione();\n  Organizzazione studi = new Universita();\n  Universita rm3 = new Universita();\n//org.immatricola(p);   // ERRORE: NON COMPILA\n//org.immatricola(ps);  // ERRORE: NON COMPILA\n  org.immatricola(s);\n//studi.immatricola(p) ; // ERRORE: NON COMPILA\n//studi.immatricola(ps); // ERRORE: NON COMPILA\n  studi.immatricola( s);\n  rm3.immatricola( p);\n  rm3.immatricola(ps);\n  rm3.immatricola( s);\n}L’esecuzione stampa:\nOrganizzazione.immatricola(Studente)\nOrganizzazione.immatricola(Studente)\nUniversita.immatricola(Persona)\nUniversita.immatricola(Persona)\nOrganizzazione.immatricola(Studente)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#34": "Programmazione orientata agli oggetti\nStudente\nPersona\nRiassunto: \nCovarianza & Controvarianza\nOrganizzazione\nUniversità\nprotected Persona immatricola(Studente)\npublic Studente immatricola(Persona)\ncovarianza controvarianzaSpecializzando ↑ il tipo che ospita un metodo, \naffinché si possa sovrascrivere:\nvisibilità↓ – tipo restituito ↑ – tipo parametro ↓@Override\ntipo + generale\ntipo + speciale",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#35": "Programmazione orientata agli oggetti\nLa Gerarchia delle Classi Java:\nLa Classe Object\n•Abbiamo già visto che in Java tutte le classi \nestendono automaticamente la classe Object\n•E' una classe predefinita, che viene \nautomaticamente estesa da ogni nuova classe \n(direttamente o indirettamente)\n•N.B. scrivere:\npublic class MiaClasse {\n}\nè del tutto equivalente a scrivere:\npublic class MiaClasse extends Object  {\n}\n36",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#36": "Programmazione orientata agli oggetti\nGerarchie di Classi\n•In Java una classe può essere estesa da molte \nclassi, ma ogni classe estende sempre una ed \nuna sola  classe\n–tranne Object , che è la radice predefinita della \ngerarchia di classi\n•Non ci può essere «ereditarietà multipla»  delle \nimplementazioni \n37",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#37": "Programmazione orientata agli oggetti\nLa Gerarchia delle Classi Java\n•Un'unica radice: Object\n•Ogni classe* ha una e una sola superclasse\n•Ogni classe può avere zero o più sottoclassi \n38* con l'eccezione di ObjectClasse1 Classe2\nClasse3 Classe4Object\nClasse5",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#38": "Programmazione orientata agli oggetti\nEreditarietà Multipla non Ammessa\n•Se Classe4  ereditasse sia da Classe1  \nche da Classe2\nN.B. In Java non è possibile!\n•Quali problemi?\n39Classe1 Classe2\nClasse3 Classe4",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#39": "Programmazione orientata agli oggetti\nProblemi con l'Ereditarietà Multipla: \nl'Ereditarietà a Diamante\n40Classe1 Classe2\nClasse4Classe0",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#4": "Programmazione orientata agli oggetti\nLa Stanza Magica (1)\n•Una stanza magica ha delle particolarità, che la \nrendono diversa dalla stanza ordinaria:\n–dopo N volte che in tale stanza viene posato (aggiunto) \nun qualsiasi attrezzo da parte del giocatore, la stanza \ninizierà a comportarsi «magicamente»\n–quando la stanza si comporta magicamente, ogni volta \nche posiamo un attrezzo, la stanza \"inverte\" il nome \ndell'attrezzo e ne raddoppia il peso. Ad esempio: se \nposiamo (togliamo dalla borsa e aggiungiamo alla \nstanza) l'attrezzo con nome ' chiave' e peso 2, la stanza \nmemorizza un attrezzo con nome ' evaihc' e peso 4\n–quando la stanza non si comporta magicamente, il \ncomportamento rimane quello usuale\n5",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#40": "Programmazione orientata agli oggetti\nLa gerarchia del Frogosauro\nclass Animal {\n   void talk() {\n      System.out.println(\"…\");\n}\nclass Frog extends Animal {\n   void talk() {\n      System.out.println(\"Ribit, ribit.\");\n   }\n}\nclass Dinosaur extends Animal {\n   void talk() {\n      System.out.println(\"I'm a dinosaur: I'm cool! \");\n   }\n}\n41Dinosaur Frog\nFrogosaurAnimal",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#41": "Programmazione orientata agli oggetti\nIl Frogosauso\n// NON COMPILA\nclass Frogosaur extends Frog, Dinosaur {\n} \n✔ Cosa dovrebbe fare la seguente chiamata a \ntalk() ?\nAnimal animal = new Frogosaur();\nanimal.talk();\n42",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#42": "Programmazione orientata agli oggetti\nProblemi di Ereditarietà Multipla (1)\n•Se un membro (metodo o campo) è definito in \nentrambe le classi base, da quale delle due la classe \nestesa «eredita» l’implementazione?\n•E se le due classi base a loro volta estendono una \nsuperclasse comune ?\n•Il problema è legato alle implementazioni, che \nvengono ereditate e potenzialmente «confuse»\n✔Per questo motivo, in Java si adotta una scelta molto \nconservativa:\n–una classe può implementare tante interfacce\n–ma può estendere sempre e solo una unica superclasse\n43",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#43": "Programmazione orientata agli oggetti\nProblemi di Ereditarietà Multipla (2)\n•In realtà il problema di quale implementazione \nscegliere, anche in questi casi, è perfettametne \nrisolvibile e risolto in alcuni linguaggi di \nprogrammazione\n–(C++, Scala, in una forma molto limitata anche Java 8+)\n•Il Problema del Frogosauro  fa quasi parte del folklore \noramai, quasi impossibile non parlarne!\n✔Ma solo lascamente chiarisce il vero problema:\n–“mischiare” due implementazioni diverse è estremamente \ncomplicato e quasi sempre non necessario\n–comodo in pochi e particolari casi (implementazioni che sono \nsostanzialmente ortogonali: per i più interessati vedere \nmixin’) ben oltre gli obiettivi formativi di questo corso\n✔Rapporto  costi benefici opinabile44",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#44": "Programmazione orientata agli oggetti\nImplementazione di più Interfacce\n●Al contrario in Java è possibile che una classe \nimplementi molteplici interface, una politica molto \npermissiva:\npublic interface Persona {\n  …\n}\npublic class Dirigente  implements Impiegato, Persona  {\n  …\n}\n✔ Una classe già ampiamente utilizzata implementa tre \ninterfacce: java.lang.String\npublic final class String implements \n       Serializable, Comparable<String>, CharSequence\npublic final class String\n   extends Object\n   implements Serializable, \n              Comparable<String>, \n              CharSequence45",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#45": "Programmazione orientata agli oggetti\nJava Interface ed \nEreditarietà Multipla (1)\n•Il problema dell'ereditarietà multipla (delle \nimplementazioni) non sussiste affatto con le \ninterface\n•Consideriamo infatti una classe che \nimplementa più di una interface\n•Per essere concreta e risultare instanziabile:\n–è costretta a fornire direttamente tutte le \nimplementazioni di tutti metodi implementati\n✔si evitano così alla radice tutti i problemi legati \nall’eredità multipla delle implementazioni\n46",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#46": "Programmazione orientata agli oggetti\nJava Interface ed \nEreditarietà Multipla (2)\n•Abbiamo anche visto che una interface può \nessere definita per estensione da un'altra \ninterface\n•In generale una interface può estendere anche \npiù di una interface scrivendo direttamente:\npublic interface A {}\npublic interface B {}\npublic interface C {}\npublic interface I extends A,B,C {}\n•Pertanto la gerarchia dei tipi è più articolata di \nquella delle classi in quanto contempla anche i \ntipi definiti tramite le interface...\n47",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#47": "Programmazione orientata agli oggetti\nLa Gerarchia dei Tipi Java\n✔Un'unica radice: Object\n•Ogni classe* ha una ed una sola superclasse\n•Ogni classe può avere zero o più sottoclassi\n•Ogni classe può implementare zero o più interface\n•Ogni interface può estendere zero o più interface \nClasse1 Classe2\nClasse3 Classe4Object\nClasse5\n* con l'eccezione di ObjectInterface2 Interface1\nInterface3\n48",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#48": "Programmazione orientata agli oggetti\nGerarchia delle Classi Lineare\n•Ogni classe estende sempre una ed una sola \nclasse\n•Tranne Object , che è la radice predefinita della \ngerarchia di classi (e tipi)\n•Ma una classe può essere estesa da molte classi\n•Non ci può essere ereditarietà multipla delle \nimplementazioni\n•Si usa dire che la gerarchia delle classi è lineare\nle implementazioni di tutti i metodi di una classe si trovano \nin una  sequenza lineare di classi e superclassi che \nconducono sino alla radice Object\nAttenzione: non più perfettamente vero in Java 8+\n•metodi di interface con implementazioni di default\n49",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#49": "Programmazione orientata agli oggetti\nL’importanza dei Tipi (1)\n•La scelta dei corretti tipi da definire in un programma \nè un delicato esercizio di modellazione \n•Si può affrontare a ragion veduta solo dopo studio, \npratica ed interi corsi specificatamente dedicati alla\nAnalisi & Progettazione  (come APS)\n•E’ opportuno cercare subito di prevenire alcuni degli \nerrori più ricorrenti nei primi utilizzi dei meccanismi di \ndefinizione dei tipi \n–Non considerare gli aspetti dinamici \n(il comportamento degli oggetti)\n–Ovvero considerare solo gli aspetti statici\n( dati che modellano)50",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#5": "Programmazione orientata agli oggetti\nLa Stanza Magica (2)\n•Vogliamo introdurre questa variante  nel nostro gioco\n•Una stanza magica deve poter essere usata in \nqualsiasi punto in cui usiamo oggetti Stanza\n•La classe della stanza magica pur essendo molto \nsimile a quella della stanza ordinaria, differisce per:\n–alcuni dati in più da gestire\n–alcuni metodi in più che può offrire\n–il comportamento di un metodo\n✔Possiamo usare l'estensione e sfruttare il polimorfismo \nper introdurre questa caratteristica nel gioco\n6",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#50": "Programmazione orientata agli oggetti\nL’importanza dei Tipi (2)\n•Su altri errori tipici dovuti alla mancanza di pratica \nmeglio tornarci in seguito (>>)\n•Tra i più rilevanti la tipizzazione anemica\n–possono scherzosamente chiamarsi sulla base del \nnome di un tipo di cui si ha un’irrefrenabile \ntendenza ad abusare, quasi fosse una «malattia »\n•La «stringhite » (>>)\n•La «mappite » (>>)\n•Sono «sintomi» dell’incapacità di scegliere \ncorrettamente i tipi da definire\n51",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#51": "Programmazione orientata agli oggetti\nConsiderazioni Finali:\nTipi e Sottotipi\n•Un frequente errore è sicuramente legato alla tendenza a \nfissare le relazioni tra tipi concentrandosi solo sugli aspetti \nstatici delle classi e trascurando invece quelli dinamici\n•Domanda : Ma... Quadrato  è sottotipo di Rettangolo ?\nFarsi sempre guidare dal principio di sostituzione:  ogni \nqualvolta mi aspetto un Rettangolo  posso utilizzare al suo \nposto un Quadrato ?\n•Risposta : dipende!   Se devo calcolarne il perimetro si, ma \nse devo raddoppiarne l’altezza senza cambiarne la base \nsicuramente no!\nIl p.d.s. deve valere anche a tempo dinamico, anche dopo i \ncambiamenti di stato degli oggetti coinvolti\n52",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#52": "Programmazione orientata agli oggetti\nTipi e Sottotipi: \nString  vs StringBuilder  (1)\n●Altro esempio; cfr.  String  vs StringBuilder :\n• String si usa per oggetti immutabili:\n–Una volta creato un oggetto java.lang.String , non è poi più \npossibile cambiarne lo stato\n✔(E’ tuttavia possibile creare nuovi oggetti stringhe sulla base del \nprimo)\n●Per costruire progressivamente stringhe particolarmente lunghe, \nè spesso preferibile utilizzare la versione modificabile delle \nstringhe: classe java.lang.StringBuilder\n●StringBuilder  si usa per oggetti mutabili:\n public final class StringBuilder …???… {\n    StringBuilder append(String str);\n    …\n    String toString();\n}\n●StringBuilder  deve essere sottotipo di String ???\n53",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#53": "Programmazione orientata agli oggetti\nTipi e Sottotipi: \nString  vs StringBuilder  (2)\n✔Cfr.  String/StringBuilder/CharSequence :\npublic final class String extends Object \n  implements\n  Serializable,Comparable<String>, CharSequence {…}\npublic final class StringBuilder  extends Object\nimplements Serializable, CharSequence  {…}\npublic interface CharSequence {\n   public char charAt(int index);\n   public int length();\n   public CharSequence subSequence(int start, int end);\n   public String toString();\n}\nStringBuilder NON è un sottotipo (né potrebbe esserlo) \ndi String ma di un loro supertipo comune CharSequence ; \nsi prevede esplicitamente la convertibilità degli oggetti \nStringBuilder a String tramite toString()\n54",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#54": "Programmazione orientata agli oggetti\nEsercizio: \nOverriding for Overloading  (1)\n•Rafforziamo  la comprensione del legame tra \nl’overloading e l’overriding in Java svolgendo un \nparticolare esercizio\n•Vogliamo invocare un metodo sovraccarico proprio \nsulla base del tipo dinamico del suo parametro e \nNON sulla base del suo tipo statico\n•N.B. NON è possibile farlo con i meccanismi offerti \ndirettamente dal linguaggio Java\n–In Java i metodi sovraccarichi sono risolti sulla base del tipo \nSTATICO e mai sulla base del tipo DINAMICO!\n•Mostriamo come mediante l’overriding di un metodo \npolimorfo ed il late-binding sia possibile «simulare» la \nrisoluzione di un metodo sovraccarico sulla base del \ntipo dinamico di un parametro\n55",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#55": "Programmazione orientata agli oggetti\nEsercizio: \nOverriding for Overloading  (2)\n•Consideriamo un semplice esempio: è data una \ngerarchia di tipi avente come radice l’interface \nForma e sottotipi concreti Quadrato , Cerchio ,…  \nma non è possibile cambiarne il codice\n✔Per quanto l’esempio sia volutamente stilizzato, nella \npratica questa situazione si presenta non di rado\n•ad es. durante l’uso di gerarchie di tipi definite da \nlibrerie esterne il cui codice risulta immodificabile\n•ad es. quelle che gestiscono composizioni  di oggetti tutti \ndi uno stesso supertipo comune ma di diversi sottotipi \nconcreti\n56",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#56": "Programmazione orientata agli oggetti\nEsempio: Una Gerarchia di Tipi\n(Sorgente Non Modificabile)\npublic interface Forma {\n}\npublic class Cerchio implements  Forma {\n  private int raggio;\n  public Cerchio(int r)  { this.raggio = r;    }\n  public int getRaggio() { return this.raggio; } \n}\npublic class Quadrato implements  Forma {\n  private int lato;\n  public Quadrato( int l) { this.lato = l;    }\n  public int getLato()   { return this.lato; }\n}\n•Si vuole calcolare l’area delle forme, che dipende dal tipo, ma NON \nè possibile modificare l’interface Forma per aggiungere il metodo\n  public float getArea();\ne quindi non è possibile implementarlo in Quadrato , Cerchio,  …\n57",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#57": "Programmazione orientata agli oggetti\nRisoluzione Tramite Overriding (1)\npublic class CalcolatoreDiArea {\n    public float areaDi(Cerchio c) {\n       int r = c.getRaggio();\n       return 3.14f * r * r;\n    }\n    public float areaDi(Quadrato q ) {\n       int l = q.getLato();\n       return l * l;\n    }\n    public static void main(String args) {\n      CalcolatoreDiArea calcolatore = new CalcolatoreDiArea();\n      Cerchio cerchio = new Cerchio(1);\n      Quadrato quadrato = new Quadrato(2);\n      System.out.println(calcolatore.areaDi(cerchio));          \n      System.out.println(calcolatore.areaDi(quadrato)); \n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#58": "Programmazione orientata agli oggetti\nRisoluzione Tramite Overriding (2)\npublic class CalcolatoreDiArea {\n   public float areaDi(Cerchio c) {\n       int r = c.getRaggio();\n       return 3.14f * r * r;\n   }\n   public float areaDi(Quadrato q) {\n       int l = q.getLato();\n       return l * l;\n   }\n   public static void main(String args) {\n      CalcolatoreDiArea calcolatore = new CalcolatoreDiArea();\n      Forma cerchio = new Cerchio(1);\n      Forma quadrato = new Quadrato(2);\n      System.out.println(calcolatore.areaDi(cerchio)); // ERRORE: NON COMPILA\n      System.out.println(calcolatore.areaDi(quadrato)); // ERRORE: NON COMPILA\n   }\n}\n•Si supponga invece di voler calcolare l’area totale di una \ncollezione di forme di cui non sia possibile prevedere il tipo a \ntempo dinamico…",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#59": "Programmazione orientata agli oggetti\nRisoluzione Tramite Overriding (3)\npublic class CalcolatoreDiArea {\n   public float areaDi(Cerchio c) {\n       int r = c.getRaggio();\n       return 3.14f * r * r;\n   }\n   public float areaDi(Quadrato q) {\n       int l = q.getLato();\n       return l * l;\n   }\n   static public float sommaAll(CalcolatoreDiArea calcolatore, Forma[] forme) {\n       float acc = 0;\n       for(Forma forma : forme) {\n          acc += calcolatore.areaDi( forma); // ERRORE: NON COMPILA\n       }\n       return acc;\n   }\n   public static void main(String args) {\n      CalcolatoreDiArea calcolatore = new CalcolatoreDiArea();\n      Forma[] forme = { new Cerchio(1), new Quadrato(2) } ;\n      System.out.println( sommaAll(calcolatore, forme));      \n   }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#6": "Programmazione orientata agli oggetti\nDefinizione di Classe Estesa (1)\n•La classe StanzaMagica  rispetto alla classe Stanza \n–possiede nuove variabili: \n–contatoreAttrezziPosati: memorizza il numero di attrezzi \nposati (aggiunti)\n–sogliaMagica : memorizza il numero di attrezzi da posare prima \nche si attivi il comportamento «magico» della stanza\n–SOGLIA_MAGICA_DEFAULT:  valore di default per la soglia  \n–possiede un nuovo metodo privato\nprivate Attrezzo modificaAttrezzo(Attrezzo attrezzo)  che \nrestituisce un attrezzo a partire dall'attrezzo passato come \nparametro\n–ridefinisce il metodo addAttrezzo(Attrezzo attrezzo)  per \nimplementare l'effetto magico\n–ha due costruttori: uno prende nome e soglia, l'altro solo il nome (e \nimposta la soglia al valore di default)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#60": "Programmazione orientata agli oggetti\nOverriding for Overloading  (1)\n●Una libreria che pubblica una gerarchia di tipi (con sorgente \nimmodificabile) può essere predisposta per la definizione di codice \ncliente che sappia specializzare i propri comportamenti sulla base del \ntipo dinamico\n●L’interface Forma  viene cambiata per ospitare un metodo che \npermetta a tutti i tipi di forme di «accettare»  un generico \nCalcolatore:\npublic interface  Forma { \npublic float  accetta(Calcolatore c);\n}\n●Questo deve saper distinguere tutti i tipi di forme della gerarchia \npublic interface  Calcolatore {\n    public float calcola(Cerchio c);\n    public float calcola(Quadrato q);\n}  \n✔Parliamo di «generici» calcoli; il calcolo dell’area è solo un \nesempio di uno dei loro possibili «instanziamenti »...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#61": "Programmazione orientata agli oggetti\nOverriding for Overloading  (2)\n●Il metodo calcola()  trova questa implementazione nei sottotipi \nconcreti Cerchio e Quadrato :\npublic class Cerchio    // lo stesso dicasi per Quadrato\n       implements Forma {\n   @Override\n   public float accetta(Calcolatore c) { // N.B. il tipo statico\n           //  di this è quello della classe corrente, qui Cerchio\n      return c.calcola( this); // tipo statico  Cerchio\n   }\n}\n✔La chiamata al metodo polimorfo float accetta(Calcolatore)   \ndi Forma come sovrascritto (overload) in Cerchio finisce per \nservire a fissare il tipo statico dell’argomento alla chiamata al \nmetodo sovraccarico (override) Calcolatore.calcola(Cerchio) !\n✔In Quadrato  servirà per chiamare l’altro metodo sovraccarico \nCalcolatore.calcola(Quadrato)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#62": "Programmazione orientata agli oggetti\nOverriding for Overloading  (3)\npublic interface Forma {\n  public float accetta(Calcolatore c);\n}\npublic class Cerchio implements Forma {…\n  @Override\n  public float accetta(Calcolatore c) {\n    return c.calcola( this);\n  }\n}\npublic class Quadrato implements Forma {…\n  @Override\n  public float accetta(Calcolatore c) {\n    return c.calcola( this);\n  }\n}\npublic interface Calcolatore {\n    public float calcola(Cerchio c);\n    public float calcola(Quadrato q);\n}Tipo statico Cerchio\nTipo statico Quadrato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#63": "Programmazione orientata agli oggetti\nOverriding for Overloading  (4)\npublic class CalcolatoreDiArea implements Calcolatore  {\n    @Override\n    public float calcola(Cerchio c) {\n       return 3.14f * c.getRaggio() * c.getRaggio();\n    }\n    @Override\n    public float calcola(Quadrato q) {\n       return q.getLato() * q.getLato();\n    }\n    static public float sommaAll(Calcolatore calc, Forma[] forme) {\n       float acc = 0;\n       for(Forma forma : forme) {\n          acc += forma.accetta(calc) ; // COMPILA!\n       }\n       return acc;\n    }\n \n   public static void main(String args) {\n      Calcolatore calcAree = new CalcolatoreDiArea();\n      Forma[] forme = { new Cerchio(1), new Quadrato(2) } ;\n      System.out.println( sommaAll(calcAree, forme));      \n   }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#64": "Programmazione orientata agli oggetti\nUlteriori Calcolatori \n(Dipendenti dal Tipo Dinamico)\n●E’ banalmente possibile implementare altre tipologie \ndi Calcolatore. Ad es.:\npublic class CalcolatoreDiPerimetro implements Calcolatore {\n    @Override\n    public float calcola( Cerchio c ) {\n       Return 2 * 3.14f * c.getRaggio();\n    }\n    @Override\n    public float calcola( Quadrato q ) {\n       return 4 * q.getLato();\n    }\n}\n●Il metodo statico sommaAll()  continua a funzionare:\n   public static void main(String args) {\n      Calcolatore calcPerim = new CalcolatoreDiPerimetro();\n      Forma[] forme = { new Cerchio(1), new Quadrato(2) } ;\n      System.out.println( sommaAll(calcPerim , forme));      \n   }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#65": "Programmazione orientata agli oggetti\nConclusioni:\nOverriding for Overloading  (1)\n●Riassumendo: si è aggiunto un livello di indirezione \naffinché il corpo del metodo polimorfo \naccetta(Calcolatore)  di Forma fissi il tipo statico \ndell’argomento della chiamata al metodo sovraccarico \ncalcola(…)  di Calcolatore\n●Per ottenere una chiamata sovraccarica risolta (a \ntempo di esec.) sulla base del tipo dinamico  del suo \nunico parametro vengono fatte:\n✔prima, una chiamata polimorfa  risolta dinamicamente\n✔poi, una chiamata sovraccarica  risolta staticamente",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#66": "Programmazione orientata agli oggetti\nConclusioni:\nOverriding for Overloading  (2)\n●Lo scopo di questo esercizio era chiarire i legami (ma anche \nribadire le significative differenze) esistenti tra due \nmeccanismi tipici della programmazione orientata agli oggetti\n✔Overriding  vs Overloading\n●Si potrebbe ripetere lo stesso esercizio anche per metodi che \nricevono DUE o più parametri polimorfi (ad es. 2+ forme)\n✔Il numero di varianti di metodo in overload si moltiplica ad ogni \nparametro aggiuntivo per ogni possibile tipo contemplato nella gerarchia\n✔Sino a risolverli tutti tramite una sequenza di chiamate polimorfe, una per \nciascun parametro\n●Il risultato sarebbe significativamente più lungo e meno \nleggibile, ma non per questo concettualmente più complicato\n●Nella pratica, anche grazie a questi meccanismi alternativi, la \nmancanza in Java della risoluzione dei metodi sovraccarichi \nsulla base del tipo dinamico di uno o più parametri non viene \npercepita come una significativa carenza",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#7": "Programmazione orientata agli oggetti\nEsempio: StanzaMagica\n class StanzaMagica extends Stanza {\n    final static private int SOGLIA_MAGICA_DEFAULT = 3;\n    private int contatoreAttrezziPosati ;\n    private int sogliaMagica;\n  public StanzaMagica(String nome) {\n        this(nome, SOGLIA_MAGICA_DEFAULT);\n  }\n   public StanzaMagica(String nome, int soglia) {\n        super(nome);\n        this.contatoreAttrezziPosati  = 0;\n        this.sogliaMagica = soglia;\n  }\n    @Override\n    public boolean addAttrezzo (Attrezzo attrezzo) {  \n        …\n    }\n    private Attrezzo modificaAttrezzo (Attrezzo attrezzo) {\n        … // (>>)\n    }\n }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#8": "Programmazione orientata agli oggetti\nEsempio: StanzaMagica\n●Un nuovo metodo privato modificaAttrezzo():\n  class StanzaMagica extends Stanza {\n    … … …\n    \n    private Attrezzo modificaAttrezzo(Attrezzo attrezzo) {\n       StringBuilder nomeInvertito;\n       int pesoX2 = attrezzo.getPeso() * 2;\n       nomeInvertito  = new StringBuilder(attrezzo.getNome());\n       nomeInvertito = nomeInvertito.reverse();\n       attrezzo = new Attrezzo(nomeInvertito.toString(),\n                               pesoX2);\n       return attrezzo;\n    }\n }\n9",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-13-estensione-protected.pdf#9": "Programmazione orientata agli oggetti\nDefinizione di Classe Estesa (2)\n•Con oggetti StanzaMagica  è comunque possibile \nusare i metodi pubblici definiti nella superclasse \nquali getDescrizione() , getStanzeAdiacenti()\n  \n  StanzaMagica labIA = …\nString s = labIA.getDescrizione();\n•Questi metodi non sono definiti esplicitamente in \nStanzaMagica , ma vengono ereditati\n•Allo stesso modo vengono ereditate le variabili di \nistanza\n–(ma solo quelle non private solo visibili nella \nsottoclasse)\n10",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#0": "Programmazione \nOrientata agli Oggetti\nGenerics: Concetti Base",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#1": "Programmazione orientata agli oggetti\nObiettivi della Lezione\n•I generics sono uno strumento per scrivere classi \n(e metodi) parametriche rispetto ad un tipo\n•Ci concentriamo soprattutto su come usare classi \ngeneriche\n–Al temine del corso lo studente dovrà essere in grado di \nusare classi generiche (in particolare quelle del package  \njava.util relative alla gestione di collezioni di oggetti)\n•La progettazione di classi generiche va oltre gli \nobiettivi del corso\n–Anche se, da un punto di vista puramente didattico, \nrisulta invece utile introdurre i Generics  progettando una \nsemplice classe contenitrice di oggetti: Coppia",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#10": "Programmazione orientata agli oggetti\nControllo sui Tipi Statici vs Dinamici\n•Il problema nasce dal controllo a tempo dinamico \noperato dal downcast esplicito: \n–nell'oggetto coppia è atteso come primo elemento \nun oggetto di tipo dinamico Persona\n•Invece vi si trova un riferimento ad un oggetto di \ntipo dinamico String\n•Tutto perfettamente lecito per il compilatore che \neffettua verifiche solo sul tipo statico:\n–l’istruzione coppia.setPrimo(pippo) riceve come \nparametro attuale la variabile locale pippo, di tipo \nstatico String , sottotipo del tipo Object atteso",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#11": "Programmazione orientata agli oggetti\nTipizzazione Lasca\n•A ben vedere sono tutte conseguenze di una \ntipizzazione lasca\nobiettivo : coppie di oggetti dello stesso tipo\n•A differenza di come originariamente desiderato, \nper ovviare alla scarsa espressività del sistema \ndei tipi Java, siamo finiti per progettare una classe \nche può ospitare un coppia di oggetti qualsiasi\nrisultato : coppie di oggetti non necessariamente \ndello stesso tipo!\n•Solo un’approssimazione del tipo desiderato\n–pratica frequentemente utilizzata precedentemente \nall’introduzione dei Java Generics nella piattaforma Java",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#12": "Programmazione orientata agli oggetti\nConseguenze della Tipizzazione Lasca\n•Un controllo lasco dei tipi a tempo di compilazione \ncomporta almeno le seguenti conseguenze:\n–Costringe ad inserire downcast ogni volta che accediamo ad \nun elemento della coppia\nPrima di Java 5 molti sviluppatori consideravano i downcasting \nquantomeno ineleganti, ma pochi lo consideravano un \nsostanziale limite, nella pratica, del linguaggio\n–Rimanda a tempo di esecuzione alcuni errori che \nrisulterebbero facilmente rilevabili già a tempo di \ncompilazione con una migliore analisi dei tipi statici\nQuesto è il vero problema! riconsiderare il costo dei bug \nrispetto al costo degli errori di compilazione>>\n•In fase di definizione della coppia, si vorrebbe poter \nspecificare un unico tipo per entrambi  i riferimenti ad \noggetti che la coppia è destinata a memorizzare",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#13": "Programmazione orientata agli oggetti\nTipi e Metodi Generici\n•I generics sono uno strumento per scrivere classi, \ned interfacce, il cui tipo diventa parametrico \nrispetto ad uno o più tipi\n•Si applica anche ai metodi per renderne \nparametrica la segnatura\n•Nella definizione di un tipo generico, il codice \nviene scritto in maniera parametrica rispetto ad \nun tipo formale\n•Nell'uso di un tipo generico, il tipo formale  deve \nessere istanziato con un tipo attuale per renderlo \neffettivamente utilizzabile e completamente \ndefinito",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#14": "Programmazione orientata agli oggetti\nLa Classe Generica Coppia<T>  (1) \n•La definizione di una classe generica prevede la \ndichiarazione del parametro formale di tipo racchiuso \ntra parentesi acute\npublic class Coppia <T> {\n…\n}\n•In questo modo si specifica che all'interno della \ndefinizione della classe  Coppia , il simbolo T indica il \ntipo sulla base del quale la definizione di classe è \nparametrica\n•Convenzione sul nome del parametro formale di tipo \n–Singola lettera maiuscola: T, E, V …",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#15": "Programmazione orientata agli oggetti\nLa Classe Generica Coppia<T>  (2) \n•Nella definizione di campi e metodi all'interno della \nclasse T viene usato come una dichiarazione di tipo:\npublic class Coppia< T> {\n    private T primo;\n    private T secondo;\n    public Coppia( T primo, T secondo) {\n        this.primo = primo;\n        this.secondo = secondo;\n    }\n    public T getPrimo() {\n        return this.primo;\n    }\n    public T getSecondo() {\n        return this.secondo;\n    }\n    …\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#16": "Programmazione orientata agli oggetti\nOggetti Coppia<T>  Mutabili\npublic class Coppia<T> {\nprivate T primo;\nprivate T secondo;\npublic Coppia(T primo, T secondo) {\n    this.primo = primo;\n    this.secondo = secondo;\n}\npublic T getPrimo() {\n    return this.primo;\n}\npublic T getSecondo() {\n    return this.secondo;\n}\npublic void setPrimo(T primo) {\n    this.primo = primo;\n}\npublic void setSecondo(T secondo) {\n    this.secondo = secondo;\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#17": "Programmazione orientata agli oggetti\nUsare una Classe Generica ( T=Persona )\n•Quando  usiamo una classe generica, dobbiamo \nistanziarne completamente il tipo  fornendo il tipo \nattuale di tutti i tipi formali di cui fa uso\n•Ad esempio, usiamo la nostra classe generica \nCoppia<T> , per gestire coppie di oggetti Persona\npublic class CoppiaTest {\n   @Test\n   public void testCoppiaDiPersone() {\n       Coppia<Persona>  coppia;\n       Persona p1 = new Persona(\"Stanlio\");\n       Persona p2 = new Persona(\"Olio\");\n       coppia = new Coppia<Persona>(p1, p2);\n       assertSame(p1,coppia.getPrimo());\n       assertSame(p2,coppia.getSecondo());\n   }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#18": "Programmazione orientata agli oggetti\nUsare una Classe Generica ( T=Color )\n•Vediamo la classe parametrica Coppia<T>  \nistanziata su un altro tipo qualsiasi (ad es. \njava.awt.Color )\nimport java.awt.Color;\n… \npublic class CoppiaTest {\n   @Test\n public void testCoppiaDiColori() {    \n        Coppia<Color>  colori;\n      Color rosso = new Color(255,0,0);\n      Color blue = new Color(0,0,255);\n      colori = new Coppia<Color>(rosso, blue);\n     assertSame(rosso,colori.getPrimo());\n     assertSame(blue,colori.getSecondo());\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#19": "Programmazione orientata agli oggetti\nControllo sui Tipi, con Generics\n•Riconsideriamo il codice di prima, quello che \navremmo voluto non compilasse affatto:\npublic class CoppiaTest {\n  @Test\n  public void testCheSmetteDiCompilare() {\nCoppia<Persona> coppia = new Coppia<Persona>();\nString pippo = new String(\"Pippo\");\nString pluto = new String(\"Pluto\");\nPersona p1 = new Persona(pippo);\nPersona p2 = new Persona(pluto);\ncoppia.setPrimo(pippo);\ncoppia.setSecondo(pluto);\nassertSame(pippo,\n         ((Persona)coppia.getPrimo()).getNome());\n  }\n}NON COMPILA!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#2": "Programmazione orientata agli oggetti\nIntroduzione\n•Notare che le coppie sono una forma molto \nrudimentale di collezione\n•In effetti i Generics  furono introdotti in Java 5 \nproprio per migliorare il JCF, la libreria dedicata \nalle collezioni già presente sin da Java 2 (ovvero \n1.2)\n•Obiettivo: una classe generica rispetto al tipo dei \ndue oggetti ospitati, purché sia lo stesso per \nentrambi. Ad esempio la classe dovrà gestire:\n–Coppie di stringhe (istanze della classe String)\n–Coppie di attrezzi (istanze della classe Attrezzo )\n–Coppie di URL (istanze della classe URL)\n–… ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#20": "Programmazione orientata agli oggetti\nTipo Formale – Tipo Attuale\n•Non è difficile trovare una similitudine tra\n–il concetto di parametro formale/attuale inerente \nl’invocazione dei metodi\n–il concetto di tipo formale/attuale inerente la \ntipizzazione di classi generiche\n•Solo superficialmente sono concetti simili: tra le \ntante differenze, non dimenticare mai la prima e \npiù importante:\n–il legame tra parametri formali/attuali è operato \ndalla JVM a tempo di esecuzione\n–il legame tra tipi formali/attuali è operato dal \ncompilatore a tempo di compilazione solo sulla \nbase dei tipi statici",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#21": "Programmazione orientata agli oggetti\nGenerics a più Parametri\n•È possibile definire classi, interfacce e metodi \ngenerici con più parametri di tipo\n–Sintatticamente, si separano i vari parametri \ncon una virgola\npublic class Esempio<T, S> {…}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#22": "Programmazione orientata agli oggetti\nGenerics e Tipi Primitivi\n•Java è un linguaggio ibrido ci sono informazioni che si \nrappresentano senza utilizzare oggetti, ad es. int\n•Non è possibile istanziare i tipi di una classe, di una \ninterfaccia o di un metodo generico con tipi primitivi \n•Per ovviare al problema è possibile rappresentare i \ntipo primitivi mediante le cosidette classi wrapper\npublic class TestCoppia {\n   @Test\npublic void testCheNonCompila() {\n   Coppia<int>  coppia;      // ERRORE: NON COMPILA!\n   int i1 = 100;\n   int i2 = 200;\n   coppia = new Coppia<int> (i1, i2);  // ERRORE\n}\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#23": "Programmazione orientata agli oggetti\nClassi Wrapper  (1)\n•Per ogni tipo primitivo esiste una corrispondente \nclasse wrapper  che consente di «oggettificare» il \ndato primitivo, costruendoci un oggetto \n«attorno», per ospitarlo:\n–int →Integer\n–double →Double\n–float → Float\n–char →Character\n–boolean →Boolean\n24",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#24": "Programmazione orientata agli oggetti\nClassi Wrapper  (2)\n25int i; \ni = 18;\nInteger iwrap = new Integer(i);  \n…\nint value = iwrap.intValue();\n …il valore della variabile \nint è “avvolto” in un \noggetto Integer\n\"scarto\" il \nvalore",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#25": "Programmazione orientata agli oggetti\nClassi Wrapper  (3)\n•Le classi wrapper sono definite nel package \njava.lang\n–quindi non è necessario importarle esplicitamente\n–per approfondimenti sui dettagli dei loro metodi è \nsufficiente vedere la documentazione\n•Metodi più frequentemente usati:\n–metodi xxxValue()\n–metodi valueOf()  e parseXxx()\n–metodo equals()\n26",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#26": "Programmazione orientata agli oggetti\nGenerics e Tipi Primitivi\n•Esempio:\npublic class TestCoppia {\n   @Test\n public void testCheCompila() {\n   Coppia<Integer>  coppia;      // OK\n   Integer i1 = new Integer(100);\n   Integer i2 = new Integer(200);\n   coppia = new Coppia<Integer>(i1, i2);  // OK\n}\n}\nDecisamente prolisso",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#27": "Programmazione orientata agli oggetti\nBoxing/Unboxing\n•Per ovviare alla eccessiva verbosità,  dalla versione 5 \ndi Java (non a caso la stessa dell’introduzione dei \ngenerics ), la gestione di oggetti wrapper è \nsemplificata dalle funzionalità di boxing e unboxing\n•In sostanza una tecnica di conversione automatica di \ntipi primitivi nei corrispondenti oggetti di un tipo \nwrapper  e viceversa\n–per certi aspetti simile alla promozione di tipi che già si \nopera ad es. da int a float \n–per altri ancora, differente: sono coinvolti sia tipi che \nnon sono oggetti, sia tipi che invece lo sono\n28",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#28": "Programmazione orientata agli oggetti\nBoxing\n•Boxing:  è possibile assegnare direttamente tipi \nprimitivi a oggetti wrapper\n•Le seguenti istruzioni sono equivalenti:\nint i = 0;        int i = 0;\nInteger iWrap;    Integer iWrap;\niWrap = i;        iWrap = new Integer(i);\niWrap = 5;        iWrap = new Integer(5);\n•È il compilatore che inserisce automaticamente le \nistruzioni per gestire boxing/unboxing!\n29",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#29": "Programmazione orientata agli oggetti\nUnboxing\n•Unboxing:  è possibile assegnare direttamente \noggetti wrapper a tipi primitivi\n•Le seguenti istruzioni sono equivalenti:\nint i = 0;        int i = 0;\nInteger iWrap;    Integer iWrap;\niWrap = 5;        iWrap = new Integer(5);\ni = iWrap;        i = iWrap.intValue();\n•È il compilatore che inserisce automaticamente le \nistruzioni per gestire boxing/unboxing!\n30",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#3": "Programmazione orientata agli oggetti\nLa Classe Generica Coppia\n•La classe Coppia  deve offrire dei servizi per \ngestire una coppia di oggetti del medesimo  \ntipo:\n–Metodi per ottenere/cambiare\n•il primo elemento della coppia\n•il secondo elemento della coppia\n–Un costruttore che riceve come parametri due \nriferimenti ad oggetti del medesimo tipo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#30": "Programmazione orientata agli oggetti\nBoxing, Unboxing e Collezioni\n•Grazie a boxing e unboxing, anche la gestione \ndi collezioni che memorizzano informazioni \nriconducibili a tipi primitivi risulta semplificata\n•Le seguenti operazioni sono lecite (sempre \ngrazie a boxing e unboxing):\n31Coppia<Integer> c;\nc = new Coppia<Integer>();\nint i = 4;\nc.setPrimo(i);\nc.setSecondo(5);\nc.setPrimo(new Integer(i));\nc.setSecondo(new Integer(5));",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#31": "Programmazione orientata agli oggetti\nAttenzione allo Zucchero (Sintattico) \n•Le versioni più recenti del linguaggio hanno \nsemplificato la gestione dei tipi primitivi nascondendo \nalcune conversioni di tipo\n•Tuttavia, è necessario comprendere a fondo\nla differenza tra il concetto di tipo primitivo e la loro \ncontroparte ad oggetti, i wrapper\nquali operazioni non sono necessarie solo grazie ai servizi \nofferti dalle ultime versioni del compilatore Java (sarebbero \nnecessarie con versioni precedenti)\nquali operazioni il compilatore inserisce per conto nostro\n•Perché conviene avere queste competenze?\n–per stimare meglio il numero di oggetti creati dalle nostre \napplicazioni\n–per migliorare la nostra capacità di ricerca delle origine degli \nerrori sia a tempo di compilazione che di esecuzione\n–per riuscire ad usare versioni precedenti del compilatore\n32",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#32": "Programmazione orientata agli oggetti\nGenerics: \nRappresentazione Diagrammatica\n•Rappresentazione diagrammatica di una \nclasse generica\n•oppure \nAT\nA<T>",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#33": "Programmazione orientata agli oggetti\nGenerics: Wildcard (1)\n•A seguire approfondiamo alcuni concetti più \navanzati relativi ai generics per mezzo di esempi \n•N.B. In questo corso ci concentriamo sull'uso di \nclassi generiche e non sulla loro progettazione\n–Ma l’utilizzo di alcune librerie pressuppone la \ncapacità di comprendere alcun tipi e segnature \ndi metodi generici\n✔sviluppare la propria capacità di astrazione è comunque \nsempre importante nella programmazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#34": "Programmazione orientata agli oggetti\nGenerics: Wildcard (2)\n•Aggiungiamo alla classe Coppia<T>  il metodo \ncopyAll()  \n–copia nella coppia corrente gli elementi di un'altra \ncoppia che viene passata come parametro\n•La coppia che passiamo come parametro per \n«fornire» gli elementi da copiare deve essere \nistanziata su un qualunque sottotipo degli oggetti \ndella coppia corrente che finirà per ospitarli\n•Questa particolarità si esprime con il carattere \njolly ? e (di nuovo) con la parola chiave extends",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#35": "Programmazione orientata agli oggetti\nGenerics: Upper-Bounded Wildcard (1)\n•Vediamone la segnatura del metodo di istanza \ncontenuto della classe generica Coppia<T> : \n public class Coppia<T> {\n  …  \n    public void copyAll(Coppia<? extends T>  c)\n     …\n   }\n✔Cosa significa questa segnatura?\n✔Di che tipo deve essere il parametro?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#36": "Programmazione orientata agli oggetti\nGenerics: Upper-Bounded Wildcard (2)\nCoppia<? extends T> c \n•Significa: un riferimento ad un oggetto (qui usato come \nparametro di un metodo) del tipo generico Coppia \nistanziato sullo stesso tipo T, o su un suo sottotipo, su cui è \nistanziata la coppia/oggetto corrente  Coppia<T> \n•Esempio di utilizzo:\nCoppia<Strumento> strumenti;\nCoppia<Chitarra> chitarre;\n…\nstrumenti.copyAll(chitarre); // OK!   T = ???\n•Definizione del metodo copyAll()  (nella classe Coppia<T> ):\npublic void copyAll(Coppia<? extends T> coppia) {\nthis.setPrimo(coppia.getPrimo());\nthis.setSecondo(coppia.getSecondo());\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#37": "Programmazione orientata agli oggetti\nMetodi Statici Generici\n•È possibile definire anche metodi statici generici \n(cioè parametrici rispetto ad un tipo formale)\n•Un metodo generico definisce i tipi formali nella \nsegnatura del metodo, subito prima del tipo \nrestituito. Ad es.:\n  public static <T> int metodoGenerico(\n          Coppia<T> c, \n          T e\n           ) \nAnche in questo caso, come già per le classi generiche, è \npossibile invocare il metodo solo dopo aver fornito (a \ntempo statico, in fase di compilazione) tutti i tipi attuali \nnecessari a completare definitivamente la segnatura",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#38": "Programmazione orientata agli oggetti\nMetodi Generici: Wildcard\n•Definiamo ora la classe Coppias, classe contenitrice \nper ospitare metodi generici e di utilità generale per \nmanipolare oggetti di tipo Coppia \n•In particolare la classe Coppias  offre alcuni metodi \n(statici), di cui stiamo per definire le segnature, \nperseguendo la loro generalità rispetto al tipo degli \noggetti ospitati nelle coppie:\n–??? reverse(??? coppia) \nprende come parametro una coppia e ne inverte gli elementi \n(il primo elemento diventa il secondo e viceversa)\n–??? fill (??? coppia, ??? e ) \nprende due parametri: una coppia ed un elemento; riempie \nentrambi gli elementi della coppia con l'elemento ricevuto",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#39": "Programmazione orientata agli oggetti\nMetodi Generici: Esempio\npublic class Coppias {\npublic static <T> void reverse(Coppia< T> c) {\nT tmp;\ntmp = c.getPrimo();\nc.setPrimo(c.getSecondo());\nc.setSecondo(tmp);\n}\n  …\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#4": "Programmazione orientata agli oggetti\nUna Possibile Soluzione Basata sul \nPolimorfismo\n•Una possibile soluzione (l’unica possibile prima \ndell’introduzione dei Generics  in Java 5) consiste \nnello sfruttare il polimorfismo, ed in particolare il \nprincipio di sostituzione\n•Definiamo una classe che gestisce una coppia di \noggetti istanza di Object\n–per il principio di sostituzione (e per la gerarchia dei \ntipi Java a singola radice in Object) la nostra classe \npuò gestire coppie di oggetti istanza di qualsiasi \nclasse (in quanto sottotipo di Object)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#40": "Programmazione orientata agli oggetti\nGenerics: Lower-Bounded Wildcard (1)\n•Il metodo fill(??? coppia , ??? e)\n–imposta entrambi gli elementi della coppia che viene \npassata come primo parametro, con lo stesso \nriferimento ad oggetto nel secondo parametro\n•E' un metodo parametrico: il tipo del secondo \nparametro deve essere un qualunque sottotipo del \ntipo istanziato dalla coppia\novvero: il tipo su cui la coppia è instanziata deve essere \nun supertipo del tipo del parametro\n•Si esprime così:\nstatic <T> void fill(Coppia<? super T>  coppia,\n         T elemento)  ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#41": "Programmazione orientata agli oggetti\nGenerics: Lower-Bounded Wildcard (2)\nCoppia<? super T>\n✔Significa: un riferimento ad un oggetto Coppia  \nistanziato su T o su un qualsiasi supertipo di T\n•Esempio di utilizzo:\nCoppia<Strumento> strumenti;\nChitarra fender;\n…\nCoppias.fill(strumenti, fender); // OK!   T = ???\n•Definizione:\npublic static <T> void fill( Coppia<? super T> coppia, \n     T elemento) {\ncoppia.setPrimo(elemento);\ncoppia.setSecondo(elemento);\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#42": "Programmazione orientata agli oggetti\nGenerics: Bounded Wildcard\nstatic <T> void copy(Coppia<? super T> dest,\n                     Coppia<? extends T> src)\n✔Attenzione a non confonderlo con copyAll()\n•Esempio di utilizzo:\nCoppia<Strumento> strumenti = new Coppia<Strumento>();\nCoppia<Chitarra> chitarre = new Coppia<Chitarra>();\n…\nCoppias.copy(strumenti, chitarre); // OK T = ???\n•Definizione:\nstatic <T> void copy(Coppia<? super T> dest,\n                     Coppia<? extends T> src) {\ndest.setPrimo(src.getPrimo());\ndest.setSecondo(src.getSecondo());\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#43": "Programmazione orientata agli oggetti\nLa Regola Mnemonica PECS\n•Semplice regola per ricordare il tipo dei parametri \nformali nelle segnature di collezioni\nProducer Extends Consumer Super\n•Utilizzare <? extends T>  per i parametri di \ncollezioni che “producono” elementi \n–ad es. il parametro del metodo copyAll()\n•Utilizzare <? super T>  per i parametri di \ncollezioni che “consumano” elementi\n–ad es. il parametro del metodo fill()\n•Esempio di utilizzo contestuali di entrambi: i due \nparametri del metodo statico copy()",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#44": "Programmazione orientata agli oggetti\nPersonaggi ed Interpreti\n•Ogni riferimento ad API esistenti o a metodi realmente \nesistenti NON è affatto puramente casuale\n–Coppia nel ruolo di Collection , List…\n–Coppias  nel ruolo di Collections (>>)\n•Dentro java.util.Collections  si trovano :\nstatic <T> void fill(List<? super T> list, T obj)\nstatic <T> boolean addAll(Collection<? super T> c, \n                T... elements)\nstatic <T> void copy(List<? super T> dest,\n                     List<? extends T> src)\nstatic void reverse(List<?> list)\nPreferibile quando non è \nstrettamente necessario dare \nun nome al tipo formale",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#45": "Programmazione orientata agli oggetti\nEsercizi\n•Scrivere il codice della classe generica Coppia<T>\n•Scrivere il codice della classe Coppias  \n•Scrivere, utilizzando JUnit, classi di test per le \nclassi Coppia<T>  e Coppias\n•Cercare di capire sino al dettaglio quante più \npossibili segnature generiche dei metodi statici \nfornite nella classe  java.util.Collections",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#46": "Programmazione orientata agli oggetti\nEsercizi\nSupponendo che Studente  sia sottotipo di \nPersona , è vero che Coppia<Studente > risulta \nessere sottotipo di Coppia<Persona >?\n•Ripetere l’esercizio di sopra, nel caso di coppie \nimmutabili , ovvero prive dei metodi setXXX()\n•Il tipo di Coppia<T> è covariante  o controvariante \nrispetto al tipo T?\n–Nel caso di coppie mutabili ?\n–Nel caso di coppie immutabili ?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#47": "Programmazione orientata agli oggetti\nRiferimenti ed Approfondimenti\n•Alcuni articoli spiegano molti altri dettagli:\nhttp://www.oracle.com/technetwork/java/javase/generics-tutorial-159168.pdf\n•Per sapere (quasi) tutto  sui java generics:\nhttp://www.angelikalanger.com/GenericsFAQ/JavaGenericsFAQ.html",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#5": "Programmazione orientata agli oggetti\nLa Coppia  di Object  \npublic class Coppia {\n private Object primo;\n private Object secondo;\n public Coppia(){}\n public Coppia( Object primo, Object secondo) {\n        this.primo = primo;\n        this.secondo = secondo;\n }\n public Object getPrimo() {\n return this.primo;\n }\n       \n public Object getSecondo() {\n return this.secondo;\n }\n public void setPrimo( Object primo) {\n this.primo = primo;\n }\n public void setSecondo( Object secondo) {\n this.secondo = secondo;\n }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#6": "Programmazione orientata agli oggetti\nTipo degli Elementi Ospitati\n•Considereremo di seguito del codice che fa \nriferimento alla semplice classe Persona\nclass Persona {\n  private String nome;\n  public Persona(String nome) {\n    this.nome = nome;\n  }\n  public String getNome() {\n    return this.nome;\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#7": "Programmazione orientata agli oggetti\nControllo sui Tipi Senza Generics:\nScomodi (ed Inutili?) Downcast\n•Il seguente codice compila, e funziona \ncorrettamente:\npublic class CoppiaSenzaGenericsTest {\n    @Test\n    public void testCheCompilaEdEsegue() {\n        Coppia coppia = new Coppia();\n        String pippo = new String(\"Pippo\");\n        String pluto = new String(\"Pluto\");\n        Persona p1 = new Persona(pippo);\n        coppia.setPrimo(p1);\n        Persona p2 = new Persona (pluto);\n        coppia.setSecondo(p2);\n        Persona persona = (Persona)coppia.getPrimo();\n        assertSame(pippo, persona.getNome()));\n    }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#8": "Programmazione orientata agli oggetti\n•Per un utilizzo più semplice delle coppie, \nvorremmo poter scrivere il seguente codice:\npublic class CoppiaSenzaGenericsTest {\n@Test\npublic void testCheNonCompila() {\nCoppia coppia = new Coppia();\nString pippo = new String(\"Pippo\");\nString pluto = new String(\"Pluto\");\nPersona p1 = new Persona(pippo);\ncoppia.setPrimo(p1);\nPersona p2 = new Persona(pluto);\ncoppia.setSecondo(p2);\nassertSame(pippo, coppia.getPrimo() .getNome());\n}\n}NON COMPILA!\nIl tipo statico \nObject\nnon possiede il \nmetodo getNome()Controllo sui Tipi Senza Generics:\nnon Compila Quando Vorremmo...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-14-generics.pdf#9": "Programmazione orientata agli oggetti\nControllo sui Tipi Senza Generics: \nnon Vorremmo che Compilasse!\n•Al contrario, il seguente codice compila \ncorrettamente ma l’esecuzione fallisce:\npublic class CoppiaSenzaGenericsTest {\n@Test\npublic void testCheCompilaMaNonEsegue() {\nCoppia coppia = new Coppia();\nString pippo  = new String(\"Pippo\");\nString pluto = new String(\"Pluto\");\nPersona p1 = new Persona(pippo);\nPersona p2 = new Persona(pluto);\ncoppia.setPrimo(pippo);\ncoppia.setSecondo(pluto);\nassertSame(pippo,( (Persona)coppia.getPrimo() ).getNome());\n}\n}\nClassCastException  a tempo di esecuzione!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#0": "Programmazione \nOrientata agli Oggetti\nCollezioni\nListe",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#1": "Programmazione orientata agli oggetti\nSommario\n•Introduzione alle Collezioni\n–Interface Collection<E>\n–Iterare una collezione: Iterator<E>\n–Rimuovere elementi da una collezione\n•Liste\n–aggiungere elementi\n–iterare sugli elementi della lista\n•Ordinamento di liste\n–Comparable , Comparator\n2",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#10": "Programmazione orientata agli oggetti\nUn Primo Sguardo: Implementazioni\n•List<E>\n–ArrayList<E>\n–LinkedList<E>\n•Set<E>\n–HashSet<E>\n–TreeSet<E>\n•Map<K,V>  \n–HashMap<K,V>\n–TreeMap<K,V>\n... le più diffusamente utilizzate, ma ne esistono \nmolte altre di uso più specifico\n11",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#11": "Programmazione orientata agli oggetti\nUn Primo Sguardo: Collections\n•La classe java.util.Collection s \n(al plurale: attenzione alla ‘s’ finale!)\n–offre un vasto insieme di metodi (statici) generici \nche implementano utili e diffusi algoritmi per la \nmanipolazione di liste quali:\n•ordinamento\n•ricerca max e min\n•shuffle (mescolamento casuale)\n•reverse \n•fill…\nA meno di forti (anzi fortissime) motivazioni in senso contrario, \nimplementare funzionalità equivalenti a questi (o ad uno degli \naltri) metodi statici offerti da Collections  è solo una perdita di \ntempo12",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#12": "Programmazione orientata agli oggetti\nSommario\n•Introduzione alle Collezioni\n–Interface Collection<E>\n–Iterare una collezione: Iterator<E>\n–Rimuovere elementi da una collezione\n•Liste\n–aggiungere elementi\n–iterare sugli elementi della lista\n•Ordinamento di liste\n–Comparable , Comparator\n13",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#13": "Programmazione orientata agli oggetti\nInterface Collection<E>\n•L'interface Collection<E>  dichiara i metodi di \nuna collezione generica \n•Questi metodi permettono di svolgere \noperazioni di tre categorie:\n–Manipolazione di base\n–Bulk\n–Conversione da e verso array\n14",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#14": "Programmazione orientata agli oggetti\nInterface Collection<E>\npublic interface Collection<E> extends Iterable<E> { \n//Basic operations \nint size(); \nboolean isEmpty(); \nboolean contains(Object element); \nboolean add(E element); //optional \nboolean remove(Object element); //optional \nIterator<E> iterator(); \n//Bulk operations \nboolean containsAll(Collection<?> c); \nboolean addAll(Collection<? extends E> c); //optional \nboolean removeAll(Collection<?> c); //optional \nboolean retainAll(Collection<?> c); //optional \nvoid clear(); //optional \n//Array operations \nObject[] toArray(); \n<T> T[] toArray(T[] a);\n}\n15?",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#15": "Programmazione orientata agli oggetti\nCollection<E> : Metodi Base\nConsultare i Javadoc! In sintesi:\n●boolean isEmpty(); \nritorna true se la collezione è vuota\n●int size();  \nritorna il numero di elementi presenti nella collezione\n●boolean contains(Object element); \nritorna true se la collezione contiene un elemento uguale a quello passato come \nparametro (l'uguaglianza è verificata dal metodo equals())\n●boolean add(E element); \naggiunge alla collezione l'elemento passato; ritorna true se la collezione è \ncambiata dopo la chiamata a questo metodo\n●boolean remove(Object element); \nrimuove dalla collezione gli elementi uguali all'oggetto passato come parametro \n(l'uguaglianza è verificata dal metodo equals()). Ritorna true se la collezione è \ncambiata dopo l'invocazione del metodo\n•Iterator<E> iterator(); \nrestituisce un oggetto Iterator, per iterare sugli elementi della collezione\n16",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#16": "Programmazione orientata agli oggetti\nCollection<E> : Metodi Bulk\nConsultare i Javadoc! In sintesi:\n●boolean containsAll(Collection<?> c);  \nritorna true se la collezione contiene tutti gli elementi della collezione passata \ncome parametro\n●boolean addAll(Collection<? extends E> c); \naggiunge alla collezione tutti gli elementi d ella collezione passata come parametro; \nritorna true se la collezione è cambiata dopo l'invocazione di questo metodo\n●boolean removeAll(Collection<?> c); \nrimuove dalla collezione tutti gli elementi uguali (l'uguaglianza è verificata dal \nmetodo equals()) che sono contenuti nella collezione passata come parametro; \nritorna true se la collezione è cambiata dopo l'invocazione di questo metodo\n•boolean retainAll(Collection<?> c);  \nrimuove dalla collezione tutti gli elementi che non sono presenti nella collezione \npassata come  parametro; ritorna true se la collezione è cambiata dopo l'invocazione di \nquesto metodo\n•void clear();\nrimuove tutti gli elementi dalla collezione\n17",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#17": "Programmazione orientata agli oggetti\nSommario\n•Introduzione alle Collezioni\n–Interface Collection<E>\n–Iterare una collezione: Iterator<E>\n–Rimuovere elementi da una collezione\n•Liste\n–aggiungere elementi\n–iterare sugli elementi della lista\n•Ordinamento di liste\n–Comparable , Comparator\n18",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#18": "Programmazione orientata agli oggetti\nIterazione: Interface  Iterator<E>\n•L'iterazione di una collezione avviene attraverso \nun oggetto iteratore dedicato allo scopo\n•Gli iteratori sono creati invocando il factory \nmethod iterator()  sulla collezione che si vuole \nscandire\n•L’oggetto ottenuto implementa l'interface \nIterator<E> , munita dei metodi\n–boolean hasNext()\n–E next()\n–void remove()>>\nSolo i primi due metodi sono considerati strettamente \ncaratterizzanti gli iteratori\n19",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#19": "Programmazione orientata agli oggetti\nSemantica degli Iteratori\n•Nella sostanza, un cursore   che scandisce la \ncollezione sottostante ricordando la sua posizione \nnella scansione\n•Posizioni lecite: subito prima o subito dopo un \nelemento della collezione che si sta iterando \nN.B. mai “sopra” un elemento\n20",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#2": "Programmazione orientata agli oggetti\nIntroduzione\n•Molte applicazioni richiedono di gestire \ncollezioni di oggetti\n•Gli array sono uno strumento di basso livello\n–La dimensione di una collezione in genere non è \nnota a priori e può variare notevolmente\n–Negli esercizi fatti finora abbiamo ipotizzato un \nnumero massimo di elementi proprio per facilitarne \nl’utilizzo\n•un indicatore di riempimento serve a ricordarsi il \nnumero di elementi già memorizzati nell'array \n–Possiamo avere bisogno di molte modalità di \naccesso\n•(non solo indicizzato; ad es. LIFO, FIFO, ecc. ecc.)\n–Ci può essere la necessità di mantenere gli \nelementi ordinati\n3",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#20": "Programmazione orientata agli oggetti\nCollezioni di Tante Tipologie, \nun Unico Modo per Enumerarle\nL’iterazione si effettua sempre nello stesso \nidentico modo indipendentemente dal tipo di \ncollezione sottostante che lo ha generato\n➢Con notevole “economia di pensiero”\n•Per questo motivo è possibile discuterli ancora \nprima delle implementazioni che sanno generarli,  \ncon riferimento all’interface  Collection<E>\n21",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#21": "Programmazione orientata agli oggetti\nIterator<E> : Metodi\n•boolean hasNext (); \nritorna true se e solo se esiste un altro \nelemento da scandire\n•E next(); \nrestituisce il prossimo elemento della \ncollezione nella scansione corrente ed avanza\n22",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#22": "Programmazione orientata agli oggetti\nIterator<E> : Iterazione\n•La chiamata ripetuta di next() permette di scorrere \ngli elementi della collezione uno alla volta\n•Se si raggiunge la fine della collezione viene sollevata \nuna eccezione (che interrompe il programma) \njava.util.NoSuchElementException\n•Per evitare questa situazione, prima di chiamare \nnext() si usa il metodo  hasNext() , che ritorna  true \nse e solo se esiste un altro elemento su cui iterare\n23",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#23": "Programmazione orientata agli oggetti\nEsercizio: \nla Semantica di Iterator\n•Per comprendere la semantica dei metodi di \nuna classe non esiste metodo migliore di una \nbatteria di test-case che la documenti \nprecisamente\n•Per Iterator<E>  scriviamo test per i due \nmetodi principali dell’interfaccia Iterator<E>\n•Utilizziamo come implementazione concreta \ndella collezione   ArrayList<E> (>>)\n24",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#24": "Programmazione orientata agli oggetti\nUnit-Testing per Documentare la \nSemantica di Iterator<E>\nimport …\npublic class IteratorTest {\n  private List<String> vuota;\n  private List<String> singoletto;\n  private String solitario;\n  @Before\n  public void setUp() {\n    this.vuota = new ArrayList <>();\n    this.singoletto = new ArrayList<String>();\n    this.solitario = new String(\"solitario\");\n    this.singoletto.add(this.solitario);\n  }\n  @Test … … … \n}\n25// da Java 7  \nArrayList <>();\nDiamond Operator",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#25": "Programmazione orientata agli oggetti\nTest di Iterator.hasNext()\n@Test \npublic void testHasNext_noListaVuota() {\n  Iterator<String> it = this.vuota.iterator();\n  assertNotNull(it);\n  assertFalse( it.hasNext() );\n}\n@Test \npublic void testHasNext_primaSiPoiNoSuSingoletto() {\n  Iterator<String> it = this.singoletto.iterator();\n  assertNotNull(it);\n  assertTrue( it.hasNext() );\n  it.next();\n  assertFalse( it.hasNext() );\n}\n26",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#26": "Programmazione orientata agli oggetti\nTest di Iterator.next()\n@Test \npublic void testNext_singoletto() {\n  Iterator<String> it =\n this.singoletto.iterator();\n  assertNotNull(it);\n  assertTrue(it.hasNext());\nassertSame(this.solitario,  it.next() );\n}\n27",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#27": "Programmazione orientata agli oggetti\nTest di Iterator.next()\n@Test \npublic void testNext_suListaDiDueElementi() {\n  List<String> doppietta = new ArrayList<>();\ndoppietta.add(new String(\"primo\"));\ndoppietta.add(new String(\"secondo\"));\n Iterator<String> it = doppietta.iterator();\n assertNotNull(it);\n assertTrue(it.hasNext());\nassertEquals(\"primo\", it.next() );\n assertTrue(it.hasNext());\nassertEquals(\"secondo\", it.next() );\nassertFalse(it.hasNext());\n}\n28",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#28": "Programmazione orientata agli oggetti\nTest di Iterator.next()\n@Test(expected  = NoSuchElementException. class)\npublic void testNext_oltreLaFineSollevaEccezione() {\n Iterator<String> it = this.vuota.iterator();\nit.next();\n}\n29•Posizioni lecite: subito prima o subito dopo un \nelemento della collezione che si sta iterando\nraggiunto l’ultimo elemento non si può andare \noltre: il metodo Iterator.next()  solleva una \neccezione (>>)\nNoSuchElementException!",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#29": "Programmazione orientata agli oggetti\nIterazione di Array – Ciclo for\npublic class Borsa {\n    private Attrezzo[] attrezzi;\n    private int numeroAttrezzi ;\n    …\n    public int getPeso(){\n        int pesoTotale = 0;\n        for(int i=0; i<this.numeroAttrezzi; i++) {\n           Attrezzo a = this.attrezzi[i];\n           pesoTotale += a.getPeso();\n        }\n      return pesoTotale;  \n    }\n    …\n}\n30",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#3": "Programmazione orientata agli oggetti\nLe Collezioni del Package  java.util\n•Nella libreria di base di Java abbiamo un package \nche ci offre un vasto insieme di interfacce e di \nclassi per la gestione di collezioni di oggetti\n•Introdotte già in Java 2:\n•ma sostanzialmente rivisitate in seguito alla \nintroduzione dei Generics in Java 5\nN.B. I Generics di Java 5 sono stati introdotti \nprincipalmente per il loro utilizzo nelle collezioni \npreesistenti\n• significativamente estese nelle versioni successive\nper coprire scenari di utilizzo via via conclamatesi \ncome importanti\n–ad es. java.util.concurrent : collezioni concorrenti \nCertamente tra le librerie più utilizzate in assoluto\n4",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#30": "Programmazione orientata agli oggetti\nIterazione Mediante Iteratori\nimport java.util.List;\nimport java.util.ArrayList;\npublic class Borsa {\n    private List<Attrezzo>  attrezzi;\n    …\n    public int getPeso(){\n        int pesoTotale = 0;\n        Iterator<Attrezzo> iteratore =\n            this.attrezzi.iterator();\n        while (iteratore.hasNext ()) {\n            Attrezzo a = iteratore.next ();\n            pesoTotale += a.getPeso();\n        }\n        return pesoTotale;\n    }\n    …\n}\n31",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#31": "Programmazione orientata agli oggetti\nEsercizio List<E> : Iterazione\n•L’utilizzo degli iteratori è decisamente ripetitivo\n•Sempre le stesse identiche operazioni:\n–Iterator<Attrezzo> iteratore = \nthis.attrezzi.iterator();\nAbbiamo chiesto alla lista di creare un oggetto per \ngestire l'iterazione su una collezione di oggetti \nAttrezzo …\n–while (iteratore.hasNext())\n…fintanto che ci sono ancora elementi da scandire …\n–Attrezzo a = iteratore.next();\n…attraverso il metodo next() otteniamo dall'iteratore il \nprossimo elemento della scansione\n•E’ ben motivata una forma sintattica abbreviata...\n32",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#32": "Programmazione orientata agli oggetti\nIterazione : for-each  (1)\n•Per iterare su tutti gli elementi di una collezione \nè possibile usare la forma \"for-each\" \ndell'istruzione for\nfor( Tipo elemento : iterable  )\n<<blocco_di_operazioni_su_elemento>>\n•Dove iterable   è un qualsiasi sottotipo di \njava.lang.Iterable<E> , una interface che offre \nil factory method:\n–Iterator<E> iterator()\nLa sintassi for-each è conveniente solo se non è \nnecessario accedere all’indice di iterazione\n33",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#33": "Programmazione orientata agli oggetti\nIterazione : for-each  (2)\nSi tratta solo di “zucchero sintattico”: il compilatore traduce\n   for( E elemento  : iterable  )\n<<blocco_di_codice_su elemento>>\n… in …\nfor (Iterator<E> iter = iterable.iterator();\n   iter.hasNext(); ) {\nE elemento = iter.next();\n <<blocco_di_codice_su elemento>>\n}\n•Tutte le collezioni del JCF sono già Iterable\n–Si può usare il for-each su qualunque classe, anche di nuova \ndefinizione,  purché implementi Iterable<E>\n–Ed anche sugli array che pure non lo sono affatto (sottotipi di \nIterable<E> )\n✔grazie ad una gestione peculiare e specifica da parte del \ncompilatore\n34",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#34": "Programmazione orientata agli oggetti\nEsercizio:\nScansione di Liste con for-each\nimport java.util.List;\nimport java.util.ArrayList;\npublic class Borsa {\n  private List<Attrezzo> attrezzi ;\n…\n  public int getPeso(){\n   int pesoTotale = 0;\n     for(Attrezzo a : this.attrezzi)\n         pesoTotale += a.getPeso();\n   return pesoTotale;\n  }\n…\n}\n35",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#35": "Programmazione orientata agli oggetti\nSommario\n•Introduzione alle Collezioni\n–Interface Collection<E >\n–Iterare una collezione: Iterator<E >\n–Rimuovere elementi da una collezione\n•Liste\n–aggiungere elementi\n–iterare sugli elementi della lista\n•Ordinamento di liste\n–Comparable , Comparator\n36",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#36": "Programmazione orientata agli oggetti\nLegame Iterator / Collezione\n•Tra un iteratore e la collezione che lo ha creato \npermane un legame anche successivamente alla \nsua creazione\n–Interessante anche per i meccanismi di costruzione dei \ntipi utilizzati nell’occasione (>>)\n•L’esistenza di un terzo metodo nell’interface \nIterator<E> per la rimozione di elementi rende  \npiù evidente la natura di questo legame:\nvoid remove(); \n–rimuove dalla collezione l'ultimo elemento che è stato \nrestituito da una precedente chiamata di next()\nattraverso l’iteratore vengono operate modifiche sulla \ncollezione sottostante\n37",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#37": "Programmazione orientata agli oggetti\nTest di Iterator.remove()\n@Test \npublic void testRemove() {\n    Iterator<String> it = \nthis.singoletto.iterator();\n  String solitario = it.next();\n  assertFalse(this.singoletto.isEmpty());\n  it.remove();\n    assertTrue(this.singoletto.isEmpty());\n}\n38",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#38": "Programmazione orientata agli oggetti\nIl Metodo remove()  di Iterator\n•Il metodo remove()  rimuove l'elemento restituito \ndall'ultima chiamata di next()  \n•Non è ammesso chiamare remove()  a meno che \nprima non si sia provveduto ad invocare next()\n•Ad es. per eliminare due elementi consecutivi:\nit.next();\nit.remove();\nit.remove();   // ERRORE\nprima bisogna richiamare next():\nit.next();\nit.remove();\nit.next();\nit.remove();  // OK\n39",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#39": "Programmazione orientata agli oggetti\nRimuovere Elementi da una Collezione\n•Pertanto esistono due diversi modi per \nrimuovere un elemento da una collezione, \nciascuno dettato da specifiche esigenze:\n–per la rimozione di un elemento uguale (secondo \nquanto stabilito dal metodo boolean \nequals(Object o) ) ad un elemento dato (passato \ncome parametro) si usa il metodo remove(Object \no) di Collection\n–per la rimozione di un elemento durante la \nscansione si usa il metodo remove()  di Iterator\n40",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#4": "Programmazione orientata agli oggetti\nIl Java Collection Framework (JCF)\n5\n<<interface>>\nCollection\n<<interface>>\nSet\n<<interface>>\nSortedSet\n<<interface>>\nList\n<<interface>>\nMap\n<<interface>>\nSortedMap\nE E\nK,VK,VE\nE\nCollections\n<<interface>>\nIterableE\n+ iterator()\n<<interface>>\nIteratorE\n",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#40": "Programmazione orientata agli oggetti\nIl Metodo remove(Object o)\ndi Collection<E>\nboolean remove(Object o) \nRemoves a single instance of the specified element from \nthis collection, if it is present (optional operation). More \nformally, removes an element e such that \n(o==null ? e==null :  o.equals(e) ), if this collection contains \none or more such elements. Returns true if this collection \ncontained the specified element (or equivalently, if this \ncollection changed as a result of the call). \n–Parameters:\n•o - element to be removed from this collection, if present \n–Returns:\n•true if an element was removed as a result of this call\n(dalla documentazione)\n41",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#41": "Programmazione orientata agli oggetti\nIl Metodo remove() di Iterator<E>\n•void remove() \n–Removes from the underlying collection the last \nelement returned by the iterator (optional \noperation). This method can be called only once \nper call to next. The behavior of an iterator is \nunspecified if the underlying collection is \nmodified while the iteration is in progress in \nany way other than by calling this method .\n(dalla documentazione)\n42",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#42": "Programmazione orientata agli oggetti\nCome Rimuovere Elementi?\n•Attenzione: è un errore cercare di rimuovere elementi \nda una collezione con il metodo\n–boolean remove(Object o)  \ndi Collection proprio mentre si sta ancora visitando \nla stessa collezione con un iteratore\n–la collezione verrebbe modificata \"sotto i piedi\" \ndell'iteratore che la sta ancora scandendo\n•Se si stanno cercando elementi da rimuovere \nattraverso un iteratore, deve poi essere usato il \nmetodo remove() dell'iteratore stesso per renderlo \n«consapevole» dei cambiamenti alla «sua» collezione\n43",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#43": "Programmazione orientata agli oggetti\njava.util.ConcurrentModificationException\nimport java.util.*;\npublic class ConcurrentModificationMain {\n  public static void main(String args[]) {\n    List<Object> list = new ArrayList<>();\n    Iterator it = list.iterator();\n    list.add(new Object());\n    it.next(); // Qui solleva  ConcurrentModificationException\n  }\n} \nL’eccezione è sollevata solo alla prima occasione utile \n(semantica fail-fast , vedere Javadoc)\nException in thread \"main“\njava.util.ConcurrentModificationException\n  at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:448)\n  at java.util.AbstractList$Itr. next(AbstractList.java:419)\n  at ConcurrentModificationMain.main(ConcurrentModificationMain.java:9)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#44": "Programmazione orientata agli oggetti\nSommario\n•Introduzione alle Collezioni\n–Interface Collection<E >\n–Iterare una collezione: Iterator<E >\n–Rimuovere elementi da una collezione\n•Liste\n–aggiungere elementi\n–iterare sugli elementi della lista\n•Ordinamento di liste\n–Comparable , Comparator\n45",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#45": "Programmazione orientata agli oggetti\nListe: Interface List<E>  (1)\n•Una lista è una collezione che mantiene gli \nelementi ordinati secondo l'ordine di \ninserimento (il primo elemento aggiunto alla \nlista, è in prima posizione, il secondo in \nseconda posizione, …, l'ultimo elemento \naggiunto è in ultima posizione)\n•Cfr. ASD (<<)\n46",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#46": "Programmazione orientata agli oggetti\nListe: Interface List<E>  (2)\n•L'interface List<E>  estende l'interface \nCollection<E>\n•Oltre ai metodi della interface Collection<E> , \nList<E>  offre specifici metodi che consentono \naccesso e inserimento indicizzati degli \nelementi. Ad esempio:\n–E get(int index):  Returns the element at the \nspecified position in this list\n–int indexOf(Object o) : Returns the index of \nthe first occurrence of the specified element in \nthis list, or -1 if this list does not contain the \nelement\n47",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#47": "Programmazione orientata agli oggetti\nEsercizio (per Casa)\n•Analizzare, compilare ed eseguire la classe di \ntest ListTest  riportata subito di seguito.\n–Aggiungere opportuni metodi di test per \nverificare e comprendere la semantica dei \nmetodi:\n–indexOf(Object o); in particolare cerca la stessa \nistanza in memoria od un oggetto equals()  ???\n–contains(Object  o); idem...\n–retainAll(Collection<?> c)\n–containsAll(Collection<?> c)\n48",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#48": "Programmazione orientata agli oggetti\nEsercizio (per Casa, continua )\npublic class ListTest {\nprivate Collection<Integer> c;\nprivate Collection<Integer> t; \n@Before\npublic void setUp () {\n      c = new LinkedList<Integer>();\n      t = new ArrayList<Integer>();\n      c.add(1);\n      c.add(2);\n      c.add(3);\n      t.add(1);\n      t.add(2);\n}\n  \n@Test\npublic void testRemoveAll() {\n      assertTrue(c.removeAll(t));\n      Iterator<Integer> it = c.iterator();\n      assertTrue(it.hasNext());\n      assertEquals(3,it.next().intValue());\n      assertFalse(it.hasNext());\n}\n}\n49",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#49": "Programmazione orientata agli oggetti\nImplementazioni di List<E>\n•Il package java.util  offre due diverse \nimplementazioni di List<E>\n–ArrayList<E>\n–LinkedList<E>\n50•Forniamo qualche (grossolana) indicazione su \ncome scegliere l'implementazione più \nopportuna\nNOTA: Questi aspetti sono stati già approfonditi \nnel corso \"Algoritmi e Strutture Dati\"",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#5": "Programmazione orientata agli oggetti\nUn Primo Sguardo: Collection<E> (1)\n•L'interface Collection<E>  dichiara i metodi di \nuna generica collezione:\n•Generalizza sia List<E>  sia Set<E>\nList<E>:\n–Collezioni sequenziali i cui elementi possiedono \nuna posizione\n–Senza gestione dei duplicati\nSet<E>:\n–Collezioni che non ammettono duplicati\n–Gli elementi non possiedono posizione \n6",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#50": "Programmazione orientata agli oggetti\nArrayList<E> : Implementazione\n•Gli elementi sono memorizzati in un contenitore \nrappresentato con array ed indicatore di riempimento\n•Al momento della creazione, la dimensione dell'array \n(ovvero la capacità della collezione) è inizializzata ad \nun valore prestabilito \n•Quando il numero di elementi è prossimo alla capacità \ndell'array, viene istanziato un nuovo array di \ndimensione maggiore (ad esempio doppia) nel quale \nvengono travasati tutti gli elementi dell'array \noriginario\nNOTA: Questi aspetti sono stati approfonditi nel corso \n\"Algoritmi e Strutture Dati\"\n51",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#51": "Programmazione orientata agli oggetti\nLinkedList<E> : Implementazione\n•Gli elementi sono memorizzati in una lista \nconcatenata\n•Ogni elemento della lista contiene \n–un riferimento all'elemento successivo \n–un riferimento all'oggetto memorizzato\n•Non è necessario stabilire una capacità iniziale\nNOTA: Questi aspetti sono stati approfonditi nel \ncorso \"Algoritmi e Strutture Dati\"\n52",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#52": "Programmazione orientata agli oggetti\nLinkedList<E>  o ArrayList<E> ?\n•Molto schematicamente:\n–ArrayList<E>  conviene se:\n•la dimensione è abbastanza stabile\n•è necessario un accesso indicizzato (la classe ArrayList  \noffre un metodo opportuno)\n–LinkedList<E>  conviene se:\n•la dimensione può variare anche significativamente\n•gli accessi sono perlopiù sequenziali\n•Nella pratica la complessità delle JVM moderne rende \nle differenze spesso impercettibili e/o comunque molto \ndifficilmente prevedibili\nBasare le ottimizzazioni sempre su misurazioni sperimentali \nche ne palesino ed accertino la reale necessità\n53",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#53": "Programmazione orientata agli oggetti\nImplementazioni di List<E>:  \nCostruttori\n•I costruttori sono sovraccarichi. In particolare facciamo \nosservare che esiste un costruttore che permette la \ncreazione di una lista a partire da una collezione\n•Costruttori di ArrayList<E>\n–ArrayList<E>()  Constructs an empty list with an initial \ncapacity of ten\n–ArrayList( Collection<? extends E>  c) Constructs a list \ncontaining the elements of the specified collection, in the order \nthey are returned by the collection's iterator \n–ArrayList<E>(int initialCapacity) Constructs an empty \nlist with the specified initial capacity\n•Costruttori di LinkedList<E>\n–LinkedList<E> Constructs an empty list\n–LinkedList<E>( Collection<? extends E>  c) Constructs a \nlist containing the elements of the specified collection, in the \norder they are returned by the collection's iterator\n54",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#54": "Programmazione orientata agli oggetti\nEsercizio: List<E>\n•La classe ArrayList<E>  implementa l'interfaccia \nList<E>  (e quindi è sottotipo anche di \nCollection<E> )\n•Proviamo a rivedere il codice della classe Borsa \nnello studio di caso:\n–anziché usare un array per memorizzare l'insieme \ndi attrezzi, usiamo un ArrayList<E>\n•Vediamo come gestiamo\n–aggiunta di un elemento\n–scansione della lista\n55",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#55": "Programmazione orientata agli oggetti\nEsercizio List<E> : Aggiunta di  Elementi\npublic class Borsa {\n    private Attrezzo[] attrezzi;\n    private int numeroAttrezzi;\n    public Borsa() {\n        this.numeroAttrezzi = 0;\n        this.attrezzi = new Attrezzo[10];\n    }\n    public boolean addAttrezzo(Attrezzo attrezzo){\n        if (numeroAttrezzi == 10)\n            return false;\n        this.attrezzi[this.numeroAttrezzi] = attrezzo;\n        this.numeroAttrezzi++;\n        return true;\n    } …\n}\nimport java.util.List;\nimport java.util.ArrayList;\npublic class Borsa {\n    private List<Attrezzo> attrezzi;\n    public Borsa() {\n        this.attrezzi = new ArrayList<Attrezzo>();\n    }\n    public boolean addAttrezzo(Attrezzo attrezzo) {\n        return this.attrezzi.add(attrezzo);\n    }…\n}\n56array\nArrayList",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#56": "Programmazione orientata agli oggetti\nEsercizio List<E> : Osservazioni\n•Dobbiamo importare:\njava.util.List\njava.util.ArrayList\n•Principali benefici rispetto all’uso degli array\n–non ci dobbiamo preoccupare di stabilire a priori le \ndimensioni massime della collezione\n–non ci dobbiamo preoccupare di gestire l'indicatore \ndi riempimento, che memorizza il numero di \nelementi effettivamente memorizzati nell'array\n57",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#57": "Programmazione orientata agli oggetti\nEsercizio List<E> :\nAggiunta di Elementi\n•L'aggiunta di elementi in un ArrayList<E>  viene \nrealizzata tramite il metodo  add(E el)\n•Questo metodo aggiunge un riferimento ad oggetto \n(istanza di tipo E) nell'ultima posizione della collezione\n•Gli elementi della lista rimangono ordinati secondo \nl'ordine di inserimento\n–l'oggetto inserito per primo è nella prima posizione\n–l'oggetto inserito per secondo è nella seconda posizione\n–… \n–l'oggetto inserito per ultimo è in ultima posizione\n58",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#58": "Programmazione orientata agli oggetti\nEsercizio List<E> : Osservazioni\n•La lista aumenta la sua capacità se necessario\n•Mantiene un conteggio del numero di elementi \n•Il metodo int size() restituisce questo valore\nI dettagli di come tutto ciò viene realizzato ci \nviene nascosto\n–È importante? \n–Non conoscere questi dettagli ci impedisce di usare la \ncollezione?\nQuanto  risulta difficile cambiare la scelta \ndell’implementazione?\n–Ad es. passare da ArrayList  a LinkedList … \n59",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#59": "Programmazione orientata agli oggetti\nMetodi Specifici di LinkedList<E>\nLinkedList<E>  offre anche alcuni metodi “fuori” dalla \ninterface List<E> e quindi non offerti anche da ArrayList<E>\nSimilarmente ArrayList<E>  offre costruttori (basati sulla \ncapacità ) che LinkedList<E>  non offre\nSono metodi tesi ad evidenziare l’accesso efficiente da parte \ndelle LinkedList<E>  in testa ed in coda\nAd es. \naddFirst()/Last()\ngetFirst()/Last()\nremoveFirst()/Last()\n… \nAttenzione: utilizzarli rende meno intercambiabili le due \nimplementazioni\nPer questo si consiglia di dichiarare i tipi, a meno di forti \nmotivazioni in senso contrario, utilizzando sempre le interface",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#6": "Programmazione orientata agli oggetti\nUn Primo Sguardo: Collection<E> (2)\n•L'interface Collection<E>  dichiara i metodi di \nuna generica collezione\n•Questi metodi permettono di svolgere \noperazioni quali:\n–aggiungere un elemento alla collezione\n–verificare la dimensione della collezione\n–verificare se la collezione è vuota\n–aggiungere tutti gli elementi di un'altra \ncollezione\n–ottenere un iteratore  con cui scandire la \ncollezione\n7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#60": "Programmazione orientata agli oggetti\nEsercizio List<E> : \nRimozione di un Elemento\n61import java.util.List;\nimport java.util.ArrayList;\npublic class Borsa {\n    private List<Attrezzi> attrezzi;\n    …\n    public Attrezzo removeAttrezzo(String nomeAttrezzo) {\n        Attrezzo a = null;\n        Iterator<Attrezzo> iteratore =\n            this.attrezzi.iterator();\n        while (iteratore.hasNext()) {\n            a = iteratore.next();\n            if (a.getNome().equals(nomeAttrezzo)) {\n         iteratore.remove();\n         return a;\n         }\n        }\n       return null;\n     }\n    …\n}Con ArrayList",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#61": "Programmazione orientata agli oggetti\nEsercizio: ListIterator<E>\nIn realtà l’interface List affianca al factory method \niterator()  di Collection  un metodo specifico per le \nliste (e non esistente per tutte le generiche collezioni)\nIl metodo List<E>.listIterator()  restituisce un \nListIterator<E>\nListIterator  estende Iterator\nStudiare (consultando i javadoc) le differenze tra le due \ninterface Iterator  e ListIterator\nScrivere dei test di unità sulla semantica dei metodi di \nListIterator  verificando che continuino ad aver \nsuccesso anche i test-case già scritti per il suo \nsupertipo Iterator",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#62": "Programmazione orientata agli oggetti\nListe: Diagramma degli Oggetti\n•Nel seguito introduciamo una notazione \ngrafica per la rappresentazione di oggetti \nArrayList  e LinkedList\n–la rappresentazione proposta è una astrazione \n(molto semplificata, ma utile a fini didattici) \ndella rappresentazione interna delle due \nimplementazioni\n63",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#63": "Programmazione orientata agli oggetti\nArrayList : Diagramma degli Oggetti \n64\n:Attrezzo[]List<Attrezzo> lista;\nlista = new ArrayList<Attrezzo>();\nlista.add(new Attrezzo(\"vite\",1);\nlista.add(new Attrezzo(\"dado\",2);\n// DIAGRAMMA DA DISEGNARE QUANDO L’ESECUZIONE RAGGIUNGE QUESTO PUNTO\n[0]\n[1]\n[2]\n[3]\n[4]\n[…]\n:Attrezzo\nnome\"vite\"\npeso  1  \n:Attrezzo\nnome\"dado\"\npeso  2  lista\n:ArrayList",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#64": "Programmazione orientata agli oggetti\nLinkedList : Diagramma degli Oggetti \n65\n:NodeList<Attrezzo> lista;\nlista = new LinkedList<Attrezzo>();\nlista.add(new Attrezzo(\"vite\",1);\nlista.add(new Attrezzo(\"dado\",2);\n// DIAGRAMMA IN QUESTO PUNTO\nnext\nvalue\n:Attrezzo\nnome\"vite\"\npeso  1  \nnome\"dado\"\npeso  2  lista\n:LinkedList\n :Node\nnext\nvaluefirst\n:Attrezzo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#65": "Programmazione orientata agli oggetti\nSommario\n•Introduzione alle Collezioni\n–Interface Collection<E>\n–Iterare una collezione: Iterator<E>\n–Rimuovere elementi da una collezione\n•Liste Generiche\n–aggiungere elementi\n–iterare sugli elementi della lista\n•Ordinamento di liste\n–Comparable , Comparator\n66",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#66": "Programmazione orientata agli oggetti\nOrdinamenti e Ricerche\n•Il JCF (in particolare la classe Collections ) \ninclude metodi che implementano algoritmi \nefficienti per \n–ordinare una lista\n–ricercare la posizione di un elemento in una lista \nordinata\n–ricercare il min/max in una lista\n67",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#67": "Programmazione orientata agli oggetti\nRelazione d’Ordine\n•Queste operazioni sono possibili solo se esiste \nuna relazione d'ordine per il tipo degli \nelementi ospitati nella collezione\n–in altri termini, gli elementi della lista devono \nsapersi confrontare\n–oppure ci deve essere un oggetto esterno che sa \ncome confrontare due oggetti della lista\nAnche una relazione d’ordine su un supertipo \ndegli elementi ospitati va bene\n68",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#68": "Programmazione orientata agli oggetti\nDefinizione di un \nCriterio di Ordinamento\n•La responsabilità di modellare il criterio di \nordinamento può essere affidata, in alternativa:\n–alla stessa classe degli oggetti contenuti, che deve \nimplementare una apposita interfaccia \njava.lang.Comparable<T> \n(il cosidetto ordinamento «naturale » o  «interno»)\n–ad una classe esterna alle classe degli oggetti \ncontenuti;  tale classe esiste solo con l’obbiettivo di \nconfrontarli, si chiama comparatore   e rispetta \nl’interfaccia java.util.Comparator<T> \n(il cosidetto ordinamento «esterno » )\n69",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#69": "Programmazione orientata agli oggetti\nL'Interface java.lang. Comparable<T>\n•L'interface java.lang.Comparable<T>   consiste di \nun solo metodo:\npublic int compareTo(T that)\n•Restituisce un valore che è: \n–minore, uguale, maggiore di zero a seconda che \nl'oggetto corrente this sia rispettivamente: \n–minore, uguale, maggiore dell'oggetto il cui riferimento \nè ricevuto come parametro formale that\n70",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#7": "Programmazione orientata agli oggetti\nUn Primo Sguardo:  Set<E>\n•L'interface Set<E>  estende Collection<E> : è una \ncollezione che non può contenere duplicati\n•Offre tutti e soli i metodi della interface \nCollection , con la restrizione che le classi che la \nimplementano si impegnano a non ammettere la \npresenza di elementi duplicati\n–sarà necessario utilizzare un meccanismo di \nmodellazione del criterio di equivalenza tra \nelementi dell’insieme\n–bisognerà utilizzarlo per definire un criterio di \nequivalenza tra gli elementi ospitati nell'insieme\n8",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#70": "Programmazione orientata agli oggetti\nUtilizzi di java.lang.Comparable<T>\n•Molte importanti classi della libreria standard già \nimplementano l’interfaccia \njava.lang.Comparable<T> , adottando una \nsemantica più o meno scontata. \n•Ad es.\n–java.lang.String\n–java.util.Calendar\n–java.util.Date\n–java.io.File\n–java.net.URI\n–tutte le classi wrapper\n… e molte altre ancora\n71",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#71": "Programmazione orientata agli oggetti\nL'interface Comparable<T> : Esempio\npublic class Persona implements Comparable< Persona> {\nprivate String nome;\nprivate int eta;\npublic Persona(String nome, int eta) {\nthis.nome = nome;\nthis.eta = eta;\n}\npublic String getNome() {\nreturn this.nome;\n}\npublic int getEta() {\nreturn this.eta;\n}\n  @Override\npublic int compareTo( Persona that) {\nreturn this.nome.compareTo(that.getNome());\n}\n}\n72",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#72": "Programmazione orientata agli oggetti\nTest di Persona.compareTo()\npublic class PersonaTest {\n  @Test\npublic void testCompareTo() {\n  Persona p1 = new Persona(\"Paolo\", 10);\n  Persona p2 = new Persona(\"Valter\", 5);\n    assertTrue(p1.compareTo(p2) < 0); // <0\n  Persona p3 = new Persona(\"Paolo\", 10);\n  assertEquals(0, p1.compareTo(p3)); // 0 \n  Persona p4 = new Persona(\"Anna\", 8);\n  assertTrue(p1.compareTo(p4) > 0); // >0 \n}\n}\n73",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#73": "Programmazione orientata agli oggetti\nOrdinamento «Naturale»\n•Un ordinamento naturale di oggetti è definito dalla \nrelazione d'ordine implementata dal metodo \ncompareTo()  nell’interface Comparable<T>\n•E’ possibile quindi operare su una  List<T> \ncontenente oggetti che implementano Comparable<T> \ncon metodi che utilizzino tale criterio di ordinamento:\n–si possono effettuare ricerche efficienti\n–si può calcolare il massimo e il minimo\n–si può effettuare l'ordinamento\nmediante i metodi offerti da Collections\n74",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#74": "Programmazione orientata agli oggetti\nOrdinamento di Liste: \nCollections.sort()\n•Metodo statico Collections.sort() , in due \nversioni sovraccariche corrispondenti ai due \ndiversi modi («naturale» vs «esterno») di fornire \nun criterio di ordinamento\n•Segnatura per l’ordinamento naturale:\npublic static <T extends Comparable<? super T>>  \nvoid sort(List<T> list)\n75",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#75": "Programmazione orientata agli oggetti\nTest dell’Ordinamento «Naturale»\npublic class OrdinamentoNaturaleTest {\n  @Test\npublic void testSort() {\n  List<Persona> l = new LinkedList<>();\n  l.add(new Persona(\"Valter\", 5));\n  l.add(new Persona(\"Paolo\", 10));\n  l.add(new Persona(\"Giacomo\", 7));\n  l.add(new Persona(\"Alessandro\", 8));\n  Collections.sort(l);\n    assertEquals(\"Alessandro\", l.get(0).getNome());\n    assertEquals(\"Giacomo\",  l.get(1).getNome());\n  assertEquals(\"Paolo\", l.get(2).getNome());\n    assertEquals(\"Valter\", l.get(3).getNome());\n}\n76Se Persona non implementasse Comparable<Persona> ,  \nsi solleverebbe un errore già a tempo di compilazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#76": "Programmazione orientata agli oggetti\nOttenere Min & Max di una Lista\n•Data una lista List<T>  i cui elementi implementino \nl'interface Comparable<T>,  può essere calcolato \nl'elemento massimo/minimo (rispetto all'ordinamento \nnaturale) mediante i metodi statici di Collections : \n•Collections.mix()\n•Collections.max()\npublic static < T extends Object &  Comparable<? super T>>\n            T min/max(Collection<? extends T> coll)\n…che semplifichiamo in:\npublic static <T extends Comparable<? super T>>\n        T min/max(Collection<? extends T> coll)\n77",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#77": "Programmazione orientata agli oggetti\nTest del Calcolo Min & Max\ndi una Lista \npublic class MinMaxTest {\n  @Test\npublic void testMinMax() {\n    List<Persona> l = new LinkedList<>();\n \n    l.add(new Persona(\"Valter\"), 5);\n    l.add(new Persona(\"Paolo\"), 10);\n    l.add(new Persona(\"Giacomo\"), 7);\n    l.add(new Persona(\"Alessandro\"), 8);\n  \n assertEquals(\"Alessandro\",Collections. min(l).getNome());\n assertEquals(\"Valter\", Collections. max(l).getNome());\n  }…\n}\n78✔ Se Persona non implementasse Comparable<Persona> ,  \nsi solleverebbe un errore a tempo di compilazione",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#78": "Programmazione orientata agli oggetti\nOrdinamento «Naturale»: Limiti (1)\nL’ordinamento naturale può essere definito al più \nuna sola volta in ogni classe\nIn effetti può essere utilizzata una sola volta per \nogni intera gerarchia di tipi !\nPer capire perché, consideriamo due classi, \nPersona  e Studente , ove Studente  estende \nPersona , e prevediamo per entrambe un \nordinamento «naturale»\n–le persone sono ordinate per nome\n–gli studenti sono ordinati per matricola",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#79": "Programmazione orientata agli oggetti\nOrdinamento «Naturale»: Limiti (2)\npublic class Persona implements Comparable<Persona>  { \n  private String nome;\n  public Persona(String nome) {  this.nome = nome; }\n  public String getNome() {   return this.nome; }\n  @Override\n  public int compareTo(Persona that) {\n    return this.getNome().compareTo(that.getNome());\n  }\n}\npublic class Studente extends Persona implements Comparable<Studente>  {\n  private String matricola;\n  public Studente(String nome, String matricola) {\n    super(nome);\n    this.matricola = matricola;\n  }\n  public String getMatricola() {  return this.matricola; }\n  @Override\n  public int compareTo(Studente that) {\n    return this.getMatricola().compareTo( that.getMatricola());\n  }\n}\nPersona\n+getNome()\nStudente\n+getMatricola()\nNON COMPILA!\nThe interface Comparable  \ncannot be implemented \nmore than once with \ndifferent arguments: \nComparable<Persona>  and \nComparable<Studente>",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#8": "Programmazione orientata agli oggetti\nUn Primo Sguardo:  List<E>\n•L'interface List<E>  estende Collection<E>  e \ncorrisponde ad una sequenza, ovvero una \ncollezione ordinata di elementi \n•Le liste, rispetto agli insiemi, possono contenere \nelementi duplicati\n•Oltre alle operazioni offerte dal supertipo \nCollection<E> , l’interface List<E>  include altre \noperazioni specifiche, quali: \n–Accesso posizionale:  permette di accedere agli elementi \ndi tipo E in base alla loro posizione nella lista (in maniera \nsimile a quanto avviene per gli array)\n–Ricerca: permette di ricercare la posizione di un \nelemento nella lista\n9",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#80": "Programmazione orientata agli oggetti\nLimitazioni dei Java Generics (1)\nAlla stessa classe/interface (in generale tipo) non è \npermesso implementare la stessa interface generica \npiù di una volta con tipi attuali distinti!\nChiaramente controintuitivo; contraddice almeno \nquesti aspetti che lasciavano sperare altrimenti:\n–una classe Java può implementare diverse interface\n–si possono definire metodi sovraccarichi di stesso nome ma \ndiversa segnatura per l’uso di tipi polimorfi (anche se pescati \ndalla medesima gerarchia)\nIn effetti è come se la piattaforma si rifiutasse di \nconsiderare diversi due tipi generici validi se differiscono \nsolo per uno dei tipi attuali utilizzati\n✔Ed è esattamente così...",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#81": "Programmazione orientata agli oggetti\nLimitazioni dei Java Generics (2)\njava.lang.Comparable  presente sin da Java 1.0\novviamente non utilizzava i generics: introdotti solo in Java 5\nSi basava su Object: Comparable .boolean compareTo(Object o)\nL’irrinunciabile  retrocompatibilità ha imposto forti assunzioni \nsull’uso dei generics: in particolare NON risultò possibile cambiare \nle informazioni sui tipi (dinamici) disponibili a tempo di esecuzione \nrispetto alle versioni pre-generics\n–Detto diversamente, ciascun tipo deve essere sempre distinguibile a \ntempo dinamico anche se cancelliamo i tipi attuali utilizzati nella \ndefinizione dei tipi generici e finalizzati a tempo statico\n–A tempo dinamico non rimane traccia dei generics, che sono sempre \nrisolti in un tipo non generico (chiamato «erasure») già a tempo di \ncompilazione\n✔Per motivazioni similari, non è possibile creare array generics\n–Per es. T[] ag = new T[10]; // T tipo  NON COMPILA",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#82": "Programmazione orientata agli oggetti\nLimitazioni dell’Ordinamento \n«Naturale» o «Interno» \nA causa di questa sfortunata interazione con le \nlimitazioni dei generics, l’ordinamento naturale può \nessere definito in uno solo dei tipi di una gerarchia\n–nell’es. di prima,  dentro Studente  oppure dentro Persona, \nma non in entrambe\nPertanto, considerando:\n–la necessità di definire molteplici criteri di ordinamento su \noggetti dello stesso tipo (o della stessa gerarchia di tipi)\n–nonché la necessità di disaccoppiare la definizione del criterio \ndi ordinamento da quella della classe le cui istanze \nrisulteranno ordinate\n✔risulta ben motivato  l’utilizzo di soluzioni (per la definizione \ndi un criterio di ordinamento) «esterne» alla classe",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#83": "Programmazione orientata agli oggetti\nOrdinamento «Esterno»\n•Se vogliamo ordinare una lista secondo un criterio \ndiverso dall'ordinamento naturale?\n•Segnatura per l’ordinamento esterno :\npublic static <T> void sort(\n         List<T> listaDaOrdinare, \n Comparator<? super T> comparatore\n  )\n•Si affida ad un oggetto esterno , passato come \nparametro,  per effettuare i confronti necessari \nall'ordinamento\n✔unica soluzione possibile quando serve più di un \ncriterio di ordinamento\n84",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#84": "Programmazione orientata agli oggetti\nL'Interface \njava.util. Comparator<T>\n•L'interfaccia java.util.Comparator<T>  consiste \ndi un solo metodo:\npublic int compare(T o1, T o2)\nche deve restituire un valore che è \n–minore, uguale, maggiore di zero a seconda che \nl'oggetto riferito da o1 sia… \n–minore, uguale, maggiore dell'oggetto riferito dal \nparametro o2\n✔(per ricordarselo: “come fosse o1 – o2 per un ord. crescente” )\n✔N.B. la segnatura è simile ma non identica a quella del \nmetodo compareTo()  di Comparable<T>\n85",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#85": "Programmazione orientata agli oggetti\nEsercizio (cont.)\nimport java.util.Comparator;\npublic class ComparatorePerEta\n           implements Comparator<Persona> {\n  @Override\n  public int compare(Persona p1, Persona p2) {\nreturn p1.getEta() - p2.getEta();\n  }\n}•Supponiamo di voler ordinare una lista di oggetti \nPersona  per età\n•Introduciamo un opportuno comparatore esterno:\n86",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#86": "Programmazione orientata agli oggetti\nTest di \nComparator<Persona>.compare()\n// import omessi\npublic class ComparatorePerEtaTest {\n@Test\npublic void testCompare() {\nPersona paolo = new Persona(\"Paolo\", 61);\nPersona anna = new Persona(\"Anna\", 55);\nComparatorePerEta comparator =\nnew ComparatorePerEta();\nassertTrue(comparator.compare(paolo, anna) > 0);\nassertTrue(comparator.compare(anna, paolo) < 0);\nassertEquals(0,comparator.compare(paolo, paolo));\nassertEquals(0,comparator.compare(anna, anna));\n}\n}\n87",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#87": "Programmazione orientata agli oggetti\nTest dell’Ordinamento «Esterno»\npublic class OrdinamentoEsternoTest {\n  @Test\npublic void testSort() {\n  List<Persona> l = new LinkedList<>();\n  l.add(new Persona(\"Valter\", 5));\n  l.add(new Persona(\"Paolo\", 10));\n  l.add(new Persona(\"Giacomo\", 7));\n  l.add(new Persona(\"Alessandro\", 8));\n    ComparatorePerEta comparatore = \nnew ComparatorePerEta();\n  Collections.sort(l, comparatore);\n    assertEquals(\"Valter\", l.get(0).getNome());\n    assertEquals(\"Giacomo\", l.get(1).getNome());\n  assertEquals(\"Alessandro\", l.get(2).getNome());\n    assertEquals(\"Paolo\", l.get(3).getNome());\n  }\n88",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#88": "Programmazione orientata agli oggetti\nNote Finali \n●L'interface Comparable<T>   è nel package java.lang , \nquindi non è necessario importarla\n•L'interface java.util.Comparator<T>  è nel package \njava.util , quindi va importata esplicitamente\n•In Collections  esistono anche i metodi per il calcolo \ndel min/max secondo l’ordinamento esterno. \nAd es.:\nstatic <T>  T min(Collection<? extends T> coll, \n  Comparator<? super T> comp)\n89",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#89": "Programmazione orientata agli oggetti\nEsercizio\n● Senza cambiare la classe  Libro (riportata di \nseguito), scrivere il metodo \nelencoOrdinatoPerPagine () della classe \nBiblioteca  affinché restituisca l'elenco dei \nlibri ordinato per numero di pagine\n90",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#9": "Programmazione orientata agli oggetti\nUn Primo Sguardo: Map<K,V>\n•L'interface Map<K,V>  offre le operazioni di una \nmappa, o dizionario: una mappa è una \ncollezione di coppie chiave-valore\n•L'interface Map<K,V>  dichiara i metodi per \noperazioni quali:\n–Accesso per chiave: ottenere il valore associato ad \nuna chiave\n–Cancellare una coppia in cui compare una chiave\n–Inserire una nuova coppia nella mappa\n–Ottenere una collezione contente tutte le chiavi o \ntutti i valori\n10",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#90": "Programmazione orientata agli oggetti\nEsercizio (cont.)\npublic class Libro implements Comparable<Libro> {\n    private String titolo;\n    private int pagine;\n    public Libro(String titolo, int pagine) {\n        this.titolo = titolo;\n        this.pagine = pagine;\n    }\n    public String getTitolo() {\n        return this.titolo;\n    }\n    public int getPagine() {\n        return this.pagine;\n    }\n    @Override\n    public int compareTo(Libro libro) {\n        return this.getTitolo().compareTo(libro.getTitolo());\n    }\n}\n91",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#91": "Programmazione orientata agli oggetti\nEsercizio (cont.)\nimport java.util.List;\nimport java.util.ArrayList;\npublic class Biblioteca {\n    private List<Libro> elenco;\n    public Biblioteca() {\n        this.elenco = new ArrayList<>();\n    }\n    public void aggiungiLibro(Libro libro) {\n        this.elenco.add(libro);\n    }\n    public List<Libro> elencoOrdinatoPerPagine() {\nComparatorePerPagine comp = new ComparatorePerPagine(); \nCollections.sort(this.elenco, comp);\n    return this.elenco;\n    }\n}\n92",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#92": "Programmazione orientata agli oggetti\nEsercizio (cont.)\npublic class ComparatorePerPagine\n implements Comparator<Libro> {\n @Override\n public int compare(Libro l1, Libro l2) {\n  return l1.getPagine() – l2.getPagine();\n }\n}\n93",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-15-collezioni-liste-generiche.pdf#93": "Programmazione orientata agli oggetti\nConclusioni\n•Introduzione alle Collezioni\n–Interacce principali\n–Principali implementazioni\n–Iteratori, e for-each\n•Java Collection Framework\n•Liste\n•Ordinamento di liste\n–Ordinamento Naturale\n–Ordinamento Esterno\n–Generics nelle gerarchie di tipi\n94",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#0": "Programmazione \nOrientata agli Oggetti\nClassi Astratte e \nCostanti Enumerative",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#1": "Programmazione orientata agli oggettiContenuti\n•Classi astratte\n•Metodi astratti\n•Classi astratte o interface?\n•Metodi e Classi finali\n•Costanti enumerative\n•Pre-Java 5\n•Java 5 Enum\n•Quando usarli\n•Enum e Collezioni\n2",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#10": "Programmazione orientata agli oggettiEsempio: Caratteristiche Generali\n•Tutte le tipologie di personaggi condividono una parte \ndella implementazione \n–le variabili per memorizzare nome e descrizione\n–metodi accessori\n–costruttori\n–il codice che gestisce la risposta al saluto\n•Tutte le tipologie di personaggi hanno un metodo \n(astratto) per modellare l'azione: \nabstract public String agisci(Partita partita);\n•Tuttavia non ha senso definire sino al dettaglio il \ncomportamento di un generico personaggio\nil comportamento dipende infatti dalle specificità di \nogni particolare personaggio\n11",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#11": "Programmazione orientata agli oggettiEsempio (cont.)\n•Ogni personaggio ha un comportamento specifico\n–il mago ci dona un attrezzo\n–la strega ci sposta in una stanza\n–il cane morde\n•Il codice che implementa l'azione è specifico del \npersonaggio: ogni tipologia di personaggio ha la \npropria, specifica, implementazione\n•In altri termini, il personaggio è definibile \nprescindendo da alcuni suoi dettagli che lo \ndifferenziano da tutti gli altri\n–alcune sue proprietà possono essere «concrete» (fornite \nanche di una implementazione) \n–altre solo «astratte» (ovvero senza l’implementazione)\n12",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#12": "Programmazione orientata agli oggettiEsempio: la Classe Astratta \nPersonaggio  (1)\npackage it.uniroma3.personaggi;\nimport …\npublic abstract class AbstractPersonaggio {\nprivate String nome;\nprivate String presentazione;\nprivate boolean haSalutato;\npublic AbstractPersonaggio(String nome, String presentaz) {\nthis.nome = nome;\nthis.presentazione = presentaz;\nthis.haSalutato = false;\n}\npublic String getNome() {\nreturn this.nome;\n}\npublic boolean haSalutato() {\nreturn this.haSalutato;\n}\n…13",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#13": "Programmazione orientata agli oggettiEsempio: la Classe Astratta \nPersonaggio  (2)\npublic String saluta() {\nStringBuilder risposta = \nnew StringBuilder(\"Ciao, io sono \");\nrisposta.append( this.getNome()+\".\"); \nif (!haSalutato)\nrisposta.append(this.presentazione);\nelse\nrisposta.append(\"Ci siamo gia' presentati!\");\nthis.haSalutato = true;\nreturn risposta.toString();\n}\nabstract public String agisci(Partita partita);\n@Override\npublic String toString() {\nreturn this.getNome();\n}\n}\n14",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#14": "Programmazione orientata agli oggettiLa Classe ComandoInteragisci\npackage it.diadia.comandi;\nimport ... \npublic class ComandoInteragisci implements Comando {\nprivate static final String MESSAGGIO_CON_CHI = \n                              \"Con chi dovrei interagire? ...\";\nprivate String messaggio;\n   private IO io;\n@Override\npublic void esegui(Partita partita) {\nAbstractPersonaggio personaggio;\npersonaggio = partita.getStanzaCorrente().getPersonaggio();\nif (personaggio!=null) {\nthis.messaggio = personaggio.agisci(partita);\nio.mostraMessaggio(this.messaggio);\n    } else io.mostraMessaggio(MESSAGGIO_CON_CHI);\n}\npublic String getMessaggio() {\nreturn this.messaggio;\n}\n@Override\npublic void setParametro(String parametro) {}\n}15",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#15": "Programmazione orientata agli oggettiEsempio: la Classe Mago\npublic class Mago extends AbstractPersonaggio  {\nprivate static final String MESSAGGIO_DONO = \" Sei un vero simpaticone, \" +\n\"con una mia magica azione, troverai un nuovo oggetto \" +\n\"per il tuo borsone!\";\nprivate static final String MESSAGGIO_SCUSE = \"Mi spiace, ma non ho piu' nulla... \";\nprivate Attrezzo attrezzo;\npublic Mago(String nome, String presentazione, Attrezzo attrezzo) {\nsuper(nome, presentazione);\nthis.attrezzo = attrezzo;\n}\n@Override\npublic String agisci(Partita partita) {\nString msg;\nif (this.attrezzo!=null) {\npartita.getStanzaCorrente().addAttrezzo(this.attrezzo);\nthis.attrezzo = null;\nmsg = MESSAGGIO_DONO;\n}\nelse {\nmsg = MESSAGGIO_SCUSE;\n}\nreturn msg;\n}\n}16",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#16": "Programmazione orientata agli oggettiClassi e Metodi Astratti\n•Le classi astratte:\n–utilizzano il modificatore abstract nella definizione\n–non possono essere istanziate direttamente  (sebbene devono avere, \nanche solo implicitamente, almeno un costruttore!)\n–definiscono implementazioni poi ereditate dalle sottoclassi\n•I metodi astratti:\n–utilizzano abstract nella segnatura\n–non possiedono corpo\n•La presenza anche di un solo metodo astratto rende \nnecessaria la dichiarazione della classe come astratta\n•ma una classe può essere dichiarata astratta anche se non possiede \nalcun metodo astratto (e non potrà essere istanziata)\n•Le sottoclassi, per essere concrete ed istanziabili, devono \ncompletare l’implementazione sovrascrivendo tutti i metodi \nastratti (oppure, a loro volta, devono dichiararsi astratte…)\n17",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#17": "Programmazione orientata agli oggettiClassi Astratte\n•Servono a definire implementazioni parziali che \nverranno completate nelle classi concrete che le \nestendono\n•Rispetto alle interface ?\n–come le interface non possono essere istanziate\n–diversamente dalle interface riportano una \nimplementazione parziale e vengono estese\n•Molto usate nei framework, ma la progettazione di \nframework va (ben!) oltre gli obiettivi formativi di \nquesto corso...\n18",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#18": "Programmazione orientata agli oggettiClassi Astratte vs Interface\n•Classe astratta\n–pro: permette di riutilizzare l’implementazione\n–contro: limita fortemente le possibilità di estensione\nla gerarchia delle implementazioni Java è lineare\n•Interface\n–pro: nessun limite di estensione\nereditarietà multipla delle interfacce\n–contro: nessun meccanismo di riutilizzo del codice\n•Come scegliere? \n–preferire le interface, meno vincolanti\n–Valutare l’alternativa solo in presenza di codice e soprattutto \nlogica da riutilizzare: per es. con i framework\n19",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#19": "Programmazione orientata agli oggettiTesting di Classi Astratte (1)\n•Qual è l’obiettivo del testing di classi astratte?\n✔testare i metodi implementati direttamente nella \nclasse astratta \n•Conviene definire una sua estensione concreta \nsolo ai fini del testing\n✔fornisce una implementazione concreta (e minimale ) dei \nmetodi astratti per permettere la creazione dell’oggetto \nda testare\n✔ad es. implementa i metodi astratti restituendo costanti \n•Si testano i metodi concreti della superclasse\n20",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#2": "Programmazione orientata agli oggettiContenuti\n•Classi astratte\n•Metodi astratti\n•Classi astratte o interface?\n•Metodi e Classi finali\n•Costanti enumerative\n•Pre-Java 5\n•Java 5 Enum\n•Quando usarli\n•Enum e Collezioni\n3",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#20": "Programmazione orientata agli oggettiTesting di Classi Astratte (2)\npublic class FakePersonaggio extends AbstractPersonaggio {\npublic FakePersonaggio (String nome, String presentazione) {\nsuper(nome, presentazione);\n}\n  @Override\npublic String agisci(Partita partita) {\nreturn “done”; \n}\n}\n•Scriviamo la classe di test AbstractPersonaggioTest  \nche verifichi il comportamento dei metodi concreti \nereditati da AbstractPersonaggio\n21",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#21": "Programmazione orientata agli oggettiEsercizi (1)\n•Definire la classe Cane, che estende la classe \nAbstractPersonaggio : quando interagiamo con un cane, \nquesti morde, togliendoci CFU!\n•Definire il comando interagisci\n•Introdurre AbstractComando  per eliminare i metodi \nreplicati nelle implementazioni dell’interface Comando  e \nrelativi alla gestione del nome e del parametro del \ncomando\n•Utilizzarla anche per ospitare l’oggetto IO per la gestione \ndell’input/output. Come offrire l’accesso alle sottoclassi?  \n✔N.B. quest’ultimo utilizzo delle classi astratte risulta utile \nper esercitarsi con l’uso dello strumento ma è in nitido \ncontrasto con la raccomandazione di limitare il loro utilizzo \nal caso di sufficiente logica duplicata\n22",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#22": "Programmazione orientata agli oggettiEsercizi (2)\n•Definire la classe Strega  come riportato nella \ndescrizione precedente\nè anche un buon esercizio sulle collezioni...\n•Definire ed organizzare i test-case per tutte le \nclassi appena introdotte; rifattorizzare i test-\ncase già prodotti per le classi modificate\n23",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#23": "Programmazione orientata agli oggettiContenuti\n•Classi astratte\n•Metodi astratti\n•Classi astratte o interface?\n•Metodi e Classi finali\n•Costanti enumerative\n•Pre-Java 5\n•Java 5 Enum\n•Quando usarli\n•Enum e Collezioni\n24",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#24": "Programmazione orientata agli oggetti\nMetodi e Classi final (1)\n•Per evitare che un metodo possa essere ridefinito \nsi usa il modificatore final\n  public class ClasseConMetodoFinale {\n    public final int nonMiPoteteSovrascrivere() {…}\n  }\n•In questo modo il metodo non può essere ridefinito \nnelle classi che estendono  ClasseConMetodoFinale\n–viene sollevato un errore a tempo di compilazione\npublic class EstendeClasseConMetodoFinale\n             extends ClasseConMetodoFinale {\n  @Override                          \n  public int nonMiPoteteSovrascrivere(){…}// ERRORE\n}\n25",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#25": "Programmazione orientata agli oggetti\nMetodi e Classi final (2)\n•E' possibile dichiarare final intere classi\npublic final class NonMiPoteteEstendere {\n  …\n}\n•La classe NonMiPoteteEstendere  non può essere \nestesa\n✔anche in questo caso errore a tempo di \ncompilazione\npublic class EstendeClasseFinale \n    extends NonMiPoteteEstendere { // ERRORE\n  …\n }\n•Molte classi della libreria standard sono dichiarate \nfinal, un esempio per tutti: java.lang.String\n26",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#26": "Programmazione orientata agli oggettiContenuti\n•Classi astratte\n•Metodi astratti\n•Classi astratte o interface?\n•Metodi e Classi finali\n•Costanti enumerative\n•Pre-Java 5\n•Java 5 Enum\n•Quando usarli\n•Enum e Collezioni\n27",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#27": "Programmazione orientata agli oggetti\nJava Enum e Costanti Enumerative\n•I metodi e le classi final sono stati inizialmente motivati da \nquestioni legate alle sicurezza\n•Hanno conosciuto un altro diffuso utilizzo, ma forse meno noto \nperché nascosto da un consistente strato di zucchero sintattico\n•Costanti Enumerative:  valori costanti raccolti in insiemi finiti e stabili. Es.:\n−i 12 mesi dell’anno ed i 7 giorni della settimana\n−i 7 tipi di tetramino nel gioco T etris! \n−i 4 punti cardinali (>>)\n•I Java Enum (da Java 5) consentono di modellare efficacemente \ninsiemi di «costanti enumerative » usando opportunamente\n–Classi astratte\n–Classi final\n–Costruttori privati",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#28": "Programmazione orientata agli oggetti\nEnumerazioni (pre Java 5)\n•Per capire il loro funzionamento, ed apprezzarne i vantaggi, \nconviene descrivere come venivano realizzati prima di Java 5,  \n(versione in cui ne fu introdotto il supporto)\n✔In effetti è la pratica tuttora utilizzata in C\n•In assenza di un meccanismo dedicato, la modellazione di costanti \nenumerative ripiegava spesso nell’uso di generiche costanti di un \ntipo «preso in prestito »\n•Ad esempio, mediante semplici numeri interi:\npublic interface Direzione {\n   // Da NORD in senso orario\n   static final public int NORD  = 0;\n   static final public int OVEST = 1;\n   static final public int SUD   = 2;\n   static final public int EST   = 3;\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#29": "Programmazione orientata agli oggetti\nEnumerazioni (pre Java 5)\n•Principale vantaggio: semplicità ed immediatezza\n•Importanti conseguenze negative, fra tutte una \ntipizzazione piuttosto lasca:\nint direzione = 5; // COMPILA! Ma non dovrebbe… \n•Ne derivano molti svantaggi, ad es. Metodi con \nsegnature di non immediata interpretazione:\n public int direzioneOpposta(int arg) { … }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#3": "Programmazione orientata agli oggettiIntroduzione (1)\n•Abbiamo visto come l'estensione può essere \nuno strumento utile per il riuso del codice\n•Le classi estese ereditano anche \nl'implementazione della classe base\n–La classe estesa può avere variabili di istanza e \nmetodi aggiuntivi a quelli ereditati dalla classe base \n–La classe estesa può ridefinire metodi della classe \nbase\n•La classe estesa è un sottotipo della classe \nbase, quindi, vale il principio di sostituzione:\npossiamo usare istanze di una classe estesa al \nposto delle istanze della classe base \n4",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#30": "Programmazione orientata agli oggetti\nEnumerazioni Tipate (pre Java 5)\n•Soluzioni più articolate prevedono infatti almeno la creazione \ndi un tipo dedicato, per rendere i controlli a tempo statico e \nla tipizzazione più stringenti\n•Una soluzione diffusamente utilizzata:\n–classi final\n–costruttori privati\npublic final class Direzione {\n  private int ordinal;\n  private Direzione(int index) { this.ordinal = index; }\n  public int getOrdinal()      { return this.ordinal;  }\n  static public final Direzione NORD  = new Direzione(0);\n  static public final Direzione EST   = new Direzione(1);\n  static public final Direzione SUD   = new Direzione(2);\n  static public final Direzione OVEST = new Direzione(3);\n  static private final Direzione[] cardinali = \n                                     { NORD, EST, SUD, OVEST };\n  static public Direzione[] values() { return cardinali; } \n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#31": "Programmazione orientata agli oggetti\nEnumerazioni Tipate (pre Java 5)\n•Risolti i problemi legati alla tipizzazione lasca:\nDirezione dir = new Direzione(5); // NON COMPILA\npublic Direzione direzioneOpposta(Direzione arg) {…}\n•Nuove possibilità, ad es. cicli for-each:\nfor(Direzione dir : Direzione.values()) { … }\n•Si può anche pensare di affiancare metodi (polimorfi) alle \ncostanti. Ad es. per il calcolo della direzione opposta() :\nimport static Direzione.*;\nassertEquals(SUD, NORD.opposta());\n•Si rende però necessario creare un sottotipo per ogni \ncostante della stessa enumerazione>>",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#32": "Programmazione orientata agli oggetti\nEnumerazioni Polimorfe (pre Java 5)\npublic abstract class Direzione {\n  private int ordinal;\n  private Direzione(int index) { this.ordinal = index; }\n  public int getOrdinal()      { return this.ordinal;  }\n  abstract public Direzione opposta();\n  static public final Direzione NORD  = new NORD();\n  static public final Direzione EST   = new EST();\n  static public final Direzione SUD   = new SUD();\n  static public final Direzione OVEST = new OVEST();\n  static final class NORD  extends Direzione   {\n    private NORD()  { super(0); }\n    @Override public Direzione opposta() { return SUD; }\n  }\n  // …similarmente per EST, SUD…  \n  … \n  static final class OVEST extends Direzione   {…}\n}Permette di dichiarare una\nnuova classe interna (>>) e di \ninvocare il costruttore privato",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#33": "Programmazione orientata agli oggetti\nLimitazioni dell’Enumerazioni \npre Java 5: Perché i Java 5+ Enum\n•Efficace: ma per N costanti è necessario creare N+1 classi...\n✔Decisamente macchinoso, anche con l’aiuto di un IDE evoluto\n•I Java Enum introdotti in Java 5 sostanzialmente adottano \nquesta stessa soluzione, ma in aggiunta offrono:\n•Una sintassi meno verbosa; ad es. per  enum senza metodi \naggiuntivi basta scrivere: \n  public enum Direzione {\n      NORD, EST, SUD, OVEST;\n  }\n•Una superclasse astratta java.lang.Enum  condivisa da tutti i tipi \nenumerati\n–con metodi condivisibili da tutti i tipi enumerati (es. ordinal() ) \n•Un generoso aiuto da parte del compilatore\n–definisce alcuni metodi statici aggiuntivi (>>)",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#34": "Programmazione orientata agli oggetti\nJava Enum: Sintassi\n•In presenza di metodi aggiuntivi, la sintassi dei Java Enum in \ncaso di enumerazioni polimorfe è più prolissa, ma comunque \npiù compatta della soluzione “manuale”:\npublic enum Direction {\n  NORD() { \n     @Override  public Direction opposta() {\n        return SUD; \n     }\n  },\n  …\n  OVEST() {\n     @Override  public Direction opposta() { \n        return EST;\n     }\n  };\n  public abstract  Direction opposta();\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#35": "Programmazione orientata agli oggetti\njava.lang.Enum : Gerarchia\n•Per ogni Enum dichiarato il compilatore genera\n•Una sottoclasse (ad es. Direzione ) della classe astratta \ngenerica java.lang.Enum che modella il tipo enumerato\n•Una sua sottoclasse per ciascun valore costante (ad es. NORD)\n  public abstract class Enum< E extends Enum<E> >\n                        implements Comparable<E>\njava.lang.Enum\nNORD\nDirezioneDirezioneDirezione\nOVEST\nSUD\nEST",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#36": "Programmazione orientata agli oggetti\njava.lang.Enum : Costruttore\n•La classe java.lang.Enum accentra funzionalità per \ntutti i tipi enumerati\n•Ad es. metodi per gestire i numeri ordinali ( ordinal() ) ed il \nnome del tipo enumerato ( name())\n•Unico costruttore:\nEnum protected  Enum(String name, int ordinal)\nSole constructor. Programmers cannot invoke this constructor. It \nis for use by code emitted by the compiler in response to enum \ntype declarations .\nParameters :\nname - The name of this enum constant, which is the identifier \nused to declare it.\nordinal - The ordinal of this enumeration constant (its position \nin the enum declaration, where the initial constant is assigned \nordinal 0).",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#37": "Programmazione orientata agli oggetti\nJava Enum: \nCostruttori delle «Costanti Enumerative »\n•I valori costanti possono avere uno o più costruttori ed uno stato\n•Si dichiara un costruttore privato solo nel tipo enumerato (es. \nDirezione )\n•Lo stato deve essere immutabile (>>)\n✔è preferibile dichiarare tutte le variabili di istanza final\npublic enum Direzione {\n    NORD(0) {     @Override\n                  public Direzione opposta() { return SUD; }\n    },…,\n    OVEST(270 ) {  @Override\n                  public Direzione opposta() { return EST; }\n    };  \n    private final int gradi;\n    private Direzione(int gradi)  { \n       this.gradi = gradi;\n    }\n    public int getGradi() { return this.gradi; }\n    public abstract Direzione opposta();\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#38": "Programmazione orientata agli oggetti\njava.lang.Enum : Metodi (1)\n●Documentiamo la semantica degli altri metodi di \njava.lang.Enum\n–tramite qualche test di unità, per es. sul tipo Direzione\n–metodo ordinal( )\nimport static org.junit.Assert.*;\nimport org.junit.Test;\nimport static Direzione.*;\npublic class EnumTest {\n    @Test\n    public void testOrdinal() {\n        assertEquals(0, NORD.ordinal());\n        assertEquals(1, EST.ordinal());\n        assertEquals(2, SUD.ordinal());\n        assertEquals(3, OVEST.ordinal());\n    }\n… // altri test-case a seguire\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#39": "Programmazione orientata agli oggetti\njava.lang.Enum : Metodi (2)\n●Ogni tipo enumerativo è associato ad una classe per l’intero tipo più \nun oggetto-singleton/classe per ciascuno dei valori costanti del tipo \n●Le costanti ricordano la classe del loro supertipo (ed associato a \ntutta la collezione di costanti enumerative) mediante il metodo\nClass<E> getDeclaringClass()\n●returns the Class object corresponding to this enum constant's enum type.\npublic class EnumTest {… \n    @Test\n    public void testGetDeclaringClass() {\n        assertSame(Direzione.class, NORD.getDeclaringClass());\n        assertNotSame(Direzione.class, NORD.getClass());\n        assertNotSame(EST.getClass(), NORD.getClass());\n        … // similarmente per le altre direzioni  \n    }\n…} ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#4": "Programmazione orientata agli oggettiIntroduzione (2)\n•Abbiamo visto che anche le interface \nfavoriscono il riuso del codice\n•Grazie al principio di sostituzione possiamo \nscrivere codice con metodi polimorfi\n–accettano come parametro formale un riferimento \nad un tipo (statico) astratto\n–il parametro attuale sarà un riferimento ad un \nsottotipo (dinamico) concreto di tale tipo\n•Talvolta, classi diverse che implementano una \nmedesima interface possono voler condividere \nanche una significativa porzione \ndell'implementazione\n5",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#40": "Programmazione orientata agli oggetti\njava.lang.Enum : Metodi (3)\n●Ad ogni valore costante è associato un singolo oggetto (singleton)\n●Evidente con un test sul metodo valuesOf() , che consente di \ncreare gli oggetti associati alle costanti di un tipo enumerato anche \nsenza scomodare la più “pesante” API sull’introspezione (>>)\npublic class EnumTest {… \n     @Test\n     public void testTuttiSingleton() {\n         assertSame(NORD, Direzione.valueOf(\"NORD\"));\n         final Direzione singleton = Direzione.valueOf(\"NORD\");\n         assertSame(singleton, NORD);\n         …\n         assertNotSame(EST, NORD);\n        … // similarmente per le altre direzioni  \n     }\n }",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#41": "Programmazione orientata agli oggetti\nEnumerazioni: Criterio di Equivalenza (1)\n●Quindi riassumendo:\n–ogni tipo enumerato di N valori costanti genera N+1 classi\n–di queste N sono singleton (classi a singola istanza) che \nmodellano i valori\n–ad ogni valore corrisponde quindi una sola classe che \npossiede una sola istanza \n●Tutto questo si riflette sul criterio di equivalenza dei \ntipi enumerati che risulta essere quello più naturale\n●Ciascuno dei valori costanti è un oggetto che  \n–è equivalente solo a se stesso\n–non è equivalente a nessuno degli altri  ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#42": "Programmazione orientata agli oggetti\nEnumerazioni: Criterio di Equivalenza (2)\n●equals()  e l’operatore  ==  finiscono per modellare lo stesso \ncriterio di equivalenza\n✔ATTENZIONE! NON è affatto vero in generale: vale solo per i tipi \nenumerativi!\n●hashCode()  ritorna il valore restituito dal metodo ordinal()\n✔HashSet ed HashMap su tipi enumerativi possiedono prestazioni \nottimali (non esistono conflitti!)\npublic class EnumTest {… \n   @Test\n   public void testCriterioDiEquivalenza() {\n       assertEquals(NORD, NORD);\n       assertNotEquals(NORD, EST);\n       assertNotEquals(NORD, SUD);//…\n        … // similarmente per le altre direzioni  \n   }…\n} ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#43": "Programmazione orientata agli oggetti\nEnumerazioni: Ordinamento\n●E’ direttamente quello basato sul valore ordinale\n✔N.B. Induce anche un criterio di equivalenza basato \nsull’identicità degli oggetti\n✔Ovvero come l’operatore == \npublic class EnumTest {… \n   @Test\n   public void testCompareTo() {\n     assertTrue(NORD.compareTo(EST)<0);\n     assertTrue(EST.compareTo(SUD)<0);\n     assertTrue(SUD.compareTo(OVEST)<0);\n     assertTrue(OVEST.compareTo(NORD) >0);\n   }  \n…} ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#44": "Programmazione orientata agli oggetti\nEnum.toString() / name()\n●Non è necessario ridefinire un metodo toString()  \nin quanto è naturalmente definito sulla base del \nnome stesso della costante\nString name()\n–Returns the name of this enum constant, as declared in its enum declaration.\nString toString()\n–Returns the name of this enum constant, as contained in the declaration.\npublic class EnumTest {… \n   @Test\n   public void testToStringAndName() {\n     assertEquals(\"NORD\", NORD.toString());\n     assertEquals(\"NORD\", NORD.name());\n    … // similarmente per le altre direzioni  \n   }…\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#45": "Programmazione orientata agli oggetti\nJava Enum e Metodi «Fantasma » (1)\n●I Java Enum introducono «zucchero sintattico »: la controparte nei sorgenti \ndi codice oggetto generato non risulta osservabile\n✔si pensi a boxing-unboxing (<<)\n●Sorprendentemente, anche alcuni metodi appaiono, in particolare il \nmetodo statico values() ed anche valueOf()  \n✔Metodi di cui non esiste il codice sorgente!\n✔Eclipse non fornisce meta-informazioni ! (e javadoc)\n●Il compilatore genera automaticamente questi metodi adottando un \ncomportamento particolare per tutte le classi che estendano  \njava.lang.Enum :\npublic class EnumTest {… \n   @Test\n   public void testValues() {\n      final Direzione[] expected = { NORD, EST, SUD, OVEST } ;\n      assertArrayEquals (expected , Direzione. values());\n   }…\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#46": "Programmazione orientata agli oggetti\nJava Enum e Metodi «Fantasma » (2)\n●Per avere documentazione è necessario riferirsi \ndirettamente alla Java Language Specification\n/**\n* Returns an array containing the constants of this enum \n* type, in the order they're declared.  This method may be\n* used to iterate over the constants as follows:\n*    for(E c : E.values())\n*        System.out.println(c);\n* @return an array containing the constants of \n* this enum type, in the order they're declared */\npublic static E[] values();\n/**\n* Returns the enum constant of this type with the specified name.\n* The string must match exactly an identifier used to declare\n* an enum constant in this type.  (Extraneous whitespace \n* characters are not permitted.)\n* @return the enum constant with the specified name\n* @throws IllegalArgumentException if this enum type has no\n* constant with the specified name */\npublic static E valueOf(String name);Il metodo statico \nvalues()  rende \npossibile usare cicli \n«for-each» sopra tipi \nenumerati",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#47": "Programmazione orientata agli oggetti\nJava Enum e «Switch-Statement »:\n●E’ possibile usare tipi enumerati direttamente negli \n«switch statement »;  in precedenza era possibile farlo \nsolo per le costanti letterali\nimport static Direzione.*;\npublic class DirezioneUtils {\n  public static Direzione opposta(Direzione direzione) {\n    switch (direzione) {\n      case NORD:  return SUD;\n      case EST:   return OVEST;\n      case SUD:   return NORD;\n      case OVEST: return EST;\n      default:    return null;\n    }\n  }\n}",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#48": "Programmazione orientata agli oggetti\nConclusioni:\nCostanti Enumerative - Quando Usarle? (1)\n•Quando è opportuno utilizzare costanti \nenumerative?\n•Quando l’insieme di elementi da modellare:\n✓è finito\n✓tutti i suoi elementi siano già noti\n✓è «stabile»: molto difficilmente cambierà nel tempo\n•Quando per gli elementi ospitati:\n✓ne conosciamo i nomi... «propri»\n•ad es. “NORD”, “GENNAIO”\n✓possono anche avere uno stato, purché immutabile\n•inutile/impossibile crearne diverse «istanze»",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#49": "Programmazione orientata agli oggetti\nConclusioni:\nCostanti Enumerative - Quando Usarle? (2)\n•Ed inoltre:\n–Il dominio suggerisce\n•una tipizzazione forte (concetto di “primo ordine”)\n•un tipo dedicato alla collezione di costanti\n–Anche per  l’esigenza di aggiungere metodi (polimorfi) \nalle costanti \n•Ad es. Direzione.opposta()",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#5": "Programmazione orientata agli oggettiMotivazioni\n•Nella pratica, capita di voler definire classi base \npensate solo per essere estese e non per essere \ndirettamente istanziate\n✔allo scopo di evitare duplicazioni nel codice\n✔Per favorire la qualità interna (<<) \n•Queste classi contengono una parziale \nimplementazione da condividere con le sottoclassi\n–variabili di istanza\n–implementazione di alcuni metodi\n•cosidetti metodi «concreti»\n–segnatura di altri metodi\n•cosidetti metodi «astratti»\n•I metodi che rimangono privi di implementazione \nnella classe base possono poi essere «completati » \nnelle classi derivate6",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#50": "Programmazione orientata agli oggetti\nEsercizio (Studio di Caso)\n●Introdurre l’enumerazione Direzione  nello studio \ndi caso per modellare con un tipo dedicato le \ndirezioni degli spostamenti e delle adiacenze tra \noggetti Stanza\n✔Attualmente si «prende in prestito» il tipo String\n✔Ma attenzione, è un sintomo di cattiva \n«modellazione »:\nhttp://c2.com/cgi/wiki?StringlyTyped",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#51": "Programmazione orientata agli oggetti\nEsercizio (Tetris)\n●Studiare il codice dell’esercitazione «T etris», in \nparticolare nel package tetris.tetramino\n●Perché si è deciso di separare la modellazione del \ntetramino (classe tetris.tetramino.Tetramino ) da \nquella del suo tipo ( enum tetris.tetramino.Tipo )?\n●Suggerimento:\n–Da quale campi è composto lo stato degli oggetti istanza dei due \ntipi? \n–Quali sono o possono essere dichiarati final?\n–Quale dei due tipi possiede uno stato mutabile?\n●Cosa accadrebbe se fondessimo le due classi \naddossando le responsabilità di Tipo alla classe \nTetramino ? ",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#52": "Programmazione orientata agli oggetti\nEnum e Collezioni\n●Le costanti enumerative finiscono per costituire un \ntipo  di collezione di caratteristiche particolari\n●Sono possibili rappresentazioni ancora più compatte \ned efficienti rispetto alle generiche collezioni (che già \nsono particolarmente efficienti sui tipi enumerativi)\n●Dai javadoc di java.util.EnumSet\n–Implementation note: All basic operations execute in \nconstant time. They are likely (though not guaranteed) to \nbe much faster than their HashSet counterparts. Even bulk \noperations execute in constant time if their argument is \nalso an enum set.\n●Vedere anche java.util.EnumMap\n–Specializzazione di java.util.Map  che utilizza come chiavi \nun tipo enumerativo",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#53": "Programmazione orientata agli oggetti\nConclusioni\n●L’insieme dei meccanismi offerti da un linguaggio per \nla modellazione dei tipi è uno degli aspetti più \ncaratterizzanti lo stesso\n●Con le classi astratte abbiamo coperto l’insieme dei \nprincipali meccanismi per la definizione dei tipi:\n●Interface\n●Classi\n●Classi astratte\n●Generics\n●Esistono ancora altri meccanismi:\n●Tipi enumerativi\n●Classi nidificate (>>)\nmeno importanti dei precedenti, ma che \ncontribuiscono alla richezza del sistema dei tipi in Java",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#6": "Programmazione orientata agli oggettiClassi Astratte\n•Una classe astratta contiene una definizione parziale della \nimplementazione\n•Una classe astratta non può essere istanziata , ma possono \nessere istanziate le classi (concrete) che la estendono\n•Ovviamente vale la relazione sottotipo-supertipo, e quindi il \nprincipio di sostituzione\ni riferimenti alle istanze delle classi che estendono una classe \nastratta possono essere usate quando è atteso un riferimento \nad una istanza della classe base\n✔Analogie con le interface: \n–le interface non possono essere istanziate, ma possono essere istanziati \noggetti di classi che le implementano\n–vale il principio di sostituzione\n✔Differenze  con le interface:\n–le interface NON contengono implementazione (non più vero da Java 8+!!!)\n7",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#7": "Programmazione orientata agli oggettiEsempio (Caso di Studio)\n•Supponiamo di voler introdurre nel nostro gioco dei \npersonaggi (es. mostri, maghi, ecc.)\n•I personaggi sono nelle stanze del gioco (per semplicità \naccontentiamoci di un singolo personaggio per stanza)\nimport … \npublic class Stanza {    \nprivate String nome;    \nprivate Map<String, Stanza> uscite;    \nprivate Map<String, Attrezzo> nome2attrezzo;\nprivate AbstractPersonaggio personaggio;\n… … …\npublic void setPersonaggio(AbstractPersonaggio personaggio) {\nthis.personaggio = personaggio;\n}\npublic AbstractPersonaggio getPersonaggio() {\nreturn this.personaggio;\n}\n…\n}\n8",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#8": "Programmazione orientata agli oggettiEsempio (Caso di Studio)\n•I personaggi hanno un nome (una stringa) ed una \ndescrizione (una stringa)\n•I personaggi possono rispondere al saluto del \ngiocatore\nointroduciamo il comando saluta per salutare il \npersonaggio presente nella stanza\n•I personaggi possono agire\nointroduciamo il comando interagisci : provoca \nl’interazione del giocatore con il personaggio presente\n9",
    "data_test\\rootfolder\\università\\ProgrammazioneOggetti\\POO-20-classi-astratte-enum.pdf#9": "Programmazione orientata agli oggettiEsempio: i «Personaggi»\n•Possiamo introdurre diverse tipologie di \npersonaggi. Ad esempio potremmo avere:\n–Mago: possiede un attrezzo che può donare\n–Strega: se interagiamo con una strega questa ci \ntrasferisce in una stanza tra quelle adiacenti. \nSiccome è permalosa:\n•se non l’abbiamo ancora salutata, ci «trasferisce» nella \nstanza adiacente che contiene meno attrezzi\n•altrimenti in quella che contiene più attrezzi\n–Cane: morde! Ogni morso diminuisce i CFU del \nprotagonista\n10",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#0": " \n \n \n \n \n \n \nPOSITION PAPER  \nAlimentazione e sost enibilità  \nQuale relazione e quali sfide p er i prossimi anni  \nDialoghi Italo -Francesi per l’Europa , 20 giugno 2019 , Science s Po – Parigi  \n \n \n \nPerché oggi è importante parlare di \nsostenibilità  \n1. Il Position Paper  ha l’obiettivo di \nsostanziare la crescente rilevanza \ndell’alimentazione per lo sviluppo \nsostenibile. Verranno declinati gli impatti \ndell’adozione di abitudini alimentari sane e \nsostenibili  sul benessere individuale e \ncollettivo  (salute e qualità della vita e \nsostenibilità economica per i sistemi \nsanitari nazionali) e sull’ ambiente . Il \nPosition Paper  rientra nell’ambito \ndell’ iniziativa “ Dialoghi Italo -Francesi per \nl’Europa ”: la re lazione tra alimentazione e \nsostenibilità e i relativi impatti saranno  \nquindi  contestualizzati per l’Italia e la \nFrancia, all’interno del lo scenario  europeo.   \n2. Il contesto socio -economico attuale è \ncaratterizzato da grande velocità e \naccelerazione del cambi amento . Per poter \nperseguire un continuo miglioramento in \ntermini di competitività economica, \ninnovazione e prestazioni ambientali, i \nsistemi -Paese devono affiancare ai propri \npercorsi di crescita una nuova dimensione , \n                                                 \n1 Fonte: elaborazione The European House – Ambrosetti \nsu dati Global Footprint Network, 2019.  quella della  sostenibilità . Ciò implica  che \ni processi di cambiamento devono essere \ntali per cui lo sfruttamento delle risorse, la \ndirezione degli investimenti, lo sviluppo \ntecnologico e i processi decisionali delle \nIstituzioni siano orientati a  garantire i \nbisogni del le generazioni future , oltre che \ndi quelle attuali.  \n3. La sfida dello sviluppo sostenibile deve fare \ni conti con  una popolazione mondiale in \nrapida crescita:  secondo le Nazioni Unite,  \nla popolazione globale raggiungerà  i 9,8 \nmiliardi  entro il 2050  (+27%  rispetto ad \noggi) . In aggiunta, l’attuale regime  di \nsfruttamento delle risorse ambientali  sta \ncompromettendo la capacità naturale di \nrigenerazione necessaria dell’ ecosistema: \nbasti pensare  che nel 2018 l’ Earth \nOvershoot Day , il giorno in cui la \npopolazione mondiale ha consumato tutte \nle risorse terrestri disponibili per l’anno, è \nstato raggiunto il 1° agosto , oltre 150  giorni  \nprima di quanto accadeva 40 anni fa .1 \n La sostenibilità ha impatto a 360˚ sull'industria , le imprese e la vita di tutti i giorni ed è diventata una \npriorità strategica per le aziende. Nel settembre 2015, l’Organizzazione delle Nazioni Unite ha varato \nl’Agenda 2030 per lo Sviluppo Sostenibile, introducendo 17 Obiettivi di Sviluppo Sostenibile  (SDGs)  che \nriguardano tutte le dimensioni della vita umana e del pianeta.  \nUna sana e corretta alimentazione ha un impatto diretto su 7 dei 17 SDGs  e può contribuire allo sviluppo \nsostenibi le attraverso due leve fondamentali: salute  e benessere individuale e collettivo  e \nsostenibilità ambientale . Abitudini alimentari sane ed equilibrate contribuiscono a ridurre il rischio di \ncontrarre alcune patologie ( tra le quali diabete, tumori, malattie cardiovascolari), con impatti positivi anche \nsulla sostenibilità economica dei sistemi sanitari nazionali. In aggiunta, una dieta equilibr ata è associata \nanche ad un minor impatto sull’ecosistema ambientale, con effetti positivi sulla sostenibilità del pianeta.  \nIn questo contesto, una solida partnership  tra Italia e Francia  può dare un contributo significativo \nallo sviluppo sostenibile . I due Paesi , caratterizzati da una tradizione alimentare consolidata, potrebbero \nfarsi ambasciatori dei benefici della dieta mediterranea nel mondo e promuovere  un dibattito sui benefici \nassociati a stili alimentari sani e sostenibili anche nelle sedi europe e di riferimento.  \n \n \n  \n",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#1": "2 \n 4. Il benchmark su cui governi, imprese e \nsocietà civile devono fondarsi per costruire \nprocessi decisionali attuativi aperti e \npartecipati verso un sentiero di \nsostenibilità  sono i Sustainable \nDevelopment Goals . Il 25 settembre \n2015 l’Assemblea Generale delle Nazioni \nUnit e ha adottato l’Agenda 2030 per lo \nSviluppo Sostenibile , introducendo  17 \nObiettivi di Sviluppo Sostenibile  \n(Sustainable Development Goals  – SDGs) , \ndeclinati in  169 target  che riguardano a \n360° tutte le dimensioni della vita umana e \ndel pianeta.  \n5. Riconoscendo l’importanza della titolarità \nnazionale degli SDGs, le Nazioni Unite \nincoraggiano i Paesi a sviluppare una \npropria strategia basata sui 17 indicatori di \nriferimento.  \n6. In Italia , il progresso verso lo sviluppo \nsostenibile viene monitorato attra verso gli \nindicatori BES  (Indicatori di Benessere \nEquo e Sostenibile): l’Istat ha sviluppato \nun approccio multidimensionale per \nmisurare le fondamentali dimensioni del \nbenessere del Paese, corredando dati di \nattività economiche c on misure relative a \ndisegu aglianze , aspetti sociali, di \ninnovazione  e di sostenibilità.  Sono stati \nindividuati 130 indicatori, raggruppati in \n12 domini del benessere considerati di \nmaggior rilievo  per il sistema -Paese . Dal \n2016 , il Governo italiano ha posto questi \ntemi al centro de lla propria politica di \nprogrammazione economica, includendo \nun set di indicatori del BES nel Documento \ndi Economia e Finanza, in cui  annualmente  \nviene elaborata un’analisi sul loro \nandamento e una valutazione d’impatto \ndelle politiche proposte sulle dimen sioni \nchiave dello sviluppo sostenibile . \n7. La Francia ha deciso di intraprendere un \npercorso analogo , definendo una serie di \nindicatori che consentono  di monitorare le \npolitiche pubbliche nazionali  che \ncontribuiscono alla sostenibilità. \nNell’ambito del Conseil National de \nl’Information Statistique (CNIS) , sono \nstati istituiti 98 indicatori di sostenibilità al \n2030  che entreranno a far parte della \nprogrammazione economica del Governo \nentro giugno 201 9, insieme alla definizione \ndi una “tabella di marcia verso la \nsostenibilità ”.  \n8. L’alimentazione può dare un contributo \nsignificativo allo sviluppo sostenibile,  in \nquanto trasversale ad una pluralità di \ndimensioni  fondamentali per il benessere \ndelle persone , degli ecosistemi ambientali \ne dei sistemi economici . \n                                                 \n2 Fonte: elaborazione The European House – Ambrosetti  \nsu dati FAO, OMS e Nazioni Unite, 2019.    \n9. Ad oggi  esistono ancora  grandi paradossi \nche ostacolano il raggiungimento di \nmodelli sostenibili  e che legano cibo, salute \ne sostenibilità ambientale . Tra principali : \n— la denutrizione colpisce 821 milioni \ndi persone  e allo stesso tempo quasi \n2 miliardi di persone  sono in \ncondizioni di sovrappeso o obesità ; \n— i decessi causati  da obesità , 30 milioni \nogni anno, stanno gradualmente \nraggiungendo quell i causa ti da \ndenutrizione  (35 milioni) ;2 \n— denutrizione e obesità sono spesso \nstrettamente correlate , soprattutto nei \nPaesi in via di sviluppo . Alcune  \nricerche in campo medico hanno \nmesso in evidenza  come la \ndenutrizione infantile possa  essere uno \ndei fattori predittivi dell’obesità da  \nadulti e dei disturbi ad essa correlati ;3 \n3 Fonte: elaborazione The European House – Ambrosetti \nsu dati The Lancet, 2019.  \nFigura 1. I 17 Obiettivi di Sviluppo Sostenibile \ndell’Organizzazione delle Nazioni Unite e quelli \ndirettamente influenzati da un’alimentazione sostenibile \n(riquadrati in rosso). Fonte: elaborazione The European \nHouse – Ambrosetti su dati Nazioni Unite, 2019 . ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#10": "11 \n 69. I benefici di un’alimentazione \ncorretta in termini di costi evitati  \nsarebbero notevoli : ipotizzando che tutta la \npopolazione  europea segua  i dettami della \ndieta mediterranea, e che quindi le quote \npercentuali correlate alla malnutrizione tra \ni fattori di rischio si annullino , l’ordine di \ngrandezza ammonterebbe a decine di \nmiliardi di Euro  risparmiati, anche a livello \ndi singolo  sistema -Paese . Un caveat  da \nconsiderare nell’interpretazione di questi \nrisultati è  che la correlazione tra \nl’abbattimento del fattore di rischio ed un  \neventuale costo non più sostenuto, però, \nnon è 1:1 .  \nIl contributo dell’alimentazione alla \nsostenibilità ambientale  \n70. La seconda leva attraverso la  quale \nl’alimentazione ha un impatto \nconsiderevole  sulla sostenibilità  è quella \nambientale : basti pensare che un \nquarto  dei gas serra generati ogni anno \ndagli esseri umani proviene dal sistema \nalimentar e. La filiera agroalimentare \nproduce numerose esternalità sia a monte, \ncon le attività del settore dell’agricoltura, \nsia a valle, attraverso l’uso alimentare \ndomestico o di operatori del settor e.  \n71. Diverse analisi sulla correlazione tra cibo e \nambiente hanno fatto emergere come gli \nalimenti per cui i nutrizionisti \nsuggeriscono un consumo maggiore in una \ndieta salutare  siano anche quelli che \ngenerano il minor impatto sull’ecosistema \nambientale . In altre parole, “ ciò che fa \nbene all’uomo fa bene anche al \npianeta ”.  \n72. Nel capitolo precedente è  stato descritto \ncome la rappresentazione triangolare su \npiù livelli della piramide alimentare abbia \nla funzione di illustrare con quale \n                                                 \n18 Le proiezioni delle Nazioni Unite sono ulteriormente al \nrialzo, secondo cui il consumo di carne aumenterà del 76%.  frequenza è preferibile il consumo degli \nalimenti caratterizzanti le diete per uno \nstile di vita sano . Sulla base delle  scoperte \nche correlano alimentazione e ambiente, la \nstessa piramide può essere rovesciata  \n(Figura 19) per illustrare questo assunto: \nl’impatto  ambientale degli alimenti che \nfanno bene alla salute (base della piramide \nalimentare ) è il più limitato  e rappre senta  \nla punta della piramide alimentare \nrovesciata, mentre l’impatto del consumo \ndegli alimenti più dannosi per la salute \n(vertice  della piramide alimentar e) è il più \nelevato, dove troviamo la base della \npiramide alimentare rovesciata.  \n \n73. L’impatto ambienta le dei singoli alimenti è \nquantificabile tramite l’analisi del ciclo \ndi vita , utilizzando tre dimensioni:  \n— water footprint , che quantifica i \nconsumi e le modalità di utilizzo delle \nrisorse idriche , ed è misurato in volumi \nd’acqua (litri) ; \n— ecological footprint , che calcola la \nquantità di terra (o mare) \nbiologicamente produttiva necessaria \nper fornire le risorse al sistema ed \nassorbire le emissioni associate a un \nsistema produttivo ; \n— carbon footprint , che misura le \nemissioni di gas serra responsabili dei \ncambiamenti climatici in termini di \nmassa di CO 2 equivalente .  \n74. Gli ecosistemi sono sottoposti ad un a forte \npressione  a causa del  costante aumento \ndella popolazione  e del reddito pro-capite , \nsoprattutto in quei Paesi  dove la carne \nrappresenta l’elemento portante della dieta \ndelle popolazioni residenti. Secondo la \nFAO, le proiezioni  sul consumo di carne \nvedono un incremento del 70% entro il \n2050  rispetto ai livelli odierni18: gli \nallevamenti intensivi  – così dannosi per il \nbenessere degli animali oltre che per le \nesternalità negative provocate – nei Paesi \nFigura 19. La piramide alimentare rovesciata.  Fonte: \nelaborazione The European House – Ambrosetti per Barilla \nCenter for Food and Nutrition, 2019.  \nFigura 20. Consumi di carne annui pro -capite nel \nmondo per macro -area geografica (kg), 2015.  Fonte: \nelaborazione The European House – Ambrosetti su dati \nFAO, 2019. \n95,7\n65,3\n50\n41,3\n31,6\n28,6\n10,9Paesi industrializzati\nAmerica Latina\nSud-Est Asiatico\nMondo\nPaesi in via di sviluppo\nNord Africa/Medio Oriente\nAfrica Sub-Sahariana",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#11": "12 \n in via di sviluppo risultano cruciali per \nsfamare una popolazione in continua \nespansione.   \n75. Non solo le popolazioni in via di sviluppo \nstanno aumentando in numeri e \npotere  d’acquisto, ma stanno anche \nevolvendo la  loro dieta verso abitudini \n“occidentalizzate”. Ad oggi, però, il  \nconsumo pro-capite  di carne dei Paesi  \nindustrializzati è estremamente superiore \na quello delle altre macro -regioni  (Figura \n20), e non sarebbe quindi da utilizzare \ncome benchmark  per una dieta corretta.  \n76. I trend  futuri accendono un forte \ncampanello d’allarme per i sistemi \nproduttivi, in quanto per produr re proteine \nanimali è necessario un ammontare di \nrisorse notevolmente superiore se \nraffrontato alla produzione della stessa \nquantità di proteine vegetali : \nappezzamenti di terreno più grandi, più \nenergia e più acqua.  \n77. Guardando alle risorse idriche necessarie \nalla produzione degli alimenti – il water \nfootprint  – in Figura 2 2 si evince come la \nproduzione di carne abbia un impatto di \ngran lunga maggiore rispetto ad ogni altro \ntipo di  alimento  e come invece i prodotti \n                                                 \n19 Fonte: elaborazione The European  House – Ambrosetti \nsu dati FAO, Barilla Center for Food and Nutrition e The \nEuropean House – Ambrosetti, 2019.  alla base della piramide alimentare (o alla \npunta della piramide alimentare \nrovesciata) siano quelli che necessitano  di \nridotti volumi di risorse idriche.  \n78. L’Organizzazione delle Nazioni Unite ha \nstimato un range  tra 20 e 50 litri d’acqua \nquale fabbisogno minimo giornaliero pro-\ncapite  necessario ad assicurare i bisogni \nprimari legati all’alimentazione e all’igiene. \nDai dati sull’ impronta idrica degli alimenti , \nemerge come l’acqua necessaria per \nprodurre solamente un kg di carne di \nmanzo sarebbe sufficiente a garantire il \nfabbisogno idr ico medio di una persona per \n310 giorni.  \n79. Oltre a d ingenti volumi d’acqua , la \nproduzione di un kg di carne bovina \nnecessita anche di 7 -8 kg di grano: in media, \nper ogni caloria di origine animale \nprodotta , ci vogliono ben 7 calorie di cereali. \nDi conseguen za, un aumento dei consumi \ndi carne sta contemporaneamente \nrichiedendo un incremento del fabbisogno \ndi terreni agricoli  destinati esclusivamente \nagli allevamenti di bestiame. Il paradosso è \nche se la quantità di cereali destinata \nall’allevamento di bestiam e venisse \nimpiegata nell’alimentazione umana, \nteoricamente si potrebbero nutrire 2,5 \nmiliardi di persone.19 \n80. Il settore dell’allevamento rappresenta il \nmaggiore fattore di uso antropico \ndelle terre  a livello mondiale. Secondo le \nultime stime, il 71%  dei terreni agricoli in \nUE è utilizza to per gli allevamenti di \nbestiame, di cui solo l’8% destinato alle \naree di pascolo e agli stabilimenti ; la \nrestante parte diventa terra arabile \ndestinata alla coltivazione di mangime per \ngli animali.20 Allo stesso modo, la \nconsiderevole crescita dell’allevamento in \natto dagli anni ’80 ha determinato un \nsignificativo fenomeno di deforestazione  e \ndi perdita di ettari di aree incontaminate , \nsoprattutto in America Latina.  \n81. Ne consegue che l’ ecological  footprint  di \nqueste attività in termini di consumi di \nsuolo ed erosione degli habitat  assume un \nvalore significativo in un contesto in cui la \ncapacità di rigenerazione degli ecosistemi \nambienta li è sempre più messa a rischio.  \n82. La tutela degli ecosistemi può essere \ngarantita da lla tracciabilità in ogni fase del \nciclo di vita dei prodotti. Oltre ad \nassicurare maggiore food security , le \ncertificazioni ambientali  possono \n20 Fonte: elaborazione The European House – Ambrosetti \nsu dati Eurostat e Commissione Europea, 2019.  \n673,6\n359,8\n294,1\n241,6\n187,2\n162,8\n135,450150250350450550650\n1965 1975 1985 1995 2005 2015 2030\nSud-Est Asiatico Paesi in via di sviluppo Nord Africa/Medio Oriente\nAmerica Latina Mondo Paesi industrializzati\nAfrica Sub-Sahariana100\nFigura 21. Trend  dei consumi di carne nel mondo per \nmacro -area geografica (1965 = 100), 1965 -2030E. Fonte: \nelaborazione The European House – Ambrosetti su dati \nFAO, 2019.  \n15.500 \n6.000 \n3.900 \n1.800 \n1.300 \n1.000 \n700 \n460 \n180 \n130 1kg di carne bovina\n1kg di carne suina\n1kg di pollo\n1kg di zucchero di canna\n1kg di pane\n1l di latte\n1kg di mele\n1kg di arance\n1kg di pomodori\n1kg di insalata\nFigura 22. Water footprint  per tipologia di alimento \n(valori in litri), 2019.  Fonte: elaborazione The European \nHouse – Ambrosetti su dati Water Footprint Network, \n2019 . ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#12": "13 \n sostenere pratiche agricole e di \nallevamento efficienti e sostenibili, in \ngrado di tutelare la salvaguardia e il \nbenessere degli animali e degli ecosistemi \nambientali in terra e in mare (si pensi non \nsolo all’allevamento di bestiame, ma anche \nall’importanza delle acquacolture per la \ntutela dell’ecosistema marino).  \n83. L’ultimo impatto rilevante del settore \nagroalimentare sull’ambiente è quello \nrelativo alle emissioni di gas inquinanti. \nAnche in questo caso, l’impronta più \nelevata è data dalla produzione di carne \nrossa, come presentato  in Figura 2 3. Il \nbestiame è infatti  responsabile del 14,5% \ndei gas serra  generati ogni anno  a livello \nglobale .21 \n84. Anche con riferimento alla relazione tra \nsana alimentazione e sostenibilità \nambientale,  occorre  considerare la \ndieta nel suo insieme  e non l’impatto \ndel singolo alimento . Come le proteine \nanimali possiedono il loro apporto proteico \nutile per un maggior equilibrio nutrizionale \nall’interno di una dieta, così dal punto di \nvista ambientale gli allevamenti di \nbestiame sono in grado di fornire un \ncontributo positivo, ad esempio  \neliminando gli sprechi di residui colturali e \nproducendo concime da usare come \nfertilizzante naturale.  \n85. Nella filiera agroalimentare a valle u n \nparadosso legato alle ricadute ambientali  è \nlegato allo spreco alimentare . La FAO \nstima che un terzo della prod uzione \nannuale mondiale di cibo, equivalente a 1,3 \nmiliardi di tonnellate , finisca nella \nspazzatura, pari a quattro volte la quantità \nnecessaria per nutrire gli 821 milioni di \n                                                 \n21 Per fornire un ordine di grandezza compar abile, la stessa \nquota delle emissioni totali di auto, camion, aeroplani e \nnavi. Fonte: elaborazione The European House – \nAmbrosetti su dati FAO, 2019.  persone affette da denutrizione . \n86. Ogni anno, in Unione Europea, vengono \ngettate 8 8 milioni di tonnellate di cibo  a \nlivello domestico , 98 kg per nucleo \nfamiliare. In Italia, il dato per nucleo \nfamiliare ammonta a 85kg, mentre in \nFrancia a 99kg.22  \n87. Lo spreco provoca inoltre specifiche \nemissioni di gas inquinanti  che \nincrementano ulteriorm ente il carbon \nfootprint  del settore : il gas metano \ngenerato  dal cibo che finisce in discarica è \ncirca 21 volte  più dannoso della CO 2.  \nIl contributo dell’alimentazione ad una \nterza dimensione di sviluppo sostenibile , \nla riduzione delle disuguaglianze  \n88. Le food inequalities  rappresentano uno \ndei principali  gap di accessibilità \nnell’odierna società . Un’alimentazione \nsana è spesso associata a costi più elevati , \ncon il rischi o di accentuare le \ndiseguaglianze  nell’accesso a cibi sani e con \nil corretto apporto nutrizionale . Abitudini \nalimentari sane e sostenibili  (e accessibili) \nsono in grado di ridurre t ali diseguaglianze, \nspezzando quella che può essere definita la \n“spirale socio -ecologica ” entro la quale \nrischiano di essere confinate le fasce di \npopolazione a basso potere di spesa.   \n89. Uno studio dell’Università di Cambridge  \nha dimostrato come 1 .000 calorie derivanti \nda alimenti sani costino oggi intorno ai 9 \nEuro , contro i 3 Euro  necessari per \nottenere l’equivalente apporto calorico dal \ncosiddetto “ cibo spazzatura ”, influenzando \nautomaticamente le scelte dei consumatori \npiù vulnerabili dal punto di vista \neconomico  e sociale . \n22 Fonte: elaborazione The European House – Ambrosetti \nsu dati Coldiretti, FAO e altre fonti, 2019.  \n39,2\n27,0\n12,1\n6,9\n6,1\n4,8\n2,7\n2,0\n1,9\n1,11kg di agnello\n1 kg di carne bovina\n1kg di carne suina\n1kg di pollo\n1kg di tonno\n1kg di uova\n1kg di riso\n1kg di verdura (media)\n1l di latte\n1kg di frutta (media)\nFigura 23.  Carbon footprint  per tipologia di alimento (kg \ndi CO 2 equivalenti), 2019. Fonte: elaborazione The \nEuropean House – Ambrosetti su dati EPA e \nEnvironmental Working Group, 2019.  \nSITUAZIONE DI\nVULNERABILITÀ\nSOCIO -\nECONOMICAABITUDINI ALIMENTARI\nSCORRETTEEFFETTI NEGATIVI SU\nSALUTE , AMBIENTE E\nSISTEMA SANITARIOAGGRAVARSI\nDELLA SITUAZIONE\nDIVULNERABILITÀ\nSITUAZIONE DI\nVULNERABILITÀ\nSOCIO -\nECONOMICAABITUDINI ALIMENTARI\nSANE ESOSTENIBILIEFFETTI POSITIVI SU\nSALUTE , AMBIENTE E\nSISTEMA SANITARIO\nATTENUARSI\nDELLA SITUAZIONE\nDIVULNERABILITÀ\nFigura 24. La spirale socio -ecologica dell’alimentazione.  \nFonte: elaborazione The European House – Ambrosetti, \n2019. ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#13": "14 \n 90. Abitudini  alimentari scorrette provocano a \nloro volta effetti negativi sul benessere \nindividuale e collettivo,  incrementando le \nspese per cure mediche e \nconseguentemente quelle del sistema  \nsanitario , oltre ad essere generalmente \nassociate ad un maggior impatto \nambientale . Tale condizione  porta \nall’aggravarsi della situazione di \nvulnerabilità iniziale , creando così un \ncircolo vizioso .  \n91. Il secondo schema in Figura 2 4 testimonia \ncome una sana e corretta \nalimentazione , a prezzi accessibili,  \nsia in grado di spezzare in parte \nquesto circolo vizioso , con ricadute \npositive per i singoli individui e la società \nnel complesso . \nQuale contributo della ristorazione \ncollettiva e quali proposte d’azione \nper il raggiungimento degli Obiettivi \ndi Sviluppo Sostenibile  \n92. La ristorazione collettiva ha un ruolo \nchiave nella  promozione di scelte \nalimentari sane e sostenibili.  \n93. Gli operatori de l settore svolgono un ruolo \ncruciale poiché  spesso si interfacciano \ncon le fasce di popolazione più \nvulnerabili e strategiche per favorire  \nabitudini alimentari corrette, quali \nbambini  e anziani . La possibilità di \nsostenere un buono stato di salute per \nqueste categorie di individui rafforza \nindirettamente la sostenibilità economica \ndei sistemi sanitari nazionali, per esempio \ncontrastando l’incremento del fenomeno \ndell’obesità infantile o favorendo una \nminor incidenza delle spese sanitarie per \ngli anziani . \n94. La ristorazione collettiva può farso garante \ndi una “ democratizzazione” di stili \nalimentari sani e sostenibili , \ncontribuendo significativamente alla \nriduzione delle disparità sociali.   \n95. La garanzia di affidabilità e qualità di tale \nservizio, che ogni gior no in Europa è \nofferto a 67 milioni di persone, è messa \nperò a dura prova dal contesto in cui il \nsettore sta operando, caratterizzato da tagli \ne strategie di spending review  in settori \ncollegati.   \n96. Il caso italiano è emblematico in tal senso: \nle imprese della ristorazione collettiva del \nPaese servono annualmente 1,65 miliardi \ndi pasti, di cui 657 milioni destinati agli \n                                                 \n23 Fonte: elaborazione The European House – Ambrosetti \nsu dati Angem e Anac, 2019.  oltre un milione e mezzo di ammalati e \nanziani nelle strutture sanitarie.  Da anni gli \ninvestimenti in ristorazione sanitaria sono \nsottoposti a forti contrazioni , anche a \ncausa di interventi di spending review  che \nhanno ridotto del 10% il  corrispettivo sui \ncontratti della sanità e , di conseguenza , \nproporzionalmente ridotto la quantità e \nqualità dei pasti serviti nelle strutture. A \nquesti tagli si è aggiunto l’obbligo di \nadeguamento dei prezzi alle direttive \nstabilite dall’autorità nazionale di \nriferimento: il prezzo medio per un menù \ncomposto da colazione, pranzo e cena è \nstato fissato a 11,74 Euro, inferiore \ndell’8,2% rispetto alla me dia nazionale \nattuale  (12,70 Euro).23  \n97. La ristorazione collettiva – quale operatore \na valle della filiera alimentare – può fornire \nun importante sostegno  anche dal punto di \nvista della riduzione dell’impatto \nambientale, attraverso la lotta allo \nspreco alime ntare , la promozione di \nbuone pratiche di economia circolare  e \nla riduzione dell’utilizzo di packaging  \nnon riciclabile.  \n98. Considerata la  relazione esistente tra sana \nalimentazione e sostenibilità, sia da un \npunto di vista socio -economico sia da un \npunto di vista ambientale, sono state \nformulate alcune proposte d’azione, \nnell’ambito dei dialoghi italo -francesi, per \nla promozione di corretti  stili alimentari.  \nProposta d’azione 1. Fare rete tra Italia e \nFrancia per affermarsi come “Paesi \nambasciatori” di una alimentazione \nsana e sostenibile  \n99. Italia e Francia sono due Paesi dalla forte \ntradizione alimentare , entrambi \naderenti ai principi della d ieta \nmediterranea, allo stesso tempo con \npeculiarità proprie anche a livello regionale \nche ne aumentano l’attrattività \ninternazionale.   \n100. In questo contesto, i due Paesi hanno \nl’opportunità  di assumersi il ruolo di \nportavoce di un’alimentazione sana \ne sostenibile, facendosi \nambasciatori dei benefici della dieta \nmediterranea nel mond o.  \n101. Si riportano di seguito alcune linee \nd’azione per l ’implementazione di questa \nproposta:  \n— stimolare il dialogo tra Italia e Francia \nsui temi dell’alimentazione e della \nsostenibilità, anche attraverso la \ncreazione di “progetti pilota” ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#14": "15 \n cross -border  con il coinvolgimento \ndi aziende della filiera agrifood  \nestesa  (agricoltura, industria di \ntrasforma zione, distribuzione e \nristorazione), università e centri di \nricerca;  \n— farsi portavoce dell’importanza di \nun’alimentazione sana e sostenibile \npresso le principali sedi decisionali \neuropee  in materia, come ad \nesempio la Direzione Generale di \nHealth and Food Safety della \nCommissione Europea, l’Autorità \nEuropea per la Sicurezza Alimentare – \nEFSA, l’ High Level Group on \nNutrition and Physical Activity . \nProposta d’azione 2. Definire una \nstrategia di comunicazione e \nsensibilizzazione congiunta sui benefici \ndi un’al imentazione sana e sostenibile  \n102. Spesso una significativa quota di \npopolazione non ha piena consapevolezza \ndelle ricadute negative di una dieta \nscorretta sulla propria salute e \nsull’ecosistema  ambientale,  una limitazione  \nspesso correlat a all’appartenenza a \ncategorie con bassi livelli di educazione.  \n103. Italia e Francia potrebbero promuovere  \nuna cultura diffusa sull’importanza di \nun’alimentazione sana e sostenibile, \nattraverso il lancio di una strategia multi -\nlivello di comunicazione e sensib ilizzazione.  104. Le linee d’azione inerenti a tale strategia \ncomprendono:  \n— promuovere, con la guida del Governo \nitaliano e francese e il coinvolgimento \ndella filiera agroalimentare estesa, \nun’azione strutturata di \nsensibilizzazione, informazione \ned educazione , verso:  \no l’opinione pubblica, con una \ncampagna nazionale di \ncomunicazione  nei due Paesi \n(“Pubblicità Progresso”), sui media  \ntradizionali e sui social network  per \ndiffondere la consapevolezza dei \nbenefici – per la salute individuale \ned il benessere del pianet a – \nassociati ad un’alimentazione sana \ne sostenibile e/o eventi -bandiera  \nad alta visibilità mediatica;  \no la filiera agroalimentare integrata, \nattraverso partnership  \npubblico -private  (ad esempio, \nconsorzi sulla Ricerca e Sviluppo) \ned iniziative di comunicazione \nmirate  (ad esempio, roadshow  \nterritoriali e /o workshop  tematici \nsettoriali).  \n \n \n \n \n \nUn ringraziamento a:  \n— Eloi Laurent, Professore Ordinario di alimentazione e sostenibilità e Senior Economist e \nConsigliere Scientifico, Observatoire français des conjonctures économiques , SciencesPo  \n— Matteo Caroli, Professore Ordinario di Economia, Universi tà LUISS  \n \n \n \n \n \n ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#2": "3 \n — nonostante la produzione mondiale di \ncibo sia in costante aumento, le \npersone denutrite hanno r ipreso ad \naumentare dal 2015, evidenziando \nquanto sia rilevante il fenomeno dello \nspreco alimentare;  \n— il 60%  della popolazione vive in aree \ncon elevato stress  idrico e 1 miliardo \ndi persone  non ha ancora accesso ad \nacqua potabile;  \n— ogni anno, 8 milioni di tonnellate \ndi plastica finiscono negli oceani. A \nquesto ritmo, la quantità di plastica in \nmare supererà quella dei pesci entro i \nprossimi 30 anni.4 \n10. Questi aspetti sono un  importante  fattore \ndi rischio per la sostenibilità e la \nresilienza del settore \nagro alimentar e. I cambiamenti  climati ci \ndeterminano crescente insicurezza \nalimentare e rischio di denutrizione nelle \nfasce più vulnerabili della popolazione, \nsoprattutto nei Paesi a reddito medio -\nbasso: si pensi alla  perdita delle colture, ad \neventi metereologici estremi che \nproducono siccità o alluvioni , ad epidemie \nlegate a patogeni nel cibo.5  \n11. Diventa fondamentale garantire la \nresilienza a 360°  a livello di sistema \neconomico, energetico, infrastrutturale, \ntecnologico, ma anche di ecosistema \necologico e di ambiente urbano. È proprio \nin questi ultimi due ambiti che la resilienza \npuò essere assicurata e rafforzata da \nmodelli di alimentazione sosten ibili, in \ngrado di contribuire  positivamente alla \ncapacità degli ambienti e degli ecosistemi \ndi resistere a shock  esogeni, di ridurre la \ndipendenza da altri territori e di rispondere \nai cambiamenti climatici e all’esaurimento \ndelle risorse.  \n                                                 \n4 Fonte: elaborazione The European House – Ambrosetti \nsu dati OMS, IPCC, World Bank e ONU, 2019.  \n5 Basti pensare che , nel solo 2018 , il settore agroalimentare \nitaliano ha perso 1,5 miliardi  di Euro  a causa degli effetti \ndel cambiamento climatico. Fonte: elaborazione The \nEuropean House – Ambrosetti su dati Coldirett i, 2019.  Il ruolo dell’alimentazione verso lo \nsviluppo sostenibil e \n12. Come illustrato nel capitolo precedente, \nsono diverse le dimensioni della \nsostenibilità correlate direttamente o \nindirettamente all’alimentazione .  \n13. Una corretta alimentazione contribuisce \nallo sviluppo soste nibile (e alla resilienza \ndegli ambienti urbani ed ecosistemi \nambientali) attraverso due leve \nfondamentali:  \n— salute e benessere  individuale e \ncollettivo ; \n— sostenibilità ambientale .  \nIl contributo dell’alimentazione alla \nsalute e al benessere individuale e \ncollettivo  \n14. Divers i studi medico -scientifici pubblicati \nnegli ultimi anni hanno dimostrato  come \nesista una forte correlazione tra una dieta \nsana ed equilibrata e il mantenimento di un  \nbuono stato di salute. Secondo i dati del \nGlobal Burden of Disease , la \nmalnutrizione  è stata la 1° causa di \nmorte  a livello mondiale nel 2017 (161 \nmorti ogni 100.000 abitanti ). Allo stesso \nmodo, la malnutrizione si posiziona come \n2° fattore di rischio  correlato al \nDALY6 (nel 1990 era il 5°).  \n15. Il modello alimentare  indicato come \nfondamento delle principali diete  è la \npiramide alimentare , intes a come \nl’insieme di regole nutrizionali volte a \ngestire l’alimentazione  nel suo insiem e.  \n6 Disability -Adjusted Life Years : misura della gravità \nglobale di una patologia , espressa come il numero di anni \npersi a causa della malattia in virtù di un cattivo stato di \nsalute, di disabilità o di morte prematura.  \nBASSO\nCarne bovina\nFormaggio\nUova\nCarne avicola\nPesce\nBiscotti\nLatte\nYogurtDolci\nOlio\nFrutta secca\nPane, Pasta\nPatate, Riso\nLegumi\nFrutta\nOrtaggi\nALTO\nFigura 3. La piramide alimentare. Fonte: elaborazione  \nThe European  House – Ambrosetti per Barilla Center for \nFood and Nutrition, 2019.  \n9,910,110,410,710,911,211,511,812,112,512,813,113,414,5\n13,8\n13,1\n12,6\n12,2\n11,8\n11,3\n11,0\n10,710,610,810,9\n2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017\nPersone affette da obesità Persone affette da malnutrizione\nFigura 2. Persone affett e da obesità e malnutrizione nel \nmondo (% sul totale della popolazione), 2005 -2017.  Fonte: \nelaborazione The European House – Ambrosetti su dati \nOMS e FAO, 2019.  ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#3": "4 \n 16. Il valore della piramide alimentare è \nduplice: oltre a rappresentare una sintesi \ndelle principali conoscenze acquisite dalla \nscienza nutrizionale, è uno strumento \nefficace per l’educazione ai consumi \nalimentari.  \n17. È importante sottolineare il concetto di \ndieta  come assunto sottostante alle analisi \ndel presente capitolo. Il dibattito odierno  in \nsedi tecniche quali organismi di controllo e \nregolatori internazionali  è spesso \nincentrato  sul singolo alimento e \nsull’analisi del suo contenuto per la salute \ndelle perso ne. Per quanto gli standard  di \nalcuni alimenti siano essenziali  in termini \ndi apporto di  ingredienti  più o meno \nsalutari, un’analisi focalizzata sul tipo di \ndieta da seguire risulta l’approccio più \nindicato per ottenere una visione d’insieme \nsulla sana e c orretta alimentazione , anche  \nper evitare di demonizzare singole \ncategorie di  alimenti.7 \n18. Il modello di dieta basato sui dettami della \npiramide alimentare e considerato come il \nbenchmark  di riferimento per la salute \ndegli individui è quello mediterraneo. La \ndieta mediterranea  si distingue per il \nsuo equi librio nutrizionale, sia in termini di \nquantità consumate sia di proporzioni \ndegli alimenti assunti. La rigorosa \naderenza alle raccomandazioni medico -\nscientifiche e l’elevata conformità con i \nrequisiti nutrizionali del  modello della \npiramide alimentare fa di questa dieta la \npiù efficace in termini di prevenzione di \nalcune importanti  malattie croniche  \ne di mantenimento di uno stato di \nbenessere . Il modello dietetico \nmediterraneo , attraverso un sano profilo di \nassunzione dei grassi, una bassa \npercentuale di carboidrati, un basso indice \nglicemico, un elevato contenuto di fibre \nalimentari, di composti antiossidanti ed \neffetti antinfiammatori, riduce il rischio di \ncontrarre alcune patologie, come verrà \nanalizzato nei paragrafi successivi.  \n19. Lungo il  bacino mediterraneo europeo, \nItalia  e Francia  rappresentano i Paesi in \ncui le tradizioni alimentari nazionali hanno \nmaggiore aderenza  con la dieta \nmediterranea , insieme a Spagna e Grecia . \nPrima di entrare nel dettaglio dell’impatto \ngenerato dall’alimentaz ione sui sistemi \nsanitari nazionali , è essenziale sviluppare \ndue ulteriori premesse , che riguardano \nl’evoluzione demografica  della popolazione  \n                                                 \n7 Il tipico esempio in questo senso è la carne. La filiera della \ncarne rischia troppo spesso di essere danneggiata da \ncampagne che ne co ndannano l’assunzione, mentre il suo e la sostenibilità  economica dei sistemi \nsanitari .  \n20. Tra i grandi cambiamenti che stanno \nmutando gli scenari di riferimento per la \nbusiness community  e le Istituzioni  non si \npossono non considerare  i trend  \ndemografici  che stanno sostanzialmente \nmodificando la struttura dell a popolazion e \ndei Paesi sviluppati e , di conseguenza , le \nesigenze da considerare prioritarie per lo \nsviluppo sostenibile.  \n21. Analizzando il  caso italiano, il Paese sta \nassistendo ad un progressivo \ninvecchiamento della popolazione, un \ntrend  destinato a d acuirsi nei prossimi \nanni a fronte  dell’innalzamento della \nsperanza di vita ( 82,7  anni nel 2017, vs. \n69,1 nel 1960 ) e dei bassi tassi di natal ità \n(1,35 figli per donna nel 2017, vs. 2,37 nel \n1960).  Nel 2018, per la prima volta nella \nstoria italiana, la popolazione over -60 \n(28,7%  vs. 21,2% nel 1990) ha superato \nquella dei giovani under -30 (28,4%  vs. \n39,6% nel 1990).8  \n22. La ragione per cui è importante guardare a \nquesto tipo di trend  demografici in uno \nstudio sull’alimentazione e la salute delle \npersone risiede nelle differenti esigenze \nnutrizionali che gli individui detengono  al \nvariare delle fasce  d’età  di appartenenza , \nsoprattutto guardando alle categorie più \nvulnerabili, a partire da bambini ed anziani. \nL’invecchiamento della popolazione \nitaliana è quindi un aspetto da tenere in \nconsiderazione, in quanto le persone \nanziane necessitano di un fabbisogno \ncalorico  – e di co nseguenza di un modello \ndietetico – diverso e più limitato . \n23. In Francia si riscontrano trend  demografici \nsimili, anche se meno marcati rispetto al \ncaso italiano. La piramide demografica di \nconsumo – se equilibrato  – è presente e caldamente \nconsigliato nella piramide alimentare.  \n8 Fonte: elaborazione The European House – Ambrosetti \nsu dati Istat e World Bank, 2019.  \n8,6%\n9,5%\n10,3%\n11,9%\n15,5%\n15,2%\n12,1%\n9,7%\n5,7%\n1,2%0-9\n10-19\n20-29\n30-40\n40-49\n50-59\n60-69\n70-79\n80-89\n90+9,9%\n13,5%\n16,2%\n14,0%\n13,0%\n12,4%\n11,1%\n6,6%\n3,1%\n0,3%\nFigura 4. Popolazione italiana per fascia d’età (% sul \ntotale), 1990 (a sinistra) e 2018 (a destra).  Fonte: \nelaborazione The European  House – Ambrosetti su dati \nIstat, 2019. ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#4": "5 \n Figura 5 mostra  come la quota di under -30 \nsul totale della popolazione sia diminuit a \ndal 42,5%  del 1990 al 35,4%  del 2018; gli \nover -60, invece, costituivano il 19,1% della \npopolazione nel 1990, mentre ad oggi \npesano per il 26,2% . \n24. Questa tendenza, comunemente \nconosciuta come  Ageing Society , è \ndestinat a a continuare nel prossimo futuro, \nincrementando la pressione per la  \nsostenibilità economica dei sistemi \nsanitari  nazionali . È indubbio che una \npopolazione sempre più anziana \nrappresenti già oggi una sfida molto \nsignificativa in ambito sanitario (oltre che \nin quello previdenziale), considerando  che \nl’allungamento della vita media sta \nmodificando la natura epidemiologica \ndella popolazione. Nella maggior parte dei \nPaesi OCSE, oltre il 50%  degli over -65 è \naffett o da una malattia cronica  e l’età \nmedia dalla quale si manifestano le prime \npatologie è in costante calo. Q uesto \ninfluisce profondamente sulla qualità della \nvita media di u na popolazione : in Italia, da \ninizio secolo , sono stati persi  2,5 anni  \nvissuti in buona salute, che ad oggi sono \ncirca  62.9 \n25. L’insieme di questi fattori rappresenta un \nserio rischio per i sistemi sanitari nazionali , \ngravati negli ultimi anni da costi crescenti. \nIn Italia e in Francia la spesa sanitaria  \n(pubblica e privata) ha registrato un \naumento rispettivamente del 6,4% e del \n17,3%  dal 2010 al 2017, un trend  \nconfermato anche negli altri “ Big Five ” \ndell’Unione Euro pea (Figura 6).  \n26. L’alimentazione gioca un ruolo chiave in \nquesto contesto , poiché strettamente \ncorrelata allo stato di salute delle persone e \nai costi ad esso associati .  \n                                                 \n9 Fonte: elaborazione The European House – Ambrosetti \nsu dati  OECD e  XIII Rapporto Meridiano Sanità  di The \nEuropean House – Ambrosetti , 2019.  27. La cattiva alimentazione si posiziona tra i \nprimi cinque fattori di rischio  nella \ncontrazione delle  tre malattie croniche non \ntrasmissibili più impattanti  per i sistemi \nnazionali  in quanto responsabili del 94% \ndei decessi  globali :  \n— i tumori , per i quali  la malnutrizione  \nè la 2° causa di morte a livello globale, \nla 4° in Italia e in Fran cia, e la 1° causa \ndi DALY nel mondo, 2° in Italia e in \nFrancia;  \n— le malattie cardiovascolari , per le \nquali è la 2° causa di morte a livello \nglobale, così come in Italia e in Francia, \ne la 2° causa di DALY al mondo, la 4° \nin Italia e in Francia;  \n— il diabete , dove è la 5° causa di morte \na livello mondiale, così come in Italia \ne in Francia, e la 4° causa di DALY \nglobalmente, terza in Italia e in \nFrancia.10 \n28. Si tratta di malattie ad altissimo impatto \nper il sistema sanitario , sia per il numero di \npersone che ne sono affette , sia per i costi \nsanitari e sociali associati .11  \n29. Alla base di queste tre malattie croniche ci \nsono due tipi di fattori di rischio:  \n— non modificabili  (età e predisposizione  \nereditaria );  \n— modi ficabili (tra i quali  rientrano \ndieta scorretta e ipercalorica , \ninsufficiente attività fisica e consumo \ndi tabacco ).  \n30. All’intern o dei fattori modificabili, le \nabitudini che incorporano una dieta \nscorretta e ipercalorica – talvolta unit e ad \nun’insufficiente attività fisica – aumentano \nle probabilità dell’insorgere di condizion i \nfisic he di sovrappeso o obesità .  \n10 Fonte: elaborazione The European House – Ambrosetti \nsu dati Global Burden on Disease, 2019.  \n11 In Italia, ad esempio, le tre patologie rientrano tra le \nprime cinque voci di costo per il sistema sanitario nazionale.  \n11,7%\n12,4%\n11,3%\n12,4%\n12,9%\n13,2%\n12,0%\n8,1%\n4,8%\n1,3%0-9\n10-19\n20-29\n30-40\n40-49\n50-59\n60-69\n70-79\n80-89\n90+13,3%\n13,9%\n15,3%\n15,0%\n12,8%\n10,5%\n10,1%\n5,3%\n3,2%\n0,5%\nFigura 5. Popolazione francese per fascia d’età (% sul \ntotale), 1990 (a sinistra) e 2018 (a destra).  Fonte: \nelaborazione The European House – Ambrosetti su dati \nInsee, 2019.  \n146,8\n129,6\n117,3\n106,4\n105,5\n90100110120130140150\n2010 2011 2012 2013 2014 2015 2016 2017\nRegno Unito Germania Francia Italia Spagna\nFigura  6. Spesa sanitaria pubblica e privata nei Paesi “ Big \nFive ” dell’UE (2010 = 100), 2010 -2017.  Fonte: \nelaborazione The European House – Ambrosetti su da ti \nOECD, 2019. ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#5": "6 \n 31. L’Organizzazione Mondiale della Sanità \ndefinisce l’obesità e il sovrappeso \nattraverso il Body Mass Index  – BMI, un \ndato biometrico che mette a co nfronto peso \ne altezza: sono considerati affetti da obesità  \ni soggetti con un BMI maggiore di 30 kg/m2, \nmentre gli individui con un BMI compreso \ntra 25 e 30 kg/m2 sono ritenuti in \ncondizioni di sovrappeso.   \n32. L’eccesso di peso può portare effetti \nnegativi  sulla salute e sull’aspettativa di \nvita degli individui : l’OCSE ha stimato \ncome una persona gravemente obesa perda \nin media dagli 8 ai 10 anni di vita  e come \nper ogni 15 kg in eccesso, il rischio di \ndecesso increment i del 30% .  \n33. L’Italia è uno dei Paesi dell’Unione \nEuropea  con la più bassa incidenza di \npersone sovrappeso/obese, anche se la \nquota rimane elevata: il 31,7%  della \npopolazione italiana adulta (età uguale o \nsuperiore a 15 anni) è in sovrappeso, contro \nil 46,1% della medi a UE, mentre il 10,7% è \naffetto da obesità.  \n34. In Francia l’incidenza delle persone \nsovrappeso sale al 48,9%  (+2,8 punti \npercentuali rispetto alla media dell’Unione \nEuropea ), mentre l’obesità colpisce il \n                                                 \n12 Fonte: elaborazione The European House – Ambrosetti \nsu dati Università Tor Vergata, 2019.  15,3%  della popolazione adulta (+4,6 \npunti percentuali al di sopra della media \nUE).  \n35. Oltre alle conseguenze negative sulla salute, \nle condizioni di obesità/sovrappeso \nconducono a numerose problematiche in \ntermini economici a diversi livelli, a partire \nda quello del singolo individuo affetto dalla \npatologia (e d el suo nucleo familiare), fino \nalle imprese e ai governi, che si ritrovano \nad affrontare ingenti spese sanitarie . È \nstato stimato infatti come una persona \naffetta da obesità  con un BMI compreso tra \n35-40 kg/m2 costi annualmente in media il \n50%  in più  al sistema sanitario  rispetto a \nuna persona normopesa , fino a \nraggiungere il 100% in più per le persone \ncon un BMI maggiore di 40 kg/m2.12  \n36. I bambini e gli adolescenti sono le categorie \ndi popolazione maggiormente esposte ai \nrischi derivanti da un’alimentazione \nscorretta  poiché  facilmente influenzabili \ndalle cattive abitudini delle persone vicine \na loro : i bambini con almeno uno dei due \ngenitori affetti da obesità , infatti, \npresentano probabilità 3-4 volte superiori \ndi essere affetti da obesità  rispetto ai \nbambini con genitori normopeso. Circa  il \n70%  dei bambini obesi rimarrà  in questa \ncondizione anche da adulto.13  \n37. L’obesità infantile in Italia è un fenomeno \ndi assoluta rilevanza: nel 2016 , il 14% delle \nbambine e il  21% dei bambini  tra i 6 -9 anni \nrisulta essere in sovrappeso o obeso , \nrispettivamente 4 e 8 punti percentuali \nsopra la media dell’Unione Europea .  \n \n38. In Francia, il fenomeno è presente in \nmisura minore, e colpisce il 6% delle \nbambine e il  9% dei bambini  (4 punti \npercentuali  sotto la media UE).  \n13 Fonte: elaborazione The European House – Ambrosetti \nsu dati OMS, 2019.  \n44%\n7-41%\n23%Diabete\nTumori\nMalattie\ncardiocircolatorie\nFigura  7. Malattie croniche attribuibili a condizioni di \nobesità/sovrappeso (% sul totale), 2015.  Fonte: \nelaborazione The Euro pean House – Ambrosetti su dati \nOMS e XIII Rapporto Meridiano Sanità di The European \nHouse - Ambrosetti, 2019.  \n10,7%\n12,3%\n12,8%\n14,7%\n14,9%\n15,3%\n15,9%\n16,6%\n16,7%\n17,0%\n18,6%\n23,0%\n23,6%\n24,8%\n26,9%Italia\nSvezia\nPaesi Bassi\nAustria\nDanimarca\nFrancia\nUE-28\nPortogallo\nSpagna\nGrecia\nBelgio\nIrlanda\nGermania\nFinlandia\nRegno Unito\nFigura  8. Tasso di obesità nella popolazione adulta in \nalcuni Paesi selezionati UE e media UE (% sul totale della \npopolazione), 2017.  Fonte: elaborazione The European \nHouse – Ambrosetti su dati Eurostat e OECD, 2019.  \n17%\n14%\n14%\n11%\n10%\n9%\n6%\n7%\n6%\n5%\n5%19%\n21%\n20%\n12%\n13%\n11%\n12%\n10%\n9%\n9%\n5%Spagna\nItalia\nGrecia\nPortogallo\nUE-28\nFinlandia\nAustria\nSvezia\nFrancia\nIrlanda\nDanimarca\nFemmine MaschiFigura 9.  Sovrappeso/obesità in età infantile nei \nprincipali Paesi UE e media UE (% sul totale dei bambini \ntra i 6 -9 anni), 2016.  Fonte: elaborazione The European \nHouse – Ambrosetti  su dati OMS , 2019.  ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#6": "7 \n 39. La prevenzione , sottoforma di \neducazione alla sana alimentazione , \nassume  un ruolo fondamentale in questo \nambito.  Da analisi di diversi  studi in campo \nmedico è stato  dimostrato come un \naumento dell’1% del ratio  della spesa in \nprevenzione  – in cui possono rientrare \ninvestimenti per il miglioramento delle \nabitudini nutrizionali delle persone – sul \ntotale della spesa sanitaria pubblica si \ncorreli ad una riduzione del 3% della spesa \nin servizi terapeutici e di riabilitazione.   \n40. Una sana e corretta alimentazione ha un \nruolo fondamentale anche come fattore \ndi prevenzione .  Come illustrato ad \ninizio capitolo, la piramide alimentare \nrappresenta il benchmark  su cui basare \nuna di eta salutare ed equilibrata: alla base \ndella piramide – e di conseguenza alla base \ndella dieta mediterranea – troviamo \nfrutta e verdura , il cui consumo \nquotidiano è il prerequisito fondamentale \nper uno stile di vita sano.  \n41. Secondo l’Organizzazione Mondiale della \nSanità, con un consumo di 600 grammi \ndi frutta e verdura al giorno – equivalente \na oltre  5 porzioni – si eviterebbero soltanto \nin Unione Europea 135.000 decessi, 1/3 \ndelle malattie coronariche e l’11% degli \nictus.  \n42. Si registra  una correlazione positiva  \ntra aspettativa di vita e consumo di \nfrutta/verdura  nei Paesi dell’Unione \nEuropea .  \n43. Da questa elaborazione  si evince come la \nquasi totalità dei Paesi UE che adottano la \ndieta mediterranea come modello \nnutrizionale di riferimento  risieda nel \nquadrante in alto a destra: Italia, Francia, \n                                                 \n14 Fonte: elaborazione The European House – Ambrosetti \nsu dati Sorveglianza Passi, Istituto Superiore di Sanità  e \nEurostat, 2019.  Spagna, Grecia, Portogallo . Oltre alla \nposizione geografica lungo il bacino \nmediterraneo, esiste un dato oggettivo che \nconvalida  le abitudini alimentari comuni \ntra i Paesi in oggetto: dal 2010 l’Unesco ha \nriconosciuto la dieta mediterranea come \nbene protetto e inserito nella lista dei \npatrimoni orali e immateriali dell’umanità , \ngrazie alla sua unicità nel prevenire \nmalattie croniche . \n44. Le raccomandazioni sull’apporto \ngiornaliero di frutta e verdura \ndell’Organizzazione Mondiale della Sanità \ne d ei singoli Ministeri della Salute \nnazionali sono di assumerne 5 porzioni al \ngiorno, equivale nti a circa 400 grammi . La \nquota di popolazione  che raggiung e i \nconsumi suggeriti  è però risibile: i n Italia  \nsolo il 9,6%  della popolazione consuma 5 e \npiù porzioni di frutta e verdura, mentre in \nFrancia la percentuale sale al 14,9% . \n45. I consumi di frutta e verdura sono correlati \nalle caratteristiche sociodemografiche ed \neconomiche della popolazione. Il consumo \ndelle 5 porzioni raccomandate  cresce \nsolitamente con l’avanzare dell’età, è \nmaggiore nelle donne e tra le persone  con \nun maggior livello di istruzione o maggiore \ndisponibilità economica .14 \n46. Anche per i Paesi più virtuosi nelle \nabitudini alimentari , si sta gradualmente \nassistendo ad una minor aderenza  alla \ndieta mediterranea (persino in Italia) a \nfavore di cibi pronti, più veloci da cu cinare \nma meno salutari  e con un alto contenuto \ndi grassi .  \n47. La concomitante presenza di quest a \nmolteplicità di  elementi  rappresenta una \nsignificativa fonte di stress  per i sistemi \nsanitari nazionali,  sia a livello gestionale -\noperativo sia economico . \n48. Sebbene calcolare il peso economico totale \ndel fenomeno  sia complesso , diversi studi \nhanno fornito delle stime sui costi  associati \na condizioni di obesità/sovrappeso : \n— in Unione Europea, il costo è stimato \nintorno ai 70 miliardi  di Euro  \nannui , tra spesa dei sistemi sanitari e \ncosti indiretti;  \n— la spesa  totale dell’obesità/sovrappeso \nper il sistema di sanità pubblica \nitaliano si attesta tra i 6 e i 16 \nmiliardi di Euro all’anno  (4-10% \ndella spesa sanitaria del Paese), di cui \nla metà impiegati direttamente nella \nFigura 10. Correlazione tra speranza di vita alla nascita e \nconsumo di frutta e verdura nei Paesi UE (età (asse y) e quota % \ndi popolazione che consuma almeno una volta al giorno una \nporzione di frutta e/o verdura (asse x)), 2018.  Fonte: \nelaborazione The European House – Ambrosetti su dati \nEurostat, 2019.  \nBelgioPortogallo\nRegno UnitoItaliaSpagna\nSlovenia\nCroaziaGreciaAustriaCipro\nPolonia\nUngheriaIrlanda\nUE-28Francia\nEstoniaMalta\nLussemburgoSvezia\nDanimarca\nLituaniaFinlandia\nGermaniaPaesi Bassi\nRep. Ceca\nSlovacchia\nLettonia Bulgaria\n7475767778798081828384\n40,0 45,0 50,0 55,0 60,0 65,0 70,0 75,0 80,0 85,0Speranza di vita alla nascita\nQuota di popolazione che consuma frutta e/o verdura quotidianamente",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#7": "8 \n cura dei problemi di salute correlati a \ntali condizioni, cui si sommano \nulteriori 3 -8 miliardi di Euro per i costi \nindiretti non sanitari, quali perdita di \nproduttività, assenteismo e mortalità \nprecoce ;15 \n— in Francia, il dato di costo sociale \n(diretto + indiretto) è stato stimato dal \nTesoro intorno a 20 miliardi  di \nEuro .  \n49. Nei prossimi paragrafi verranno invece \nanalizzate l e ricadute negative  della \nmalnutrizione  per la sostenibilità \neconomica dei sistemi sani tari nazionali \nattraverso un focus sulle  principali \nmalattie croniche ad essa correlate. Il \ndettaglio riguarderà gli impatti di  diabete, \nmalattie cardiovascolari e tumori per i 5 \nprincipali Paesi UE (Italia, Francia, \nGermania, Spagna e Regno Unito) e a \nlivello comunitario.  \n50. Il diabete è una malattia cronica legata al \nmetabolismo, quindi strettamente \ncorrelata alle  abitudini alimentari degli \nindividui . Obesità/sovrappeso e dieta \nsquilibrata, insieme a insufficiente attività \nfisica e pressione alta, sono condizioni che \naumentano esponenzialmente le \nprobabilità di contrarre tale patologia. \nOltre ad essere una malattia  ad alto \nimpatto per la salute della persona, la \npresenza del diabete può favorire lo \nsviluppo di altre patologie non trasmissibili, \ncome le malattie cardiovascolari.  \n51. Secondo l’International Diabetes \nFederation , ad oggi 425 milioni di \nadulti (20-79 anni) ne sono affetti  nel \nmondo , proporzione destinata ad \naumentare a quasi 650 milioni entro il \n2050. In Unione Europea, sono circa 60 \nmilioni  le persone affette da diabete.  \n                                                 \n15 Fonte: elaborazione The European House – Ambrosetti \nsu dati  European  Center for International Political 52. In Italia, la prevalenza del diabete è in \ncontinuo aumento: nel 2002 la patologia \ncolpiva il 3,9% della popolazione, mentre \nnel 2017 il dato è salito al 5,7%  (3,4 milioni \ndi persone), che diventa 7,6%  \nconsiderando la sola popolazione adulta . \n53. I casi di diabete in Francia nel 2017 sono \ninvece 3,3 mi lioni, con una prevalenza del \n7,3%  sul totale della popolazione.  \n54. La patologia  rappresenta un problema \nsanitario molto diffuso in Europa anche \nconsiderati gli elevati costi di \ntrattamento  associati, solitamente \nsostenuti per ospedalizzazioni, farmaci, \nmoni toraggio e prestazioni specialistiche.  \n55. Tali costi sono soggetti a grandi variazioni \nda Paese a Paese. Come illustrato in Figura \n12, le spese medie per paziente in Unione \nEuropea sono di 2.834  Euro , in Italia il \ndato è leggermente superiore ( 2.934  Euro ), \nmentre in Francia le spese sostenute sono \nestremamente al di sopra della media, \nraggiugendo un importo di 5.342  Euro .  \n56. Al fine di quantificare  in modo completo gli \nimpatti economici derivanti da  tale \npatologi a, è necessario includere  anche \nquei costi che non rientrano nel computo \ndelle voci direttamente imputabili al \nsistema sanitario , ma che producono \nimportanti ricadute sui sistemi nazionali \ndovuti alla perdita di produzione  \ncausata dalla malattia.  La dimensione \nindiretta, non facilmente identificabile a \nlivello quantitativo poiché  include aspetti \nquali perdita di produttività o assenze \nlavorative, detiene un peso ugualmente \nsignificativo per la sostenibilità economica \ndi un Paese.  \n57. Prendendo in esam e il caso italiano del \ndiabete, il cui costo diretto cumulato \nammonta a 9,6 miliardi  di Euro , la \nEconomy,  Fondazione Polic linico Tor Vergata  e \nSorveglianza  Passi , 2019.  \n7.476.800\n3.584.5003.809.1193.402.300 3.276.40012,2% 10,4%\n8,1%7,6%7,3%\n01. 000. 00 02. 000. 00 03. 000. 00 04. 000. 00 05. 000. 00 06. 000. 00 07. 000. 00 08. 000. 00 0\n0, 0%2, 0%4, 0%6, 0%8, 0%10, 0%12, 0%14, 0%\nGermania Spagna Regno Unito Italia Francia\nNumero malati Prevalenza nella popolazione adulta\nFigura 11. Popolazione affetta da diabete e prevalenza \nnella popolazione adulta nei P aesi “ Big Five ” dell’UE \n(numero persone e % sul totale della popolazione adulta), \n2018.  Fonte: The European House – Ambrosetti su dati \nInternational Diabetes Federation e Diabetes UK, 2019.  \n5.899 \n5.342 5.107 \n2.934 2.834 \n2.362 \nGermania Francia Regno\nUnitoItalia UE-28 Spagna€17,5 mld €9,6 mld €8,5 mld €18,9 mld €44,1 mldValori \ncumulati€170 mld\nFigura 12. Costi diretti del diabete per paziente e cumulati \nnei Paesi “ Big Five ” dell’UE e media UE (valori in €), 2016.  \nFonte: elaborazione The European  House – Ambrosetti su \ndati XIII Rapporto Meridiano Sanità di The European \nHouse – Ambrosetti, International Diabetes Federation e \nLondon School of Economics, 201 9. ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#8": "9 \n stima  di costi indiretti associati alla \npatologia raggiunge un valore di 10,7 \nmiliardi  di Euro , pesa ndo per il 53% \ndell’ intero impatto economico sul sistema -\nPaese (20,3 miliardi  di Euro ). \n58. Come evidenziato, la presenza di diabete \npuò essere un campanello d’allarme per \nl’insorgere di altre malattie non \ntrasmissibili, ad esempio  le malattie \ncardiovascolari. A livello UE, le malattie \ncardiovascolari rappresentano la 1° causa \ndi morte  (37% sul totale dei decessi  – \noltre 1,8 milioni)  e la 2° in Italia (37,1% dei \ndecessi) e in Francia  (24,5% dei decessi) .  \n59. Oltre  al diabete, tra i principali fattori di \nrischio di queste malattie vi sono elevati \nlivelli di colesterolo , condizioni di \nobesità/sovrappeso16 e, più in generale, \nstili di vita inadeguati, come elevato \nconsumo di alcool e tabacco , dieta \nsquilibrata  e ridott a attività fisica . Molti  \nfattori sono  influenzati da un elemento \ncomune, cioè abitudini alimentari  \nscorrette . Tra le più diffuse, elevati \nconsumi di sale e zucchero incrementano \nnotevolmente le probabilità di contrarre \ntali patologie .  \n60. Una corretta dieta f unge sia da strumento \ndi prevenzione per le malattie \ncardiovascolari sia da principale linea \nguida per la convalescenza post -operatoria \n                                                 \n16 L’Organizzazione Mondiale della Sanità stima come il \n23% delle malattie ischemiche del cuore siano provocate da \neccesso ponderale.  e la successiva quotidianità del paziente. \nNella convalescenza post -infarto , ad \nesempio, è stato dimostrato come una \nregolare e costante aderenza ai principi \ndella dieta mediterranea riduca del 18% il \nrischio di decesso  correlato all’occorrenza \ndi tale evento patologico .   \n61. Le malattie cardiovascolari sono una \nproblematica molto rilevante in termini \nepidemiologici e sociali,  che si traduce \nanche in costi economici per i sistemi \nsanitari e previdenziali. Le stime a  livello \neuropeo  attestano  l’ammontare di tali costi \noltre i 200 miliardi  di Euro , di cui circa \n113 miliardi di Euro direttamente associati \nal trattamento patologico e la restante \nparte legata alla perdita di produttività e \nalle spese sostenute dal sistema \nprevidenziale responsabile di fornire \nprestazioni assistenziali sottoforma di \npensioni di inabilità e assegni di inv alidità . \nQuest’ultima voce di costo è estremamente \nonerosa per i sistemi previdenziali \nnazionali: a titolo di esempio, secondo \nun’analisi condotta dall’Università Tor \nVergata di Roma in collaborazione con \nl’INPS, l’istituto previdenziale italiano ha \nerogat o il 19% delle proprie prestazioni dal \n2009 al 2015 per malattie del sistema \ncardiocircolatorio (seconda voce di costo \ndopo i tumori) , per un totale di 13,7 \nmiliardi  di Euro  ed una spesa annua di \noltre 1,9 miliardi  di Euro . \n62. Guard ando alle spese dirette (Fi gura 1 5), \nl’impatto economico delle malattie \ncardiovascolari in Europa ha quindi un \npeso molto rilevante: in Italia, il costo \nmedio pro-capite  per il loro trattamento \nammonta a 235  Euro , valore che scende a \n207 Euro per il sistema sanitario francese.  \n€ 20,3 mld\n€ 9,6 mld€ 10,7 mldCosti indiretti\n53%\nCosti diretti\n47%\nVoce di costo\n▪Ospedalizzazioni = €5,1 mld\n▪Farmaci = €2,1 mld\n▪Monitoraggio = €1,3 mld\n▪Prestazioni specialistiche = €1,1 mldVoce di costo\n▪Pensionamento \nanticipato = €9,1 mld\n▪Assenza dal lavoro = \n€1,6 mld\nFigura 13. Dettaglio dei costi diretti e indiretti del diabete \nin Italia (valori in €), 2017.  Fonte: elaborazione The \nEuropean  House – Ambrosetti su dati XIII Rapporto \nMeridiano Sanità di The European House – Ambrosetti, \n2019. \n357.788 \n238.460 \n157.690 145.291 123.384 38,6%37,1%\n26,3%24,5%29,3%\n - 5 0. 000 1 00. 000 1 50. 000 2 00. 000 2 50. 000 3 00. 000 3 50. 000 4 00. 000\n0, 0%5, 0%10, 0%15, 0%20, 0%25, 0%30, 0%35, 0%40, 0%45, 0%\nGermania Italia Regno Unito Francia Spagna\nNumero decessi % sul totale dei decessi\nFigura 14. Numero decessi causati da malattie \ncardiovascolari e quota sul totale dei decessi nei Paesi “ Big \nFive ” dell’UE (numero e % sul totale), 2015.  Fonte: \nelaborazione The European House – Ambrosetti su dati \nEurostat, 2019.  \n413\n313\n235 223207\n130\nGermania Regno\nUnitoItalia UE-28 Francia Spagna€20,7 mld €113,3 mld €6,1 mld €14,2 mld €34,2 mldValori \ncumulati€13,9 mld\nFigura 15. Costo pro -capite e cumulati delle malattie \ncardiovascolari nei Paesi “ Big Five ” dell’UE e media UE \n(valori in €), 2016.  Fonte: elaborazione The European \nHouse – Ambrosetti su dati O MS, Center for Economic \nand Business Research e XIII Rapporto Meridiano Sanità \ndi The European House – Ambrosetti, 2019.  ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Alimentazione-e-sostenibilità-Quale-relazione-e-quali-sfide-per-i-prossimi-anni.pdf#9": "10 \n 63. La terza patologia cronica non \ntrasmissibile in analisi  sono i tumori. \nNonostante il tasso di mortalità stia \ngradualmente diminuendo, i tumori \nrestano la seconda causa di morte in UE, \ncon circa 1,3 milioni di decessi (26,4% sul \ntotale), e la quarta in Ital ia e in Francia.  \n64. Al contrario di altre patologie per le quali i \nfattori di rischio sono più facilmente \nidentif icabili, non esistono quasi mai \nsingole cause che possono spiegare \nl’insorgenza di un particolare tipo di \ntumore, in quanto sono diversi i fattori  – \ntalvolta sovrapposti – che concorrono al \nsuo sviluppo, alcuni dei quali non \nmodificabili . Oltre ad età e pr edisposizione \nereditaria (“familiarità”), elementi sui \nquali non è possibile intervenire, i fattori \nmodificabili sono maggiormente legati \nanche in questo caso agli stili di vita .  \n65. Come mostrato in Figura 1 7, nella lista dei \nprincipali fattori di rischio per i tumori, \nsono tre quelli strettamente correlati \nall’alimentazione, ancora una volta \nelemento  fondamentale  per il benessere \npersonale . \n                                                 \n17 Fonte: elabor azione The European House – Ambrosetti \nsu dati Associazione Italiana Registri Tumori , Ligue Contre \nle Cancer  e Cancer Organisation Soutien, 2019.  66. Nonostante la mortalità per tumore sia in \nlenta ma graduale  diminuzione  grazie alla \nscoperta di cure sempre più adeguate   \n(-0,7% annuo in Italia e -0,3% in Francia) , \nil numero di malati è in costante \naumento : \n— negli ultimi decenni si è registrato in \nItalia un trend  in rialzo della \nprevalenza di pazienti con una storia \ndi cancr o, da 2,2 milioni nel 2006 a \ncirca  3,4 milioni nel 201 8. Ogni anno \nil numero di malati oncologici cresce \ndi oltre 90.000 persone (+3%);  \n— nel periodo 2007 -2016, in Francia, \nsono stati diagnosticati in media oltre \n356.000 nuovi casi di tumore ogni \nanno , ma ne l 2016 le nuove diagnosi \nsono state 385.000.17 \n67. I trend  evidenziati sono da un lato dovuti \nall’intensificazione di fattori quali \ninvecchiamento demografico, peggiora -\nmento negli stili di vita quotidiani e \naggravarsi della situazione ambientale e , \ndall’altro , ai progressi ottenuti nelle terapie, \nche migliorano la sopravvivenza nei \npazienti e cronicizzano la malattia. Questi  \nfattori incrementa no lo stress  economico \ndei sistemi sanitari in termini di costi \ndiretti, soprattutto dovuti alla durata delle \nterapie e dei successivi controlli periodici.  \n68. I dati di spesa pro-capite  diretta sostenut a \nper il trattamento dei tumori in Figura 1 8 \nrestituiscono un valore di 114  Euro  per \nl’Italia e di 110  Euro  per la Francia, \nleggermente al di sopra della media UE di \n102 Euro , il cui costo cumulato diretto \nammonta a 51,8 miliardi  di Euro . \n226.662 \n169.835 164.411 162.045 \n107.083 24,4%26,4%27,4% 27,3%\n25,4%\n - 5 0. 000 1 00. 000 1 50. 000 2 00. 000 2 50. 000\n20, 0%22, 0%24, 0%26, 0%28, 0%30, 0%\nGermania Italia Regno Unito Francia Spagna\nNumero decessi % sul totale dei decessi\nFigura 16. Numero decessi causati da tumori e qu ota sul \ntotale dei decessi nei Paesi “ Big Five ” dell’UE (numero e % \nsul totale), 2015.  Fonte: elaborazione The European House \n– Ambrosetti su dati Eurostat, 2019.  \nFattore di rischio Valore\nTabacco 33%\nDieta 5%\nSovrappeso e obesità 20%\nInattività fisica 5%\nAbuso di bevande alcoliche 3%\nFattori occupazionali 5%\nInfezioni 8%\nRadiazioni ionizzanti e esposizione a raggi UV 2%\nInquinamento ambientale 2%\nFigura 17. Principali fattori di rischio per i tumori (% sul \ntotale), 2017.  Fonte: elaborazione The European House – \nAmbrosetti su dati American Associations for Cancer \nResearch, 2019.  \n182\n114 11010290 85\nGermania Italia Francia UE-28 Spagna Regno\nUnito€6,9 mld €51,8 mld €5,6 mld €7,4 mld €15,0 mldValori \ncumulati€4,2 mld\nFigura 18. Costi pro -capite e cumulati dei tumori nei \nPaesi “ Big Five ” dell’UE e media UE (valori in €), 2016.  \nFonte: elaborazione The European House – Ambrosetti su \ndati XIII Rapporto Meridiano Sanità di The European \nHouse – Ambrosetti, ministeri nazionali della salute  e \nOMS, 2019.   ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#0": "",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#1": "Una sana alimentazione rappresenta il \nprimo intervento di prevenzione a tutela \ndella salute e dell’armonia fisica.\n“La salute è uno stato di completo benessere fisico, \nmentale e sociale e non la semplice assenza di \nmalattia o infermità”\nOrganizzazione Mondiale della Sanità",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#10": "",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#2": "LINEE GUIDA PER UNA SANA ALIMENTAZIONE ITALIANA\n( MIPAAF -INRAN )\n1.Più cereali, legumi, ortaggi e frutta\n2.I grassi: scegli la qualità e limita la quantità\n3.Zuccheri, dolci e bevande zuccherate: nei giusti limiti\n4.Bevi ogni giorno acqua in abbondanza\n5.Il sale? Meglio poco\n6.Bevande alcoliche: se si, solo in quantità controllata\n7.Controlla il peso e mantieniti sempre attivo\n8.Varia speso le tue scelte a tavola\n9.Consigli speciali per persone speciali\n10.La sicurezza dei tuoi cibi dipende anche da te\n",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#3": "L’ORGANISMO È UNA MACCHINA BIOCHIMICA CHE CONSUMA CARBURANTE\nAnche se a riposo, il corpo umano impiega comunque energia \nper il funzionamento di organi ed apparati, per il mantenimento\ndella temperatura corporea, per il continuo ricambio delle cellule\nChe si rinnovano (pelle, sangue, intestino)\nL’energia necessaria si ricava dagli \nalimenti\n",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#4": "E’ la somma dei nutrienti necessari contenuti nel cibo,\ncapaci di assicurare all’organismo uno stato di salute\nottimale e di assicurare al bambino un accrescimento\ncorrispondente al suo potenziale genetico ed alla sua etàFABBISOGNO ALIMENTARE\nLIVELLI DI ASSUNZIONE RACCOMANDATI\nSono le quantità di alimenti sufficienti a coprire i bisogni \nnutrizionali di una persona sana.",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#5": "LE SOSTANZE NUTRITIVE\nsono divise in  3 gruppi fondamentali\n(funzione energetica )\n(funzione plastica )\n(funzione energetica e plastica )\nAnche ACQUA , SALI MINERALI , VITAMINE e OLIGOELEMENTI\nsono indispensabili per la vita: pur non fornendo energia essi sono\nutilizzati per un corretto funzionamento di tutto l’organismo ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#6": "GLI ALIMENTI CONTENGONO:\n❖PROTEINE (liberano 4 cal. al gr.)\nPane, pesce, legumi, latte, formaggio, uova;\n❖GLUCIDI/CARBOIDRATI (liberano 4 cal. al gr.)\nPane, pasta, riso, mais, orzo, patate;\n❖GRASSI (liberano 9 calorie al gr.)\nOlio, burro, margarina, panna, lardo …;\n❖VITAMINE, SALI MINERALI E FIBRE\nFrutta, ortaggi, legumi freschi ",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#7": "SANA ALIMENTAZIONE:\nPIÙ CEREALI, LEGUMI, ORTAGGI E FRUTTA\nGli alimenti vegetali (cereali, legumi, ortaggi e frutta) sono molto \nimportanti nella nostra alimentazione, perché contengono amido, fibra, \nvitamine, minerali e altre sostanze preziose per la salute. Cereali e legumi \ncontengono anche proteine.\nMangiare prodotti vegetali aiuta a ridurre le calorie, saziando senza \nappesantire. L'ideale, quindi, è ricordarci di consumare tutti i giorni \ndiverse porzioni di frutta e verdura (almeno 5 porzioni ).\nOgni giorno dovremmo mangiare anche pane, pasta o altri prodotti a base di \ncereali, meglio se integrali.\nGli alimenti vegetali costituiscono la base \ndell’alimentazione mediterranea",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#8": "LA PIRAMIDE ALIMENTARE\n«DIETA MEDITERRANEA»",
    "data_test\\rootfolder\\varie\\Alimentazione\\Educazione-Alimentare.pdf#9": "LA PIRAMIDE ALIMENTARE\n«DIETA DELLO SPORTIVO»",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#0": "Fondamenti della Scienza\ndell’Alimentazione\nVia Icilio, 7 - 00153 Roma - www.onb.it\nFondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#1": "1\nFondamenti della Scienza\ndell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#10": "10",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#100": "Le proprietà chimico-fisiche degli acidi grassi dipendono sostanzialmente dal\nnumero di atomi di carbonio presenti e dal loro grado di insaturazione. Tantopiù corta è la catena e più elevato è il grado d’insaturazione, tanto più è bassala temperatura di fusione. Per esempio, l’olio d’oliva, è liquido a temperaturaambiente e contiene principalmente acidi grassi monoinsaturi, come l’acidooleico (~ 65 %), e solo una piccola percentuale di acidi grassi saturi (~ 16 %).Al contrario il burro o la margarina, costituiti principalmente da acidi grassisaturi, sono solidi a temperatura ambiente.Gli acidi grassi presenti nei tessuti e nelle cellule dell’uomo possono essere diderivazione alimentare o generati attraverso sintesi endogena, a partire daprecursori più semplici ottenuti, per esempio, dalla decomposizione deglizuccheri. Il nostro organismo è in grado di sintetizzare, grazie all’azione dienzimi quali l’acido grasso sintasi, acidi grassi contenenti non più di 16 atomidi carbonio (acido palmitico, C16:0). Alcuni enzimi (elongasi) presenti sulversante citosolico del reticolo endoplasmatico, sono in grado di addizionareunità bicarboniose all’acido palmitico, favorendo così l’allungamento degliFigura 2: formula di struttura di un acido grasso saturo (A), acido grasso monoinsaturo con\ndoppi legami in configurazione “cis” (B), acido grasso monoinsaturo con doppi legami inconfigurazione “trans”.\n100Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#101": "acidi grassi. Altri enzimi (desaturasi) provvedono poi ad inserire doppi legami,\ngenerando così acidi grassi insaturi quali l’acido palmitoleico (C16:1 Δ9) e\nl’acido oleico (C18:1 Δ9). Tuttavia l’uomo non possiede gli enzimi in grado\ndi introdurre, in un acido grasso, doppi legami oltre l’atomo di carbonio inposizione 9. Per tale motivo molecole quali l’acido linoleico (C18:2 Δ\n9Δ12),\nl’acido α-linolenico (C18:3 Δ9Δ12Δ15) e l’acido arachidonico (C20:4\nΔ5Δ8Δ11Δ14) devono essere necessariamente introdotti con l’alimentazione e\nvengono definiti acidi grassi essenziali (Essential Fatty Acid, EFA). L’acidolinoleico e linolenico possono essere convertiti nell’organismo in altri acidigrassi polinsaturi definiti essenziali di derivazione , indispensabili per la\nbiosintesi di prostaglandine, prostacicline, trombossani e leucotrieni,metaboliti attivi in molte importanti funzioni, tra cui la contrazione dellamuscolatura liscia, l’aggregazione piastrinica, la risposta infiammatoria, ecc(Figura 3).\nNegli alimenti, la maggior parte degli acidi grassi si trova esterifica con altri\ncomponenti, originando molecole complesse quali i triacilgliceroli, le cere, ifosfolipidi ed i glicolipidi. \n4.3 I triacilgliceroli\nI triacilgliceroli o trigliceridi, sono esteri del glicerolo con tre acidi grassi,\ngeneralmente differenti per il numero di atomi di carbonio (acidi a catena corta,media o lunga), per la presenza di doppi legami e per la loro posizioneall’interno della catena idrocarburica (Figura 4). Figura 3: formula di struttura di alcuni derivati di acidi grassi a 20 atomi di carbonio\n(eicosanoidi)\n101Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#102": "Essi sono sostanzialmente apolari, molto meno solubili dei singoli acidi grassi,\ne vengono generalmente accumulati in cellule specializzate, gli adipociti,largamente presenti nei depositi sottocutanei, nelle cellule mammarie e nellacavità addominale. Dal momento che vengono conservati in forma pressochéanidra, essi occupano, da un punto di vista fisico, poco spazio. In un soggettodi 1,75 m di altezza e 70 Kg di peso (soggetto normopeso), i triacilglicerolicostituiscono circa 10-12 Kg del peso corporeo e rappresentano una importanteriserva di energia. Una volta ossidati forniscono circa 9 kcal/gr, una quantitàdi energia circa doppia rispetto a quella ricavata dall’ossidazione deicarboidrati (4 kcal/gr). Ciò è dovuto principalmente al fatto che gli acidi grassiin essi contenuti sono molecole fortemente ridotte e quindi maggiormenteossidabili rispetto a glucidi ed amminoacidi. In un soggetto normopeso, lariserva di energia presente sotto forma di triacilgliceroli ammonta a circa 100-130.000 kcal, quantità sufficiente, in caso di digiuno protratto, a garantirne lasopravvivenza per molti giorni. \n4.4 I lipidi di membrana: i fosfolipidi\nI fosfolipidi sono molecole strutturalmente simili ai trigliceridi ma a differenza\ndi questi ultimi derivano dal fosfatidato o diacilglicerolo-3-fosfato, una\nmolecola contenete glicerolo in cui due gruppi ossidrilici (generalmente quelliin posizione C1 e C2) sono esterificati con acidi grassi ed il terzo gruppoossidrilico è esterificato con una molecola di acido fosforico. A differenza deitrigliceridi, i fosfolipidi possiedono generalmente una porzione polare,rappresentata dalla molecola legata al gruppo fosfato (Figura 5).Figura 4: formula di struttura generica di un triacilglicerolo. R1, R2e R3, rappresentano acidi\ngrassi di varia lunghezza e differente grado di insaturazione\n102Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#103": "In molti casi il gruppo polare è costituito da un composto azotato, quali serina,\netanolammina o colina; in altri casi esso è costituito da un polialcol, come ilglicerolo o l’inositolo. Alcuni fosfolipidi non contengono glicerolo mamolecole complesse come la sfingosina, un aminoalcol che presenta una lungacatena idrocarburica insatura (Figura 6). In questo caso, i derivati prendono ilnome di sfingolipidi. \nSe disciolti in ambiente acquoso, i fosfolipidi si dispongono a formare un\ndoppio strato lipidico, una struttura lamellare estesa in cui le porzioniidrofiliche dei fosfolipidi sono esposte verso l’ambiente acquoso e quelleidrofobiche verso la parte interna del doppio strato, dalla quale le molecole diacqua vengono escluse (Figura 7). A volte tali strutture si ripiegano formandostrutture vescicolari chiuse (liposomi). La stabilità di tali strutture è garantitadalle numerose interazioni idrofobiche e legami di Van der Walls che siinstaurano tra le molecole di acidi grassi che formano i fosfolipidi. \nFigura 7: disposizione dei fosfolipidi in una membrana a doppio strato\nFigura 5: formula di struttura di un fosfolipide generico. Il simbolo “X” indica il gruppo\nsostituente polare\nFigura 6: formula di struttura della sfingosina\n103Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#104": "Grazie a tale tipo di organizzazione, i doppi strati lipidici risultano assai poco\npermeabili a ioni o molecole polari. I fosfolipidi rivestono una funzione moltoimportante dal momento che sono i principali costituenti delle membranebiologiche, strutture formate da un doppio strato fosfolipidico contenenteproteine di membrana (Figura 8). \n4.5 Il colesterolo\nOltre a fosfolipidi, glicolipidi e proteine, le membrane biologiche contengono\nanche colesterolo , un alcol a struttura complessa, caratterizzata da un anello\ntetraidro-pentano-peridro-fenantrenico (Figura 9). Il colesterolo è presentenelle membrane di tutte le cellule eucariotiche ed in particolare nellemembrane plasmatiche dove può arrivare a rappresentare il 25% dei lipiditotali. Esso, grazie alla sua catena idrocarburica e alla sua struttura steroidea,sprofonda all’interno del doppio strato fosfolipidico, dove forma legamiidrofobici con gli acidi grassi che compongono i fosfolipidi. Il gruppoidrossilico del colesterolo invece sbuca sul lato esterno della membrana doveforma legami ad idrogeno con la componente idrofilica dei fosfolipidi.\nFigura 9: formula di struttura del colesterolo\nFigura 8: schema semplificato di una membrana plasmatica. Nel doppio strato fosfolipidico\nsono immerse proteine di membrana (colore rosso e verde). Le porzioni idrofiliche deifosfolipidi, in grigio scuro, sono esposte verso l’ambiente acquoso mentre gli acidi grassi, ingrigio chiaro, sono disposti verso la parte più interna del doppio strato\n104Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#105": "Formando un ampio network di legami con i fosfolipidi, il colesterolo riduce\nla fluidità delle membrane biologiche; è ben noto che un eccesso di colesterolopuò arrivare a compromettere la funzionalità cellulare. Il colesterolo può essereintrodotto con gli alimenti (colesterolo esogeno), oppure essere sintetizzato alivello epatico (colesterolo endogeno). Un complicato meccanismo permettedi regolare la sintesi endogena epatica, controllando così anche i livelli dicolesterolo ematico (colesterolemia). Il colesterolo è un precursore essenzialeper la sintesi degli acidi biliari (i principali sono glicocolato e taurocolato),essenziali per la assicurare la corretta digestione intestinale degli acidi grassialimentari (Figura 10). \nIn aggiunta il colesterolo è il precursore di tutti gli ormoni steroidei quali il\nprogesterone, l’aldosterone, il cortisolo, il testosterone e l’estradiolo oltre chedella vitamina D, ormone essenziale per la regolazione dell’accrescimentoosseo (Figura 11). La sintesi della vitamina D inizia grazie all’azione fotoliticadei raggi ultravioletti che convertono il colesterolo in previtamina D3; questaviene poi isomerizzata a vitamina D3 e poi convertita in calcitrolo (1,25-Diidrossicolecalciferolo), la forma biologicamente attiva della vitamina D. Lacarenza di vitamina D causa, nei bambini, il rachitismo, e l’osteomalacia, negliadulti.\nFigura 10: formula di struttura dei principali sali biliari\n105Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#106": "4.6 Digestione ed assorbimento dei lipidi\nI lipidi sono costituenti essenziali di quasi tutti gli alimenti, sia di quelli di\norigine animale che di quelli di origine vegetale. La frazione lipidica deglialimenti comprende i trigliceridi, i più rappresentati (circa il 90-95 % dei lipiditotali), i fosfolipidi, il colesterolo, la maggior parte del quale in formaesterificata, e le vitamine liposolubili. La quota di lipidi presenti negli alimentipuò variare molto. Per esempio il burro e l’olio sono costituiti quasiesclusivamente da lipidi; la maionese contiene quasi il 70% di lipidi, la fruttasecca il 60%, e la cioccolata il 35%. Le carni suine ed i salumi contengonocirca il 30 % di lipidi, le uova il 15%, vitello, pollo e coniglio il 12%, mentreil latte vaccino solo il 4-5%. Per evitare un eccessivo accumulo di acidi grassi,fenomeno che predispone ad un maggior rischio di sviluppare patologiecardiovascolari o il diabete, si raccomanda di un’assunzione giornaliera diacidi grassi non superiore ai 60 gr, di cui circa 25 preferibilmente di originevegetale e i rimanenti 35 di origine animale. A causa della loro natura idrofobica, il processo di digestione dei lipidi è unfenomeno abbastanza complesso. In soluzione acquosa i lipidi, oltre ai doppistrati, possono formare piccole micelle sferiche. Gli enzimi coinvolti nelladigestione dei lipidi, le lipasi, sono molecole idrofile; questo implica che ilprocesso di digestione dei lipidi debba avvenire all’interfaccia lipide-acqua.Per tale motivo più elevata è la superficie dell’interfaccia, maggiore è lavelocità di digestione dei lipidi. I continui movimenti peristaltici dello stomacoe dell’intestino favoriscono l’idratazione, l’estrazione dei lipidi dagli altricomponenti e la loro dispersione in piccole gocciole lipidiche contenentiprevalentemente triacilgliceroli. Tuttavia per ottenere una perfetta emulsionei soli movimenti peristaltici non sono sufficienti. Ciò è dovuto al fatto che, insoluzioni acquose, le goccioline lipidiche tendono ad aggregare rapidamente,Figura 11: principali funzioni del colesterolo\n106Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#107": "formando complessi di maggiori dimensioni, difficilmente attaccabili dalle\nlipasi. Perciò, per ottenere un’adeguata emulsione dei lipidi è necessaria anchela presenza dei sali biliari, molecole anfipatiche derivate dal colesterolo,sintetizzate dal fegato e accumulate nella cistifellea. I sali biliari, insieme aglienzimi digestivi, vengono secreti nel momento in cui il chimo inizia il suopercorso all’interno dell’intestino. Questi si organizzano in micelle di piccoledimensioni (micelle biliari, contenenti altri agenti surfattanti quali acidi grassie fosfolipidi) che interagiscono con la superficie delle gocciole lipidiche,impedendo a queste ultime, una volta disperse nella soluzione acquosa, diaggregare nuovamente (Figura 12). Per tale motivo si è soliti affermare che isali biliari hanno azione detergente e contribuiscono ad emulsionare i lipididisperdendoli in gocciole di piccole dimensioni, facilitando così l’azione dellelipasi. \nL’uomo possiede diversi lipasi: una lipasi linguale, una lipasi gastrica ed una\npancreatica. Sebbene la lipasi linguale e quella gastrica contribuiscano alladigestione dei lipidi, quella più importante è, senza dubbio, quella pancreatica.Tuttavia tale enzima non riesce, a causa della presenza delle micelle biliari,ad interagire facilmente con le gocciole lipidiche. Per poter agire efficacementeesso necessita dell’intervento di una colipasi, una proteina in grado, da unaparte, di legare saldamente la lipasi, e dall’altra, di scalzare le micelle biliaridalla superficie delle gocciole lipidiche ed interagire con i lipidi che lecompongono, permettendo alla lipasi di venire a contatto con il suo substrato.La lipasi pancreatica idrolizza i legami estere in posizione 1 e 3 dei\nFigura 12: goccia lipidica circondata da micelle biliari\n107Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#108": "triacilgliceroli, liberando due molecole di acidi grassi e una molecola di 2-\nmonoacilglicerolo. I prodotti dell’idrolisi vengono estratti dalle gocciolelipidiche e trasferiti sulle micelle biliari (Figura 13). \nIn tal modo il contenuto di triacilgliceroli all’interno delle gocciole lipidiche\nsi riduce velocemente mentre gli acidi grassi e le molecole di 2-monoacilglicerolo vengono trasportati in prossimità delle cellule intestinalidalle micelle biliari e poi assorbiti dagli enterociti. In maniera simile, il colesterolo esterificato presente negli alimenti verrà idro-lizzato dalla colesterolo esterasi pancreatica e trasformato in colesterolo libero,il quale verrà poi veicolato dalle micelle biliari fino agli enterociti, che lo as-sorbiranno. Una volta esaurita la loro funzione, la maggior parte dei sali biliariviene riassorbita nell’ileo distale (solo il 2% viene perso con le feci), per poiessere trasportati al fegato attraverso la vena porta. Dal fegato i sali biliari ven-gono trasferiti nuovamente nella cistifellea dove vengono accumulati fino alpasto successivo. Tale meccanismo viene descritto come circolazione ente-roepatica degli acidi biliari. Una volta all’interno degli enterociti, la maggiorparte degli acidi grassi vengono nuovamente esterificati per originare trigli-ceridi. Una quota minore di acidi grassi, generalmente quelli a catena corta,passa direttamente nel sangue dove si lega all’albumina la quale provvede aFigura 13: idrolisi dei trigliceridi presenti nelle gocce lipidiche da parte della lipasi gastrica. La\ncolipasi spiazza le micelle biliari e si lega ai trigliceridi, veicolando la lipasi gastrica in corrispondenzadel proprio substrato. Gli acidi grassi liberati, ed il 2-monoacilglicerolo, insolubili nell’ambienteacquoso, vengono trasferiti sulle micelle biliari le quali li trasportano fino agli enterociti\n108Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#109": "trasportarli ai vari tessuti. I triacilgliceroli, così come il colesterolo, gli esteri\ndel colesterolo ed i fosfolipidi, sono fortemente insolubili e non potrebberomai essere riversati, come tali, nel torrente circolatorio. Il trasporto di tali mo-lecole è assicurato dalle lipoproteine. \n4.7 Il trasporto dei lipidi: le lipoproteine\nLe lipoproteine plasmatiche sono sostanzialmente aggregati lipido-proteici incui la componente proteica, chiamata apolipoproteina, è in grado di associarsiai trigliceridi, ai fosfolipidi, al colesterolo e agli esteri del colesterolo. Nellelipoproteine i gruppi polari degli amminoacidi, dei fosfolipidi e del colesterololibero sono rivolti all’esterno della molecola, mentre i lipidi non polari (esteridel colesterolo e trigliceridi) sono situati al centro della molecola (Figura 14).Le lipoproteine trasportano anche le vitamine liposolubili. \nLa concentrazione plasmatica delle lipoproteine raggiunge valori di circa 500\nmg/dl; il loro contenuto è tuttavia variabile, a seconda dell’età, sesso, fattoriormonali e dietetici. Si conoscono diverse classi di lipoproteine le qualipossono essere classificate in base alla loro densità; queste sono i chilomicroni,le very low density lipoprotein, VLDL, le intermediate density lipoprotein,IDL, le low density lipoprotein, LDL e le high density lipoprotein, HDL. Ogni\nFigura 14: struttura delle lipoproteine plasmatiche. La porzione più esterna è costituita da\nlipidi polari e proteine, mentre la porzione più interna da lipidi idrofobici ed esteri delcolesterolo\n109Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#11": "11Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#110": "particella lipoproteica contiene la quantità sufficiente di proteine, fosfolipidi\ne colesterolo per formare, sulla sua superficie, un monostrato di questicomponenti di circa 20 Ǻngstrom di spessore. La densità delle lipoproteineaumenta con il diminuire del diametro della particella, in quanto la densità delrivestimento più esterno è maggiore di quella del nucleo più interno. Così, leHDL, che sono le lipoproteine a maggiore densità, sono anche le particellepiù piccole. I chilomicroni vengono sintetizzati nell’intestino, le VLDLnell’intestino e nel fegato, le HDL nel fegato; le IDL e le LDL derivano invecedalle VLDL. Il contenuto ed il tipo di lipidi varia molto tra le varielipoproteine: i chilomicroni contengono prevalentemente triacilgliceroli diorigine alimentare, le VLDL triacilgliceroli, fosfolipidi ed esteri del colesterolodi sintesi endogena, le IDL triacilgliceroli e fosfolipidi, le LDL esteri delcolesterolo e fosfolipidi, le HDL, colesterolo e fosfolipidi. In aggiunta,ciascuna classe contiene differenti tipi di apolipoproteine; esse sono statedenominate: A, B\n48, B100, C, E, A-1, A-2, A-4 (vedi Tabella 1). La componente\nproteica è in genere rivolta verso la superficie della molecola, ed è costituitada segmenti, in parte a-elicoidali, in parte a foglietti β antiparalleli. \nI chilomicroni sono sintetizzati nella mucosa intestinale ed i triacilgliceroli\nin essi contenuti derivano dai lipidi della dieta: vengono rilasciati nel circolo\nlinfatico per poi defluire nel circolo sanguigno; attraverso il sistemacircolatorio raggiungono tutti gli altri tessuti. Prevalentemente a livello deltessuto adiposo e muscolare, i trigliceridi contenuti nei chilomicroni vengonoidrolizzati dalla lipoproteina lipasi, ancorata agli endoteli dei capillari. Gli\nacidi grassi rilasciati possono essere ossidati nel muscolo o essere depositaticome trigliceridi nel tessuto adiposo. In seguito al rilascio progressivo deitrigliceridi, i chilomicroni si restringono fino a formare particelle chiamateChilomicroni VLDL IDL LDL HDL\nOrigine Intestino Fegato, VLDL VLDL Fegato\nIntestino\nDensità < 0.95 0.95-1.006 1.006-1.019 1.019-1.063 1.019-1.20\nComposizione in Trigliceridi Trigliceridi, Colesterolo, Colesterolo Fosfolipidi\nlipidi (prevalenti) Fosfolipidi Trigliceridi Colesterolo\nApoproteine C-III, C-II, C-I, B-100, C-III, B-100, B-48, E B-100 A-I, A-II, D,\nB48, E, A-I, E, C-II, C-I, C-III, C-I, EA-II A-II, A-ITabella 1: proprietà delle lipoproteine plasmatiche\n110Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#111": "chilomicroni residui, ricchi di colesterolo, che vengono trasportati al fegato,\ndove, in seguito all’interazione con recettori specifici presenti sugli epatociti,vengono internalizzati, per endocitosi, all’interno delle cellule epatiche. Le\nVLDL sono sintetizzate nel fegato per trasportare trigliceridi, colesterolo e\nfosfolipidi di derivazione endogena (neosintesi); anch’esse sono substrato dellalipoproteina lipasi presente nei capillari del tessuto adiposo e del muscolo. Gliacidi grassi rilasciati sono assorbiti dalle cellule e ossidati per produrre energiaoppure sono utilizzati dagli adipociti per sintetizzare trigliceridi. Dopo averceduto i loro trigliceridi, le VLDL residue che hanno perso una parte delle\nloro apolipoproteine compaiono nel circolo sanguigno come IDL e\nsuccessivamente come LDL. Queste interagiscono con i recettori presenti sulla\nmembrana plasmatica di varie cellule, per poi essere internalizzate perendocitosi e idrolizzate dagli enzimi lisosomiali. Le LDL sono coinvolte nel\ntrasporto del colesterolo esterificato ai vari tessuti. Le HDL hanno una\nfunzione opposta rispetto alle LDL: rimuovono il colesterolo dai tessuti e lo\ntrasportano al fegato. Si formano nel sangue da componenti ottenuti per lamaggior parte dalla degradazione di altre lipoproteine. Le HDL circolanti\nprobabilmente assumono il colesterolo dalle membrane della superficiecellulare e lo convertono in esteri del colesterolo grazie all’enzima lecitina\ncolesterolo acil tranferasi plasmatica, che trasferisce acidi grassi dalle lecitine\nal colesterolo. Le HDL sono internalizzate nel fegato per endocitosi e\nidrolizzate dagli enzimi lisosomiali. Una volta nel fegato, gli esteri delcolesterolo vengono idrolizzati ed il colesterolo libero può essere inserito nelleVLDL o convertito in sali biliari. In conclusione, il colesterolo esterificato è\nlegato sia alle HDL (per il 25%) che alle LDL (per il 75%), con le quali è\ntrasportato ai vari tessuti, dove si trovano recettori soprattutto per le LDL; il\nfegato, oltre a possedere i recettori per le IDL e LDL, possiede anche quelliper le HDL (Figura 15). \n111Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#112": "4.8 La biosintesi dei lipidi\nNell’uomo, ed in molti animali, un eccesso di nutrienti, ed in particolare di\ncarboidrati, viene generalmente convertito in acidi grassi, e poi in trigliceridi,i quali vengono accumulati nel tessuto adiposo. La sintesi degli acidi grassi èparticolarmente attiva nel fegato, nell’intestino, nella ghiandola mammaria enel tessuto adiposo. Nel fegato, dopo un pasto abbondante, l’afflusso diglucosio permette di saturare rapidamente le riserve epatiche di glicogeno;l’ulteriore eccesso viene così trasformato in acidi grassi. L’insulina svolge unafunzione essenziale in questo processo: essa stimola la conversione delglucosio in piruvato, il quale viene poi trasformato in acetil-CoA, precursoreessenziale per la sintesi degli acidi grassi. A sua volta, l’acetil-CoA è ancheun potente attivatore allosterico dell’enzima piruvato carbossilasi il qualeconverte il piruvato in ossalacetato. Piruvato ed ossalacetato sono entrambisubstrati dell’enzima citrato sintasi. Di conseguenza, dopo un pastoabbondante, nei mitocondri la quantità di citrato sintetizzato aumenta molto,e parte di esso viene esportato nel citoplasma. Una volta nel citoplasma, ilcitrato viene scisso in acetil-CoA ed ossalacetato ad opera dell’enzima citratoliasi. Nel citoplasma, l’acetil-CoA agisce da attivatore allosterico dell’enzimaacetil-CoA-carbossilasi il quale catalizza la sintesi del malonil-CoA,precursore fondamentale per la sintesi degli acidi grassi (Figura 16). Figura 15: funzioni delle lipoproteine plasmatiche \n112Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#113": "Il malonil-CoA è un potente inibitore dell’enzima acil-CoA: carnitina\naciltranferasi ed è in grado di bloccare il trasporto degli acidi grassi nelmitocondrio, inibendo così anche il loro processo di degradazione. In tal modo,sintesi e degradazione degli acidi grassi risultano reciprocamente regolati:l’abbondanza di zuccheri attiva la sintesi dei lipidi ma blocca la lorodegradazione. Nel fegato, i livelli di espressione di enzimi quali la piruvato deidrogenasi, lacitrato liasi, l’acetil-CoA carbossilasi e quelli appartenenti al complessodell’acido grasso sintasi aumentano in seguito alla presenza di glucosio edinsulina. In aggiunta l’insulina è in grado di stimolare l’attività della citratoliasi e dell’acetil-CoA carbossilasi, incrementando così la sintesi degli acidigrassi a catena lunga quali l’acido palmitico. Al contrario, il glucagone stimolala fosforilazione, e quindi l’inibizione, dell’enzima acetil-CoA carbossilasi,bloccando la sintesi degli acidi grassi. La sintesi di acidi grassi è un processocomplesso che richiede energia.\n8 acetil-CoA + 7ATP + 14 NADPH + 14 H\n+Palmitato + \n8 CoA + 7 ADP + 7 Pi + 14 NADP++ 6 H2O\nNelle cellule epatiche, il NADPH necessario alla sintesi degli acidi grassi vieneFigura 16: regolazione della sintesi degli acidi grassi\n113Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#114": "generato attraverso il ciclo dei pentosi fosfati o grazie all’azione dell’enzima\nmalico, altro enzima la cui sintesi è stimolata dall’insulina. Nell’uomo,l’enzima acido grasso sintasi produce molecole non più lunghe di 16 atomi dicarbonio: per tale motivo il palmitato è il principale prodotto dell’attività delcomplesso multienzimatico dell’acido grasso sintasi. Il complessomultienzimatico denominato acido grasso elongasi, localizzato nel reticoloendoplasmatico, contribuisce poi ad allungare la catena degli acidi grassisintetizzati fino ad ottenere, come avviene nel cervello, acidi grassi a catenamolto lunga (22-24 atomi di carbonio) che poi verranno utilizzati per la sintesidegli sfingolipidi. Altri enzimi, le desaturasi, sono in grado di inserire doppilegami nella struttura degli acidi grassi saturi generando acidi grassi mono- epolinsaturi. Spesso l’inserimento di doppi legami avviene tra gli atomi dicarbonio in posizione 9 e 10, generando così molecole quali l’acidopalmitoleico (C16:1Δ\n9) e l’acido oleico (C18:1Δ9). \nIn caso di necessità, le cellule del nostro organismo, ed in particolare le celluleepatiche, possono sintetizzare anche il colesterolo. La sintesi di questo steroloinizia con la condensazione di due molecole di acetil-CoA, per ottenereacetoacetil-CoA. Un’ulteriore molecola di acetil-CoA viene utilizzata pergenerare il 3-idrossi-3-metilglutaril-CoA (HMG-CoA), il quale viene poiridotto a mevalonato, il primo intermedio specifico della via di sintesi delcolesterolo. Attraverso una complessa serie di reazioni, il mevalonato vieneconvertito in un composto con 30 atomi di carbonio, lo sqalene, il quale vienepoi trasformato, attraverso altre reazioni, in colesterolo. In condizioni ideali,\nesiste una diretta correlazione tra la quota di colesterolo sintetizzato e quellaassunta attraverso la dieta: maggiore è la quota di colesterolo proveniente dalladieta, tanto minore sarà la quota sintetizzata dalle stesse cellule. Tra i fattoriche contribuiscono a regolare la sintesi endogena di colesterolo vi è la quantitàdi colesterolo libero presente all’interno delle cellule e l’azione di ormoni qualil’insulina ed il glucagone. Quando i livelli di colesterolo intracellularidiminuiscono, le proteine che legano gli elementi di regolazione degli steroli(Sterol Regulatory Element-Binding Protein, SRBEP) vengono attivate,migrano nel nucleo dove stimolano la trascrizione dei numerosi geni, tra cuiquello dell’enzima che sintetizza il mevalonato, l’HMG-CoA reduttasi. Alcontrario, un aumento della quantità intracellulare di colesterolo liberodetermina la riduzione dell’attività della HMG-CoA reduttasi, una riduzionedi espressione del recettore per le LDL, un aumento dell’attività dell’enzimaacil-CoA colesterolo aciltransferasi (ACAT) il quale favorisce l’accumulo dicolesterolo esterificato all’interno delle cellule. L’insulina stimola la glicolisi,\n114Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#115": "la conversione del piruvato in acetil-CoA e attiva la HMG-CoA riduttasi,\nfavorendo la sintesi di colesterolo; questo spiega perché una dieta ricca inglucidi possa determinare un aumento dei livelli di colesterolo circolante,incrementando il rischio di insorgenza di patologie a carico dell’apparatocardiocircolatorio. Il glucagone, al contrario, inattiva l’enzima HMG-CoAriduttasi, inibendo così la sintesi del colesterolo (Figura 17). La mancataespressione dei recettori per le LDL o difetti del loro funzionamento sonoimportanti fattori di rischio per patologie quali l’aterosclerosi. Con una certafrequenza nella popolazione si manifestano mutazioni genetiche a carico deigeni responsabili della sintesi del recettore per le LDL o a carico di geni cheesprimono proteine coinvolte nella regolazione di tali recettori. I soggettiportatori di tali mutazioni soffrono di ipercolesterolemia familiare, unapatologia caratterizzata da un costante eccesso di colesterolo circolante chedifficilmente può essere ridotto, anche se i pazienti vengono sottoposti ad unrigido controllo della dieta.\nLa maggior parte delle volte, la causa è proprio una mutazione del gene per il\nrecettore delle LDL. I pazienti portatori di queste mutazioni presentano unbasso numero di recettori per le LDL e, di conseguenza, la loro captazione da\nFigura 17: regolazione ormonale della sintesi del colesterolo\n115Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#116": "parte del fegato e dei tessuti periferici è fortemente compromessa. Individui\nomozigoti per la mutazione mostrano elevati livelli di LDL e IDL circolanti,e livelli di colesterolo tre volte superiori alla norma; depositi di colesteroloappaiono sulla pelle, sui tendini e su altri tessuti, ma l’accumulo più pericolosoè quello a livello dell’endotelio arterioso. L’accumulo di colesterolo causadisturbi cardiaci, alterazioni della circolazione periferica, ed infartomiocardico. Gli individui omozigoti per questo tipo di mutazione, muoionogeneralmente per patologie coronariche durante l’infanzia; al contrario, isoggetti eterozigoti presentano disturbi più moderati con prognosi variabile.Le ricerche effettuate hanno permesso di identificare differenti tipi dimutazione che possono contribuire all’insorgenza di tale patologia. Alcune diqueste determinano la perdita della capacità di sintetizzare il recettore, mentrein altri casi il recettore viene sintetizzato ma non può essere trasferito sullamembrana plasmatica delle cellule. In altri casi ancora, il recettore sintetizzatoha difetti strutturali e non è in grado di riconosce in maniera appropriata, equindi legare, le LDL. Infine vi sono soggetti in cui il recettore riconosce leLDL ma non è in grado di modulare la loro internalizzazione. In ogni caso,l’unica opzione terapeutica per gli individui omozigoti per laipercolesterolemia familiare è il trapianto di fegato. Al contrario, per i soggettieterozigoti, è disponibile oggi un trattamento farmacologico (statine) in gradodi migliorare la loro condizione. Dal momento che l’espressione dei recettoriper le LDL riflette la necessità di colesterolo della cellula stessa, negli individuieterozigoti l’approccio farmacologico è destinato ad abbassare i livellicitoplasmatici di colesterolo attraverso specifici inibitori della sua biosintesi.Le statine (Simvastatina, Pravastatina e Lovostatina) sono molecole simili almevalonato, ed agiscono da inibitori competitivi di uno degli enzimi chiaveper la sintesi del colesterolo, l’enzima HMG-CoA reduttasi. Lasomministrazione di statine è associata ad una riduzione dei livelli dicolesterolo circolante ed, in molti casi, ciò comporta anche un’aumentataespressione di recettori per le LDL per poter captare il colesterolo in essecontenuto.\n4.9 Il catabolismo dei lipidi\nIn caso di digiuno o attività fisica prolungata, i lipidi, ed in particolare i\ntrigliceridi, possono soddisfare la richiesta di energia anche per lunghi periodi.Ormoni quali il glucagone e adrenalina si legano a recettori specifici presenti\n116Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#117": "sulla membrana degli adipociti, innescando una via di segnalazione che\nculmina con l’attivazione della lipoproteina lipasi ormone-dipendente. Taleenzima è in grado d’idrolizzare tutti i legami estere dei triacilgliceroli,producendo acidi grassi liberi e glicerolo che vengono poi espulsi dagliadipociti. Il glicerolo, molecola solubile, attraverso il sangue raggiunge ilfegato, dove viene trasformato in diidrossiacetonefosfato che verrà poiutilizzato come intermedio per sintetizzare glucosio. Gli acidi grassi, una voltariversati nel sangue si legano all’albumina e, attraverso questa, vengonoveicolati alle cellule del nostro organismo che necessitano di substrati daossidare per produrre energia. Grazie a trasportatori di membrana specifici(CD36, FAT, FABP) gli acidi grassi riescono ad entrare all’interno delle celluledove divengono substrati degli enzimi coinvolti nel loro catabolismo. Tutte lecellule, ad eccezione delle cellule nervose e degli eritrociti, sono in grado dimetabolizzare gli acidi grassi. Una volta nel citoplasma delle cellule, gli acidigrassi divengono substrato di un enzima, l’Acil-CoA sintetasi, in grado diesterificare l’acido grasso con una molecola di CoA; tale reazione vienegeneralmente definita come reazione di attivazione degli acidi grassi dalmomento che, solo se legati al CoA, gli acidi grassi a catena lunga potrannoessere metabolizzati. La reazione richiede il consumo di due molecole di ATP. \nAcido grasso + ATP + CoA Acil-CoA + AMP + PPi\nUna volta attivati, gli acidi grassi devono essere trasportati all’interno delmitocondrio, dove si trovano gli enzimi in grado di catalizzare la lorodegradazione. Gli acidi grassi a catena corta, a contrario di quelli a catenalunga, non hanno bisogno di trasportatori per entrare all’interno delmitocondrio. Al contrario, gli acidi grassi a catena lunga vengono trasportatiall’interno del mitocondrio attraverso uno specifico trasportatore, la carnitina\n(Figura 18).\n                                                                                   \nL’enzima acil-CoA: carnitina aciltranferasi I catalizza il trasferimento della\nFigura 18: struttura chimica della carnitina\n117Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#118": "molecola di acido grasso sulla carnitina.\nAcil-CoA + carnitina Acil-carnitina + CoA\nA questo punto, grazie all’intervento di un trasportatore specifico presente\nsulla membrana mitocondriale interna, il complesso acil-carnitina vienetrasferito all’interno del mitocondrio, mentre, una molecola di carnitina vieneportata all’esterno del mitocondrio. Una volta nella matrice mitocondriale,l’enzima carnitina aciltranferasi II provvede a liberare l’acido grasso e\nrigenerare la carnitina nella forma libera\nAcil-carnitina + CoA Acil-CoA + carnitina\nA questo punto inizia il vero e proprio processo di degradazione degli acidigrassi, definito β-ossidazione. Tale processo consiste in quattro reazioni,\ncatalizzate da altrettanti enzimi, grazie alle quali due atomi di carboniovengono staccate dalla struttura dell’acido grasso e liberate sotto forma diacetil-CoA (Figura 19). \nFigura 19: schema della β-ossidazione\n118Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#119": "4.10 Sintesi e funzione fisiologica dei corpi chetonici\nNel fegato, il destino metabolico dell’acetil-CoA generato attraverso la\ndegradazione degli acidi grassi dipende fondamentalmente dalla disponibilitào meno di ossalacetato. In condizioni di moderata disponibilità di glucosio,l’enzima citrato sintasi utilizza ossalacetato e acetil-CoA, per sintetizzare unamolecola di citrato, indirizzando così l’acetil-CoA proveniente dalladegradazione degli acidi grassi nel ciclo degli acidi tricarbossilici (ciclo diKrebs). L’ossalacetato necessario viene sintetizzato a partire dal piruvato,grazie all’azione dell’enzima piruvato carbossilasi, il quale, a sua volta, vieneattivato dall’ acetil-CoA. Quindi, elevate concentrazioni di acetil-CoAstimolano la produzione di ossalacetato, intermedio utile ad assicurare latrasformazione dell’acetil-CoA prodotto dalla degradazione degli acidi grassiin citrato (Figura 20). \nIn condizioni di digiuno prolungato, il glucagone, legandosi ai recettori\npresenti sulla membrana delle cellule epatiche, inibisce la glicolisi e stimolala gluconeogenesi, la quale sottrae molecole di ossalacetato al ciclo di Krebs,destinandole alla sintesi del glucosio. In tal modo gran parte dell’acetil-CoA\nFigura 20: Metabolismo degli acidi grassi in condizioni di moderata disponibilità di glucosio\n119Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#12": "MANUALE\nI Fondamenti della Scienza dell’Alimentazione\nCAPITOLO I\nI MACRO e MICRONUTRIENTI\nRiassuntoIn questo capitolo si forniranno cenni su alcuni micro e macro nutrienti, questiultimi verranno approfonditi nei capitoli III, IV e V . I micronutrienti, sali minerali e vitamine, sono sostanze ingerite con gli alimenti(frutta e verdura), la cui funzione non è direttamente correlata alla produzione dienergia e alla crescita, ma necessari al mantenimento delle funzioni dell’organismo. Per garantire un corretto sviluppo e mantenimento dell’organismo, oltre aduna alimentazione equilibrata e una regolare attività fisica, l’OMS promuovel’assunzione di almeno 400 g al giorno per persona, di frutta e verdura (con-tenenti principalmente vitamine e sali minerali), quantità che corrisponde acirca cinque porzioni.Secondo gli studi dell’Ufficio Regionale Europeo dell’OMS, l’assunzione difrutta e verdura è in quasi tutti i paesi a livelli ben più bassi di quanto racco-mandato. A rischio, in particolare i ragazzi tra i 9 e i 16 anni, per uno scarsoconsumo quotidiano di frutta e verdura.Un’alimentazione equilibrata, in cui sono presenti tutti i gruppi di alimenti, risultafondamentale, insieme ad un’adeguata attività fisica (camminata a passo veloce,ballo, palestra e altro), per la prevenzione di diverse condizioni patologiche.I processi di cottura riducono la quantità di questi micronutrienti, quindi sarebbeopportuno utilizzare, ad esempio, la cottura a vapore, o l’assunzione di frutta everdura fresca di stagione, e possibilmente proveniente da agricoltura biologicae appena raccolta. L’unità di misura utilizzata per queste sostanze è il milligrammo o microgrammo.I macronutrienti sono princìpi alimentari introdotti in grandi quantità, poichérappresentano la più importante fonte energetica per l’organismo, utile siaper la crescita sia per mantenere il metabolismo. Appartengono a questa categoria i carboidrati o glucidi, i grassi o lipidi e leproteine o protidi.L’unità di misura utilizzata per queste sostanze è il grammo. Ognuna di queste sostanze, assunte con la dieta quotidianamente, ha funzionidifferenti e viene gestita in modo diverso dal nostro organismo.\n12Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#120": "prodotto dalla degradazione degli acidi grassi non entra nel ciclo di Krebs, ma\npiuttosto viene utilizzato per la sintesi dei corpi chetonici. La sintesi dei corpichetonici inizia con la condensazione di due molecole di acetil-CoA, pergenerare acetoacetil-CoA. Una terza molecola di acetil-CoA vienesuccessivamente addizionata per ottenere l’HMG-CoA. Quest’ultimo vienepoi scisso dall’enzima HMG-CoA liasi in acetil-CoA e acetoacetato.L’acetoacetato può avere due differenti destini: andare incontro ad un processodi decarbossilazione spontanea, generando acetone, oppure essere ridotto a 3-idrossibutirrato. Acetone, acetoacetato e 3-idrossibutirrato sono le molecoleche vengono comunemente chiamate corpi chetonici (Figura 21).\nI soggetti che seguono una dieta bilanciata producono solo una piccola quantità\ndi corpi chetonici dal momento che la quantità di ossalacetato disponibile èsufficiente ad indirizzare l’acetil-CoA prodotto nel ciclo di Krebs. Al contrario,in condizione di digiuno prolungato, nel caso di diete povere in carboidrati enel diabete non trattato, le quantità di ossalacetato disponibili non sonosufficienti a compensare il forte incremento dei livelli di acetil-CoA,determinando, specialmente nel fegato, un rapido aumento della produzioneFigura 21: Sintesi dei corpi chetonici nelle cellule epatiche in condizioni di digiuno\nprolungato. In queste condizioni, l’abbondante presenza di NADH causa l’inibizionedell’isocitrato deidrogenasi, della piruvato deidrogenasi, forzando, inoltre, la conversione diossalacetato in malato. Questo può uscire dal mitocondrio ed essere nuovamente ossidato adossalacetato, il quale verrà indirizzato nella gluconeogenesi. A causa della carenza diossalacetato, l’acetil-CoA prodotto viene utilizzato per sintetizzare corpi chetonici\n120Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#121": "dei corpi chetonici. Questi ultimi svolgono una funzione molto importante dal\nmomento che, una volta rilasciati nel sangue, sono in grado di soddisfare larichiesta energetica di molti tessuti extraepatici quali il muscolo scheletrico,il cuore ed il cervello. Quest’ultimo usa esclusivamente glucosio per ricavareenergia per il proprio sostentamento; in condizioni di ristrettezze (digiuno), ilcervello è in grado di sostituire buona parte del glucosio con i corpi chetonici,utilizzando l’acetoacetato ed il 3-idrossibutirrato per alimentare il ciclo diKrebs e ricavare ATP. E’ stato calcolato che in condizioni di digiuno prolungato, i corpi chetonicisoddisfano circa il 75% della richiesta energetica del cervello. In tal modo ilfegato può continuare a degradare acidi grassi anche se l’acetil-CoA non vieneindirizzato all’interno del ciclo di Krebs, ed i tessuti periferici possonocompensare la carenza di glucosio, utilizzando i corpi chetonici per ricavareenergia, riducendo così la richiesta di glucosio. L’acetone, generalmenteprodotto in piccole quantità rispetto agli altri due metaboliti, viene quasicompletamente allontanato dal sangue con la respirazione. \n121Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#122": "Bibliografia\n• Saliprandi & Tettamanti Biochimica Medica Edizioni Piccin 1010 pp 2011\n• D. V oet, J.G. V oet, C.W. Pratt. Fondamenti di Biochimica. Edizioni Zanichelli. 3 Ed. 1200\npp. 2013\n• John W. Baynes, Marek H. Dominiczak Biochimica per le discipline biomediche. Edizioni\nElsevier. 653 pp. 2011\n• G. Zubay, W.W. Parson, D.E. Vance. Principles of Biochemistry. Edizioni McGraw-Hill\nEducation, 992 pp. 1995\n• David L. Nelson, Michael M Cox I principi di Biochimica di Lehninger, Edizioni Zanichelli,\nV Ed. 1288 pp. 2010\n• Giuseppe Arienti Le basi molecolari della nutrizione Edizioni Piccin. Terza edizione 1111\npp. 2011\n122",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#123": "123Capitolo V Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#124": "CAPITOLO V\nLE PROTEINE\nRiassunto\nLe Proteine sono macromolecole costituite da amminoacidi che si legano con\nlegame peptidico in lunghe catene polipeptidiche. Esse svolgono funzioni es-senziali per tutti i processi biologici, quali quelle strutturali, di catalisi enzi-matica, di trasporto e deposito, di movimento coordinato, di protezioneimmunitaria. Nelle Proteine possiamo individuare una struttura primaria, unasecondaria, una terziaria ed una quaternaria. Possono essere distinte in P .semplici e Proteine coniugate. La digestione delle Proteine consiste nella di-sgregazione meccanica, chimica ed enzimatica delle proteine contenute neicibi che vengono ridotte a unità più piccole, fino a ridurle in aminoacidi o di-e tripeptidi prima che possa aver luogo l’assorbimento. In questo processointervengono enzimi proteolitici quali endo ed esopeptidasi. Seguono processidi deaminazione, transaminazione e trans-desaminazione che portano all’eli-minazione del gruppo amminico NH\n2sotto forma di ammoniaca NH3nel ciclo\ndell’urea.\n124Fondamenti della Scienza dell’Alimentazione Capitolo V",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#125": "5.1 Proteine\nLe Proteine sono sostanze organiche con un alto grado di complessità, macro-\nmolecole costituite da una, o diverse, lunghe catene polipeptidiche (detteanche protidi). Le proteine sono la classe di molecole organiche più abbondanti in tutti gli or-ganismi viventi; si trovano in tutte le cellule e costituiscono più della metà delloro peso secco. Si stima che esistano più di 50.000 proteine mane e che il numero di proteinedistinte all’interno di una cellulari tra le 3.000 e le 5.000.Nel solo siero possono essere identificate più di 1.400 proteine.Le proteine sono essenziali per tutti i processi biologici legati alle funzioni vi-tali. Le loro funzioni sono molteplici:- intervengono come componenti primari nella struttura e nelle funzioni cellulari- svolgono un ruolo fondamentale nei processi di catalisi enzimatica, infatti\ntutti gli enzimi sono proteine \n- sono deputate a funzioni di trasporto e deposito, come l’ emoglobina , la mio-\nglobina, la transferrina, la ferritina.\n- permettono le funzioni di movimento coordinato, come l ’actina-miosina , la\ntubulina-dineina\n- costituiscono basi di supporto meccanico, come il collagene , la cheratina\n- esercitano funzioni di protezione immunitaria, infatti gli anticorpi sono pro-\nteine altamente specifiche\n- sono in grado di esercitare la funzione di generazione e trasmissione del-\nl’impulso nervoso, come la rodopsina, presente nei bastoncelli della retina,\nproteina in grado di funzionare come recettore della luce\n- svolgono azioni di controllo della crescita e della differenziazione in grado\ndi stimolare la proliferazione e il differenziamento cellulare, come il growth\nfactor\n- permettono la comunicazione tra le cellule di un organismo, come le cito-\nchine, molecole infiammatorie o gli ormoni che si legano a specifici recettorisulla membrana cellulare dei loro target, come insulina e glucagone\n- hanno azione aggressiva, come le esotossine batteriche, t. botulinica , t.te-\ntanica, t. carbonchiosa, e come alcuni veleni di serpente che rappresentano\nuna complessa secrezione proteica costituita da neurotossine ad azione pre-\nsinaptica o post-sinaptica\n- svolgono la funzione essenziale di sintesi proteica da parte degli organismi\nviventi, detta protidopoiesi .\n125Capitolo V Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#126": "5.2 Struttura\nLe Proteine sono polimeri (macromolecole costituite da numerosi gruppi mo-\nlecolari) costituite da un insieme di molecole di a-amminoacidi (detti ancheresidui amminoacidici), legate fra loro mediante legami peptidici tra il gruppocarbossilico di una molecola e il gruppo amminico della successiva, costi-tuendo nel loro insieme una catena polipeptidica (Figura 1). \nIl peso molecolare delle proteine può variare da circa 5000-10.000 in quelle\npiù piccole (40-80 amminoacidi) sino ad alcuni milioni per quelle più grandie complesse. Gli amminoacidi costituenti le proteine sono 20, tutti L-isomeri.Nelle proteine possiamo individuare una struttura primaria, una secondaria,una terziaria ed una quaternaria.\n5.2.1 La struttura primariaE’ determinata dalla sequenza dei diversi amminoacidi che costituiscono la\ncatena polipeptidica (Figura 2).Figura 1: struttura chimica di un amminoacido\nFigura 2: esempio di catena polipeptidica. I singoli amminoacidi sono legati tra loro da un\nlegame peptidico\n126Fondamenti della Scienza dell’Alimentazione Capitolo V",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#127": "La struttura primaria, cioè tipo e sequenza degli aminoacidi, condiziona la\nconfigurazione spaziale e la forma globale della molecola, dalle quali dipen-dono le proprietà biologiche.\n5.2.2 La struttura secondariaSi distinguo principalmente due tipi di struttura secondaria: la struttura ad a-\nelica e quella a b-foglietto. Queste strutture sono stabilizzate da legami a idro-geno tra gli amminoacidi appartenenti a una stessa catena, o tra gliam mi noacidi di catene diverse (Figura 3). Le l’α-cheratine, proteine che costituiscono i capelli, la lana e le unghie, hannocome struttura prevalente proprio quella dell’ α-elica.\nNella struttura β-foglietto si ha una disposizione di catene proteiche l’una ac-canto all’altra.La β-cheratina, proteina che costituisce la seta, ha fondamentalmente questastruttura.\n                                            \n5.2.3 La struttura terziaria\nE’ la forma stereoisomerica della proteina e determina la sua disposizione spa-\nziale (Figura 4). Scaturisce dall’interazione tra le catene laterali degli ammi-noacidi della proteina, che assume così una ben definita conformazionetridimensionale. Tali interazioni sono determinate dalle cariche elettriche at-trattive e repulsive, da ponti idrogeno e ponti salini, da interazioni idrofobiche,da ponti disolfuro, che ripiegano ad ansa la proteina. Quindi la successioneamminoacidica della struttura primaria determina la disposizione tridimensio-\nFigura 3: struttura dell’a-elica, a sinistra, e del b-foglietto, a destra\n127Capitolo V Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#128": "nale della proteina, che a sua volta definisce l’attività funzionale della proteina\nstessa.\n5.2.4 La struttura quaternaria\nSi tratta di una più complessa composizione strutturale in presenza di più ca-\ntene amminoacidiche all’interno della proteina determinata dalle interazioninon covalenti o da legami covalenti trasversali (Figura 5).\n5.3 Proteine semplici e proteine coniugate.\nUna delle più importanti classificazioni delle proteine è quella che distingue\nfra proteine semplici e proteine coniugate. Figura 4: organizzazione di una catena polipeptidica nello spazio. Gli amminoacidi apolari\n(in verde) formano il core idrofobico della proteina. Quelli polari, in rosso, si dispongono aformare le porzioni più esterne della proteina stessa\nFigura 5: proteina con struttura quaternaria, formata da quattro catene polipeptidiche, uguali\na due a due.\n128Fondamenti della Scienza dell’Alimentazione Capitolo V",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#129": "Le proteine vengono dette semplici se sono costituite soltanto da aminoacidi,\nmentre si parla di proteine coniugate e a esse è legato un gruppo non proteico(detto anche gruppo prostetico); le proteine coniugate possono essere distintein diverse classi a seconda del tipo di gruppo prostetico. Le diverse tipologiedi proteine coniugate sono le seguenti:• glicoproteine• lipoproteine• nucleoproteine• emoproteine • metalloproteine• fosfoproteine• flavoproteine. \n5.3.1 Funzioni La classe più numerosa delle proteine è costituita dagli enzimi con funzione\nregolatrice, che influenzano la velocità dei cicli metabolici.Gli ormoni regolano l’attività cellulare fisiologica, come l’adrenalina che mo-dula la trasmissione degli impulsi nervosi, come la tiroxina che accelera le os-sidazioni intracellulari, come l’insulina regolatrice del metabolismo glucidico.Ci sono proteine che fungono da materiale di riserva, come l’albumina del-l’uovo, la caseina del latte e la ferritina, che permette l’accumulo di ferro nellamilza.Anche la gliadina e la zeina presenti rispettivamente nel seme di grano e di\nmais, hanno la stessa funzione di riserva.Altre proteine hanno funzione di trasporto, legando e trasportando sia attra-\nverso le membrane cellulari sia nel flusso sanguigno ioni o molecole com-plesse, come l’ emoglobina presente negli eritrociti, come la mioglobina nel\ntessuto muscolare, le lipoproteine nel sangue\nCi sono ancora proteine che servono da elementi strutturali , come le glico-\nproteine del collagene e l’ elastina nei legamenti o la cheratina nei capelli e\nnelle unghieAlcune proteine hanno una funzione protettiva o difensiva, come la trombina\ned il fibrinogeno nel sistema di coagulazione del sangue, come la catalasi e\nla glutationeperossidasi, che svolgono azione di difesa dei radicali liberi, come\nle immunoglobuline del sistema immunitario.\n129Capitolo V Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#13": "1.1 I Micronutrienti\n1.1.1 I Sali Minerali\nI Sali minerali, sostanze inorganiche, hanno assunto notevole importanza nu-trizionale negli ultimi anni poiché, grazie a raffinate tecniche analitiche, è statopossibile evidenziare le svariate funzioni che esercitano.I sali minerali rappresentano circa il 6.2% del peso corporeo e sono costituitiin prevalenza, da composti di fosforo, calcio, sodio e potassio, di cui la mas-sima parte è localizzata nello scheletro sotto forma di fosfato e carbonato dicalcio. I macro ed i microminerali svolgono funzioni essenziali per la vita dell’uomo,entrano nella costituzione delle cellule e dei tessuti dell’organismo, come laformazione di denti e ossa, sono coinvolti nella regolazione dell’equilibrioidrosalino, nell’attivazione di numerosi cicli metabolici e costituiscono fattorideterminanti per la crescita e lo sviluppo di tessuti e organi.Non forniscono energia e durante la cottura o il riscaldamento degli alimenti,non si alterano, ma possono disperdersi nell’acqua utilizzata per la cottura. L’organismo umano non è in grado di sintetizzarli e li assume tramite gli ali-menti e l’acqua. Altro elemento da tenere in considerazione, per la salute del-l’organismo umano, è la loro biodisponibilità , intesa come “la quota di\nelementi ingerita che è effettivamente assorbita, trasportata al sito di azione econvertita nella forma fisiologicamente (o tossicologicamente) attiva. Pertantoun alimento è in grado di coprire il fabbisogno di un oligoelemento se questoè presente non solo in quantità corretta ma anche in forma biodisponibile”(Fonte SINU-Minerali LARN 1996). L’assorbimento dipende dallo stato fi-siologico del soggetto e dalla interazione con altri componenti della dieta,come, ad esempio, fitati ed ossalati.Il fabbisogno giornaliero è basso, una corretta ed equilibrata alimentazione èsufficiente ad reintegrarli, anche quando vengono persi con sudore, urine efeci. Si possono classificare in: • Macroelementi o elementi presenti in discrete quantità nell’organismo: Ca,\nP, Mg, S, Na, K, Cl, il cui bisogno giornaliero è dell’ordine di grammi o didecimi di grammo.\n• Microelementi o oligoelementi o elementi presenti in tracce nell’organismo,\nil cui bisogno giornaliero è dell’ordine di milligrammi o microgrammi, aloro volta classificati in :\n- Essenziali: Fe, Cu, Zn, I, Se, Cr, Mn, Mo, Co per i quali è dimostrabile\nche una loro carenza può compromettere funzioni fisiologiche importanti,\n13Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#130": "5.4 Digestione delle proteine\n5.4.1 Enzimi pancreatici\nLa digestione delle proteine consiste nella disgregazione meccanica, chimicaed enzimatica delle proteine contenute nei cibi che vengono ridotte a unità piùpiccole. La digestione comprende diversi passaggi, tra cui l’estrazione meccanica delleproteine dal cibo, la denaturazione delle proteine e l’idrolisi dei legami pepti-dici. La proteina viene estratta meccanicamente dal cibo nel processo di mastica-zione e dall’azione dello stomaco. Il basso pH dello stomaco svolge un ruolofondamentale nella denaturazione della proteina estratta, rendendola così piùaccessibile agli enzimi proteolitici dell’intestino. \nNell’intestino tenue enzimi provenienti da tale tratto intestinale e dal pancreas\nscindono le proteine alimentari in peptidi e in aminoacidi singoli. Le proteine devono essere digerite fino a ridurle in aminoacidi o di- e tripeptidiprima che possa aver luogo l’assorbimento, sebbene a volte possano essereassorbiti peptidi di dimensioni maggiori.Per le prime fasi della digestione delle proteine sono importanti quattro tipi dienzimi:- pepsine (secrete dalle ghiandole gastriche sierose dello stomaco), \n- tripsina, - elastasi- chimotripsine (tutte prodotte dagli acini pancreatici).\nIl prodotto dell’azione di questi enzimi proteolitici è una serie di peptidi di di-verse dimensioni. Oltre alle endopeptidasi esistono delle esopeptidasi pancreatiche o carbossi-\npeptidasi A e B che staccano l’amminoacido all’estremità carbossi-terminale\ndel peptide.Le carbossi-peptidasi, prodotte in forma inattiva come procarbossi-peptidasi,sono attivate per azione della tripsina.Per completare la liberazione degli amminoacidi delle proteine alimentari ènecessario l’intervento di proteasi prodotte delle cellule dell’epitelio intesti-nale, le ammino-peptidasi o eso-peptidasi. Questi enzimi staccano gli amminoacidi all’estremità ammino-terminale deipeptidi.La digestione viene completata per l’intervento delle di- e tri-peptidasi presentisui villi dell’orletto a spazzola delle cellule intestinali.\n130Fondamenti della Scienza dell’Alimentazione Capitolo V",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#131": "Gli amminoacidi sono assorbiti dalle cellule epiteliali intestinali ed immessi\nnel circolo sanguigno.\n5.4.2 Deaminazione\nLa rimozione del gruppo NH2(deaminazione) può avvenire in tre modi di-\nversi:-Deaminazione ossidativa : consiste nella rimozione del gruppo amminico –\nNH\n2mediante deidrogenazione dell’amminoacido, seguita da un’idrolisi.\nUn esempio di deaminazione ossidativa è quella catalizzata dalla glutammicodeidrogenasi presente nel fegato, rene, cuore, muscolo, cervello.Con questa reazione, l’amminoacido acido glutammico è trasformato in acidoalfa-chetoglutarico ed ammoniaca. La reazione è reversibile.\n5.4.3 Transaminazione\nI processi di transaminazione consistono in uno scambio di un gruppo ammi-nico NH2 da un amminoacido (donatore) ad un chetoacido (accettore)\nGli amminoacidi donatori si trasformano in chetoacidi, ed i chetoacidi accettorisi trasformano in amminoacidi.I chetoacidi accettori sono solo:- acido piriuvico , che si trasforma nell’amminoacido alanina\n-acido alfa-chetoglutarico, che si trasforma nell’amminoacido acido glutam-mico \n- acido ossalacetico , che si trasforma nell’amminoacido acido aspartico\nLe reazioni sono catalizzate da enzimi che prendono il nome di transaminasi\n5.5 Transaminasi\nLe transaminasi sono presenti in grandi quantità soprattutto nel fegato e la loro\ndeterminazione nel plasma è utilizzata nello studio delle malattie epatiche.ALT (alanina transaminasi) o GPTL- alanina + 2-chetoglutarico piruvato + L-glutammatoAST (aspartato transaminasi) o GOTL-aspartato + 2-chetoglutarato ossalacetato + L-glutammato\n5.5.1 Trans-desaminazione\nL’acido alfa-glutarico è l’accettore principale delle reazioni di transaminazionecon formazione dell’amminoacido acido glutammico.\n131Capitolo V Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#132": "Quest’ultimo, se non è impegnato in particolari destini metabolici, viene at-\ntaccato dall’enzima glutammico-deidrogenasi, riformando l’acido alfa-cheto-glutarico.Questa combinazione di transaminazione e deamminazione ossidativa prendeil nome di trans-desaminazione\n5.6 Ciclo dell’urea\nIl gruppo amminico NH\n2degli amminoacidi può essere trasferito ad un che-\ntoacido tramite la transaminazione, dando origine a nuovi amminoacidi, ma\nalla fine con la trans-desaminazione o la deaminazione ossidativa, è sempreliberato sotto forma di ammoniaca NH\n3.\nL’ammoniaca è tossica per l’organismo, perché è una base relativamente forte.Questo fa sì che nel nostro organismo l’ammoniaca si trovi in forma protonatacome ione ammonio NH\n4+\nL’ammoniaca prodotta in tutti i tessuti è trasformata in urea nel fegato.Solo in piccola parte viene usata per sintetizzare l’amminoacido glutammina.La formazione di urea è un processo ciclico\n5.7 Destino dello scheletro carbonioso degli amminoacidi\nI processi di deaminazione ossidativa, transaminazione, transdesaminazione,\ndecarbossilazione, permettono la degradazione degli amminoacidi con libera-zione del gruppo NH\n2.\nVie metaboliche specifiche comportano la formazione di intermedi metabolici,che possono essere convertiti in glucosio o in Acetil CoA.Pertanto gli aminoacidi si dividono in:- aminoacidi glucogenici (che formano acido piruvico o intermedi del ciclo\ndi Krebs)\n- aminoacidi chetogenici (che formano Acetil CoA o corpi chetonici)\n132Fondamenti della Scienza dell’Alimentazione Capitolo V",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#133": "Bibliografia\n• Christopher K. Mathews, Kensal Edward Van Holde, Biochemistry: Guide, Benjamin-Cum-\nmings Publishing Company, 1996, p. 1159, ISBN 978-0-8053-3931-4.\n• David L. Nelson, Michael M. Cox,  I Principi di Biochimica di Lehninger, 3ª  ed.,\nBologna, Zanichelli, febbraio 2002. ISBN 8808090353\n• Campbell, Neil A. (2003), Biologia, Edizione Italiana, Zanichelli\n• T.W.G. Solomons. Amminoacidi e Proteine. In Chimica Organica. Ed. Editoriale Grasso.\nBologna.\n• H. Hart, L.E. Craine and D.J. Hart. Amminoacidi, Peptidi e Proteine. In:Chimica Organica,\nquarta edizione italiana. Ed. Zanichelli. (2003).\n• Lodish H, Berk A, Matsudaira P, Kaiser CA, Krieger M, Scott MP, Zipurksy SL, Darnell\nJ, Molecular Cell Biology, 5th, New York, New York, WH Freeman and Company, 2004.\n133Capitolo VI Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#134": "CAPITOLO VI\nRIPARTIZIONE DEI MACRONUTRIENTI:\nIL CONCETTO DI DIETA EQUILIBRATA\nRiassunto\nL’equilibrio di una dieta deve essere garantito dall’armonica integrazione e\ngiusta proporzione dei diversi alimenti, tenendo conto della loro quantità equalità. La dieta quindi, intesa come la corretta alimentazione razionale edequilibrata che un soggetto sano deve assumere quotidianamente in funzionedelle proprie necessità biologiche e nutrizionali al fine di assicurare e man-tenere un buono stato di salute, deve soddisfare sua etimologia greca“\nδιαιτα ”, che significa “stile di vita e quindi anche alimentare”. Così, una\nvolta stabilito il fabbisogno energetico, la quota calorica deve essere ripartitatra i nutrienti fondamentali (proteine, lipidi, glucidi). Nel soggetto adulto e incondizioni fisiologiche normali, si considera ottimale una ripartizione ener-getica quotidiana che preveda il 10-15% di proteine, 25-30% di lipidi ed il45-60% di carboidrati, come risulta anche dal rapporto dell’Organizzazioneper l’Alimentazione e l’Agricoltura delle Nazioni Unite (F AO) insieme conl’Organizzazione Mondiale della Sanità (OMS). Le principali Linee Guidadelle più importanti associazioni scientifiche mondiali sulla base dei sempremaggiori e recenti studi epidemiologici e delle continue evidenze scientifichesono concordi nel riconoscere nella dieta mediterranea, un modello alimen-tare bilanciato, equilibrato e corretto, da anni illustrato attraverso la figuradi una piramide, che ha un ruolo fondamentale nella prevenzione di malattiemetaboliche, cardiovascolari e tumorali. \n134Fondamenti della Scienza dell’Alimentazione Capitolo VI",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#135": "6.1 Il concetto di dieta.\nE’ largamente diffuso nel linguaggio comune identificare il concetto di “ dieta”\ncon svariati significati tra cui: “l’evitare di mangiare un alimento o più di uno\nper perdere peso” oppure “l’astinenza dal cibo per motivi di salute” e così via.Quindi il termine dieta viene utilizzato per indicare un regime alimentare par-\nticolare, quale una dieta dimagrante per soggetti in soprappeso o obesi oppureuna dieta speciale indicata in determinate situazioni patologiche. Ma prima diassumere un tale significato nella patologia, è importante identificare in questoconcetto di dieta, il significato derivante dalla sua etimologia greca “\nδιαιτα ”,\nche significa “ stile di vita e quindi anche alimentare ”. Di conseguenza la dieta\ndeve essere intesa come la corretta alimentazione razionale ed equilibrata cheun soggetto sano deve assumere quotidianamente in funzione delle proprie ne-cessità biologiche e nutrizionali al fine di assicurare e mantenere un buonostato di salute. La dieta dunque, deve avere i requisiti dell’ “adeguatezza”:\ndeve cioè, consistere essenzialmente nella qualità e nella quantità degli ali-menti consumati in misura tale da assicurare il soddisfacimento dei bisogni dienergia e nutrienti, rispettando combinazioni e proporzioni tali da non arrecarerischi potenziali per la salute.Ne consegue che non esiste una alimentazione standard che vada bene pertutti, in quanto le esigenze biologiche del proprio organismo variano da indi-viduo a individuo. Nella sua più ampia e concreta accezione, la dieta non com-\nprende solo la cosiddetta razione alimentare, cioè il quantitativo alimentare insenso stretto, ma anche ogni possibile variazione della razione stessa in rela-zione alle modalità di cottura e di preparazione dei cibi, al numero e ritmo deipasti nell’arco della giornata. L’alimentazione è dunque un fenomeno dina-mico e vitale, che si diversifica nei giorni, nelle diverse età rappresentandoanche un momento di piacere nella percezione cosciente dei sapori. Per questomotivo, gli alimenti possiedono due importanti proprietà: un valore nutritivoe un valore edonistico. Quindi il cibo e gli alimenti in senso lato non rappre-sentano unicamente l’oggetto dei nostri bisogni metabolici, la modalità attra-verso la quale possano essere soddisfatti i diversi fabbisogni, ma ancheun’esperienza sensoriale in grado di trasformare lo stato d’animo di una per-sona, di influenzare comportamenti oltre che modulare o anche condizionaremolte funzioni nervose e fisio-biologiche. L’ alimentazione è dunque stile di\nvitanell’accezione più completa e vasta del termine. Alimentarsi implica sce-\ngliere, preparare, condividere con l’ambiente circostante e quindi l’atto delmangiare non è solo un’esperienza sensoriale o un processo fisio-biologico\n135Capitolo VI Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#136": "correlato ai bisogni nutrizionali, ma rappresenta anche un atto culturale in cui\nstimoli ambientali di varia origine e natura, meccanismi complessi, ormonalie neurosensoriali, le abitudini familiari e tutto ciò che comprende la storia e ilvissuto si intrecciano determinando e condizionando il nostro stile di vita equindi la nostra\nδιαιτα .C’è da precisare comunque, che la modifica della\ndieta fisiologica in relazione allo stato nutrizionale e metabolico del soggettoal fine di correggere gli errori alimentari e gli squilibri metabolici assume ilsignificato di dietoterapi a. La dietoterapi a ha lo scopo di eliminare i sintomi\ne i segni di malattie correlabili ad errori dietetici o causati dall’alimentazionestessa, minimizzare o ritardare l’evoluzione di molte malattie degenerativecroniche, correggere o prevenire la malnutrizione, fornire un supporto nutri-zionale nelle condizioni patologiche caratterizzate da deficienze nutritive cheidentificano uno stato più o meno evidente di malnutrizione. Quindi, risultafondamentale che l’elaborazione di una dieta tenga conto della valutazione\ndello stato nutrizionale, che in base alle funzioni svolte dai nutrienti (strutturali,energetiche e regolatrici) può essere considerato come la risultante di tre va-riabili: composizione corporea, bilancio energetico e funzionalità corporea.Per il tramite di queste variabili, lo stato nutrizionale è strettamente correlatoallo stato di salute. \n6.2 Caratteristiche di una dieta varia ed equilibrata.\nAlimentazione e prevenzione agiscono in sinergia definendo una nutrizione\nottimale, che abbia come obiettivo quello di potenziare le funzioni fisiologichein modo tale da garantire uno stato di benessere e di salute ottimale, ponendoparticolare attenzione su determinati alimenti in base alle loro peculiari carat-teristiche. La dieta diventa il mezzo più idoneo per prevenire malattie, disor-dini metabolici e disturbi funzionali e la sua composizione, intesa come sceltarazionale di alimenti e come ripartizione armonica dei vari nutrienti, prevedel’utilizzo dei LARN (Livelli di Assunzione di Riferimento di energia e Nu-trienti per la popolazione italiana, Revisione 2012). La dieta quindi, non èuguale per tutti: il bisogno energetico e nutrizionale di un bambino in fase diaccrescimento è sicuramente diverso da quello di un adulto che non ha neces-sità di crescere più, ma che deve mantenere un corretto funzionamento dellesue strutture biologiche; allo stesso modo, una donna in gravidanza o che al-latta, ha dei bisogni energetici e nutrizionali maggiori rispetto alla propria dietain condizioni normali; così, una persona con un livello di attività fisica mode-\n136Fondamenti della Scienza dell’Alimentazione Capitolo VI",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#137": "rato o pesante, nonché lo sportivo a livello amatoriale o agonistico, non può\navere la stessa dieta di un’altra persona dello stesso sesso ed età, ma sedentaria.E quindi, una volta stabiliti i presupposti fisiologici e teorici di un interventodietetico, un’alimentazione è razionale e capace di rispettare i gusti e le abi-tudini personali e di corrispondere in pieno alle esigenze reali dell’organismo,se presenta le caratteristiche di: - assicurare un’idonea introduzione di energia mediante una corretta propor-\nzione dei vari nutrienti\n- prevenire carenze o eccessi nutrizionali (assoluti o relativi per un determinato\nnutriente), che in un certo arco di tempo possono provocare disturbi o malattie.\nUn’alimentazione è razionale solo se è completa ed adeguata, cioè quando lascelta degli alimenti è quanto più possibile varia. Variare opportunamente ladieta, aumenta la probabilità di assumere regolarmente e nelle giuste quantitài nutrienti per coprire i relativi fabbisogni, che servono a garantire una nutri-zione ottimale e a proteggere il proprio equilibrio metabolico e psicofisico.Un’alimentazione variata ha inoltre il “potenziale obiettivo” di ridurre al mi-nimo i rischi correlati all’incompletezza di alcuni alimenti e talora, i pericoliconseguenti all’assunzione ricorrente di sostanze potenzialmente nocive pre-senti in certi cibi, o naturalmente o come conseguenza di manovre tecnologi-che di conservazione e/o di manipolazione industriale. Non esiste dunque,l’alimento completo in grado di fornire tutti i nutrienti nelle giuste quantità enei giusti rapporti, ad eccezione del latte nell’alimentazione del neonato neiprimi mesi di vita. La varietà di un’alimentazione razionale è strettamente le-gata anche alla moderazione nelle porzioni. L’equilibrio di una dieta deve essere garantito dall’armonica integrazione egiusta proporzione dei diversi alimenti, tenendo conto della loro quantità equalità.Quindi, una dieta per essere definita equilibrata deve considerare: - una necessità energetica individuale- un apporto ideale dei macronutrienti (carboidrati, proteine, grassi) che serve\na coprire il fabbisogno calorico quotidiano\n- una necessaria presenza nelle quantità raccomandate, di vitamine e sali mi-\nnerali, bioregolatori dei processi metabolici, nonché di acqua per il mante-nimento dell’equilibrio idro-salino\n- una buona presenza di fibra alimentare - una razionale distribuzione dei pasti nell’arco della giornata- una scelta di metodi di cottura più idonei al fine di preservare al meglio il\nvalore nutrizionale degli alimenti \n137Capitolo VI Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#138": "6.3 Dieta e attività fisica\nNessuna dieta, per quanto equilibrata e corretta, non sarà mai abbastanza ef-\nficace da sola nel controllare il peso corporeo o combattere l’eccesso di grassosenza un adeguato stile di vita attivo, che preveda un programma di attivitàfisica associato. La dieta di per sé, comporta sempre una perdita più o menorilevante di massa magra. Solo aumentando la quantità di movimento praticatospontaneamente (attività della vita di tutti i giorni) e/o in forma organizzata(sport, programmi di allenamento ecc.) possiamo incrementare quindi, in ma-niera significativa la spesa energetica del nostro organismo, consumare le ca-lorie che introduciamo con gli alimenti ed ottenere un mantenimento o, aseconda del grado di allenamento, indurre un aumento della massa metaboli-camente attiva. Secondo le principali organizzazioni scientifiche e come evi-denziato anche dal rapporto FAO/OMS, che fornisce raccomandazioni generaliin merito all’assunzione di nutrienti e agli obiettivi relativi all’attività fisicarispetto alla prevenzione delle principali patologie croniche, lo stile di vita at-tivo riduce il rischio di sviluppare una condizione di soprappeso e obesità el’insorgenza di malattie cardiovascolari e di molte altre patologie cronico-de-generative, quali ipertensione arteriosa, diabete di tipo II, osteoporosi e di al-cune forme di neoplasie. Per la riduzione del rischio di queste malattie, leindicazioni prevedono un impegno di tutti i giorni di almeno 30 minuti nellosvolgere qualunque attività fisica capace di indurre un impegno di entità mo-derata-intensa e di almeno 60 minuti, per mantenere un opportuno peso cor-poreo. \n6.4 Principi generali per impostare un profilo nutrizionale: ripartizione\ndei macronutrienti\nNella formulazione di una dieta individuale, in base ai parametri dell’età, del\nsesso, dello stile di vita e dell’attività fisica e del metabolismo basale, deveessere determinato il fabbisogno energetico giornaliero, definito sulla base distime del dispendio energetico che tenga presente informazioni relative al pesoreale o ideale e al profilo il più possibile esatto delle attività svolte. Una volta stabilito il fabbisogno energetico, la quota calorica deve essere ri-partita tra i nutrienti fondamentali (proteine, lipidi, glucidi). Nel soggetto adulto e in condizioni fisiologiche normali, si considera ottimaleuna ripartizione energetica quotidiana che preveda il 10-15% di proteine, 25-\n138Fondamenti della Scienza dell’Alimentazione Capitolo VI",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#139": "30% di lipidi ed il 45-60% di carboidrati, come risulta anche dal rapporto\ndell’Organizzazione per l’Alimentazione e l’Agricoltura delle Nazioni Unite(FAO) insieme con l’Organizzazione Mondiale della Sanità (OMS). Tale rap-porto è stato elaborato da vari esperti mondiali per esaminare e valutare lostato delle conoscenze sul ruolo dei vari nutrienti nell’alimentazione umana,in modo da fornire a tutti i Paesi informazioni aggiornate e raccomandazionipratiche relative al loro uso. \nproteine 10-15%\ngrassi 25-30%carboidrati 45-60%\nLe proteine non devono essere superiori ai valori raccomandati dai LARN per\nkg di peso corporeo (LARN, Revisione 2012) che corrispondono al 10-15%dell’energia totale e di queste 2/3 devono essere di origine vegetale e 1/3 diorigine animale. Questa quota dovrà aumentare solo in condizioni fisiologichedi accrescimento, gravidanza e allattamento. L’apporto di lipidi non deve superare il 30% delle kcal totali: questa quotacome livello di assunzione adeguata (AI) raggiunge il 40% nei lattanti, mentrein un’età compresa tra 1-3 anni, l’intervallo di riferimento per l’assunzionedei di macronutrienti (RI) è compresa tra 35-40% e dai 4 anni in poi, scendea 20-35%. La quota accettabile nell’adulto e nell’anziano, come pure in gra-vidanza e allattamento, è del 25% con un intervallo di riferimento per l’assun-zione dei di macronutrienti (RI) compreso tra 20-35%. La ripartizione suggerita della qualità dei lipidi prevede che: - gli acidi grassi saturi rappresentino il 10% delle kcal totali- gli acidi grassi monoinsaturi, in particolare l’acido oleico, rappresentino fino\nal 20% delle kcal totali\n- gli acidi grassi polinsaturi siano compresi tra il 5 -10% delle kcal totali: di\nquesti, il 4-8% deve essere rappresentato da n-6 PUFA, l’0.5-2% da n-3PUFA \ngli acidi grassi idrogenati trans non più dell’1% delle kcal totaliPer quanto riguarda i carboidrati, la loro assunzione fornisce il restante fabbi-sogno calorico, compreso tra il 45-60% dell’energia totale e di questi, gli zuc-cheri semplici devono rappresentare una quota inferiore al 15% dell’energiatotale. L’importanza di rispettare questi quantitativi è legata a varie ragioni:essi forniscono energia facilmente disponibile per il metabolismo ossidativo,aiutano a mantenere la glicemia in omeostasi e una buona funzionalità ga-\n139Capitolo VI Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#14": "oppure che fanno parte di strutture organiche preposte a ruoli vitali\nnell’organismo;\n- Probabilmente essenziali : Si, V , Ni, B;\n- Potenzialmente tossici , alcuni dei quali svolgono forse funzioni essenziali\na basse concentrazioni: As, Al, Pb, Cd, Hg, Li, Sn, F;\nOccorre chiarire che la tossicità è una caratteristica di tutti gli elementi e di-pende solo dalla quantità del minerale che perviene all’organismo. Gli ioni di alcuni elementi metallici in particolare Fe, Ca, Mg, Mn, Zn, Cuhanno rilevante importanza come cofattori (ioni o molecole non proteiche),\nla cui presenza è indispensabile perché l’enzima possa svolgere la sua attivitàcatalitica. Ad esempio, tutti gli enzimi che utilizzano ATP richiedono la pre-senza di ioni Mg\n2+.\nMacroelementiIl Calcio è il minerale presente in maggiore quantità nell’organismo, tanto da\nessere fondamentale per la costruzione di scheletro e denti (99%). Ma svolgealtre importanti funzioni quali la determinazione della regolazione della con-trazione muscolare, la conduzione dell’impulso nervoso, la coagulazione delsangue, regola la permeabilità cellulare e l’attività di numerosi enzimi. Una sua carenza può comportare rachitismo, crisi tetaniche, osteoporosi, dolorimuscolari, irritabilità, sindrome premestruale, spasmofilia. Possono verificarsianche condizioni da eccesso che portano a sonnolenza, nausea, vomito, statoconfusionale.Gli alimenti che principalmente lo contengono sono legumi, latte e derivati,uova e pesci, alghe di mare, basilico, maggiorana, timo. Le acque mineralicon residuo fisso di almeno 500 mg/l e tenore di Ca superiore a 150 mg/l. Leacque mediominerali (acque bicarbonato-calciche, ricche di calcio, povere disodio), assieme alle acque potabili a più alto tasso di Ca (200-300 mg/l), pos-sono concorrere alla copertura del fabbisogno di calcio, ad esempio, durantela menopausa e la senilità, in questo caso a causa della riduzione dell’assorbi-mento di circa il 50% di calcio e della diminuita capacità di sintesi endogenadella vitamina D. Solo il 35 - 45 % del calcio apportato dagli alimenti viene assorbito. La vita-mina D esercita un ruolo importante, in qualità di ormone, sull’omeostasi delcalcio. Con diete povere di calcio si attiva il meccanismo di assorbimento di-pendente dalla vitamina D. Alcuni costituenti dei vegetali possono diminuireil suo assorbimento come ad esempio ossalati e fitati. Lo stesso accade peraltri sali minerali. Il fabbisogno giornaliero varia a seconda del periodo di vita,\n14Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#140": "strointestinale e contribuiscono a mantenere sotto controllo il peso corporeo.\nInoltre, anche il rapporto FAO/OMS evidenzia come il consumo di carboidrati,con un basso indice glicemico, tenda a ridurre le possibilità di sovra-consumoed il rischio di obesità, soprattutto in età evolutiva.Particolare attenzione viene posta alla fibra alimentare, il cui livello ottimaledi introito di deve raggiungere i 25 g/die anche in caso di apporti energetici <2000 kcal/die. L’intervallo di riferimento per l’assunzione dei di macronu-trienti (RI) negli adulti indica 12.6-16.7 g/1000 kcal. Un consumo costante difrutta, verdura, legumi, cereali integrali può assicurare questo apporto e comeindica il documento FAO/OMS, può essere raggiunto con un quantitativo difrutta e verdura ≥ 400 g/die. In questo modo, anche l’apporto di vitamine, oli-goelementi e minerali è garantito.Inoltre, come i LARN (Revisione del 2012) evidenziano, è importante che cisia un’adeguata assunzione di acqua, che nell’adulto corrisponde a 2.0 L perle femmine, 2.5 L per i maschi. Per una idonea strategia preventiva dell’ipertensione arteriosa, come indicatodalle Linee Guida di una Sana Alimentazione (2003) anche dal rapportoFAO/OMS, viene indicato un quantitativo di sale discrezionale non superiorea 5 g/die . L’EFSA (European Food Safety Authority) nel 2010 ha aggiornato i precedentipareri europei in questo settore, tenendo conto delle nuove evidenze scienti-fiche e delle recenti raccomandazioni emanate a livello nazionale e interna-zionale. Sono stati presentati così, i primi pareri sui valori dietetici diriferimento (DRV) per i carboidrati, le fibre alimentari, i grassi e l’acqua chesono del tutto simili a quanto già sostenuto e acclarato negli ultimi anni dallacomunità scientifica internazionale. Così come risulta anche dalla revisione2012 dei nostri LARN, particolare attenzione viene posta all’assunzione di250 mg/die di n-3PUFA per il loro ruolo fondamentale nella riduzione del ri-schio di cardiopatie. Una dieta equilibrata e bilanciata prevede anche una corretta distribuzionedelle calorie durante la giornata: le Linee Guida Italiane per una Sana Alimen-tazione (2003) suggeriscono di assumere con la prima colazione circa il 15-20% delle calorie giornaliere (il 15% se la colazione è abbinata ad unospuntino di metà mattina, il 20% se lo spuntino non c’è), con il pranzo il 45%(il 40% si può raggiungere se nel pomeriggio è abbinato uno spuntino del 5%)e con la cena il 35%.\n140Fondamenti della Scienza dell’Alimentazione Capitolo VI",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#141": "Tale distribuzione può subire delle variazioni in funzione dello stato nutrizio-\nnale e delle abitudini del soggetto senza che queste siano necessariamentecausa di danno o disordini metabolici rilevanti. Numerose evidenze scientifiche sottolineano l’importanza della prima cola-zione: l’abitudine a consumare regolarmente la prima colazione, si associa admigliore stato di salute e benessere a tutte le età e soprattutto, può esercitareeffetti positivi sui parametri metabolici correlati al rischio cardiovascolare in-fluenzando sia direttamente che indirettamente, la composizione della dieta.Una serie sempre più numerosa di studi osservazionali, conferma i benefici diuna prima colazione consumata regolarmente, specie se ricca di cereali ed abase di carboidrati a basso indice glicemico: si assumono macro e micronu-trienti in quantità più adeguate rispetto a chi non ha questa abitudine. Più fibra,calcio, vitamine, minerali e meno grassi, colesterolo e calorie totali sembranocaratterizzare nello specifico, il profilo nutrizionale di chi fa regolarmente laprima colazione. \n6.5 Esempio di schema dietetico secondo i LARN\nPer esprimere i fabbisogni energetici e in nutrienti indicati dai LARN in quan-\ntità di alimenti, si rende necessario quantificare in modo standardizzato le porzioni di alimenti. Per questo motivo,si definisce porzione la quantità standard di alimento espressa in g, che si as-\nsume come unità di misura da utilizzare per un’alimentazione equilibrata, chesi è cercato di ricavare sulla base dei consumi medi di alimenti della popola-zione italiana, degli alimenti e pietanze tipici della nostra tradizione e delle\n141Capitolo VI Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#142": "grammature di alcuni prodotti confezionati.\nAl di là del contenuto in nutrienti, la porzione deve essere innanzitutto di di-mensioni “ragionevoli”; deve cioè soddisfare le aspettative edonistiche delconsumatore ed essere conforme alla tradizione alimentare. Le quantità digrammi proposte per ciascuna porzione assumono perciò il significato di“unità pratica di misura della quantità di alimento consumata”.\nPorzioni standard nell’alimentazione italiana e numero di porzioni per\ncomporre una razione alimentare giornaliera di circa 2000 kcal\n142Fondamenti della Scienza dell’Alimentazione Capitolo VI\n",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#143": "Per impostare uno schema dietetico è necessario indicare il numero di porzioni\ndi alimenti di ciascun gruppo, da consumare in una giornata per un determinatofabbisogno di energia. Nella composizione di uno schema dietetico giornalierodi circa 2000 kcal (la scelta di tale fabbisogno è esemplificativa e non limital’applicazione di questo metodo ad altre situazioni fisiologiche) prevede che:- il pane va consumato tutti i giorni nelle porzioni indicate. I prodotti da forno\npossono essere consumati a colazione o fuori pasto. \n- per i secondi piatti, si consigliano nell’arco della settimana, le seguenti fre-\nquenze di consumo: 3-4 porzioni di carne, 2-3 porzioni di pesce, 3 porzionidi formaggio, 2 porzioni di uova, 1-2 porzioni di salumi. Almeno 1-2 voltela settimana, il secondo piatto va sostituito con un piatto unico a base di pastaN°GRUPPO DI ALIMENTI ALIMENTI PORZIONE (g)PORZ/DIE\nLATTE E DERIV ATI • latte • 125 (un bicchiere) 2\n• yogurt • 125 (un vasetto)\n• formaggio fresco • 100 0-1\n• formaggio stagionato • 50\nCARNE, PESCE, UOV A - carne fresca • 100 (a crudo) \n- carne conservata (salumi) • 50 1\n- pesce • 150 (a crudo)\n- uova • un uovo 0-1\n(circa 50 g a crudo)\nLEGUMI •legumi freschi • 100 (a crudo) 0-1\n• legumi secchi • 30 (a crudo)\nCEREALI E TUBERI • pane • 50 3-4\n• prodotti da forno • 50 0-1\n• pasta o riso (*) • 80 (a crudo)\n• pasta fresca all’uovo (*) • 120 (a crudo) 1\n• pasta fresca ripiena (*) • 180 (a crudo)\n• patate  200 (a crudo) 0-1\nORTAGGI E FRUTTA • insalate • 50\n• ortaggi • 250 (a crudo) 2-4\n• frutta o succo • 150 \nCONDIMENTI •olio • 10 3\n• burro • 10 0-1\n• margarina • 10\n(*) in minestra, la porzione è dimezzata. (tabella adattata dai LARN, 1996)\n143Capitolo VI Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#144": "o riso con legumi, nelle porzioni indicate per ognuno dei due alimenti. \n- il latte e/o lo yogurt vanno consumati tutti i giorni (due porzioni). Una tazza\ndi latte equivale a circa due bicchieri. \n- tra le porzioni di verdure e ortaggi (2-4) viene inclusa una eventuale porzione\ndi minestrone o passato di verdure, nonché una porzione utilizzata quale\ncondimento per pasta e riso (zucchine, melanzane, funghi, pomodori freschi,carciofi, asparagi, ecc.). \n- le porzioni di frutta e succo di frutta si possono consumare anche fuori pasto. - per i grassi da condimento preferire sempre il consumo di olio di oliva; burro\no margarina sono ammessi saltuariamente. \nTenendo conto così, della corretta distribuzione dei tre pasti durante la giornatasecondo le combinazioni del numero di porzioni e alternando gli alimenti dellostesso gruppo, la valutazione nutrizionale su di uno schema dietetico di 2000kcal di 7 giorni, mostra che:- le proteine sono 75 g e rappresentano circa il 15%dell’ energia totale (ET)- i carboidrati sono 290 g (220g di amido e 70 g di carboidrati solubili) e\nrappresentano circa il 55%dell’ ET\n- i lipidi sono 65 g e rappresentano circa il 30%dell’ ET, di cui i saturi sono il\n7%, i monoinsaturi il 18% e i polinsaturi il 4%\n- la fibra è presente in 23 g- il colesterolo in 255 mgPer quanto riguarda i minerali, i quantitativi sono riportati quanto segue:- il calcio è presente in 876 mg, il fosforo in 1200 mg, il ferro in 11 mg, il\nsodio in 2270 mg, il potassio in 3042 mg \nPer quanto riguarda le vitamine, i quantitativi sono riportati quanto segue:- la vit B\n1è presente in 1,02 mg, la vit B2è presente in 1,6 mg, la vit C è\npresente in 163 mg, la vit PP è presente in 29 mg, la vit A è presente in 935mg\nSi evince che la copertura dei LARN è soddisfacente per quanto riguarda imacronutrienti, mentre non sempre lo è per alcuni micronutrienti. Quindi,anche in vista della nuova e prossima revisione dei LARN, è necessario porreparticolare attenzione e fare le dovute valutazioni del caso.\n6.6 Piramide alimentare e dieta mediterranea\nAl fine di orientare la popolazione verso comportamenti alimentari più salutari,\nil Ministero della Salute ha affidato ad un Gruppo di esperti (D.M. del\n144Fondamenti della Scienza dell’Alimentazione Capitolo VI",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#145": "1.09.2003) il compito di elaborare un modello di dieta di riferimento che sia\ncoerente con lo stile di vita attuale e con la tradizione alimentare del nostroPaese. La Piramide Alimentare è la “chiave grafica” per visualizzare il modellomediterraneo e capire come adottarlo. Nasce così la piramide settimanale dellostile di vita italiana che si basa sulla definizione di Quantità Benessere (QB)sia per il cibo che per l’attività fisica. Da questo modello di dieta scaturisce lapiramide alimentare italiana, che elaborata dall’Istituto di Scienza dell’Ali-mentazione dell’Università di Roma “La Sapienza”, indica i consumi alimen-tari giornalieri consigliati. Vengono date indicazioni sulle quantità di cibo daconsumare ogni giorno secondo il criterio della quantità benessere QB (por-zioni di alimenti in grammi). Le QB di cibo e di movimento, se opportuna-mente adattate alle esigenze del singolo individuo, consentono di orientare lostile di vita verso un equilibrio tra consumo alimentare e spesa energetica. Le caratteristiche di questo tipo di alimentazione le ritroviamo nella dieta me-diterranea, che nella sua concezione generale adotta pienamente la definizionedi \nδιαιτα in quanto non rappresenta soltanto un insieme di indicazioni sul re-\ngime alimentare da seguire, ma racchiude in sé un vero e proprio stile di vita. La dieta mediterranea infatti, affianca ad un’alimentazione bilanciata, com-posta essenzialmente da prodotti freschi locali e di stagione, lo svolgimentodi una moderata ma costante attività fisica, il rispetto per il territorio e per labiodiversità e una quotidianità fatta di pasti conviviali, feste e tradizioni in unclima di accoglienza che la rendono un eccellente modello, unico nel suo ge-nere. Il riconoscimento nel 2010 da parte dell’UNESCO come patrimonio im-materiale dell’umanità, ne dimostra l’importanza nella vita delle popolazionimediterranee e il suo potenziale impatto sulla vita e la salute delle popolazionidi tutto il mondo.Sulla base di più recenti studi epidemiologici e delle continue evidenze scien-tifiche sul ruolo fondamentale della dieta mediterranea nella prevenzione dimalattie metaboliche, cardiovascolari e tumorali, è stata presentata nel 2011dalla Mediterranean Diet Foundation, in associazione con il Forum on Medi-terranean Food Cultures la piramide alimentare mediterranea. \nLa nuova Piramide della Dieta Mediterranea si basa su una dieta mediterranearivisitata all’insegna della modernità e del benessere, che tiene conto dellediverse tradizioni culturali e religiose e le differenti identità nazionali, maanche dell’evoluzione dei tempi e della società, evidenziando l’importanzabasilare dell’attività fisica, della convivialità a tavola e dell’abitudine di bereacqua e suggerendo poi, di privilegiare il consumo di prodotti locali su base\n145Capitolo VI Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#146": "stagionale. Si rivolge a tutti gli individui sani di età compresa tra i 18 e i 65\nanni. La piramide alimentare della dieta mediterranea fornisce tutti gli elementiper poter seguire un’alimentazione equilibrata.\nBibliografia \n• Shils M.E., Olson J.A., Shike M., Modern NUTRITION in health and disease, eighth edi-\ntion, Lea & Febiger \n• Willet CW (1994) Diet and health: what should we eat? Science, 264, 532-537, 1994\n• OMS-WHO Global recommendations on physical activity for health. Geneva: WHO, 2010• INRAN, Linee Guida per una sana alimentazione italiana” (revisione 2003), Roma • LARN “Livelli di Assunzione Raccomandati di Energia e Nutrienti per la Popolazione Ita-\nliana SINU (Revisione 1996) EDRA\n• LARN “Livelli di Assunzione di Riferimento di Nutrienti ed energia per la popolazione ita-\nliana SINU (Revisione 2012)\n• The Joint WHO/FAO Expert Consultation on diet, nutrition and the prevention of chronic\ndisease: process, product and policy implications Public Health Nutrition: 7 (1A), 245-2502003, DOI: 10.1079/PHN2003592 \n• EFSA, Scientific Opinion on establishing Food-Based Dietary Guidelines, 2010• Marangoni F., Poli A. et al Documento di consenso sul ruolo della prima colazione nella ri-\ncerca e nel mantenimento della buona salute e del benessere, Nutrition Foundation of Italy(NFI) 2009\n• Vannozzi, Leandro. Lineamenti di Dietoterapia e nutrizione clinica. Pensiero Scientifico\nEditore \n• www.piramideitaliana.it• Keys A. Coronary heart disease in seven countries. Circulation 1970; 41• Bach-Faig A, Berry EM, Lairon D et al. Mediterranean diet pyramid today. Science and\ncultural updates. Public Health Nutr 2011; 14(12A): 227-84\n146",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#147": "147Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#148": "CAPITOLO VII\nV ALUTAZIONE DELLO STATO NUTRIZIONALE: \nCENNI DI ANTROPO-PLICOMETRIA\nRiassunto\nLo stato nutrizionale definisce gli effetti dei nutrienti e degli altri componenti\ndegli alimenti sulle funzioni corporee, nonché l’integrità anatomica dei tessuti,organi e apparati del corpo. Questa affermazione è corretta per tutti gliindividui, tuttavia occorre tener presente altre variabili che influenzanoulteriormente lo stato di nutrizione. La variabile biologica, cioè lecaratteristiche intrinseche e peculiari di ciascun individuo (composizionecorporea) e la funzionalità corporea. La valutazione dello stato nutrizionalesi propone di identificare problemi legati ad una cattiva alimentazione inindividui che richiedono un intervento specifico. Questo capitolo fornirà indicazioni nella valutazione dello stato nutrizionalee metodi di studio nella popolazione adulta.\n148Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#149": "7.1 Valutazione dello stato nutrizionale\nLa valutazione dello stato si propone di identificare in modo più certo gli in-\ndividui con problemi nutrizionali sia per eccesso che per difetto, alcuni deiquali richiedono un intervento specifico. Il metodo ideale per la valutazionedello stato nutrizionale deve possedere caratteristiche quali, semplicità, mo-desto impiego di tempo, costo contenuto.La sua utilità può essere giudicata in termini di precisione, accuratezza, sensi-bilità e specificità. La precisione corrisponde alla riproducibilità, in altre parolealla capacità di ottenere un valore dato costante nella misurazione di una datavariabile. L’accuratezza indica quanto la misura ottenuta è vicina (fino a corri-spondere) al valore reale. La sensibilità rappresenta nel caso specifico la ca-\npacità di identificare gli individui che rientrano in una malnutrizione perdifetto e per eccesso, mentre la specificità è la capacità nel discriminare inmaniera corretta individui malnutriti con un normale stato di nutrizione.Lo stato di nutrizione di un uomo è la condizione biologica presente all’atto\ndell’osservazione , considerata come risultante dell’equilibrio dinamico che,\nin ciascun momento, si instaura fra bisogni di nutrienti e di energia ed illoro soddisfacimento, in dipendenza della disponibilità dei nutrienti e dellaloro corretta utilizzazione .\nFatte queste premesse dobbiamo considerare che, ormai da diversi anni, l’Or-ganizzazione Mondiale della Sanità (OMS/WHO) sottolinea la relazione uni-voca esistente tra stato di salute e stato nutrizionale. Si definisce stato di salute: “Uno stato di completo benessere fisico, mentale e sociale e non la semplice\nassenza dello stato di malattia o infermità ”\nPossiamo quindi affermare che il deterioramento, per eccesso o per difetto,dello stato di nutrizione influenzi lo stato di salute e viceversa. Questo ci per-mette di superare la definizione iniziale di stato di nutrizione che mostra uncontenuto limitato per quanto riguarda gli aspetti pratici della valutazione dellostato nutrizionale. Possiamo così elaborare una definizione “operativa” fondatasulla relazione esistente tra composizione corporea, funzionalità corporea, bi-lancio energetico, stato nutrizionale e stato di salute.Nel soggetto sia in condizioni fisiologica (adolescente, sportivo, gravidanza e allat-tamento) che patologica, la relazione univoca tra stato di salute e stato di nutrizionesi sta imponendo in tutta la sua importanza in quanto si assiste a rapide modificazionimetaboliche che possono essere monitorate e quantificate attraverso lo studio dellostato nutrizionale. Diventa quindi di fondamentale importanza saper cogliere, nellemetodiche che gli specialisti hanno a disposizioni, i relativi vantaggi e svantaggi.\n149Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#15": "adolescenza, gravidanza e allattamento. In generale, per l’adulto sono neces-\nsari 800 mg/die di calcio.Lo zolfo si trova nell’organismo in numerosi composti organici come gli am-\nminoacidi solforati, metionina e cisteina (e quindi nelle proteine e negli ormonipolipeptidici), vitamine e coenzimi (tiamina, biotina, acido pantotenico, co-enzima A), alcuni mucopolisaccaridi (eparina, condroitinsolfati), nel glutatione(importante sistema redox delle cellule), nei solfatidi e negli acidi biliari (acidotaurocolico), nell’insulina.E’ assorbito nel tenue soprattutto sotto forma di amminoacidi (un adeguatoapporto di proteine con la dieta soddisfa anche il bisogno di zolfo) e di solfatiinorganici. Lo zolfo viene in gran parte utilizzato dall’organismo per la sintesidella cisteina, dei mucopolisaccaridi, dei solfatidi. Sottoforma di solfato faci-lita l’eliminazione urinaria di sostanze fenoliche e di ormoni, legandosi adesse e rendendole più solubili. Entra a far parte della composizione della car-tilagine, del sistema di detossicazione epatico, della composizione di cute, un-ghie e capelli. Si trova in alimenti di origine vegetale, quali germe e crusca di grano, semi dilino, aglio, cipolla, cavoli e in alimenti di origine animale quali ostriche, trota,caciotta, coniglio, tacchino, uova, formaggi.È difficile riscontrare carenze da zolfo se la dieta contiene quantità adeguatedi proteine animali: è per questo che non è stato stabilito uno specifico valoreper il fabbisogno di questo minerale. Una sua carenza può provocare artrosi,dolori alle articolazioni, fragilità di unghie e capelli, intossicazioni da alcol edinquinanti. È invece provato che l’assunzione eccessiva di aminoacidi solforaticausa problemi di sviluppo fisico e una crescita scarsa.\nMicroelementi o oligoelementi\nIl Ferro è il costituente dell’emoglobina, della mioglobina, componente di nu-\nmerosi sistemi enzimatici (sintesi e degradazione di amine quali dopamina e se-rotonina), dei citocromi, per il trasferimento di elettroni nella catena respiratoria. L’organismo umano adulto contiene generalmente 3-4 grammi di ferro, distri-buiti tra emoglobina, mioglobina, fegato, milza e midollo osseo. “Il ferro cheassumiamo è contenuto negli alimenti in due forme distinte: in pesce, carne ealcuni vegetali è presente il ferro emico, mentre nelle uova e nei prodotti lat-tiero caseari si trova il ferro non emico (più difficilmente metabolizzabile)”(Fonte http://www.epicentro.iss.it).Una sua carenza può provocare astenia, affaticabilità, facilità alle infezioni,anemia ferropriva, fragilità delle unghie e dei capelli; un suo eccesso, danni\n15Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#150": "7.2 Composizione corporea \nNello svolgimento di un’indagine antropometrica si procede selezionando, tra\nquelli opportunamente studiati e documentati, un modello standard adatto altipo di indagine desiderata. Dopodiché sarà possibile considerare singolar-mente e incrociare tra loro i parametri caratterizzanti il modello selezionato,traendone le opportune conclusioni. Ovviamente ogni tipo di modello saràstrutturato in funzione del tipo di indagine svolta, quindi ci si potrebbe trovarea considerare per modelli di indagine differenti parametri estremamente dif-ferenti, come il diametro di un’articolazione, la circonferenza di un distrettocorporeo o lo spessore di una plica cutanea e così via.La valutazione della composizione corporea significa studio della strutturadell’organismo. Le informazioni possono essere stratificate secondo 5 livelli.1) Modello atomico. La somma di tutti gli elementi presenti nel corpo fornisce\nmassa corporea, quindi: Massa corporea totale = O+C+H+N+Ca+ piùelementi presenti in minor concentrazione (Fe, Mg, Cu, K, ecc.). Questo tipo di misure viene effettuato generalmente su un cadavere oppuresu campioni isolati di tessuto. Nell’ambito di questo tipo di misura è pos-sibile infatti determinare il potassio totale, il sodio, il cloro, il fosforo, ilcalcio, l’azoto e infine il carbonio. Le tecniche impiegate per il dosaggiodi questi elementi sono decisamente sofisticate: richiedono complesse ap-parecchiature e competenze estremamente specialistiche.\n2) Modello molecolare. Gli elementi appena visti, si organizzano nel formare\nmolecole. Nell’organismo sono presenti più di 100.000 diverse molecole.Le componenti derivate da questa ulteriore organizzazione sonoprincipalmente: acqua, lipidi, glicidi, proteine e minerali. Gli elementipossono essere generalmente identificati mediante tecniche di marcaturacon specifici isotopi. Inoltre, tramite particolari tecniche di eccitazione deinuclei dei minerali, è possibile definire la componente minerale ossea(considerando che le ossa contengono più del 99% del calcio e dell’86%del fosforo presenti nell’intero organismo) e non ossea.\n3) Modello cellulare . A livello cellulare, l’organismo può essere suddiviso in\ntre principali compartimenti corporei: liquido intracellulare, liquidoextracellulare e componente solida extracellulare. Queste tre componentipossono essere determinate mediante l’utilizzo di traccianti radioattivi etecniche di diluizione.\n4) Modello anatomico . A questo livello vengono identificati dieci diversi\nsistemi: circolatorio, respiratorio, nervoso, tegumentario, muscolare,\n150Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#151": "endocrino, linfatico, digestivo, scheletrico e riproduttivo. Dal punto di vista\ndella composizione corporea questa suddivisione viene semplificataindividuando quattro componenti principali di interesse: tessuto adiposo,tessuto muscolare, tessuto osseo e sangue. Tecniche quali l’ecografia, latomografia assiale computerizzata e la risonanza magnetica nuclearepossono essere impiegate con successo per determinare queste quattrocomponenti.\n5)Nel quinto livello sono studiate le caratteristiche proprie dell’essere umanoquali taglia corporea, in termini pratico applicativi, la valutazione dellacomposizione corporea risponde all’idea che l’organismo sia suddiviso indifferenti compartimenti con specifico significato fisiologico e nutrizionale.\n151Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#152": "Il modello bi compartimentale distingue fra depositi adiposi dell’organismo\n(massa adiposa) e tessuti non adiposi (massa magra).\n7.2.1 Grasso essenziale e grasso di depositoNell’ambito della composizione corporea il grasso viene identificato secondo\ndue criteri funzionali diversi. Con il termine “grasso essenziale” (o primario)si identifica la frazione di grasso contenuta in alcuni distretti come midolloosseo, miocardio, polmone, milza, reni, intestino, muscolo scheletrico e alcuneparti del sistema nervoso. Il grasso essenziale è soggetto a un continuo utilizzometabolico da parte dei tessuti. Esistono differenze legate al sesso per quantoriguarda il grasso primario. Per esempio nei maschi a livello cardiaco è\nGrassi\nMassa metab. attivaMassa MagraAltri componenti\nN, Ca, P, K, S,\nNa, etc.\nIdrogeno\nCarbonio\nOssigeno\nATOMICOLipidi\nAcqua\nProteine\nMinerali\nMOLECOLARELipidi\nLiquidi extracellulari\nLiquidi\nintracellulari\nSolidi\nintracellulari\nSolidi  extracellulari\nCELLULARESangue\nTessuto adiposo sottocutaneo\nTessuto adiposo addominale\nMuscolatura scheletrica\nMuscolatura non scheletrica\n(organi)\nOssa\nANATOMICO\n152Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#153": "presente una quantità di circa 18,4 g di grasso, ovvero circa il 5,3% della massa\ndel miocardio, che in media è di 349 g; nelle femmine il grasso cardiaco è dicirca 22,7 g, ovvero l’8,6 % su una massa media di 256 g.Con il termine “grasso di deposito” si identifica la restante quota di grassocorporeo: nello specifico si tratta del grasso che ricopre i visceri, come lamaggioranza degli organi contenuti nella cavità addominale, proteggendolidal punto di vista meccanico e del grasso sottocutaneo. Esaminando datistatistici riferiti ai valori medi, rilevati su un campione di soggetti in buonasalute, si riscontra che il livello percentuale di grasso corporeo èsostanzialmente simile nei due sessi: 12% circa nei maschi e 15% circa nellefemmine, mentre la percentuale di grasso primario si rivela quattro voltesuperiore nelle donne rispetto agli uomini. Si ritiene che il maggiorquantitativo di grasso primario nella donna sia attribuibile alle specificheesigenze correlate alla maternità, dipendendo quindi dal complesso quadroormonale che la governa. Non è tuttora chiaro in quale misura il grasso dideposito possa direttamente rappresentare una effettiva riserva energetica dalpunto di vista calorico per i tessuti che lo contengono.\n7.2.3 Massa magra e free fat mass Spesso i concetti di “massa magra” e “ free fat mass” vengono erroneamente\nritenuti dei sinonimi, ma le cose stanno molto diversamente. La cosiddetta\n“massa magra” comprende nella sua composizione il 3% circa di grassoprimario, principalmente contenuto, come si è visto, a livello del sistemanervoso centrale, del midollo osseo e negli organi parenchimali. Con il terminedi “massa fat free” ci si riferisce a massa corporea a cui è stata completamentesottratta la massa grassa. Secondo quanto proposto da Behnke con il suomodello, la free fat mass rappresenta un valore determinabile esclusivamente\ntramite la misura diretta della composizione chimica effettuata su un cadavere.La massa magra invece è considerata una variabile misurabile in vivo, capacedi conservare nel corso della vita una costanza relativa, soprattutto per quantoriguarda la componente acquosa, la componente organica e quella inorganica.È quindi possibile affermare che in un soggetto adulto, normale e con normalelivello di idratazione, l’unica differenza tra massa fat free e massa magra èrappresentata dalla frazione di grasso primario. Nel calcolo della massa magrabisogna quindi ricordare che la quota di grasso primario rimane inclusa, quotaa cui si aggiungono la massa proteica, la massa ossea e la massa d’acquacontenuta nei tessuti.\n153Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#154": "Le metodiche più semplici nella pratica quotidiana per lo studio della\ncomposizione corporea sono l’antropometria, la psicometria e labioimpedenziomentria, mentre quelle più complesse, si ricordano:\nTecniche monocompartimentali \nLa distribuzione selettiva di traccianti introdotti dall’esterno o naturalmentepresenti in specifici compartimenti dell’organismo, che permette, misurandola diluizione dei traccianti stessi di calcolare le dimensioni del rispettivo pooldi distribuzione. Le tecniche che si valgono di questo principio sono definitetecniche della diluizione e misurano singoli componenti da cui vengono poiestrapolati altri compartimenti.\nMonocompartimentali \n• Misura dell’acqua corporea • diluizione di traccianti (isotopi radioattivi, isotopi freddi, sostanze chimiche) • impedenza bioelettrica• conduttanza magnetica • Misura del potassio corporeo (K40 spontaneamente presente o K40 iniettato) \nTecniche pluricompartimentali \nLe differenti proprietà fisico-chimiche dei componenti corporei quali pesospecifico, conduttività elettrica e magnetica, attenuazione di energia di raggiX o fotoni, risonanza magnetica nucleare o attivazione neutronica. Su questebasi è possibile misurare contemporaneamente più compartimenti\nPluricompartimentali\n• Analisi chimica diretta su cadavere • Peso specifico per pesata subacquea • Tomografia assiale computerizzata • Densitometria a doppio raggio fotonico • Risonanza magnetica nucleare • Attivazione neutronica• Pletismografia \nla idrodensitometra, dilutometria, determinazione dell’azoto corporea, DXA,\nTAC e NMR. Queste ultime utilizzate in ambito di ricerca. Fra le tecniche più utilizzate va subito citata l’antropometria (termine che\n154Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#155": "significa misurazione dell’uomo). Nel caso specifico essa prevede la\ndeterminazione di grandezze quali il peso, l’altezza, lunghezze segmentali,pliche adipose e sottocutanee e circonferenze. Il peso rappresenta un indicesemplice ed immediato nella valutazione dello stato nutrizionale, presentamodifiche e variazioni casuali nell’arco delle 24 ore (modifiche dell’acquacorporea). Nel caso dell’altezza la sua determinazione è fondamentale percalcolare l’IMC (indice di massa corporea) dato dal peso in kg e diviso per ilquadrato dell’altezza in metri). Questo indice nasce nel 1 9° secolo ad opera\ndel matematico belga da Lambert Adolphe Jacques Quetelet , il quale ha\ninventato la formula del calcolo del BMI per fornire un modo semplice eveloce il grado di obesità della popolazione ed assistere quindi il governo\nnella ripartizione delle risorse. Benché in modo approssimato, l’IMC nontiene conto dello sviluppo delle masse muscolari, esso fornisce delleindicazioni indirette sulle riserve adipose dell’organismo, definendo il soggettoin esame in sottopeso, normopeso, sovrappeso, obeso in diversi livelli.\nAntropometria \nIl peso, la statura e l’IMC rappresentano i parametri antropometrici piùutilizzati nella valutazione dello stato nutrizionale. Si riportano le tecniche di rilevazione più diffuse:\nPeso\nDefinizione: Si definisce fisiologico il peso che in relazione alle esigenzeenergetico-metaboliche, meccaniche, termoregolatrici è associato ad unaquantità di massa grassa ottimale e fisiologica e quindi ad un rapportoarmonico tra questa e la massa magra. Il peso corporeo è un indicatore grossolano della composizione corporea e delbilancio energetico. Rappresenta la somma di TBW, PM, MM, Gn e FM. Per-tanto, al livello molecolare, una modificazione del peso corporeo può dipen-dere dalla modificazione di uno o più di cinque compartimenti corporei. Poiché\nPM, Gn e FM hanno anche un significato energetico, BW è pure un indicatoregrossolano del bilancio energetico. Un bilancio energetico a lungo negativocausa infatti la contrazione di PM, Gn e FM ed uno a lungo positivo la loroespansione. La natura grossolana di BW come indicatore della composizionecorporea e del bilancio energetico deve essere tenuta ben presente per la pos-sibilità che le modificazioni di un compartimento corporeo mascherino quelledi un altro compartimento. La presenza di edema può infatti mascherare unaperdita di FM e PM e la rialimentazione di un paziente malnutrito per difetto\n155Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#156": "può produrre un aumento di BW dovuto all’espansione di ECW piuttosto che\nall’incremento di PM e FM. Un rapido incremento ponderale (ore o giorni)suggerisce l’occorrenza di un’espansione di ECW. Un lento aumento di BW(settimane o mesi) suggerisce invece l’occorrenza di un’espansione di FM.D’altro canto, è sempre necessario interrogarsi sulla composizione del calo\nponderale di un soggetto sovrappeso sottoposto a trattamento dietetico: la con-dizione ideale è ch’esso consista prevalentemente di FM e risparmi il più pos-sibile PM. Diete fortemente ipocaloriche o sbilanciate possono produrre unacontrazione di PM il cui risultato finale è la perdita di massa metabolicamente\nattiva , documentata dalla riduzione di BEE. Il controllo seriato di BEE può\nessere in effetti utilizzato come indicatore della massa metabolicamente attivanel paziente obeso in trattamento dietetico. Inoltre, prima di prescrivere untrattamento ad un paziente sovrappeso che ha praticato numerose diete forte-mente ipocaloriche o sbilanciate, è utile stabilire la sua “dotazione” attuale dimassa metabolicamente attiva attraverso la misurazione di BEE. La calorime-tria indiretta è l’unica tecnica suscettibile di applicazione clinica che consentadi seguire nel tempo la massa metabolicamente attiva ed il suo impiego è net-tamente superiore a quello di formule predittive, sempre sconsigliabile nel sin-\ngolo individuo .\nBW = peso corporeo; FM= massa grassa; TBW = acqua totale corporea; PM = massa proteica; \nMM = massa minerale; BCM massa = cellulare corporea; ECF = fluidi extracellulari; ECS = solidiextracellulari\n156Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#157": "Misurazione\nIl peso corporeo viene misurato impiegando una bilancia. Le bilance consi-gliate sono quella a bascula e quella elettronica. Per la misurazione del pesocon bilancia a bascula, l’operatore si pone di fronte al soggetto da misurare(dopo minzione). Questi sale sulla bilancia indossando solo la biancheria in-tima, sistema i piedi al centro della piattaforma di misurazione e distribuisceil peso uniformemente su di essi. Il peso viene approssimato a 0,1 kg. \nStatura\nUtilizzata congiuntamente al peso, la statura o altezza (BH, body height) con-sente di valutare le dimensioni corporee. La combinazione di peso e statura\nnella forma degli indici pondero-staturali consente una prima valutazione\nobiettiva della malnutrizione per eccesso o difetto e delle turbe dell’accresci-mento. La statura viene misurata con lo stadiometro. Al momento della misurazioneil soggetto è scalzo e pochi abiti cosicché l’operatore possa controllare costan-temente la posizione. I piedi poggiano su una superficie piana sistemata adangolo retto rispetto alla tavola verticale dello stadiometro ed il peso è egual-mente distribuito su di essi. La testa si trova nel piano orizzontale di Franco-forte (linea ideale tracciata tra il margine posteriore dell’orbita sx e il tragoomolaterale); le braccia pendolano liberamente ai lati del tronco con il palmodelle mani rivolto verso le cosce; i calcagni, uniti, poggiano contro il basa-mento della tavola verticale ed i margini mediali dei piedi formano un angolodi circa 60°. Le scapole e le natiche devono essere in contatto con la tavolaverticale. Si chiede al soggetto di fare un’inspirazione profonda mentre man-tiene la posizione eretta. Si porta quindi la barra mobile dello stadiometro incorrispondenza del punto più alto del capo esercitando una pressione suffi-ciente a comprimere i capelli. La misura viene approssimata al più vicino 0,1cm e si annota l’ora del giorno a cui è stata effettuata.IMC = peso in Kg diviso altezza in metri al quadrato (m\n2)\nSottopeso < 18,5\nNormopeso 18,5 – 24,9\nSovrappeso 25,0 – 29,9\nObesità I livelli 30,0 – 34,9\nObesità II livello 35,0 – 39,9\nObesità III livello > 40\n157Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#158": "FORMULE PER LA V ALUTAZIONE del peso corporeo fisiologico Kg\nPfis= 50 + (statura cm–150) x 0.75 Van Der Vael\nPfis= statura cm– 100 Broca\nPfis= statura cm– 104 corretta per il sesso femminile Broca \nPfis= 0.8 x (statura cm–100) + etàanni/ 2 Bertheam\nMaschi Pfis= cm – 100 – cm-150/4 Lorentz\nFemmine Pfis= cm – 100 – cm-150/2 Lorentz \nCirconferenze e diametri corporei\nEssi sono l’espressione delle dei vari segmenti corporei, inoltre sono indiciriconosciuti per la valutazione della distribuzione della massa grassa. Glistrumenti presenti in questa tecnica sono il calibro ed il metro flessibile edanelastico che aderisca alla cute senza comprimere i tessuti. Per ognicirconferenza il piano deve essere perpendicolare all’asse longitudinale dellaregione corporea in esame.\nDiametro del gomito\nIl diametro del gomito è il principale indicatore della taglia corporea.\nLa misurazione può essere effettuata con un calibro estensibile o fisso. La pro-cedura illustrata di seguito è quella da utilizzare con il calibro estensibile. Ilsoggetto, che si trova in posizione eretta, flette il braccio di 90° in modo cheildorso della mano sia rivolto anteriormente. L’operatore localizza palpatoria-mente gli epicondili mediale e laterale dell’omero e applica le barre del calibroin loro corrispondenza \nDiametro del polso\nIl diametro del polso è utilizzato principalmente come indicatore della tagliacorporea.La misurazione viene effettuata con un calibro estensibile. Il soggetto, che sitrova in posizione eretta, flette l’avambraccio di 90° sul gomito, tenendo il brac-cio vicino al torace. L’operatore localizza palpatoriamente i processi stiloideiulnare e radiale e sistema le estremità del calibro in corrispondenza di essi \nCirconferenza vita\nIndicatore del tessuto adiposo sottocutaneo addominale, correlato al grassoviscerale e al rischio di morte.\n158Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#159": "Il soggetto è in posizione eretta, l’addome è rilassato, le braccia pendono ai\nlati del corpo ed i piedi sono uniti. L’operatore, che si trova di fronte alsoggetto, sistema un metro a livello della vita, la parte più stretta dell’addome.\nSi richiede l’aiuto di un secondo operatore il quale deve accertarsi che il metrosia in posizione orizzontale.Se il soggetto è obeso si misura la circonferenza orizzontale più piccolanell’area compresa tra le coste e la cresta iliaca. \nValutazione delle circonferenze addominali : \nuomini >\n94 cm (aumentata) \n>102 cm ( sostanzialmente aumentato)\ndonne > 80 cm (aumentata)\n>88 cm ( sostanzialmente aumentato)\nCirconferenza addominale\nE’ un indicatore del tessuto adiposo sottocutaneo addominale. Essa differisceda WC per essere la circonferenza massima dell’addome.\nIl soggetto è in posizione eretta, con i piedi uniti, l’addome rilassato e le brac-cia pendenti ai lati del corpo. L’operatore misura la circonferenza massimadell’addome. Questo livello corrisponde spesso, ma non sempre, a quello\nnell’ombelico. (Idealmente, un secondo operatore dovrebbe controllare il cor-retto posizionamento del metro sul lato non visibile al primo operatore.) Lamisurazione viene effettuata al termine di una normale espirazione\nCirconferenza dei fianchi\nLa circonferenza dei fianchi (HC, hip circumference) è un indicatore di adi-posità, muscolarità e struttura ossea della regione dei fianchi. Utilizzata con-giuntamente a WC, nella forma del rapporto vita:fianchi (WHR, waist-hipratio = WC/HC), essa consente di valutare il rischio metabolico associato alsovrappeso. Valori di WHR > 1.0 nell’uomo e >0.85 nella donna segnalano un aumento del rischio delle complicanze meta-boliche.Il soggetto è in posizione eretta, con i piedi uniti, e le braccia pendenti ai latidel corpo. L’operatore misura la circonferenza massima dei glutei. (Ideal-mente, un secondo operatore dovrebbe controllare il corretto posizionamentodel metro mente al di sopra degli epicondili femorali.\n159Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#16": "agli organi in cui viene depositato (ad esempio nel fegato a causa dell’emo-\ncromatosi).Il fabbisogno giornaliero è di circa 10 mg, nell’uomo, e 18 mg nella donna,durante il periodo dell’età fertile. Il fabbisogno aumenta nei bambini e negliadolescenti.Per il ferro è raccomandata la supplementazione solo in situazioni di carenzaaccertata, poiché una dieta anche se equilibrata, non sempre permette la co-pertura nel caso di aumentato fabbisogno di ferro. Situazioni carenziali di ferrosi osservano piuttosto frequentemente nelle donne in età fertile, con perditemestruali abbondanti o polimenorrea. Scegliere alimenti di origine animalequali crostacei, pesce, carne, interiora (fegato, rene, cuore), consumare ali-menti vegetali a foglia (broccoli, spinaci, indivia, radicchio verde), legumi,insieme a discrete quantità di vitamina C, che aumenta la biodisponibilità delferro non-eme. Sono comunque i comportamenti da suggerire anche perchétalvolta possono essere sufficienti per assicurare la copertura dei fabbisognisenza dover ricorrere a specifici integratori.LoZinco è cofattore di numerosi enzimi coinvolti nel metabolismo delle pro-\nteine e degli acidi nucleici, in cui svolge un ruolo catalitico e strutturale di re-golazione. Favorisce la maturazione delle gonadi, interviene nel correttofunzionamento del gusto e dell’olfatto; potenzia la risposta immunitaria, è im-portante nella riproduzione cellulare.Lo zinco dell’organismo umano è pari a circa 2 g, distribuito in tutti i tessuti,ma in particolare modo nella muscolatura striata, nelle ossa e, in piccola per-centuale, nella pelle. Situazioni da carenza di zinco possono verificarsi in pazienti trattati a lungocon nutrizione parenterale, in portatori di by-pass intestinali o in soggetti an-ziani. Rischi da carenza possono aversi in coloro che fanno un gran consumodi cereali integrali, legumi, prodotti a base di crusca, nei vegetariani stretti, acausa dell’alto contenuto di fitati, ossalati e fosfati che si comportano da che-lanti, limitando l’assorbimento dei sali minerali. La dieta media italiana assi-cura ampiamente un’assunzione totale di zinco che soddisfa la quotaraccomandata (10 mg/die per gli uomini, 7 mg/die per le donne).Le maggiori fonti alimentari sono rappresentate da carne bovina ovina, suina,uova, pesce, ostriche, latte e derivati, cereali, funghi, cacao, noci. LoIodio è il principale costituente degli ormoni tiroidei (tirosina e triiodioti-\nronina), regolatori di alcune funzioni metaboliche quali l’accrescimento cor-poreo e lo sviluppo del sistema nervoso centrale. E’ importante per laregolazione della termogenesi, nel metabolismo dei macronutrienti; per la fis-\n16Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#160": "Circonferenza braccio\nAC è una componente standard della valutazione dello stato nutrizionale , è\nun indicatore della dimensione traversale del braccio, usata per il calcolodell’AMA, e AFA e AMC. È un predittore di mortalità e malnutrizione perdifetto.Il soggetto è in posizione eretta flette il gomito a 90° l’operatore localizza ilpunto medio tra il processo coraco-acromiale della scapola e il margine\ninferiore dell’ulna.\nCirconferenza dell’avambraccio\nLa circonferenza dell’avambraccio è utile per una miglior definizione delledimensioni dell’arto superiore ma è meno impiegata di AC. Il soggetto è in posizione eretta, con le braccia leggermente distanti dal troncoe il palmo della mano rivolto anteriormente. Il metro viene fatto scorrere sullaparte prossimale dell’avambraccio fino ad identificarne la circonferenza mas-sima.\nCirconferenza del polso\nLa circonferenza del polso viene utilizzata principalmente come indicatoredella taglia corporea, poiché questa regione è relativamente priva di tessutoadiposo e muscolare.Il soggetto è in posizione eretta, con il braccio flesso e il palmo della manosul lato non visibile al primo operatore\nTipo costituzionale Grant Circonferenza del polso (cm)\nCirconferenza coscia\nLa circonferenza mediana (MThC) della coscia è un indicatore di adiposità\ne muscolarità , consente la stima di TFA, TMA e TMC.\nIl punto di repere della circonferenza mediana della coscia è localizzato tra il\npunto medio tra la piega inguinale e il margine prossimale della rotula , più\nfacilmente localizzabile a ginocchio flessoM > 20a F >20a\nBrevilineo >20 >18\nNormolineo 16-20 14-18\nLongilineo <16 <14\n160Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#161": "Plicometria \nIl termine plica (pannicolo adiposo) si riferisce allo spessore di una piega dellacute e del tessuto adiposo sottocutaneo sollevata in posizioni standardizzatedel corpo. Le pliche forniscono una buona misura del grasso sottocutaneo; poiché esisteuna relazione fra il grasso sottocutaneo ed il grasso corporeo totale, si ritieneche il risultato della misura delle pliche sia un buon indicatore dellacomposizione e della densità corporea. Alcuni autori ritengono infatti che lasomma delle varie pliche possa essere utilizzata per la stima del grassocorporeo totale. L’utilizzo della somma viene ritenuto utile per ridurre l’errore nella misura eper compensare possibili differenze nella distribuzione del grasso sottocutaneotra soggetti della stessa età, gruppo etnico e sesso. Lo spessore del pannicolo adiposo varia con l’età, il sesso e l’etnia. Le equazioni che associano i valori delle pliche sottocutanee al grasso corporeototale sono state sviluppate utilizzando modelli di regressione, per lo piùmultipla, sia lineari (popolazione-specifici) sia quadratici (generalizzati) delledimensioni, considerando come variabili dipendenti i valori di densità, massagrassa e massa magra valutati su base densitometrica e pliche, o somma dellestesse, o diametri o perimetri o statura come variabili indipendenti. Esistononumerosissime equazioni popolazione-specifiche per predire la densitàcorporea (D) da varie combinazioni di pliche, circonferenze e diametri ossei(Jackson and Pollock, 1985. Slaughter et al. 1988. Lohman 1986). Leequazioni specifiche sono state sviluppate per popolazioni relativamenteomogenee e si assume che siano valide solo per individui aventi caratteristichesimili rispetto ad età, genere, etnia e livello di attività fisica. Ottenuto il valore della densità corporea è possibile calcolare la percentualedi grasso corporeo attraverso diverse formule \nDove: \nBD = densità corporea in kg/dm3. Conosciuta la percentuale di grasso è possibile determinare la massa di grasso corporeo conla seguente equazione: FM (kg) = (FM% x BW) / 100 Per differenza è quindi possibile ottenere la massa magra: Autore Equazione \nSiri %FM = [ (4.95/BD) – 4.5] x 100 \nBrozek %FM = [ (4.57/BD) – 4.142] x 100 \nRathbun & Pace %FM = [ (5.548/BD) – 5.044] x 100 \n161Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#162": "FFM (kg) = BW – FM (kg) \nDove: BW = peso corporeo. \nLa misura delle pliche si effettua per mezzo di un plicometro (Figura 2).\nEsistono vari tipi di plicometri. Un plicometro è un particolare tipo di calibro,nel quale la pressione che agisce sulla piega cutanea sollevata, è costante econtrollata per non provocare lo schiacciamento del tessuto adiposo. Nel casodel plicometro Lange, ad esempio si hanno brevi branche foggiate a chela,solidali con una molla a pressione che tende a mantenerle unite alle loroestremità; la pressione esercitata è compresa tra 2 g/mm2 e 15 g/mm2. (20kPa e 150 kPa). \nMetodologia Plicometrica\nSollevamento della plica : \n- palpare il sito prima della misurazione in modo tale da predisporre il soggetto\nal contatto con lo strumento\n- con il pollice e l’indice della mano sinistra viene sollevato un doppio strato\ndi cute e sottocute 1 cm al di sopra del sito di misurazione\n- poi separare le dita dal sito di misurazione in maniera tale da non alterare la\nmisurazione con la pressione esercitata dalle dita stesse. \n- la plica viene sollevata in modo tale da essere perpendicolare alla superficie\ndel corpo.\nApplicazione del plicometro - mentre la mano sinistra serve a sollevare la plica la mano destra tiene il\nplicometro. \n- esercitare una pressione per separare le estremità dello strumento e\nposizionare il suo braccio fisso su di un lato della plica \n- infine, rilasciare il plicometro gradualmente, in modo tale da non far sentire\nsensazioni fastidiose al soggetto sotto esame. \nLettura della misura : \nLa misura viene rilevata 4 secondi dopo aver rilasciato il plicometro. La mi-surazione deve essere effettuata 3 volte e si assume il valore medio come ilreale\n162Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#163": "Plica tricipitale\nE’ un indicatore dei depositi adiposi sottocutanei della regione posteriore delbraccio. Essa è un riferimento, al valore prognostico nella malnutrizione perdifetto.La plica viene sollevata sulla faccia posteriore in corrispondenza del puntomedio tra il margine laterale del processo coraco-acromiale e il margine infe-riore del processo oleocranico dell’ulna \nPlica bicipitale\nLa plica bicipitale è un indicatore dei depositi adiposi sottocutanei della re-gione anteriore del braccio.La plica viene sollevata sulla faccia del braccio 1 cm al di sopra del punto con-trassegnato per la misurazione della plica tricipitale, su una linea verticale checongiunge il margine anteriore dell’acromion e il centro della fossa anticubi-tale.\nPlica sottoscapolare\nLa plica sottoscapolare è un indicatore dei depositi adiposi sottocutanei dellaregione posteriore del torace.La plica viene sollevata su una linea diagonale a indicazione infero-laterale, a45° rispetto al piano orizzontale della scapola.\nPlica soprailiaca\nE’ un indicatore dei depositi adiposi sottocutanei della regione addominale.La plica viene sollevata sulla linea medio-ascellare, immediatamente al disopra della cresta iliaca a 45° rispetto al piano orizzontale.\nPlica anteriore coscia\nE’ un indicatore dei depositi adiposi sottocutanei della regione mediana dellacoscia.La plica viene misurata sulla faccia anteriore della coscia in corrispondenzadel punto medio di una linea tracciata tra la piega inguinale e il margine pros-simale della rotula.\n163Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#164": "Equazioni con 4 pliche Durnin e Womersley\nP = somma bicipitale, tricipitale, sottoscapolare, soprailiaca\nContenuto in grasso corporeo, espresso come percentuale del peso, corrispondente ai diversi\nvalori della somma di 4 pliche cutanee (bicipitale, tricipitale, sottoscapolare, sovrailiaca) dimaschi e femmine a differenti età . (Durnin e Womersley 1974) Maschi \n(20-29a) D = 1,1631 -0,0632 logP \n(30-39a) D = 1,1422 – 0,0544 logP \n(40-49a) D = 1,1620 – 0,0700 logP \n(> 50a) D = 1,1715 – 0,0779 logP \n(17-72a) D = 1,1765 – 0,0744 logP Femmine \n(20-29a) D = 1,1599 -0,0717 logP \n(30-39a) D = 1,1423 – 0,0632 logP \n(40-49a) D = 1,1333 – 0,0612 logP \n(> 50a) D = 1,1339 – 0,0645 logP (17-72a) D = 1,1567 – 0,0717 logP\nSomma  \npliche Femmine (età in anni)\n(mm)\n16-29 30-39 40-49 50+\n20 14,10 17,00 19,80 21,40\n30 19,30 21,80 24,50 26,60\n40 23,40 25,90 28,20 30,20\n50 26,50 28,20 31,00 33,40\n60 29,10 30,60 33,20 35,70\n70 31,20 32,50 35,00 37,70\n80 33,10 34,30 36,70 39,80\n90 34,80 35,80 38,30 41,20\n100 36,40 37,20 39,70 42,60\n110 37,80 38,60 41,70 43,90\n120 39,00 39,80 42,00 45,10\n130 40,20 40,60 43,00 46,20\n140 41,30 41,60 44,00 47,20\n150 42,30 42,60 45,00 48,20\n160 43,30 43,60 45,80 49,20\n170 44,10 44,40 46,60 50,00\n180 , 45,20 47,40 50,80\n190 , 45,90 48,20 51,60\n200 , 46,50 48,80 52,40210 , , 49,40 53,00Somma  \npliche Maschi (età in anni)\n(mm)\n17-29 30-39 40-49 50+\n20 8,10 12,20 12,20 12,60\n30 12,90 16,20 17,70 18,60\n40 16,40 19,20 21,40 22,90\n50 19,00 21,50 24,60 26,50\n60 21,20 23,50 27,10 29,20\n70 23,10 25,10 29,30 31,60\n80 24,80 26,60 31,20 33,80\n90 26,20 27,80 33,00 35,80\n100 27,60 29,00 34,40 37,40\n110 28,80 30,10 35,80 39,00\n120 30,00 31,10 37,00 40,40\n130 31,00 31,90 38,20 41,80\n140 32,00 32,70 39,20 43,00\n150 32,90 33,50 40,20 44,10\n160 33,70 34,30 41,20 45,10\n170 34,50 34,80 42,00 46,10\n180 35,30 - , ,\n190 35,90 - , ,\n200 - - , ,210 - - , ,\n164Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#165": "AMA (Arm Muscle Area) AFA (Arm Fat Area) \nLe aree muscolo-adipose e le circonferenze muscolari degli arti vengono cal-colate dalla circonferenza e dalla plica di un arto. Dalla combinazione di dueindicatori antropometrici, una plica e una circonferenza, la loro performancenella valutazione dello stato nutrizionale è generalmente migliore rispetto allasingola plica o circonferenza.\nMaschi \nAMA corretta (AMA - osso) = (MAC-πxTSF)\n2/4π-10\nFemmineAMA corretta (AMA - osso) = (MAC-πxTSF)\n2/4π-6.5\nHeymsfield et al.. Am J Clin Nutr, 1982\nImpedenziometria \nL’analisi dell’impedenza biolelettrica (BIA) è un metodo rapido e non invasivoper valutare la composizione corporea. In questo metodo una corrente alternataa bassa tensione attraversa il corpo del soggetto, viene misurata in questo modol’impedenza (Z), cioè la resistenza al passaggio della corrente. Il passaggiodella corrente avviene per attivazione degli elettroliti presenti nell’acqua. Laresistenza al passaggio della corrente elettrica è maggiore nel tessuto adiposoe minore nella massa magra. I tessuti biologici si comportano infatti comeconduttori o come isolanti; la massa magra contiene grande quantità di acquaed elettroliti rendendola migliore, rispetto alla massa grassa, nella conduzionedella corrente elettrica. La misura viene effettuata con apparecchi che iniettanocorrente alternata di 800 μA (micro Ampere) a diverse frequenze (1-1000kHz). L’impedenza è il rapporto tra la differenza di potenziale (V olt) e l’in-tensità di corrente (Ampere), la sua unità di misura è l’Ohm. Considerando il corpo umano come un cilindro con differenza di potenzialetra base inferiore e base superiore, l’impedenza che si oppone al passaggiodella corrente elettrica nel corpo è direttamente proporzionale alla sua lun-ghezza (statura) ed inversamente proporzionale all’area della sezione trasver-sale del corpo. I tessuti biologici agiscono come conduttori o isolanti ed il flusso di correntesegue un percorso di minima resistenza. L’uso della bioimpedenziometria pervalutare la composizione corporea si basa su diverse proprietà conducenti edielettriche dei tessuti biologici al variare della frequenza riferita alla correnteelettrica; i tessuti che contengono acqua ed elettroliti come il liquido cerebro-\n165Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#166": "spinale, il sangue ed i muscoli sono buoni conduttori, mentre il grasso, l’osso\ne gli spazi pieni d’aria come i polmoni sono tessuti dielettrici. Nel corpoumano, il volume (V) di questi tessuti può essere dedotto dalla misura dellaloro resistenza (R).L’impedenza è una funzione di resistenza (R) e reattanza (Xc):  Z = R\n2+ Xc2\nL’impedenza (Z) è l’opposizione dipendente dalla resistenza di un conduttoreal flusso di una corrente elettrica alternata ed è scomponibile in due membri:resistenza (R) e reattanza (Xc). La resistenza (R) è la misura pura di opposi-zione al flusso di corrente elettrica ed è inversa alla conduttanza. La reattanza(Xc) è l’opposizione al flusso di corrente causato dalle massa corporea (MC)ed è il reciproco della capacitanza; nella bioimpedenziometria, resistenza (R)e impedenza (Z) sono intercambiabili perché la reattanza (Xc) è molto bassa(<4%). A 50Hz, la resistenza (R) è maggiore della reattanza (Xc) perciò la re-sistenza (R) è il miglior predittore della impedenza (Z).\nL’indice di resistenza corrisponde a:  statura (S)\n2/resistenza (R), mentre il mi-\nglior predittore di acqua extra cellulare (ECW) è:  statura (H)2/ reattanza (Xc).\nLa resistenza (R) tra due punti è definita dalla legge di Ohm:  resistenza (R) =\ndistanza tra due punti (V) / intensità di corrente (I). Come già anticipato, per un conduttore cilindrico isotropo, la resistenza (R) èdirettamente proporzionale alla lunghezza (L) ed inversamente proporzionalealla sua sezione (A), pertanto, la resistività ( ρ) specifica del tronco è 2 o 3\nvolte superiore rispetto alla resistività ( ρ) di quella delle estremità. Anche la\nresistività ( ρ)degli adulti è maggiore che nei bambini e la resistività ( ρ)\ndegli obesi è maggiore che nei normopeso.\nresistenza = espressione dell’acqua corporea totale (TBW)\nreattanza = espressione della cellularità corporea\nL’analisi dell’impedenza bioelettrica si esegue sul lato destro del corpo con il\nsoggetto disteso supino. Si posizionano 4 elettrodi, di cui 2 sull’arto superioree 2 sull’arto inferiore. A livello dell’arto superiore si pone un elettrodo prossimale a livello delprocesso stiloideo di radio e ulna ed un altro distale, alla base dell’articolazionedella seconda o terza articolazione metacarpo-falangea nella mano. Nell’arto inferiore gli elettrodi vanno posizionati prossimamente a livello deimalleoli mediale e laterale della caviglia e distalmente a livellodell’articolazione metatarso-falangea nel piede. Il livello di errore “accettabile” per un’analisi della CC previa bioimpe -\n166Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#167": "denziometria è < 3,5kg per gli uomini e < 2,5kg per le donne. \nIl livello di accuratezza e precisione del metodo bioimpedenziometrico è\ninfluenzato soprattutto dalle variabilità intra-strumentali (taratura) e dallevariabilità inter-strumentali. Negli impedenziometri a monofrequenza puòvariare sensibilmente l’intensità della corrente alternata (800:500 μA) anchecon la stessa frequenza 50KHz, così come l’equazione di predizione (diversitàdei software) e il tipo di calibrazione (interna o esterna). Attraverso le misurazioni effettuate dalle apparecchiature, è possibile ottenerevari parametri:• Acqua corporea totale (TBW -  total body water );\n• Acqua extracellulare (ECW -  extra cell water );\n• Acqua intracellulare (ICW -  intra cell water );\n• Massa cellulare (BCM -  body cell mass);\n• Massa magra (FFM - fat free mass);• Massa grassa (FM - fat mass);• Massa muscolare (MM - muscle mass);• Metabolismo basale correlato alla massa cellulare\nAngolo di Fase\nValore bioelettrico che indica il rapporto tra Reattanza e Resistenza, ovverotra volumi intra ed Extracellulari. Se un corpo fosse costituito solo damembrane cellulari, quindi senza fluidi (impossibile) si otterrebbe un Angolodi Fase di 90 gradi. Se viceversa fosse composto esclusivamente da fluidi(impossibile) si otterrebbe un Angolo di Fase di 0 gradi. In un essere umanosano il valore dell’Angolo di Fase oscilla tra 6 ed 8 gradi. Un angolo di fasebasso (3/4 gradi) è un indice prognostico molto negativo.Attenzione: massa magra, massa cellulare e massa muscolare non sonosinonimi. Ognuno di essi corrisponde ad un determinato compartimentocorporeo, ognuno dei quali è costituito da specifici elementi.Esso rappresenta il ritardo che subisce la corrente elettrica nell’attraversareun conduttore biologico (elettrico).Esprime la ripartizione dell’acqua corporea ed in particolare del rapportotra acqua intracellulare ed acqua extracellulare. Tale rapporto può essere\nconsiderato un indice dello stato di nutrizione dell’individuo esaminato. Essoviene misurato a 50 kHz (clinico).\nSodio potassio scambiabile\nSodio, soluto extracellulare: un aumento di quest’elemento segue in genere\n167Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#168": "un aumento dell’acqua extracellulare. Il potassio è invece il principale catione\nintracellulare. Il mantenimento dell’equilibrio di questi due soluti èd’importanza vitale.\nRange di normalità: 0.9 – 1.0Per un’esecuzione attendibile dell’analisi è consigliata una temperatura\nambiente attorno ai 22 °C, è necessario non mangiare né bere nelle quattro oreprecedenti il test, non avere praticato attività fisica nelle ultime dodici ore,non indossare oggetti metallici (anelli, orecchini) non avere assunto diureticinell’ultima settimana, e svuotare la vescica almeno trenta minuti prima deltest. Nelle donne è indicato considerare la fase mestruale. La precisionedell’analisi può essere influenzata da vari fattori, i principali comprendono:la strumentazione, le capacità dell’operatore, fattori ambientali (temperaturaesterna) e fattori che alterano lo stato di idratazione del paziente (mangiare,bere, etc.). Anche per questo metodo sono presenti equazioni per il calcolo degli elementidella composizione corporea a partire da resistenza, reattanza.\nFattori che influenzano l’individuo da studiare\nTemperatura ambientale: la temperatura ambientale dove avviene lamisurazione deve essere tra i 24 e i 26 °C. Temperature basse o più altepossono modificare il risultato per i motivi spiegati in precedenza.\nTemperatura cutanea: basse temperatura della pelle portano ad una\nvasocostrizione con innalzamento artificioso dell’impedenza, viceversa\ntemperatue alte (febbre) portano ad una vasodilatazione con una riduzionedell’impedenza.\nPreparazione della cute: la conducibilità elettrica migliora trattando la pelle\ncon alcool etilico in quanto elimina secrezioni e cellule desquamate chepossono causare specifiche interferenze.\nCibo e bevande: Si consiglia che il soggetto sia a digiuno da liquidi e solidi\nda almeno 5-8 oreOggetti metallici: Togliere tutti gli oggetti metallici a contatto con la pelle\n168Fondamenti della Scienza dell’Alimentazione Capitolo VII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#169": "Esercizio fisico: si consiglia di noneffettuare esercizio fisico il giorno prima\ndell’esame in quanto potrebbe portare ad una disidratazione che altera il dato\nimpedenziometrico.\nAlcool: l’assunzione di bevande alcoliche prima dell’esame può alterare il\nvalore di impedenza.Ciclo mestruale: nelle donne in età fertile l’esame deve essere effettuato tra\nil 5-15° giorno del ciclo mestruale, al di fuori di questo intervallo si possono\navere variazioni dell’acqua corporea che portano variazioni artificiosedell’impedenza. L’impiego di contraccettivi orali non modifica la valutazioneimpedenziometrica.\nProtesi: placche metalliche, protesi mammarie etc… \nIn conclusione possiamo affermare:\n1) L’analisi bioimpedenziometrica è una tecnica valida per valutare la\ncomposizione corporea quando eseguita in maniera standardizzata\n2) La composizione corporea (massa magra e massa grassa) è valutata in\nmaniera accurata quando la distribuzione dell’acqua corporea è fisiologicae non vi è una distribuzione anomala dell’acqua extracellulare.\n3) Da un punto di vista clinico l’angolo di fase sembra essere un parametro\ncapace di dare delle informazioni sulla distribuzione dell’acqua corporea(rapporto acqua extra/intracellulare) e quindi sullo stato di nutrizione\n169Capitolo VII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#17": "sazione del calcio nelle ossa. Lo iodio viene assunto tramite diversi alimenti\nche lo contengono come alcune specie di pesci (merluzzo, tonno, sgombro),molluschi (cozze), uova, derivati del latte, carne, cereali, frutta e verdura (aseconda dei terreni di coltivazione), acqua potabile e alghe marine essiccate.La cottura riduce il contenuto in iodio degli alimenti, con perdite di circa il58% con la bollitura (WHO, 1996).Gli ultimi dati dell’OSNAMI (Osservatorio Nazionale per il monitoraggiodella iodoprofilassi in Italia) mostrano il persistere di una condizione di iodo-carenza lieve o moderata nel nostro Paese. Le patologie della tiroide sono fre-quenti nella popolazione generale, soprattutto tra le donne, e possono colpiretutte le età, compresa l’età fetale e neonatale. La carenza provoca gozzo en-demico, cretinismo, mixedema, disturbi della memoria, mastodinia. E’ statostimato che in Italia circa 6 milioni di persone soffrono di gozzo ovvero piùdel 10% della popolazione (Fonte http://www.salute.gov.it).Pertanto l’OSNAMI, in collaborazione con il Ministero della Salute ed altriesperti di settore, ha messo in atto una campagna che sensibilizza gli italianialla prevenzione del gozzo e di altri disordini da carenza iodica, con il ricorsoall’uso regolare di sale arricchito con iodio (sale iodurato/iodato) nell’alimen-tazione giornaliera. La quantità di iodio aggiunto al sale da cucina (30 mg diiodio per chilo) consente di contribuire al fabbisogno giornaliero di iodio chenell’adulto, in condizioni fisiologiche, è di 150 μg. Particolarmente elevato èil fabbisogno nelle donne in gravidanza e nei bambini.L’attuazione della profilassi iodica non è però in contrapposizione con la cam-pagna a favore della riduzione del consumo di sale (non più di 4-5 g al giorno)per la prevenzione dell’ipertensione e delle malattie cardiovascolari.Il Selenio è un antiossidante, co-fattore della glutatione perossidasi, protegge\nle membrane cellulari, il DNA dai danni dei radicali liberi. È dimostrato unsuo ruolo coenzimatico anche nel metabolismo degli ormoni tiroidei; contri-buisce al normale funzionamento del sistema immunitario e della spermato-genesi (http://www.efsa.europa.eu/it/efsajournal/pub/1220.htm).Gli alimenti ricchi di selenio sono soprattutto le frattaglie (fegato, rognone),pesci, frutti di mare, carne, cereali, frutta a guscio. Il livello del minerale neivegetali è proporzionale alla sua abbondanza nel terreno.Gli apporti giornalieri assunti attraverso una dieta varia, secondo i LARN, ga-rantiscono i livelli raccomandati per l’adulto (55 μg). L’Istituto Superiore diSanità riporta che ”L’Italia è una regione selenifera a basso contenuto e quindil’apporto di questo elemento con la dieta è piuttosto scarso”. La sindrome da carenza di selenio comporta cardiopatie, ipertensione, anemieemolitiche, cirrosi, neoplasie, sclerosi multipla, invecchiamento precoce, ar-\n17Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#170": "Bibliografia\n• Fidanza F., Liguori G., Nutrizione Umana. Idelson Editore, 1988 \n• Bedogni G., Borghi A., Battistini N.C., Manuale di valutazione dello stato nutrizionale\nERDA 2001\n• Del Toma E. Prevenzione e terapia dietetica Pensiero Scientifico Editore, 2005\n• Binetti P., Marcelli M., Baisai R. Manuale di nutrizione clinica e scienze dietetiche applicate\nSeu Editrice Universo 2006. \n• Cozzani I., Dainese E., Biochimica degli alimenti e della nutrizione. Piccin Editore, 2006• Riccardi, Pacioni, Giacco, Tivellese. Manuale di Nutrizione Applicata, Sorbona, III Edizione\n• Costantini A. M., Cannella C., Tomassi G. Alimentazione e nutrizione umana Pensiero\nScientifico Editore, 2011\n• Amerio M. L., Fatati G. Dietetica e nutrizione Pensiero Scientifico Editore, 2012\n170",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#171": "171Capitolo VIII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#172": "CAPITOLO VIII\nFABBISOGNO ENERGETICO\nRiassunto\nNell’uomo, così come in tutti gli esseri viventi, il mantenimento delle funzionivitali è assicurato dal continuo svolgimento delle reazioni anaboliche e cata-boliche che assicurano la costruzione e il rinnovamento delle strutture cellu-lari e la produzione di energia necessaria allo svolgimento del metabolismodi sintesi e al mantenimento delle molteplici forme di attività vitale (mecca-nica, osmotica, elettrica, ecc).La valutazione del fabbisogno energetico è di fondamentale importanza perla determinazione di un programma nutrizionale personalizzato.\n172Fondamenti della Scienza dell’Alimentazione Capitolo VIII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#173": "Per fabbisogno energetico di un individuo si intende la quantità di energia ali-\nmentare necessaria a compensare il suo dispendio energetico, risultante dalmetabolismo basale, termogenesi indotta dalla dieta e livello di attività fisica.Si definisce FABBISOGNO ENERGETICO l’apporto di energia di origine\nalimentare necessario a compensare il dispendio energetico di individui chemantengono un livello di attività fisica sufficiente a partecipare attivamentealla vita sociale ed economica e che abbiano dimensioni e composizione cor-poree compatibili con un buono stato di salute a lungo termine.Nel caso di bambini o di donne in gravidanza o allattamento, il fabbisognodeve comprendere la quota energetica necessaria per sostenere la deposizionedi nuovi tessuti o per la secrezione di latte (WHO,1985).Il fabbisogno energetico si calcola sulla base del dispendio energetico che èla risultante di tre componenti:• METABOLISMO BASALE ( M.B.)• TERMOGENESI INDOTTA DALLA DIETA ( TID )• COSTO ENERGETICO ATTIVITA’ FISICA ( LAF )\n8.1 METABOLISMO BASALE ( M.B.)\nRappresenta la quantità di energia necessaria a garantire l’integrità funzionale\ne morfologica delle cellule e dei tessuti e a mantenere una temperatura corpo-rea costante.La misurazione viene effettuata sul soggetto a digiuno da 12 ore, in condizionidi neutralità termica, sveglio e in totale rilassamento.Rappresenta il 65-75% del dispendio energetico totale ed è correlato al pesocorporeo, al sesso, all’età e alla massa magra e varia in particolari condizionifisiologiche come gravidanza, allattamento e accrescimento. Può essere misurato attraverso specifiche tecniche calorimetriche oppure pre-detto attraverso equazioni che tengono conto del peso corporeo, del sesso edell’età.\n8.1.1 CALORIMETRIA DIRETTAMisura il calore prodotto da un individuo collocato all’interno di una camera\nmetabolica; tiene conto della quantità di ossigeno consumato, della CO2 pro-dotta e della quantità di azoto secreta attraverso le urine e le feci.\n173Capitolo VIII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#174": "8.1.2 CALORIMETRIA INDIRETTA\nSi basa sul principio che i processi ossidativi metabolici richiedono un deter-\nminato consumo di O2. Noto il coefficiente termico dell’02 in condizioni ba-sali (4,825 Kcal/l) e misurata la quantità utilizzata in un determinato periododi tempo è possibile misurare la spesa energetica dell’individuo.\nHarris Benedict ( 1919)\nUOMINI: \nKCAL/24 ORE= 66, 473 + (13,7516 X P ) + ( 5, 0033 X h) – (6,7550 X ETÀ)\nDONNE: \nKCAL 24 ORE = 655,0955 + (9,5634 X P ) + ( 1,8496 X h) – ( 4,6756 X ETÀ)\nP = peso in kg; h = altezza in cm;In queste equazioni si utilizza il valore del peso corporeo osservato se si vuoleun calcolo di tipo conservativo, il valore riferito al peso desiderabile ( bmi 20-25 per gli uomini e 18,7- 25,8 per le donne ) se si vuole una stima del fabbi-sogno per realizzare una perdita o un aumento di peso.\n8.2 TERMOGENESI INDOTTA DALLA DIETA ( TID )\nDopo l’assunzione di un pasto si assiste ad un incremento della spesa energe-\ntica che varia tra i differenti substrati nutritivi.Questo fenomeno è stato definito ADS (Azione Dinamico Specifica). La causaeffettiva di tale aumento non è chiara, la ADS delle proteine sembra legataalle trasformazioni metaboliche a cui vanno incontro gli aa ( deamminazione,gluconeogenesi e sintesi proteica ), la ADS dei glucidi forse in relazione aimeccanismi di glicogenosintesi epatica post-prandiale, la ADS dei lipidi allaliposintesi dei grassi di deposito.Corrisponde al 7-15% del dispendio energetico totale; le proteine concorronoalla spesa energetica per il 10-35%, i glucidi per il 5-10% e i lipidi per il 2-5%.\n8.3 COSTO ENERGETICO ATTIVITA’ FISICA ( LAF )\nE’ strettamente dipendente dal tipo, dalla frequenza e dall’intensità delle atti-\nvità svolte dall’individuo. Viene espresso in kcal o kj per unità di tempo (mi-nuto, ora, giorno) oppure come multiplo del Metabolismo Basale e può variare\n174Fondamenti della Scienza dell’Alimentazione Capitolo VIII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#175": "dal 15% del MB in soggetti con attività piuttosto sedentarie fino a valori pari\nal 3-4 volte il MB per attività pesanti o negli atleti. Ogni attività viene espressaattraverso il cosiddetto Indice Energetico Integrato (IEI) e dalla somma dei\ncosti energetici di tutte le attività svolte nella giornata si calcola il livello diattività fisica di un individuo (LAF) che rappresenta il dispendio energeticodi un soggetto durante un’intera giornata.Sono stati anche elaborati LAF medi divisi per tipologia di attività.Livelli di attività fisica (espressi in LAF) da utilizzare per stimare il fabbisognoenergetico per sesso e classi di età.\nPer attività fisiche auspicabili si intendono le attività consigliate ai soggetti\nsedentari per il mantenimento del tono muscolare e cardiocircolatorio.- attività leggere: corrispondono allo stare seduti o in piedi senza spostamenti\n(casalinghe, impiegati, liberi professionisti, operai)\n- attività moderate: comportano spostamenti del corpo, flessioni del tronco,\nintenso lavoro di braccia, per periodi non troppo prolungati (giardinieri,collaboratori domestici)\n- attività pesanti: sono quelle eseguite con movimenti di tutto il corpo e\nl’impegno di tutta la forza muscolare (zappare, picconare o trasportare pesinotevoli).\nMoltiplicando il LAF medio per il MB del soggetto si ottiene il dispendio ener-COMPRESE LE ESCLUSE LECLASSE DI ETÀATTIVITÀ ATTIVITÀLIVELLO DIFISICHE FISICHEATTIVITÀAUSPICABILI* AUSPICABILI*\nLAF LAF\nUomini 18-59 anni leggero 1,55 1,41\nmoderato 1,78 1,70\npesante 2,10 2,01\n60-74 anni 1,51 1,40\n>=75 anni 1,51 1,33\nDonne 18-59 anni leggero 1,56 1,42\nmoderato 1,64 1,56\npesante 1,82 1,73\n60-74 anni 1,56 1,44\n>=75 anni 1,56 1,37\n175Capitolo VIII Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#176": "getico totale espresso come quantità di energia per unità di tempo.\nEsempio di stima del fabbisogno energetico:soggetto di sesso maschile, età anni 35, peso Kg 75, h cm 170, BMI 22\nKg/mxm , impiegato (attività fisica leggera)MB = 66, 473 + (13,7516 X 75) + (5, 0033 X 170) – (6,7550 X 35) = 1711Kcal/giornoLAF attività fisica leggera 1,55\nFABBISOGNO ENERGETICO = 1711 x 1,55 = 2652 Kcal/ giorno\n176Fondamenti della Scienza dell’Alimentazione Capitolo VIII",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#177": "Bibliografia\n• Manuale di Nutrizione Applicata” Riccardi Pacioni Rivellese Ed. Sorbona\n• Nutrizione Clinica Magnati Russo Dazzi EdiSES• Nutrizione umana F. Fidanza G. Liguori Ed. IDELSON• Basi metodologiche dell’approccio psiconutrizionale Paolo De Cristofaro Ed. SEE FI-\nRENZE\n177Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#178": "CAPITOLO IX\nSTRUMENTI DI INDAGINE ALIMENTARE\nRiassunto\nLa rilevazione dei consumi alimentari costituisce un ottimo indicatore indiretto\ndello stato nutrizionale. Per le indagini nutrizionali possono essere utilizzatemetodologie diverse a seconda degli obiettivi. Le metodologie si basano so-stanzialmente sulla registrazione o sul ricordo. I principali metodi di rileva-zione della dieta sono il diario, il ricordo delle 24 ore e i questionari difrequenza. Non esiste un metodo di indagine alimentare che sia migliore inassoluto, ciascun metodo ha i suoi vantaggi e svantaggi e sta nelricercatore/operatore scegliere il più o i più adatti per la valutazione dellostato di nutrizione.\n178Fondamenti della Scienza dell’Alimentazione Capitolo IX",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#179": "9.1 Strumenti di indagine alimentare\nLa valutazione dello stato di nutrizione è molto importante in quanto permette\na livello individuale di rilevare gli eventuali problemi nutrizionali nei singolisoggetti e quindi consente di porvi rimedio attraverso un corretto riequilibriodietetico. A livello di gruppo permette di elaborare statistiche di prevalenza oincidenza di forme di malnutrizione, fornendo elementi essenziali per pianifi-care interventi istituzionali di recupero. La valutazione dello stato nutrizionalecostituisce quindi uno strumento di fondamentale importanza nella sorve-glianza nutrizionale.Oltre che mediante metodi diretti quali le misurazioni antropometriche e bio-chimiche, la rilevazione dello stato nutrizionale può avvenire tramite misura-zioni indirette quali la raccolta delle informazioni sui consumi alimentari. In generale, quattro sono i principali usi dei dati sui consumi alimentari:• Misurazione e sorveglianza del consumo di alimenti e nutrienti (Stima del-\nl’adeguatezza dell’assunzione alimentare di individui e gruppi di soggetti)\n• Formulazione e valutazione delle politiche governative sanitarie e agricole (pia-\nnificazione della produzione e distribuzione degli alimenti, definizione di re-golamenti sugli alimenti e nutrizione, creazione di programmi di educazionealimentare e di riduzione delle malattie, valutazione del successo e dei costi-benefici dei programmi di educazione alimentare e di riduzione delle malattie)\n• Conduzione degli studi epidemiologici per studiare le relazioni tra dieta e\nsalute e per identificare gruppi a rischio per lo sviluppo di alcune patologielegate alla dieta\n• Fini commerciali\nLa misura dei consumi alimentari permette di tracciare il profilo dietetico di:\n• singoli individui\n• gruppi di individui (bambini, gruppi a rischio, pazienti, anziani etc.)• popolazioni o gruppi di popolazioni (coorti, gruppi rappresentativi etc.)\nPer la rilevazione dei consumi alimentari possono essere utilizzate metodolo-\ngie diverse a seconda degli obiettivi.È importante sottolineare che non esiste un metodo di indagine alimentare chesia migliore in assoluto e che la misurazione della dieta è sempre accompa-gnata da un certo grado di errore. Ciascun metodo ha i suoi vantaggi e svan-taggi e sta nel ricercatore/operatore scegliere il metodo migliore in base alproprio scopo.\n179Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#18": "teriosclerosi e facilità alle infezioni.\n1.1.2 Vitamine\nLe vitamine (Tabella 1) sono un insieme molto eterogeneo di sostanze chimi-che, richieste in quantità pari a milligrammi o microgrammi per soddisfare ifabbisogni dell’organismo. Per la maggior parte, devono essere introdotte conla dieta, poichè non vengono sintetizzate o solo in parte dall’organismo. Nonvengono usate per produrre energia né per usi strutturali, ma sono precursoridi ormoni, agiscono da antiossidanti, partecipano come substrati a reazionispecifiche, regolano una serie di reazioni metaboliche, spesso funzionanocome coenzimi. La maggior parte delle vitamine deve essere ulteriormentetrasformata per generare coenzimi, e prendere parte direttamente all’azionecatalitica dell’enzima. Possono manifestarsi delle ipovitaminosi, a causa di una insufficiente assun-zione con gli alimenti (ad esempio la vitamina B12), di un aumentato fabbi-sogno (ad esempio in gravidanza, con i folati), o per la presenza di alterazioniintestinali che ne impediscono l’assorbimento. In generale, oltre ad una ali-mentazione corretta ed equilibrata, l’utilizzo di specifici integratori può essereutile a coprire l’aumentato fabbisogno. \nVitamine idrosolubili Vitamine liposolubili\nVitamina B1 (tiamina) Vitamina A (retinolo ed analoghi)\nVitamina B2 (riboflavina) Vitamina D (ergocalciferolo D2\ne colecalciferolo D3)\nVitamina B3 o Vitamina PP Vitamina E (tocoferolo)\n(niacina o acido nicotinico)\nVitamina B5 o Vitamina W Vitamina K (fillochinone e derivati)\n(acido pantotenico)Vitamina B6 \n(piridossina o piridossamina o piridossale)\nVitamina B8 o Vitamina H (biotina)Vitamina B9 o Vitamina Bc o Vitamina M \n(acido folico)\nVitamina B12 (cobalamina)Vitamina C (acido ascorbico)Tabella 1 - Classificazione delle vitamine in liposolubili ed idrosolubili sulla base della loro\ninsolubilità o solubilità in acqua\n18Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#180": "Un punto molto importante da tener presente nella scelta della metodologia\nda utilizzare è rappresentato dalla variazione della dieta nel tempo. In moltecircostanze il fattore tempo è estremamente importante dal momento che avolte sono necessari alcuni anni prima che si verifichi l’effetto della dieta sul-l’insorgenza di una patologia. In questi casi valutare le variazioni quotidianedella dieta è assolutamente necessario per indirizzare meglio la scelta di unmetodo più adatto di stima della dieta e per interpretare i dati raccolti. La va-riabilità individuale della dieta di una popolazione dipende dalla diversifica-zione quotidiana ed è legata a numerosissimi fattori tra cui la stagionalità dialcuni alimenti, il costo, il giorno della settimana (giorno lavorativo o fine set-timana, festività etc.), la condizione socioculturale. La variabilità è differenteper i macronutrienti e micronutrienti, in generale è minore per i primi e mag-giore per i secondi. Due sono sostanzialmente le metodologie della rilevazione dei consumi ali-mentari:La registrazione (dietary record): gli alimenti e le bevande vengono descritti\ne/o quantificati al momento del consumo in un determinato intervallo ditempo:• Breve periodo (diario)\nIlricordo (dietary recall): viene chiesto al soggetto di ricordare gli alimenti\ne le bevande consumati nel passato per certo intervallo di tempo :\n• Breve periodo, ovvero la dieta abituale. \nA tal fine vengono utilizzate delle tecniche che siano in grado di misurare\ncon un buon grado di precisione le quantità di alimenti consumati (ricordodelle 24 ore, delle 48 ore o delle 72 ore)\n• Medio periodo (settimana, mese precedente: questionario di frequenza (FFQ)• Lungo periodo (anno precedente, periodi della vita: FFQ, storia dietetica etc.)\n9.2 Diario di registrazione degli alimenti\nRegistrazione dettagliata del tipo e della quantità di tutti gli alimenti e bevande\nconsumate ad ogni pasto durante un periodo di tempo, in genere da 3 a 7giorni. Gli alimenti e le bevande consumate possono essere quantificate tramiteporzioni standard (ad esempio modelli di porzioni, tazze, cucchiai o medianteil righello) oppure pesando gli alimenti. Tipicamente se sono registrati più\n180Fondamenti della Scienza dell’Alimentazione Capitolo IX",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#181": "giorni, sono consecutivi e non includono più di 3 o 4 giorni. La registrazione\ndi periodi più lunghi di 4 giorni risulta insoddisfacente dal momento che si hauna riduzione nei consumi dovuti alla fatica da parte dell’intervistato nel doverregistrare tutti gli alimenti consumati. Teoricamente l’informazione sui consumi alimentari deve essere registrata almomento del consumo, ma non è necessario scrivere su un foglio, ma si pos-sono utilizzare anche dei registratori oppure adesso sono disponibili delle ap-plicazioni per il computer o il tablet.L’utilizzo del diario necessita di un’accurata preparazione del soggetto circale modalità di registrazione, la descrizione della tipologia e della quantità dialimento consumato. In particolare, il soggetto deve essere istruito sul dettagliocon cui deve descrivere gli alimenti e la quantità consumata, includendo nomedell’alimento (la marca laddove possibile), metodo di preparazione, ricette pergli alimenti composti e la misura della porzione. In alcuni casi si implemental’addestramento del soggetto contattando e rivedendo il diario dopo il primogiorno di registrazione. Alla fine della registrazione il diario va accuratamentecontrollato da un operatore che insieme all’intervistato deve chiarire i puntioscuri e verificare eventuali dimenticanze. In alcuni casi altre persone oltre ilsoggetto possono compilare il diario e questo viene fatto ad esempio per i bam-bini o le persone nelle case di cura.Il diario solitamente è in una forma “aperta” (open-ended), cioè lasciando lalibertà all’intervistato di riportare gli alimenti consumati, anche se in alcunicasi sono state sviluppate anche delle versioni cosiddette “chiuse” (close-ended). Queste ultime consistono in una lista predefinita di gruppi di alimentie il soggetto deve scegliere quello consumato. Le porzioni possono essere chie-ste anch’esse in un formato “open-ended” o mediante categorie predefinite. Vantaggi di questo metodo di indagine alimentare:• Può fornire informazioni dettagliate sul consumo degli alimenti durante il\nperiodo registrato. Registrare il consumo degli alimenti intanto che vengonomangiati diminuisce il problema delle omissioni e fornisce informazioni piùaccurate sulla quantità di alimenti consumati in ogni singola occasione ri-spetto al dover ricordare le porzioni consumate precedentemente. \n• Può fornire informazioni sulle abitudini alimentari• Non dipende dalla memoria \nLimiti di questo metodo di indagine alimentare: \n• estremamente costoso • dispendioso in termini di tempo \n181Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#182": "• necessaria alta motivazione e buona scolarità, quindi può creare un problema\ndi selezione del campione\n• può modificare il comportamento alimentare nel periodo di osservazione,\ncioè il dover registrare quello che si è mangiato può influenzare sia ciò che\nsi mangia che la quantità consumata. La conoscenza richiesta per descriverel’alimento consumato e il fatto di doverlo registrare può alterare le abitudinialimentari che si intendono misurare con questo strumento \n• analisi dei dati è estremamente laboriosa. Anche se attualmente sono dispo-\nnibili software che permettono un facile inserimento dei dati e quindi per-mettono un risparmio di tempo nella codifica degli alimenti, mantenere uncontrollo generale della qualità dei dati è difficile perché spesso le informa-zioni non sono registrate consistentemente da soggetto a soggetto.\nIl diario è indicato per piccoli gruppi di soggetti con buon livello culturale emolto motivati. Studi che hanno valutato il consumo di energia e nutrientisomministrando il diario in piccoli campioni di soggetti adulti sottostimavanol’energia in un intervallo dal 4 al 37% se paragonati al dispendio energeticomisurato dall’acqua marcata o al consumo proteico misurato da azoto urinario.Quindi il diario viene considerato uno strumento “gold standard” imperfetto.La sottostima è il risultato sia dell’incompleta registrazione che dell’impattodi dover registrare quello che si è mangiato. Sono stati suggeriti diversi ap-procci per cercare di superare il problema della sottostima nel diario: alcunihanno suggerito di fare un rigoroso addestramento del soggetto, altri di inseriredelle domande psico-sociali note per essere associate alla sottostima in mododa poter valutare il livello di sottostima. Infine, altri hanno suggerito di cali-brare il diario alimentare con il dispendio energetico misurato dall’acqua mar-cata per meglio predire il consumo energetico individuale.\n9.3 Il ricordo delle 24 ore\nRicordo del tipo e della quantità degli alimenti e bevande consumate nelle 24\nore precedenti. Un intervistatore ben addestrato, coadiuvato da supporti visivi(atlante fotografico, modelli di porzioni, tazze, fotografie di piatti pronti, ecc.)chiede al soggetto di descrivere con precisione tutto ciò che ha consumatonelle 24 ore precedenti per risalire alla quantità consumata di ogni alimento obevanda. Occasionalmente il periodo temporale riguarda le 48 ore precedenti,le 72 ore precedenti etc. Ovviamente i ricordi di quello che si è assunto pos-\n182Fondamenti della Scienza dell’Alimentazione Capitolo IX",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#183": "sono sbiadire piuttosto rapidamente al di là del giorno più recente o di due\ngiorni, per cui la perdita di precisione può superare il guadagno in termini dirappresentatività.L’intervista solitamente è strutturata con specifiche domande in modo da ri-cordare tutti gli alimenti consumati nella giornata. Una comune tecnica del-l’intervista sulle 24 ore precedenti prevede che le domande sul consumoinizino dalla prima cosa consumata al risveglio mattutino e ultima cosa con-sumata prima di andare a letto. L’intervista può essere condotta di persona, o telefonicamente. Idealmentel’intervistatrice dovrebbe essere una dietista o nutrizionista con una buona co-noscenza degli alimenti e della nutrizione; comunque anche non-nutrizionistiche sono stati formati per utilizzare uno strumento standardizzato possonoandar bene. Tutti gli intervistatori devono avere una buona conoscenza deglialimenti presenti sul mercato e dei diversi metodi di preparazione degli ali-menti includendo piatti regionali e alimenti etnici.Vantaggi di questo metodo di indagine alimentare:• non richiede un certo grado di scolarità dal momento che è un operatore che\nsomministra l’intervista \n• il grado di partecipazione è alto• non si alterano le abitudini alimentari in quanto l’intervista viene sommini-\nstrata dopo che la persona ha già consumato gli alimenti.\n• due o più giorni forniscono dati sulla variabilità intra e inter-individuale• la ripetizione dell’intervista nell’anno può fornire dati attendibili sull’ali-\nmentazione usuale di gruppi di popolazione, anche per alimenti non frequen-temente consumati o per tenere conto della stagionalità\nPoco costoso e richiede un tempo di somministrazione non elevato (circa 30minuti). Ci sono dei software che permettono una codifica immediata deglialimenti durante l’intervista e quindi i costi di gestione dei dati raccolti sonoridotti e c’è una standardizzazione delle interviste. Questi sistemi variano sianel numero di alimenti presenti nel database che nel modo in cui viene chiestala porzione. Ad esempio l’Istituto per la Ricerca sul Cancro Americano (Na-tional Cancer Institute) ha sviluppato un’intervista delle 24 autosomministratavia internet.Limiti di questo metodo di indagine alimentare:- un solo ricordo delle 24 ore è poco rappresentativo del consumo abituale di\nun soggetto data la variabilità giorno per giorno della dieta. Inoltre un solo24 ore non può essere usato per stimare la proporzione di una popolazione\n183Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#184": "che abbia o meno una dieta adeguata (per esempio la proporzione di indivi-\ndui che consumano meno del 30% di energia da grassi o che sono carenti invitamina C) \n- anche se diverse interviste delle 24 ore sono somministrate ad una singola\npersona, è impossibile misurare alimenti consumati raramente come ad es.il fegato\n- richiede buona memoria - i consumi fuori casa sono meno accurati- sottostima dei consumi di alcool e di altri alimenti considerati poco salutari- la validità può diminuire con l’aumentare dei giorni\nQuesto strumento è indicato per \n• stimare il consumo medio di campioni consistenti di popolazione, non indi-\ncato per piccoli gruppi o singoli individui \n• confrontare il consumo ottenuto da metodiche più sofisticate e meno agili• verificare l’efficacia di un programma di intervento che prevede il confronto\ncon un gruppo di controllo\nAnche per quanto riguarda il ricordo delle 24 ore, come per il diario, la sotto-stima è un grosso problema. Studi che hanno utilizzato marcatori biologicihanno evidenziato come l’intervista delle 24 ore sottostimi l’energia in un in-tervallo dal 3 al 26 % rispetto alla misurazione con l’acqua marcata e le pro-teine dall’11 al 28% rispetto alla misurazione con azoto urinario. \n9.4 Il questionario di frequenza degli alimenti\nQuesto strumento valuta la frequenza di consumo di determinati alimenti, ri-\nferita ad un determinato periodo di tempo. Il questionario consiste nella regi-strazione degli alimenti per frequenza di assunzione e mediante analisiquantitativa nel caso contengano foto o riferimenti a porzioni standard. Ov-viamente pochi dettagli sulle caratteristiche degli alimenti consumati vengonoraccolti come ad esempio il tipo di cottura o la combinazione di alimenti du-rante il pasto. L’intervallo di tempo a cui si riferisce il questionario può esserel’anno precedente o la settimana o il mese precedente. I questionari alimentari vengono utilizzati negli studi epidemiologici per met-tere in relazione la dieta con la prevalenza o incidenza di malattie cronico-de-generative (cancro, malattie cardiovascolari) in quanto valutano i consumi\n184Fondamenti della Scienza dell’Alimentazione Capitolo IX",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#185": "abituali, retrospettivi e relativi a lunghi periodi (anno, mese, etc.). \nLa costruzione del questionario è estremamente importante, particolare atten-zione deve essere focalizzata sulla scelta degli alimenti e sulla tipologia di fre-quenza richiesta. Il questionario deve essere costruito, mediante un’opportuna analisi statistica,a partire da una base di dati rilevata con precisione su un campione simile aquello che si vuole studiare.Il questionario si compone di:• una lista di alimenti, scelti in base all’obiettivo della ricerca, con numerosità\nadeguata, escludendo gli alimenti di scarso consumo al fine di rendere il que-stionario più gestibile e preciso. Gli alimenti nella lista devono essere con-sumati spesso e da un numero ragionevolmente grande di soggetti nellostudio, devono avere un contenuto sostanziale del nutriente o nutrienti og-getto dello studio ed il consumo di questi alimenti deve variare considere-volmente da un individuo all’altro per consentire di discriminare fra diversequantità consumate e/o modalità di assunzione. Di solito gli alimenti sonoraggruppati in categorie omogenee \n• una sezione con le risposte relative alle frequenze di consumo, nelle quali i\nsoggetti indicano quanto spesso un determinato alimento viene consumato.Le domande relative alla frequenza possono essere chiuse (prevedono unformato con risposte multiple, da 5 a 10, in relazione alla lista di alimenti,ad esempio: quanto spesso beve il latte? Mai, 1 volta al mese, 2-3 volte almese, 1 volta alla settimana ecc) oppure aperte (lasciano libertà di scelta al-l’intervistato, ad esempio quanto spesso beve il latte? 1,2,3 volte al giorno,alla settimana, al mese)\n• informazioni sulla quantità di alimenti consumata possono non essere pre-\nsenti come ad esempio per i questionari di frequenza non quantitativi (in cuisi chiede ad esempio quante volte al giorno,/mese/anno mangia pane inte-grale o il gelato). Nei questionari di frequenza semi-quantitativi viene dataun’idea della porzione (quantificazione con unità standard vicino alla rispo-sta di frequenza ad esempio una fetta di pane) mentre nei questionari di fre-quenza quantitativi si prevede una quantificazione fatta dal soggettomediante unità standard e foto (si chiede al soggetto di descrivere la gran-dezza della sua porzione abituale come piccola, media o grande rispetto aduna porzione standard)\nVantaggi di questo metodo di indagine alimentare:• fornisce dati sui consumi alimentari abituali\n185Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#186": "• non richiede una specifica preparazione dell’intervistatore\n• può essere compilato direttamente dal soggetto in esame• non sono influenzati i consumi alimentari abituali• il tasso di risposta è alto• fornisce dati utilizzabili in epidemiologia nelle relazioni tra alimentazione e\nmalattie o fattori di rischio\n• consente di raggruppare i soggetti esaminati in base ai consumi alimentari• poco costoso nella raccolta dei dati e nell’analisi \nLimiti di questo metodo di indagine alimentare:\n• l’impegno richiesto al soggetto in esame dipende dalla numerosità e com-\nplessità della lista di alimenti\n• richiede buona memoria• molti dettagli dei consumi alimentari non vengono misurati • non è idoneo per segmenti di popolazione quali individui con consumi ali-\nmentari atipici o non compresi nella lista\n• i consumi possono essere misurati male quando più alimenti sono raggrup-\npati in una singola lista.\n9.5 Strumenti brevi per valutare la dieta\nSono stati sviluppati diversi strumenti brevi per misurare la dieta che possono\nessere molto utili quando si è interessati ad indagare un singolo aspetto delladieta e non la sua totalità. Questi strumenti sono molto utili ad esempio inquelle situazioni in cui l’obiettivo è la promozione della salute e l’educazionealla salute. Strumenti brevi per misurare ad esempio frutta e verdura sono uti-lizzati come strumenti di sorveglianza in studi di intervento sulla popolazione Questi strumenti possono essere mirati a misurare un singolo aspetto delladieta oppure degli specifici comportamenti (o pattern) alimentari. Di solito unquestionario di frequenza contiene 100 o più “food items” se si vuole indagareun singolo nutriente o un gruppo di alimenti 15-30 “food items” possono es-sere sufficienti.Sono stati sviluppati diversi strumenti orientati a misurare un singolo nutrientequali le proteine, il calcio, il ferro, i prodotti della soia o frutta e verdura. Ancheper i comportamenti alimentari sono stati sviluppati diversi questionari brevi.La brevità di questi strumenti creano un’allettante opzione per coloro che in-tendono misurare la dieta a basso costo. Sebbene abbiano molte applicazioni\n186Fondamenti della Scienza dell’Alimentazione Capitolo IX",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#187": "hanno anche diversi limiti. \n• Non misurano la dieta nel suo complesso• Le misure non sono quantitative e quindi una stima dell’introito dietetico\nnon si può fare\n• Le stime non sono precise ed hanno un grande margine di errore\n9.6 Storia dietetica\nÈ un’intervista molto accurata che permette di valutare i consumi di alimentiin un periodo lungo e di avere informazioni sulle abitudini alimentari.Si procede in due fasi:• si definiscono le abitudini alimentari• si definiscono i consumi dei singoli alimenti. \nViene discusso ogni pasto, considerando tutte le alternative settimanali e sta-\ngionali. Le porzioni sono quantificate con foto e modelli.L’intervista prevede tutta una serie di domande circa il numero di pasti algiorno, l’appetito, la digestione, l’alvo, la storia ponderale, le allergie ed in-tolleranze alimentari, le interazioni farmaco-nutrizionali, la storia dieteticapregressa, l’attività fisica, la presenza di complicanze specifiche della malattiasuscettibili di trattamento dietetico e il supporto psico-sociale. Poi la sommi-nistrazione di un intervista sul consumo delle 24 ore precedenti circa le abitu-dini alimentari durante e tra i pasti ed infine la somministrazione di un diariodei 3 giorni che serve per aggiungere ulteriori informazioni all’intervista. Vantaggi di questo metodo di indagine alimentare:• fornisce un quadro abbastanza completo della dieta abituale• è utile negli studi epidemiologici su malattie che si sviluppano lentamente\nnegli anni\n• non richiede un particolare livello di istruzione per i soggetti in esame• fornisce informazioni sia sulle abitudini alimentari che sul consumo di par-\nticolari alimenti\n• può ben correlare con le misurazioni biochimiche\nLimiti di questo metodo di indagine alimentare:\n• richiede un buon addestramento dell’intervistatore• richiede una buona cooperazione dell’intervistato• è difficile riportare alla memoria, in modo accurato, il periodo da esaminare• richiede un grande impegno sia per l’intervistatore che per l’intervistato\n187Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#188": "• richiede un tempo lungo di intervista\n• sovrastima il consumo di nutrienti• il ricordo dell’alimentazione del passato può essere influenzato dalla dieta\npresente\n9.7 Elaborazione dei dati\nLa dieta rilevata in un’indagine alimentare può essere analizzata in termini di\ncomportamenti alimentari (Modelli alimentari), consumo di alimenti o introitodi nutrienti o combinazione di alimenti e nutrienti. • I modelli alimentari possono essere definiti a priori (cioè costruiti a partire\nda alimenti e /o nutrienti importanti per la salute o specifici di un determinatocontesto culturale, che vengono quantificati e sommati per fornire una misuraglobale della qualità alimentare. Esempi: Indice Mediterraneo, Healthy Eating Index, DASH Index) oppurea posteriori (cioè vengono costituiti da modelli statistici a partire dai dati diconsumo. Esempi: pattern alimentare occidentale “Western” oppure patternVegetariano).\n• Gli alimenti o i gruppi di alimenti vengono valutati in termini di frequenza\ne/o quantità consumata\n• I nutrienti possono essere valutati in termini di energia, macro- e micronu-\ntrienti e sono generalmente ricavati da tabelle di composizione degli alimentiLa traduzione in energia e nutrienti viene effettuata mediante l’uso di tabelledi composizione degli alimenti possibilmente nazionali. Le tabelle si riferi-scono per la maggior parte ad alimenti a crudo, per tale ragione il valore dialcuni nutrienti, in particolare quello delle vitamine, può non rispecchiarel’effettivo contenuto negli alimenti consumati dopo cottura. Le tabelle dicomposizione utilizzate per la traduzione in principi nutritivi sono determi-nanti agli effetti della validità dei risultati. In base alle esigenze e allo scopodello studio, devono essere revisionate, ampliate con la composizione deglialimenti locali e completate per tutte le voci necessarie ai fini dell’inchiestastessa. Ciò può essere fatto sia utilizzando valori ricavati da tabelle di altriPaesi con precisi criteri, sia campionando localmente e analizzando gli ali-menti di cui non si conosce la composizione. \nLa scelta varia in funzione dello scopo che ci si propone. Ad esempio l’analisidi un’associazione tra l’alimentazione e una patologia prevede un’analisi della\n188Fondamenti della Scienza dell’Alimentazione Capitolo IX",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#189": "dieta sia in termini di nutrienti che di alimenti. In alcuni casi lo studio dell’as-\nsociazione tra il consumo di un alimento ed una patologia può portare all’iden-tificazione di un nutriente potenzialmente responsabile. Ad esempio studiepidemiologici hanno evidenziato un’associazione protettiva tra il consumodi frutta gialla e verdure gialle e verdi nei confronti del tumore del polmone eda qui nacque l’ipotesi che il beta carotene potesse avere un effetto protettivosull’insorgenza di questa forma tumorale.Inoltre, i nutrienti possono poi essere confrontati con i relativi fabbisogni digruppi o individuali per valutare l’adeguatezza della dieta o di particolari nu-trienti. Accertata l’adeguatezza dei consumi si può desumere se la popolazionestudiata è esposta o meno al rischio di carenze o eccessi alimentari con riferi-mento a particolari nutrienti.\n189Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#19": "Vitamine liposolubili \nLa Vitamina D o calciferolo negli animali e nelle piante è presente come pro-\nvitamina D, che si converte nella forma attiva mediante esposizione ai raggi\nultravioletti. La provitamina, il 7-deidrocoletserolo, negli animali, uomo com-preso, si converte in colecalciferolo o vitamina D3. Nelle piante, la provita-mina è l’ergosterolo, che si converte, con il sole, in ergocalciferolo o vitaminaD2. La vitamina D si trova nell’olio di fegato di merluzzo, nello sgombro, sal-mone, trote, sardine, aringhe, nel latte di mucca e capra, nel tuorlo d’uovo, nelfegato di pollo, nelle frattaglie in genere, in latte e derivati, nel germe di grano.La forma attiva della vitamina D ha la funzione, come ormone, di controllarel’omeostasi di calcio e fosforo. L’assorbimento a livello intestinale è indipen-dente dalla vitamina D se la dieta è ricca di calcio, ma il meccanismo si attivase si ingerisce poco calcio. La forma metabolicamente attiva è l’1,25-(OH)\n2-colecalciferolo che agisce fa-\nvorendo l’assorbimento del calcio a livello intestinale, il riassorbimento di cal-cio e fosforo nel tubulo contorto prossimale; la deposizione di calcio a livellodel tessuto osseo. Agisce all’interno del sistema paratormone (PTH) – calci-tonina in modo sinergico con il PTH o ormone paratiroideo con conseguenteomeostasi del sistema calcio – fosforo per una corretta mineralizzazione delloscheletro. La maggior parte della vitamina D viene sintetizzata dall’organismo,per azione del sole, soprattutto nei mesi estivi. In genere, la normale esposi-zione ai raggi del sole è sufficiente a coprire il fabbisogno di vitamina D negliadulti, sono sufficienti 10 minuti. Una integrazione è prevista in fase di accre-scimento, gravidanza e allattamento, quando il fabbisogno aumenta. Gli an-ziani in particolar modo sono a rischio di carenza di vitamina D, sia per lamancanza di esposizione alla luce solare, sia per la diminuita capacità di sintesiendogena, per cui si tende ad utilizzare specifici integratori alimentari, utili acoprirne i fabbisogni. Per l’adulto, il fabbisogno è pari a 10-15 μg/die, in re-lazione alla maggiore o minore sintesi endogena. Una sua carenza determinaun inadeguato assorbimento del calcio, provoca ritenzione di fosforo nei reni,demineralizzazione ossea con rachitismo nei bambini, osteomalacia negliadulti, osteoporosi, tensione nervosa, insonnia. Un eccesso di vitamina D, in-vece, può causare calcificazioni diffuse negli organi, contrazioni e spasmi mu-scolari, vomito, diarrea.LaVitamina K è presente in due forme, K1 o fillochinone e K2 o menachi-\nnone. La prima, di origine vegetale è contenuta in particolare nelle verdure afoglie verdi (crucifere), nel kelp, nel fegato, nel tè verde e costituisce la forma\n19Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#190": "Bibliografia\n• Ammerman AS, Haines PS, DeVellis RF, Strogatz DS et al. 1991. “A brief dietary assess-\nment to guide cholesterol reduction in low-income individuals: design and validation.” J\nAm Diet.Assoc. 91:1385-1390.\n• Beaton GH. 1994b. “Approaches to analysis of dietary data: relationship between planned\nanalyses and choice of methodology.” Am J Clin Nutr . 59:253S-261S.\n• Black AE, Bingham SA, Johansson G, and Coward WA. 1997. “Validation of dietary intakes\nof protein and energy against 24 hour urinary N and DLW energy expenditure in middle-\naged women, retired men and post-obese subjects: comparisons with validation against pre-sumed energy requirements.” Eur J Clin Nutr . 51:405-413.\n• Block G, Hartman AM, Dresser CM, Carroll MD et al. 1986a. “A data-based approach to\ndiet questionnaire design and testing.” Am J Epidemiol. 124:453-469.\n• Buzzard IM, Faucett CL, Jeffery RW, McBane L et al. 1996. “Monitoring dietary change\nin a low-fat diet intervention study: advantages of using 24-hour dietary recalls vs foodrecords.” J Am Diet.Assoc. 96:574-579.\n• Byers T, Marshall J, Fiedler R, Zielezny M, and Graham S. 1985. “Assessing nutrient intake\nwith an abbreviated dietary interview.” Am J Epidemiol. 122:41-50.\n• Casey PH, Goolsby SL, Lensing SY , Perloff BP, and Bogle ML. 1999. “The use of telephone\ninterview methodology to obtain 24-hour dietary recalls.” J Am Diet.Assoc. 99:1406-1411.\n• Coates RJ, Serdula MK, Byers T, Mokdad A et al. 1995. “A brief, telephone-administered\nfood frequency questionnaire can be useful for surveillance of dietary fat intakes.” J Nutr.\n125:1473-1483.\n• Eck LH, Klesges LM, and Klesges RC. 1996. “Precision and estimated accuracy of two\nshort-term food frequency questionnaires compared with recalls and records.” J Clin Epi-\ndemiol. 49:1195-1200.\n• Field AE, Colditz GA, Fox MK, Byers T et al. 1998. “Comparison of 4 questionnaires for\nassessment of fruit and vegetable intake.” Am J Public Health . 88:1216-1218.\n• Gersovitz M, Madden JP, and Smiciklas-Wright H. 1978b. “Validity of the 24-hr. dietary\nrecall and seven-day record for group comparisons.” J Am Diet.Assoc. 73:48-55.\n• Gersovitz M, Madden JP, and Smiciklas-Wright H. 1978a. “Validity of the 24-hr. dietary\nrecall and seven-day record for group comparisons.” J Am Diet.Assoc. 73:48-55.\n• Gibson,R. 2005. Principles of Nutritional Assessment. Oxford University Press. New York.\n• Goris AH, Westerterp-Plantenga MS, and Westerterp KR. 2000. “Undereating and under-\nrecording of habitual food intake in obese men: selective underreporting of fat intake.” Am\nJ Clin Nutr . 71:130-134.\n• Green JH, Booth CL, and Bunning RL. 2002. “Assessment of a rapid method for assessing\nadequacy of calcium intake.” Asia Pac.J Clin Nutr . 11:147-150.\n• Hammond J, Nelson M, Chinn S, and Rona RJ. 1993. “Validation of a food frequency ques-\ntionnaire for assessing dietary intake in a study of coronary heart disease risk factors inchildren.” Eur J Clin Nutr . 47:242-250.\n• Hertzler AA and McAnge TR, Jr. 1986. “Development of an iron checklist to guide food\nintake.” J Am Diet.Assoc. 86:782-786.\n• Hill RJ and Davies PS. 2001. “The validity of self-reported energy intake as determined\nusing the doubly labelled water technique.” Br.J Nutr. 85:415-430.\n190Fondamenti della Scienza dell’Alimentazione Capitolo IX",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#191": "• Kirk P, Patterson RE, and Lampe J. 1999. “Development of a soy food frequency question-\nnaire to estimate isoflavone consumption in US adults.” J Am Diet.Assoc. 99:558-563.\n• Kohlmeier L, Mendez M, McDuffie J, and Miller M. 1997. “Computer-assisted self-inter-\nviewing: a multimedia approach to dietary assessment.” Am J Clin Nutr. 65:1275S-1281S.\n• Kushi LH. 1994. “Gaps in epidemiologic research methods: design considerations for stud-\nies that use food-frequency questionnaires.” Am J Clin Nutr . 59:180S-184S.\n• Lillegaard IT, Loken EB, and Andersen LF. 2007. “Relative validation of a pre-coded food\ndiary among children, under-reporting varies with reporting day and time of the day.” Eur\nJ Clin Nutr . 61:61-68.\n• Little P, Barnett J, Margetts B, Kinmonth AL et al. 1999. “The validity of dietary assessment\nin general practice.” J Epidemiol.Community Health . 53:165-172.\n• Maurer J, Taren DL, Teixeira PJ, Thomson CA et al. 2006. “The psychosocial and behavioral\ncharacteristics related to energy misreporting.” Nutr Rev. 64:53-66.\n• McDonald A, Van HL, Slattery M, Hilner J et al. 1991. “The CARDIA dietary history: de-\nvelopment, implementation, and evaluation.” J Am Diet.Assoc. 91:1104-1112.\n• Morin P, Herrmann F, Ammann P, Uebelhart B, and Rizzoli R. 2005. “A rapid self-admin-\nistered food frequency questionnaire for the evaluation of dietary protein intake.” Clin Nutr.\n24:768-774.\n• Pickle LW and Hartman AM. 1985. “Indicator foods for vitamin A assessment.” Nutr Can-\ncer. 7:3-23.\n• Probst YC and Tapsell LC. 2005. “Overview of computerized dietary assessment programs\nfor research and practice in nutrition education.” J Nutr Educ.Behav. 37:20-26.\n• Rebro SM, Patterson RE, Kristal AR, and Cheney CL. 1998b. “The effect of keeping food\nrecords on eating patterns.” J Am Diet.Assoc. 98:1163-1165.\n• Rebro SM, Patterson RE, Kristal AR, and Cheney CL. 1998a. “The effect of keeping food\nrecords on eating patterns.” J Am Diet.Assoc. 98:1163-1165.\n• Rimm EB, Giovannucci EL, Stampfer MJ, Colditz GA et al. 1992. “Reproducibility and\nvalidity of an expanded self-administered semiquantitative food frequency questionnaire\namong male health professionals.” Am J Epidemiol. 135:1114-1126.\n• Rothenberg E. 1994. “Validation of the food frequency questionnaire with the 4-day record\nmethod and analysis of 24-h urinary nitrogen.” Eur J Clin Nutr . 48:725-735.\n• Sawaya AL, Tucker K, Tsay R, Willett W et al. 1996. “Evaluation of four methods for de-\ntermining energy intake in young and older women: comparison with doubly labeled watermeasurements of total energy expenditure.” Am J Clin Nutr . 63:491-499.\n• Seale JL. 2002. “Predicting total energy expenditure from self-reported dietary records and\nphysical characteristics in adult and elderly men and women.” Am J Clin Nutr. 76:529-534.\n• Sempos CT, Liu K, and Ernst ND. 1999. “Food and nutrient exposures: what to consider\nwhen evaluating epidemiologic evidence.” Am J Clin Nutr . 69:1330S-1338S.\n• Slimani N, Bingham S, Runswick S, Ferrari P et al. 2003. “Group level validation of protein\nintakes estimated by 24-hour diet recall and dietary questionnaires against 24-hour urinarynitrogen in the European Prospective Investigation into Cancer and Nutrition (EPIC) cali-bration study.” Cancer Epidemiol.Biomarkers Prev. 12:784-795.\n• Subar AF, Kipnis V , Troiano RP, Midthune D et al. 2003a. “Using intake biomarkers to\nevaluate the extent of dietary misreporting in a large sample of adults: the OPEN study.”Am J Epidemiol. 158:1-13.\n191Capitolo IX Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#192": "• Subar AF, Thompson FE, Potischman N, Forsyth BH et al. 2007. “Formative research of a\nquick list for an automated self-administered 24-hour dietary recall.” J Am Diet.Assoc.\n107:1002-1007.\n• Thompson FE, Midthune D, Subar AF, McNeel T et al. 2005. “Dietary intake estimates in\nthe National Health Interview Survey, 2000: methodology, results, and interpretation.” J\nAm Diet.Assoc. 105:352-363.\n• Trabulsi J and Schoeller DA. 2001a. “Evaluation of dietary assessment instruments against\ndoubly labeled water, a biomarker of habitual energy intake.” Am J Physiol\nEndocrinol.Metab . 281:E891-E899.\n• Vuckovic N, Ritenbaugh C, Taren DL, and Tobar M. 2000. “A qualitative study of partici-\npants’ experiences with dietary assessment.” J Am Diet.Assoc. 100:1023-1028.\n• Zulkifli SN and Yu SM. 1992. “The food frequency method for dietary assessment.” J Am\nDiet.Assoc. 92:681-685.\n192",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#193": "193Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#194": "CAPITOLO X\nI LARN e le Linee Guida\nRiassunto. I Livelli di Riferimento di Energia e Nutrienti (LARN) per la po-\npolazione italiana e le Linee Guida per una sana alimentazione italiana sono\ni più moderni ed efficaci strumenti di politica alimentare in Italia. Sono duedocumenti di consenso che rappresentano la posizione condivisa del mondodella scienza in tutte le sue accezioni, Accademia, Enti di Ricerca, SocietàScientifiche che operano nel campo degli alimenti e della nutrizione. I LARNsi riferiscono agli apporti raccomandati di energia e nutrienti in funzione dellastima dei relativi bisogni a livelli di sicurezza, tenendo conto di specifiche con-dizioni di età, sesso, ecc. La stesura dei LARN si basa sul concetto di adegua-tezza nutrizionale della dieta. Una dieta adeguata deve essere in grado di (i)prevenire carenze nutrizionali; (ii) consentire di avere adeguate riserve cor-poree dei nutrienti; (iii) mantenere le funzioni dell’organismo umano a livelliottimali; (iv) prevenire la insorgenza della patologie a componente nutrizio-nale. Le Linee Guida si propongono la tutela della salute in situazioni in cuifattori socio-economici abbiano determinato sovrabbondanza di risorse e con-seguenti effetti sulla salute dell’uomo. I due strumenti sono correlati: le LineeGuida che traducono in indicazioni pratiche come soddisfare i fabbisogni nu-trizionali fissati dai LARN.\n194Fondamenti della Scienza dell’Alimentazione Capitolo X",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#195": "10.1 Premessa\nLe raccomandazioni nutrizionali in Italia i LARN (SINU, 2012 in corso di\nstampa) – sono il documento di consenso che fissa i valori di energia e nutrientinecessari ogni giorno per mantenere una salute ottimale e avere normali funzionifisiologiche. La traduzione delle raccomandazioni nutrizionali in indicazionidietetiche specifiche viene fatta attraverso le Linee Guida per una Sana Alimen-tazione Italiana (INRAN, 2003) che sono lo strumento di politica alimentare delPaese; le Linee Guida declinano le raccomandazioni nutrizionali in termini distile dietetico in grado di promuovere la salute. Questo percorso, raccomanda-zioni nutrizionali e strumenti di educazione alimentare per la loro messa in pra-tica, è quello che viene fatto in tutti i Paesi con la produzione della cosiddetteFood-based dietary guidelines stilate in maniera compatibile con la efficacia\nterritoriale e le tradizioni enogastronomiche delle varie aree geografiche.\n10.2 Adeguatezza nutrizionale della dieta\nIl concetto di adeguatezza della dieta si è evoluto nel corso degli anni. Tradi-\nzionalmente una dieta adeguata corrispondeva ad una assunzione dietetica taleper cui la qualità e la quantità degli alimenti consumati era tale da assicurareil soddisfacimento dei bisogni di energia e nutrienti; poi sempre di più neltempo ha acquisito valore il rispetto delle combinazioni e proporzioni tramacro- e micro-nutrienti tali da non arrecare rischi potenziali per la salute. Inquesta ottica quindi, per copertura dei fabbisogni si deve considerare quantoè necessario in termini:• Biologici, per il soddisfacimento dei bisogni di energia e nutrienti• Epidemiologici per preservare da malattie legate ad errata e squilibrata ali-\nmentazione\n• Ecologici, per le caratteristiche ambientali dei singoli sistemi e siti di pro-\nduzione agro-alimentare.\nMutuando il glossario della Federazione delle Società Italiane di Nutrizione(FeSIN, 2010), una alimentazione “sana” deve essere tale da:• Coprire i nostri fabbisogni di energia e di nutrienti essenziali.• Fornirci di sostanze protettive.• Minimizzare la nostra esposizione a contaminanti chimici e microbiologici\npresenti negli alimenti.\n195Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#196": "• Avere un impatto ambientale il più basso possibile cioè essere sostenibile.\nFabbisogno nutrizionale e adeguatezza delle dieta si sono dunque evoluti nel\ntempo. Alla sicurezza nutrizionale ( food security ) si è via via aggiunto il focus\nsulla sicurezza alimentare ( food safety ) oggi sempre di più declinato anche\ncome sostenibilità del sistemi agro-alimentari. Sostenibilità non solo ambien-tale ma anche economica e sociale.\n10.3 I livelli di assunzione di riferimento di nutrienti e energia per la po-\npolazione italiana\nI LARN sono il documento di consenso che fissa i valori di riferimento per\nenergia e nutrienti per la popolazione italiana. I dati sono presentati per gruppidi popolazione omogenea, per fasce di età, sesso, stato fisiologico (es gravi-danza, allattamento). Come riportato nella revisione del 1996 (SINU, 1996),i LARN mirano essenzialmente a:• Proteggere l’intera popolazione dal rischio di carenze nutrizionali:• Fornire elementi utili per valutare l’adeguatezza nutrizionale della dieta\nmedia della popolazione o di gruppi di essa rispetto ai valori proposti;\n• Pianificare la politica degli approvvigionamenti alimentari nazionali, nonché\nla alimentazione di comunità.\nDiverse altre applicazioni delle raccomandazioni sono tuttavia possibili e sonostate finora realizzate, quale ad esempio quella della informazione ed educa-zione alimentare, quella della etichettatura nutrizionale dei prodotti alimentarie quella della formulazione di supplementi e alimenti dietetici. Va comunquechiarito che le quantità raccomandate anche se si riferiscono a valori per per-sona per giorno, non debbono essere necessariamente assunte ogni giorno, marappresentano una media dei consumi per un certo periodo di tempo. E’ inoltrebene sottolineare che le raccomandazioni si riferiscono ad individui in buonasalute e non possono essere applicate a soggetti con necessità specifiche deri-vanti da malattie, particolari terapie o diete speciali.Nella revisione 2012 (SINU, in corso di stampa) dei LARN si è stressato dipiù il concetto di valore di riferimento più che di raccomandazione nutrizio-nale. Pur confermando quanto detto sopra sulla raccomandazione, il concettodi valore di riferimento amplia e completa la raccomandazione che diventaquindi più articolata con una serie di ulteriori riferimenti utili a una migliore\n196Fondamenti della Scienza dell’Alimentazione Capitolo X",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#197": "definizione degli apporti di nutrienti in grado di soddisfare i fabbisogni indi-\nviduali e di gruppo.La definizione della raccomandazione si basa sulla definizione del fabbisogno.Nella revisione ’96 dei LARN (SINU, 1996) si individuavano 3 differenti li-velli di raccomandazione sulla base della distribuzione di frequenza dei fab-bisogni individuali di una popolazione o in un gruppo di popolazione: quellominimo al disotto del quale è praticamente impossibile mantenere l’integritàmetabolica per la maggior parte della popolazione; quello medio, che copre ilfabbisogno del 50% degli individui della popolazione e quello cosiddetto diriferimento per la popolazione, corrispondente al fabbisogno medio più duedeviazioni standard, in grado cioè di coprire i bisogni della maggior parte dellapopolazione. Naturalmente quanto maggior sarà la variazione nel fabbisognoindividuale del singolo nutriente, tanto più ampio deve essere il margine di si-curezza.Tutti questi concetti restano validi e vengono declinati nella revisione 2012con gli acronimi mutuati dalla lingua inglese e definiti dall’EFSA (2010).Nello specifico:•PRI (Population Reference Intake ) che è il livello di assunzione del nutriente\nsufficiente a soddisfare il fabbisogno di quasi tutti (97,5%) i soggetti sani inuno specifico gruppo di popolazione.\nOltre al PRI, si indicano:• Il fabbisogno medio AR(Average Requirement) che è il livello di assunzione\ndel nutriente sufficiente a soddisfare il fabbisogno del 50% dei soggetti saniin uno specifico gruppo di popolazione.\nQuando non si hanno sufficienti dati per calcolare il PRI e all’AR (ad esempionei bambini molto piccoli, ad es. al disotto dell’anno di età, oppure per alcuniminerali) si utilizza:• l’assunzione adeguata AI(Adequate Intake ) che si ricava dagli apporti medi\nosservati in una popolazione esente da carenze manifeste.\nE’ oramai concetto consolidato che per alcuni nutrienti, oltre al rischio di ca-renza può esserci un rischio di assunzione eccessiva con la dieta. Bisognaquindi stabilire un intervallo di sicurezza nell’ambito del quale è minimo sia ilrischio di eccesso che il rischio di inadeguatezza (INRAN, 2003). In questoquadro vale la pena ricordare l’approccio particolare che viene usato per l’ener-gia. Nel caso dell’energia il rischio per la salute aumenta anche se l’apporto\n197Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#198": "giornaliero si discosta di pochissimo in eccesso (obesità) o in difetto (malnu-\ntrizione). Per l’energia quindi si definisce solo il fabbisogno medio, senza sta-\nbilire il un apporto raccomandato che copra le due deviazioni standard dellapopolazione. Nel caso invece degli altri nutrienti, viene usato un approccio con-servativo di copertura del 97,5% (media più due deviazioni standard) di popo-lazione perché i rischi per la salute sono maggiormente legati ad una carenzache non ad un eccesso e questa copertura ampia consente di assicurare un mar-gine di sicurezza.Per lipidi e carboidrati si è provveduto a definire:• gli intervalli di riferimento per l’assunzione di macronutrienti ( Reference In-\ntake range for macronutrients, RI) con valori minimi e massimi espressi in\npercentuale sull’energia totale della dieta.\nIn aggiunta, la necessità di incorporare nel documento l’evidenza scientificasulle relazioni fra stato di nutrizione e prevenzione delle patologie cronico-degenerative, al di la della semplice soddisfazione del ruolo biologico dei nu-trienti, ha portato in qualche caso, ad esempio acidi grassi saturi o gli zuccheri,all’introduzione di:• obiettivi nutrizionali per la prevenzione ( Suggested Dietary Target, SDT)\nnonché di raccomandazioni qualitative sulle scelte fra diverse fonti alimen-tari.\nPer completare il discorso dei macronutrienti vale la pena sottolineare che leproteine sono l’unico macronutriente che si comporta come un micronutriente;infatti per le proteine abbiamo un PRI a copertura di sicurezza di una ampiafascia di popolazione (97,5%). Inoltre in molti casi si è indicato anche:• il limite massimo tollerabile di assunzione ( tollerable Upper intake Level,\nUL) che rappresenta l’apporto più elevato del nutriente che non si associa aeffetti avversi sulla salute.\nGli UL utilizzati sono quelli definiti a livello europeo dalla commissione SCF-EFSA (2006, 2012).La ragione della introduzione degli STD nella revisione 2012 dei LARN ha lasua ragion d’essere in relazione ai tanti dati che stanno emergendo nella lette-ratura internazionale sul ruolo preventivo di alcuni nutrienti. Infatti i fabbiso-gni individuali di nutrienti ed energia si possono definire sulla base dellemanifestazioni cliniche da carenza, del mantenimento sia delle riserve del nu-triente nell’organismo sia delle funzioni biochimico-fisiologiche (in presenza\n198Fondamenti della Scienza dell’Alimentazione Capitolo X",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#199": "di una adeguata composizione corporea), e anche in relazione alla prevenzione\nnutrizionale delle malattie e alle relazioni tra dieta e morbosità e mortalità. Laletterature degli ultimi anni, in particolare, sembra indicare il ruolo di diversinutrienti nella riduzione del rischio per patologie cronico-degenerative che vaoltre i loro effetti biologici e nutrizionali in senso stretto. Di fatto è in genereproblematico, se non impossibile, elaborare una precisa raccomandazionequantitativa rivolta alla prevenzione nutrizionale delle malattie cronico-dege-nerative, poiché il rischio di malattia risente fortemente di altri fattori legatialla dieta, come la presenza sinergica di più nutrienti e sostanze non-nutritivenegli alimenti e delle diverse abitudini alimentari in grado di oscurare l’even-tuale ruolo preventivo dello specifico nutriente. Inoltre, non sono in generedisponibili sufficienti dati dose-risposta che permettano l’identificazione diun valore soglia efficace in termini preventivi. Proprio per prendere in consi-derazione gli aspetti nutrizionali più propriamente preventivi, sono stati uti-lizzati gli RI per lipidi e carboidrati e gli STD per acidi grassi e zuccheri.\n10.4 Come si usano i LARN\nI LARN possono essere utilizzati con diversi obiettivi di ricerca e pianifica-\nzione nutrizionale a livello sia individuale sia di gruppo o comunità. Offronoinoltre una necessaria base di conoscenze nella definizione di politiche sani-tarie e commerciali, ad esempio nella messa a punto di linee guida, nell’eti-chettatura nutrizionale o nello sviluppo di nuovi alimenti e integratorialimentari. I LARN vengono quindi tradotti nelle Linee Guida per dare indi-cazioni pratiche alla popolazione per come soddisfare i propri fabbisogni dienergia e nutrienti e per come proteggere il proprio stato di salute.\n10.5 L’evoluzione delle linee guida per una sana alimentazione\nLe Linee Guida per una sana alimentazione Italiana sono una serie di consigli\ne indicazioni nutrizionali, periodicamente aggiornate, elaborate da un’appositacommissione scientifica che raccoglie prestigiosi studiosi del mondo dell’ali-mentazione oltre che una nutrita rappresentanza della comunità scientifica.L’Ente pubblico Italiano che secondo la sua legge istitutiva (Legge n.258/63;Legge n.70/75; D.lgs 454/99) fin dalla sua costituzione nel 1963, ha il compitospecifico di promuovere la sana alimentazione anche attraverso la revisione\n199Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#2": "Editore\nAIBAAssociazione Italiana Biologia ApplicataVia Icilio, 7 - 00153 RomaTel. 06/57090200Fax 06/57090235www.onb.itE-mail:igiene.qualita.sicurezza.ONB@gmail.com\nConsiglio\ndell’Ordine Nazionale dei BiologiPresidente, Ermanno Calcatelli;Vice Presidente, Antonio Costantini;Tesoriere, Pietro Sapia;Segretario, Luciano O. Atzori;Consiglieri, Domenico L. Laurendi,Pietro Miraglia, Pierluigi Pecoraro,Franco Scicchitano, Gianni Zocchi.\nA cura\nCommissione Permanente di Studio “Nutri-zione” dell’Ordine Nazionale dei Biologi\nAutori\nGianni ZocchiPierluigi PecoraroLaura RossiSabina SieriRossella TrioFrancesca TommasiMarisa CampanilePatrizia ZulianiPaolo PaoliFilippo CarlucciRoberto CiandagliaSilvio MorettiCopertinaXXX\nGrafica e impaginazione\nFotolito Moggio srl\nUfficio Stampa e redazione\nLuca Mennuni, Claudia TancioniOrdine Nazionale dei Biologi, RomaTel. 06/57090205 - Fax 06/57090234\nSegreteria\nPina ComandèOrdine Nazionale dei Biologi, RomaTel. 06/57090204 - Fax 06/57090235e-mail: segreteria@onb.it\nStampa e rilegatura\nFotolito Moggio srl\nFinito di stampare nel mese di maggio 2015©copyright 2015 AIBA tutti i diritti sono ri-\nservati\nL’editore non si assume nessuna responsabi-\nlità sui contenuti e sulle opinioni espresse\ndagli autori dei testi.\n2",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#20": "più presente nella dieta. La seconda, può essere di origine batterica, cioè sin-\ntetizzata dai batteri simbionti normalmente presenti nella flora intestinale eu-biotica; o di origine alimentare, le cui fonti sono pollo, tuorlo d’uovo, prodottilattiero-caseari, fegato bovino. Il corretto assorbimento viene favorito dai grassi, in condizioni di normalefunzionalità biliare e pancreatica. Successivamente solo la vitamina K1 ver-rebbe inserita nei chilomicroni, poi nelle VLDL e LDL, per essere infine ce-duta ai tessuti. I menachinoni sarebbero riassorbiti nel colon.La forma attiva della vitamina K è il cofattore dell’enzima carbossilasi impor-tante per la cascata della coagulazione del sangue e interviene nella fissazionedel calcio. Una sua carenza porta a emorragie ed osteoporosi. Il fabbisogno dell’adulto è valutato intorno ai 1 μg per kg peso corporeo, cioècirca 60 μg al giorno, normalmente coperto dalla sintesi endogena a livello diflora intestinale.\nVitamine idrosolubili La Vitamine B12 è rappresentata da un gruppo di sostanze contenenti cobalto,\nl´idrossicobalamina e la cianocobalamina. Tali sostanze sono coinvolte in\nnumerose reazioni biochimiche (metabolismo degli acidi grassi, degli ammi-\nnoacidi, degli acidi nucleici, del ferro). Una sua carenza, piuttosto rara, si ma-nifesta solo nei casi di dieta vegana, in particolare durante la gravidanza, conconseguenze per il nascituro di danni neurologici. La carenza può anche deri-vare dall’assenza del fattore intrinseco, secreto dalle cellule parietali del fondodello stomaco, che ne facilita l’assorbimento a livello intestinale, con conse-guenti disturbi a carico del sistema nervoso e della produzione delle celluledel sangue, fino a forme di anemia e neuropatie e, soprattutto nella terza età,di deterioramento delle capacità cognitive. Per tali ragioni si può consigliare l’integrazione di vitamina B12 anche neglianziani, con stati di carenza per dieta povera di alimenti di origine animale, acausa di cattiva masticazione. Fonti alimentari di tale vitamina, seppure in minime quantità, sono in partico-lare il fegato, la carne, il latte e derivati, le uova e il pesce. Gli alimenti vegetalinon ne contengono. E’ resistente alla cottura. Sono necessarie minime quantitàgiornaliere di vitamina B12, largamente coperta dalla dieta, pari a circa 2μg/die per l’adulto. La vitamina B12 viene immagazzinata nell’organismo,con un’emivita calcolata in 1-4 anni.Indicatori sensibili di diete carenti di vitamina B12 sono alti livelli di acido\n20Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#200": "periodica delle Linee Guida è l’Istituto Nazionale della Nutrizione (INN) poi\ndiventato INRAN (Istituto Nazionale di Ricerca per gli Alimenti e la Nutri-zione) oggi CRA-NUT (Centro di Ricerca per gli Alimenti e la Nutrizione).La caratteristica delle Linee Guida è quella di essere un documento di con-senso, prodotto da una commissione multidisciplinare che si rivolge alla po-polazione generale in modo autorevole e scevro da interessi economici.Le Linee Guida sono uno strumento di politica alimentare in grado di tradurrele raccomandazioni nutrizionali in consigli diretti su come mangiare. Pratica-mente i LARN ci dicono di quanto ferro abbiamo bisogno o di quante proteine(ad esempio) e le Linee Guida ci dicono quanti e quali alimenti dobbiamo con-sumare per soddisfare i fabbisogni (di ferro, di proteine, ecc.) e per proteggerela nostra salute. In questa ottica infatti dobbiamo tenere presente che nel pas-sato sana alimentazione significava principalmente correggere le malattie dacarenza nutrizionale e quindi Linee Guida significava dare alla popolazionele indicazioni di quanta frutta mangiare perché non avesse lo scorbuto (carenzadi vitamina C). La scienza della alimentazione oggi chiede molto di più allanutrizione: oltre al soddisfacimento dei fabbisogni nutrizionali per non averecarenze oggi chiediamo alla sana alimentazione di proteggerci dalle malattiecronico-degenerative e garantirci salute e longevità. La nutrizione in otticamoderna è soprattutto prevenzione e le Linee Guida sono quindi uno strumentodi salute pubblica fondamentale nel quadro della politica alimentare di unPaese. Il raggiungimento dell’obiettivo, così importante, di un più correttocomportamento alimentare da parte del maggior numero possibile di italianipuò essere conseguito solo con una migliore informazione e con una miglioreconoscenza, basate su dati obbiettivi e scientificamente convalidati, da partedei consumatori.L’alimentazione sana ed equilibrata rappresenta oggi la maniera più efficacee alla portata di tutti per guadagnare salute, abbattere i costi legati alle più im-portanti patologie del nostro tempo, risparmiare anni di lavoro e di vita sanadi ognuno di noi. Mai come oggi, ove obesità e le malattie ad essa correlate(aterosclerosi, diabete, ipertensione e alcuni tipi di tumore) rappresentano uncosto altissimo in vite umane, in riduzione della qualità della vita, in ore lavo-rative perse e in spese sanitarie, si rendono improrogabili interventi mirati,forti e precisi di educazione alimentare nella popolazione che agiscano a piùlivelli e che siano capaci di orientare le politiche sanitarie verso un modellodi stile di vita virtuoso.Tali Linee Guida per una sana alimentazione sono costruite tutte sulla basedel modello alimentare mediterraneo, modello che ha ormai acquisito fama e\n200Fondamenti della Scienza dell’Alimentazione Capitolo X",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#201": "onore del miglior modello attraverso il quale coniugare salute e piacere. Gli\nalimenti a disposizione variano da Paese a Paese, così come le abitudini ali-mentari; molte sono quindi le possibilità per la realizzazione di una dieta sa-lutare nel quadro di uno stile di vita egualmente salutare. Ogni Paese declinai dettami della dieta mediterranea a seconda delle proprie possibilità di sceltealimentari e delle abitudini alimentari. Infatti gli alimenti di cui disponiamosono tantissimi, e molte sono anche le vie per realizzare una dieta salutare nelquadro di uno stile di vita egualmente salutare. Ognuno ha quindi ampia pos-sibilità di scelte.\n10.6 Attualità e modernità delle linee guida per una sana alimentazione:\ndalle carenze nutrizionali alla alimentazione come prevenzione.\nIn Italia la prima edizione delle “Linee Guida per una sana alimentazione” è\nstata pubblicata dall’allora INN nel 1986; erano e sono tutt’ora dopo le varierevisioni, le uniche indicazioni istituzionalmente valide per indirizzare il cit-tadino ad un’alimentazione equilibrata. Per tale iniziativa, l’Istituto si è avvalsosia delle competenze interne, sia della collaborazione di numerosi rappresen-tanti della comunità scientifica nazionale. Nel 1997, a distanza quindi di 10anni dalla prima edizione è stata eseguita la prima revisione, cui è seguita unaseconda nel 2003. E’ in corso d’opera la revisione 2015 delle Linee Guida dicui possiamo anticipare qualche tematica.Dal 1997 al 2003 abbiamo avuto il primo ampliamento dei temi trattati, conl’inserimento di tre nuove Direttive destinate a rispondere all’esigenza di unamaggiore completezza rispetto ai problemi e alle domande che il consumatorecomune oggi si pone in tema di nutrizione. La prima delle tre “nuove” Diret-tive è quella che affronta la questione del bilancio idrico del nostro organismo,dei bisogni di acqua e delle funzioni che essa svolge, e anche delle numerosefalse credenze che circondano questo nutriente fondamentale. La seconda“nuova” Direttiva è quella che esamina problemi particolari dell’alimentazionedi alcuni gruppi di popolazione “speciali” in quanto caratterizzati da esigenzenutrizionali specifiche, e per questi motivi più “vulnerabili”. Si tratta di bam-bini, adolescenti, gestanti, nutrici, donne in menopausa, anziani, una quota dipopolazione, quest’ultima, in forte aumento. Sulle modalità di una corretta ali-mentazione di tutti questi soggetti esiste nella conoscenza comune una note-vole confusione di idee: da qui l’opportunità di parlarne in modoparticolareggiato nell’ambito delle Linee guida. La terza “nuova” Direttiva ri-\n201Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#202": "guarda i problemi della sicurezza alimentare, con speciale riferimento a quella\ndomestica. Infatti, ferma restando la grande responsabilità delle Istituzionipubbliche nel formulare regole e prevedere controlli per tutta la filiera agro-alimentare (e la creazione di un Agenzia europea dedicata alla sicurezza ali-mentare ne è la prova), si è ritenuto utile e opportuno stimolare anche laresponsabilità e la partecipazione individuale, dal momento dell’acquisto deglialimenti alle fasi di preparazione e di conservazione degli stessi, poiché è pro-vato che gran parte degli incidenti e dei problemi di tipo igienico-sanitario siverificano proprio nella cucina di casa.In linea generale i concetti e le indicazioni presenti nelle Linee Guida del 2003sono tuttora valide, poiché le basi scientifiche che hanno portato alla loro for-mulazione continuano ad avere validità e anzi diventano ancora più valide allaluce delle ultime evidenze scientifiche, ma hanno bisogno di essere rivisitatealla luce delle mutate abitudini di vita degli italiani che sono ad esempio semprepiù sedentari, dello stato di salute, dei progressi delle conoscenze scientifiche,della disponibilità di nuovi prodotti che l’industria agroalimentare italiana haaffiancato ai prodotti tradizionali per migliorare ed aumentare la possibilitàdelle scelte del cittadino. La revisione del 2003 tenne concretamente conto dellaproblematica ancora attualissima per altro della sicurezza alimentare e dellaperdita del fiducia da parte dei consumatori nei confronti delle Autorità cheavrebbero dovuto garantirla. A fronte di minacce come la “mucca pazza” o l’in-fluenza aviaria o la più recente suina, e di una necessità da parte del consuma-tore di una informazione non “emotiva” ma autorevole, venne inclusa nellarevisione del 2003 la linee guida sulla sicurezza alimentare. E’ uno spazio cheverrà mantenuto e aggiornato anche nella revisione del 2015 delle linee guidaperché le tematiche sono ancora cogenti e perché c’è necessità di dare indica-zioni semplici e praticabili da parte della popolazione per garantire la sicurezzadei propri alimenti. Il consumatore italiano in particolare, gode oggi della di-sponibilità di una ampia gamma di prodotti dotati di ottime caratteristiche, tantoquelli della tradizione quanto quelli offerti da un settore produttivo agro-indu-striale che ha già dimostrato di volersi orientare in coerenza con le indicazionivia via fornite dalle precedenti Linee guida. È quindi fondamentale impararead usare gli alimenti disponibili nel modo più corretto, ed è proprio per questo,lo ribadiamo, che vengono predisposte le Linee guida. In questa ottica anche ilsettore della ristorazione collettiva può svolgere un ruolo di particolare impor-tanza, sia producendo e distribuendo pasti nel rispetto delle indicazioni delleLinee guida, sia diffondendo ai propri utenti una informazione alimentare coe-rente con i principi contenuti nelle stesse Direttive.\n202Fondamenti della Scienza dell’Alimentazione Capitolo X",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#203": "Due dei temi caldi, imprescindibili per la revisione 2015 delle Linee Guida\nsaranno l’impatto ambientale dei consumi alimentari e il costo economico peril consumatore di una dieta sana. Queste tematiche verranno dunque affrontatenella revisione 2015 delle Linee Guida o in appositi capitoli su questi due temioppure affrontandoli a mano a mano che vengono dati consigli alimentari neldocumento. La povertà alimentare e le nuove fasce di popolazione vulnerabiliportano sempre di più gli operatori di salute pubblica a dare indicazioni chesiano anche commensurate alle possibilità economiche. Alcuni cibi possonoessere troppo costosi per larghe fette di popolazione; per contro ci sono mol-tissime scelte alimentari che sono sia salutari che affrontabili senza appesantireil bilancio familiare. Pane, cereali, riso, pasta e patate, vegetali e frutta di sta-gione o legumi, sono gli alimenti che dovrebbero costituire la base dell’ali-mentazione non sono particolarmente costosi. Riorganizzare le scelte edindirizzarle verso questi prodotti consente di guadagnare salute e risparmiaredenaro. La sostenibilità dei sistemi agroalimentari è un altro tema caldo cheverrà affrontato nelle revisione 2015 delle Linee Guida. Anche se attualmentec’è abbastanza cibo, la produzione di cibo sta creando problemi ambientali indiversi modi e la sostenibilità a lungo termine della produzione alimentare stadiventando un problema sempre più importante. Possiamo continuare a usarel’acqua e il suolo con il nostro modo abituale di produzione del cibo? La do-manda non è di stretta pertinenza nutrizionale ma è doveroso porsela e lenuove Linee Guida dovranno promuovere un modello non solamente sano maanche sostenibileAnche nella revisione 2015 continuerà l’aggiornamento e l’inserimento dinuove tabelle e dati su vari temi, compreso l’elenco di “porzioni standard ita-liane” la cui corretta valutazione da parte del singolo è di importanza fonda-mentale se si vuole riuscire a realizzare una buona alimentazione. Il capitolodelle “porzioni” è l’anello di congiungimento tra i LARN e le Linee Guida.Nei LARN vengono definite le porzioni per coprire i fabbisogni che nelleLinee Guida vengono declinate il rapporto dalla sana alimentazione. Il rag-giungimento dell’obiettivo, così importante, di un più corretto comportamentoalimentare da parte del maggior numero possibile di italiani può essere con-seguito solo con una migliore informazione e con una migliore conoscenza daparte dei consumatori. Il consumatore italiano, in particolare, gode oggi delladisponibilità di un’ampia gamma di prodotti dotati di ottime caratteristiche,tanto quelli della tradizione quanto quelli offerti da un settore produttivo agro-industriale che ha già dimostrato di volersi orientare in coerenza con le indi-cazioni via via fornite dalle precedenti Linee Guida. È quindi fondamentale\n203Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#204": "imparare ad usare gli alimenti disponibili nel modo più corretto, ed è proprio\nper questo che vengono predisposte le Linee guida. In questa ottica anche ilsettore della ristorazione collettiva può svolgere un ruolo di particolare im-portanza, sia producendo e distribuendo pasti nel rispetto delle indicazionidelle Linee guida, sia diffondendo ai propri utenti un’informazione alimentarecoerente con i principi contenuti nelle stesse Direttive.\n10.7 Perché le revisioni periodiche delle Linee Guida?\nLa opportunità e la necessità di una periodica revisione delle Linee guida, pur\nnel solco della continuità con le precedenti edizioni, sono facilmente spiega-bili. Il primo motivo è quello del necessario aggiornamento in base alla con-tinua evoluzione delle conoscenze scientifiche circa il ruolo dei singolinutrienti e di vari componenti minori e i relativi bisogni e rapporti reciprocinell’ambito di una dieta equilibrata. Il secondo motivo è certamente quello delmutamento dei consumi, delle abitudini e degli orientamenti alimentari e deglistili di vita, nel quadro di una società che dimostra sempre più attenzione allecorrelazioni fra alimentazione e salute, ma che contemporaneamente vede au-mentare sia le patologie legate ad una dieta abituale eccessiva e/o squilibratasia la confusione e la disinformazione circa ruoli e funzioni di alimenti e dinutrienti. E tutto ciò avviene nonostante l’impressionante crescita della massadi informazioni dirette al grande pubblico, veicolate da un sempre maggiornumero di canali, anche molto innovativi. Sono proprio questi motivi a rendereancora più pressante l’esigenza di mettere a disposizione del consumatore unostrumento, come le Linee guida, i cui contenuti siano approvati dalle istituzioniscientifiche, che sia aggiornato nei suoi contenuti ma anche sempre più fun-zionale rispetto ai tempi che cambiano, e che sia facilmente comprensibile eutilizzabile nonché capace di fornire, accanto a pratiche indicazioni anche in-formazioni di carattere più spiccatamente tecnico-scientifico.Gli alimenti di cui disponiamo sono tantissimi, e molte sono anche le vie perrealizzare una dieta salutare nel quadro di uno stile di vita egualmente salutare.Ognuno ha quindi ampia possibilità di scelte. Negli ultimi decenni Istituzionipubbliche e Organismi scientifici hanno dato vita, nei principali Paesi delmondo, a Linee guida o Direttive alimentari. E nella stessa direzione si sonomosse le principali Agenzie internazionali che si occupano di alimentazione esalute. Il motivo per il quale le Linee guida vengono compilate e diffuse inmilioni di copie è proprio quello di fornire al consumatore una serie di semplici\n204Fondamenti della Scienza dell’Alimentazione Capitolo X",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#205": "informazioni alimentari del nostro Paese, proteggendo contemporaneamente\nla propria salute. Per far questo sono chiamati a collaborare studiosi apparte-nenti a varie istituzioni scientifiche e accademiche italiane, in grado di affron-tare, con le loro differenti competenze, tutto l’arco delle varie problematicheriguardanti la nutrizione. Destinatario e ragion d’essere delle Linee Guida èquindi l’universo dei consumatori, al quale le Direttive sono indirizzate e dalquale devono poter essere agevolmente comprese e utilizzate, per realizzareun’alimentazione sana ed equilibrata, garantendosi più benessere e salute senzadover mortificare il gusto e il piacere della buona tavola. E tutto ciò evidente-mente è più facile per coloro che conservano abitudini alimentari tradizionalidel nostro Paese, vale a dire un uso frequente di cibi meno densi di energia epiù ricchi di sostanze utili.Proprio la esplosione di tanta informazione incontrollata rende ancora più pres-sante l’esigenza di mettere a disposizione del consumatore uno strumento,come le Linee guida, che sia garantito dalle istituzioni scientifiche, che sia ag-giornato nei suoi contenuti, ma anche sempre più funzionale rispetto ai tempiche cambiano, e che sia facilmente comprensibile e utilizzabile nonché capacedi fornire, accanto a pratiche indicazioni (riassunte particolarmente nei “comecomportarsi”, nelle “false credenze su…”, ecc.), anche informazioni di carat-tere più spiccatamente tecnico-scientifico. Le Linee Guida si rivolgono per-tanto sia a chi desidera avere semplici chiarimenti sugli aspetti-base dell’usodegli alimenti per la vita quotidiana, sia a chi si aspetta di trovare nelle nuoveLinee guida anche maggiori approfondimenti di tipo scientifico, senza esserecostretto a ricercarli consultando altre pubblicazioni. \n10.8 Le Linee Guida nei moderni strumenti di comunicazione\nLa declinazione delle linee guida in piattaforme informatiche moderne\n(www.\nSAPERMANGIARE .MOBI ) ha lo scopo di comunicare anche attraverso il\nweb le direttive della sana alimentazione e di un corretto stile di vita ai cittadiniitaliani. In questo caso il target di riferimento sono i più giovani (dai 14 ai 35anni), in quanto la fascia d’età più abituata all’uso di tecnologie di comunica-zione avanzate. Le dieci Linee Guida sono diventati dei filmati in cui una “fa-miglia normale” declina le indicazioni nutrizionali nella vita di tutti i giornianche con criticità ed errori con uno spirito leggero e non coercitivo. E’ in pro-grammazione il lancio di filmati brevi di un minuto e mezzo sulle tematichedelle Linee Guida che saranno lanciati in modo virale sui vari server con\n205Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#206": "l’obiettivo di essere ripresi e ritrasmessi sulle varie piattaforme web, ad es\nhttp://www.youtube.com. SAPERMANGIARE .MOBI è un servizio di tutoraggio\ncontinuo nel tempo per il cittadino, ovvero quanto di più simile si possa im-maginare a un “assistente alimentare” che aiuti nel tempo l’utente a trovare ilsuoregime corretto. La piattaforma ha anche una raccolta organica delle\nschede di approfondimento. Tra i servizi offerti basti ricordare: •ChiedoeMangio, un servizio di risposte brevi e secche alle domande più fre-quenti; \n•ContoeMangio, un servizio che, attraverso un software di facile utilizzo, per-mette di calcolare il contenuto in calorie e macronutrienti di un alimento(con la sua quantità), un piatto, una ricetta, un pasto o una dieta.\n•SoBere?, un servizio per la determinazione personalizzata degli effetti del-l’alcool, che consente all’utente di calcolare gli effetti dell’alcool (fra i qualiquelli sulla guida) sulla base dei principali parametri che li influenzano(sesso, età, peso, distanza dall’ultimo pasto). La pagina del servizio proponeanche alcune schede di approfondimento sull’alcool nell’alimentazione.Questi servizi saranno accessibili anche via cellulare.\nNel solco della utilizzazione dei nuovi media, al sito istituzionale \nSAPERMAN -\nGIARE .MOBI è legata una pagina facebook (https://www.facebook.com/saper-\nmangiare.mobi?fref=ts) in cui oltre a pubblicizzare eventi e convegni sonoaperte discussioni sulle tematiche più calde emergenti dalla stampa. Ovvia-mente apertissimo è il canale della domanda/risposta dei frequentatori dellapagina, dietro cui c’è l’Istituzione e non i singoli.\n10.9 Conclusioni\nI LARN e le Linee Guida sono i due strumenti di politica alimentare che ven-\ngono redatti e periodicamente aggiornati in Italia. Si basano entrambi sul con-cetto di fabbisogno nutrizionale e della sua evoluzione con la progressivaintroduzione dei concetti di sicurezza alimentare e sostenibilità ambientale,economica e sociale. LARN e Linee Guida sono documenti di consenso pre-parati da autorità pubbliche scevre da interessi commerciali che rappresentanonon solo l’Istituzione che li redige ma anche tutti i portatori di interesse nelcampo specifico di applicazione. I LARN sono un documento tecnico destinatia un pubblico di settore, sanitario, nutrizionistico in senso ampio, ministeriale,industrie e associazioni di produttori. Le Linee Guida sono destinate princi-\n206Fondamenti della Scienza dell’Alimentazione Capitolo X",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#207": "palmente al consumatore e alle associazioni di tutela dei consumatori, sono\nuno strumento di prevenzione che dovrebbe essere divulgato anche nellescuole così come nei servizi preventivi, nella medicina di base, e in tutte lestrutture che fanno nutrizione applicata. In chiusura e in conclusione è benerimarcare che benché sia un campo promettente dal punto di vista della ricercascientifica bisogna precisare che allo stato attuale delle conoscenze le racco-mandazioni nutrizionali sono basate sui fabbisogni di gruppi di popolazione;la informazione basata sulle raccomandazioni personalizzate tarate sui fabbi-sogni individuali non è ancora matura per essere gestita e ha orizzonti ancoratroppo limitati.\n207Capitolo X Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#208": "208",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#209": "Fondamenti della Scienza\ndell’Alimentazione\nVia Icilio, 7 - 00153 Roma - www.onb.it\nFondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#21": "metilmalonico e/o di omocisteina, che sono collegati ad anomalie cliniche. La\ncarenza di vitamina B12, insieme a quella di acido folico, piridossina, zincoed enzimi può provocare iper-omocisteinemia, con conseguente danno vasco-lare. L’OMS (Organizzazione Mondiale della Sanità) considera fino a 13μmoli/l un valore ematico normale. La Task Force Internazionale per la Pre-venzione della Malattia Cardiovascolare considera fino a 12 μmoli/l un valore\nematico normale . Quindi livelli n ormali di omocisteina sono considerati tra\n5 e 9 μmol/l.L’acido folico oVitamina B9 o folacina è essenziale per la sintesi di DNA, nel\nmetabolismo degli aminoacidi e per la formazione dell’emoglobina e quindi deiglobuli rossi, rientra in numerosi processi enzimatici; riduce i livelli di omoci-steina, il cui eccesso è associato a rischio di malattie cardiovascolari e infarti.Il fabbisogno minimo di folacina viene valutato intorno ai 50-100 μg al giorno.I LARN ne raccomandano un’assunzione giornaliera di almeno 200 μg/die.Le necessità aumentano notevolmente durante la gravidanza, la crescita, l’al-lattamento ed in corso di alcune patologie (anemie, tumori). La supplementa-zione di folati è molto importante nelle prime fasi della gravidanza, per ilrischio di malformazioni nel feto, in particolare difetti del tubo neurale, spinabifida e anencefalia. In questo caso si raccomanda l’integrazione di acido fo-lico sino a 400 μg /die.I folati si trovano soprattutto nelle frattaglie (circa 500 microgrammi /100g),nei vegetali come brassicacee, asparagi, lattuga, pomodori, legumi, frutta come, arance, kiwi, fragole, cioccolato, frutta secca a guscio in forma più o menodisponibile, uova. La cottura distrugge gran parte dei folati presenti negli ali-menti, così pure gli antinutrienti contenuti, ad esempio nei legumi, ne riduconol’assorbimento a livello intestinale.La carenza di acido folico può portare ad anemia megaloblastica, ritardo del-l’accrescimento, disturbi della memoria, precoce ingrigimento dei capelli. E’spesso associata a carenza di altri nutrienti come vitamina B12 e zinco.\n1.2 I Macronutrienti\n1.2.1 Carboidrati\nI carboidrati o saccaridi o glucidi o idrati di carbonio sono i composti chimici\npiù diffusi ed abbondanti sulla terra, poiché svolgono un gran numero di fun-zioni in tutte le forme di vita. Sono costituiti da C, H, O, in rapporto C\nn(H2O)n, struttura da cui deriva il nome\n21Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#22": "di carboidrati. Classicamente i glucidi vengono suddivisi in monosaccaridi,\ndisaccaridi, oligosaccaridi e polisaccaridi.Le piante verdi, attraverso il processo di fotosintesi clorofilliana, trasformanola CO\n2in composti organici, per lo più glucidi con liberazione di O2.\nI carboidrati comprendono anche le fibre, polisaccaridi cellulari e di riserva dellepiante (cellulosa, pectine, gomme, mucillagini), definite carboidrati indigeribili\nin quanto non possono essere idrolizzati dagli enzimi digestivi dell’uomo.I carboidrati servono a conservare l’energia, come nutrienti e intermedi meta-bolici. L’amido, nelle piante ed il glicogeno, negli animali, sono i carboidratiche costituiscono la riserva energetica, facilmente disponibile per essere tra-sformata in glucosio.Il glucosio è lo zucchero biologico per eccellenza, monosaccaride, che puòessere considerato come la principale sostanza glucidica per la produzione dienergia (ATP).Gli zuccheri sono costituenti di un gran numero di molecole biologiche: ATP;DNA; RNA, oltre ad essere intermedi del metabolismo.Si suddividono in base al numero di atomi di C presenti ed al gruppo funzio-nale. Sono poliidrossialdeidi o poliidrossichetoni a seconda se il corbonile èaldeidico o chetonico.I monosaccaridi, unità strutturali più semplici, non scindibili per idrolisi, sonoaldeidi o chetoni che hanno due o più gruppi ossidrilici; la formula generale è(CH\n2O)n, i più piccoli. L’aldoso più semplice è la gliceraldeide, mentre il che-\ntoso più semplice è il diidrossiacetone. Questi due zuccheri semplici vengonochiamati triosi in quanto contengono 3 atomi di carbonio. Ai monosaccaridi,sia aldosi che chetosi, vengono dati nomi generici che descrivono i gruppi fun-zionali importanti ed il numero totale di atomi di carbonio. I monosaccaridipiù semplici sono solubili in acqua ed hanno generalmente un sapore dolce.I disaccaridi sono formati da due monosaccaridi uniti insieme da un legameglicosidico. Gli oligosaccaridi sono formati da tre a 10 unità monosaccaridiche,unite insieme da legami glicosidici. I polisaccaridi, chiamati anche glicani, sono formati da unità monosaccaridi-che, fino a qualche milione, tenute insieme sempre da legami glicosidici. Pos-sono essere formati da un solo tipo di monosaccaride (omopolisaccaride) odiversi tipi di monosaccaridi (eteropolisaccaridi); assumono una struttura li-neare o ramificata in funzione del tipo di legami che si formano. I principali polisaccaridi sono l’amido, il glicogeno (polisaccaridi di riserva),la cellulosa, la chitina (polisaccaridi di struttura), i mucopolisaccaridi (acidoialuronico). La forma principale di polisaccaride di deposito negli animali è il\n22Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#23": "glicogeno, che si trova principalmente nel fegato (fino al 10% della massa\nepatica) e nel muscolo scheletrico (1-2% della massa). \nNuova classificazione dei carboidratiNel 1981 David Jenkins propose una nuova classificazione dei carboidrati ba-\nsata sulla velocità con cui i carboidrati contenuti negli alimenti determinavanol’aumento della glicemia dopo il pasto. L ’indice glicemico è un indicatore\nstandardizzato della capacità di un carboidrato presente in un cibo, di alzarela concentrazione di glucosio nel sangue.Gli alimenti contenenti carboidrati sono suddivisi secondo la velocità di dige-stione e di assorbimento in alimenti ad alto indice glicemico e alimenti a basso\nindice glicemico . I primi sono digeriti e assorbiti velocemente, fanno alzare\nmolto il tasso del glucosio nel sangue (glicemia), di conseguenza viene secretauna quantità notevole d’insulina che causa un’altrettanto brusca diminuzionedella glicemia. Esempi di alimenti ad alto e medio indice glicemico sono ilpane bianco e integrale, pasta, biscotti e tutti i prodotti da forno, riso bianco,miglio, patate bianche, pizza, patate molto cotte e purè, fave bianche sgusciate,wafers, crepes, krafen e altri dolci, pop-corn, cereali soffiati da colazione. Glialimenti a basso indice glicemico sono digeriti e assorbiti lentamente, fannoalzare lentamente il tasso del glucosio nel sangue, di conseguenza viene secretauna quantità normale d’insulina che riporta gradualmente la glicemia ai livelliprecedenti l’assunzione di carboidrati. Esempi di alimenti ad basso indice gli-cemico: verdure, legumi, cereali integrali, frutta (ad eccezione di banane ma-ture, ananas, anguria ).L’indice glicemico è espresso in termini percentuali rispetto alla velocità concui la glicemia aumenta in seguito all’assunzione di un alimento di riferimento(IG 100), quale il glucosio oppure il pane bianco.\n1.2.2 I Lipidi\nI lipidi sono composti eterogenei di piante e animali. Variano tra loro moltis-simo per struttura chimica, alcuni sono esteri, altri idrocarburi; alcuni sonoaciclici, altri ciclici o anche policiclici. Costituiscono la maggiore riserva dienergia nell’organismo umano, hanno funzioni plastiche e bioregolatrici. Sono i costituenti principali delle membrane biologiche, del surfattante alveo-lare, degli ormoni steroidei, trasportano le vitamine liposolubili nel nostro or-ganismo. Gli acidi monocarbossilici hanno più di tre atomi di C; la catenaalifatica può essere satura, insatura, ramificata. Quelli più frequenti nei tessuti\n23Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#24": "di mammifero hanno catena lineare e numero pari di atomi di carbonio.\nDiverse sono le categorie di lipidi, acidi grassi, trigliceridi, colesterolo, fosfo-lipidi e sono tutti apolari, insolubili in ambienti acquosi e solubili nei solventiorganici.Gli acidi grassi rappresentano il materiale di costruzione delle membrane cel-lulari, si distinguono in saturi, monoinsaturi, polinsaturi a seconda della pre-senza o meno di doppi legami fra atomi di carbonio adiacenti. Gli insaturi sonodi solito in forma cis, le insaturazioni spesso cominciano dalla posizione C-9,a catena lineare, senza ramificazioni. La fluidità di membrana aumenta con ilgrado di insaturazione. Gli acidi grassi sono definiti essenziali, ciò significache queste molecole devono essere necessariamente introdotte con la dietaperchè l’organismo non è in grado di sintetizzarle, o non riesce a produrne unaquantità adeguata alle proprie necessità fisiologiche. L’organismo è in gradodi formare alcuni acidi grassi, ma solo a partire da altri acidi grassi. Acidigrassi essenziali sono, ad esempio, l’acido arachidonico, α-linolenico (C18:3ω-3) e linoleico (C18:2 ω-6). La loro deficienza nella dieta porta a disordinimetabolici e strutturali. \nAcidi grassi saturiAcido miristico e acido palimitico\nL’acido miristico, a 14 atomi di carbonio, deriva il nome dall‘albero che pro-duce la noce moscata ( Myristica fragrans ) in cui rappresenta fino all‘80%\ndella frazione degli acidi grassi.E’ prodotto dal fegato a partire dai carboidrati ed è contenuto nei grassi di ori-gine animale. E particolarmente abbondante nella materia grassa dei prodottilattiero-caseari (panna, burro), olio di cocco. Può essere trasformato in acidopalmitico nel fegato. L’acido miristico svolge un ruolo importante nel funzio-namento dei recettori degli ormoni e nel trasporto delle proteine all’internodella cellula verso i mitocondri. Sembra aumentare i livelli di colesterolo econtribuire in maniera significativa allo sviluppo dell’aterosclerosi. Evidenzescientifiche dimostrano che l’acido miristico, insieme al palmitico (a 16 atomidi carbonio, contenuto in olio di palma, carne e prodotti caseari), agli acidigrassi trans, all’uso eccessivo di sodio, al sovrappeso e ad un uso eccessivo dialcool, contribuisce all’incremento del rischio cardiovascolare (CDV). L’acido palmitico è fra i più diffusi sia nel mondo animale che vegetale. Derivail suo nome dal fatto di essere il maggior costituente dell‘olio di palma, è im-portante perchè è il primo acido grasso prodotto nella sintesi degli acidi grassi.\n24Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#25": "Recenti ricerche hanno evidenziato che l’acido palmitico è coinvolto nella re-\ngolazione degli ormoni. Gli acidi palmitico e miristico sono coinvolti nellacomunicazione tra cellule e nelle funzioni del sistema immunitario. L’acidomiristico può regolare la disponibilità di acidi grassi polinsaturi, come l’acidodocosaenoico (DHA), tuttavia ciò richiede ulteriori approfondimenti.\nAcido Stearico \nE‘ un acido saturo a 18 atomi di carbonio, presente soprattutto negli animalie limitatamente nei vegetali, ad esempio nel burro di cacao e karitè. Tra gliacidi grassi saturi è il meno aterogeno.I salumi contengono più acidi grassi saturi rispetto alle altre carni, ma la per-centuale maggiore è rappresentata dall’acido stearico, un grasso che è sì saturo,ma che non fa aumentare il tasso di colesterolo. L’acido stearico ha infatti lacapacità di convertirsi in acido oleico per attività della delta-9-desaturasi.\nAcidi grassi monoinsaturiGli acidi grassi monoinsaturi sono sintetizzabili dall’organismo umano a par-\ntire dei glucidi.Acido oleicoL’acido oleico è un acido grasso monoinsaturo della famiglia degli omega 9.E il più abbondante degli acidi grassi monoinsaturi a catena lunga del nostroorganismo. Svolge un ruolo strutturale importante in quanto favorisce la flui-dità delle membrane cellulari ed è anche una fonte di energia mitocondriale.Influisce positivamente sul tasso di colesterolo sanguigno e aiuta a ridurre ilrischio di morbilità e mortalità cardiovascolare. \nGli acidi grassi polinsaturi omega-3\nAcido Alfa-linoleico\nL’acido alfa-linoleico è il precursore degli acidi grassi polinsaturi della fami-glia omega 3. E un acido grasso essenziale, vale a dire che deve assolutamenteessere assunto con la dieta, in quanto l’organismo non è in grado di produrloa partire dal precursore. Poichè rende possibile la sintesi degli acidi grassiomega 3 allungati, in particolare dell’acido eicosapentaenoico (EPA) e del-l’acido docosaesaenoico (DHA), l’acido alfa linoleico è molto importante perla salute sia a livello strutturale, in quanto garantisce alle membrane cellulari\n25Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#26": "una fluidità ottimale, sia a livello funzionale, in quanto fornisce i precursori\ndegli eicosanoidi della serie 3 (antinfiammatori, antiaggreganti piastrinici,miorilassanti della muscolatura liscia). \nGli acidi grassi polinsaturi omega-6Acido linoleico \nL’acido linoleico, acido grasso essenziale, è il precursore degli acidi grassi po-linsaturi della famiglia omega 6. Poiché consente la sintesi degli acidi grassiomega 6 allungati, in particolare dell’acido diomo-gamma-linoleico (DGLA)e dell’acido arachidonico (AA), L’acido linoleico svolge un ruolo molto im-portante per la salute sia dal punto di vista funzionale, in quanto fornisce i pre-cursori degli eicosanoidi della serie 1 per il DGLA (antinfiammatori,antiaggreganti piastrinici, miorilassanti della muscolatura liscia) e della serie2 per l’AA (pro-infiammatori, pro-aggreganti piastrinici, pro costrittori dellamuscolatura liscia); sia dal punto di vista strutturale, in quanto favorisce lafluidità delle membrane cellulari.\nAcidi Grassi TransLa configurazione cis è l’unica riconoscibile dal nostro organismo, e dun-\nque è l’unica a poter essere metabolizzata. Gli oli di semi spremuti a freddosono di tipo cis. Quando l’olio è estratto mediante riscaldamento o mediantel’uso di solventi organici, come l’esano, l’olio ottenuto ha una configura-zione Trans.Questi trattamenti distruggono parte delle vitamine liposolubili e di altre com-ponenti utili presenti nel seme e creano dei grassi che non esistono in naturae risultano dannosi per il sistema cardiocircolatorio.Questi acidi grassi trans sembra che provochino la diminuzione del coleste-rolo HDL in modo dose dipendente, l’aumento dell’LDL-colesterolo inmodo dose dipendente e delle lipoproteine aterogeniche, aumenta il tasso dicolesterolo totale nel siero. I acidi grassi trans, quindi, potrebbero parteciparenotevolmente ai processi arterosclerotici e aumentare così i rischi per infartidel miocardio ed altri problemi correlati. Inibiscono la funzione degli enzimiassociati alle membrane, come la delta-6 desaturasi. Causano delle modifi-che della citocromo ossidasi P450, alterazioni nelle proprietà fisiologichedelle membrane biologiche, comprese la capacità di trasporto e flessibilitàdella membrana cellulare.\n26Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#27": "Acido trans-vaccenico\nL’acido trans-vaccenico è un acido grasso prodotto dalla trasformazione bat-terica degli acidi grassi insaturi nel rumine dei ruminanti. Gli acidi grassi transdetti « naturali » si possono perciò ritrovare nei prodotti lattiero-caseari (burro,panna, formaggi, latte) e nelle carni (bovine, ovine, e così via).E‘ considerato un MUFA (acidi grassi monoinsaturi) antidislipidemico.\nAcido elaidico \nL’acido elaidico è un acido grasso trans presente soprattutto negli oli vegetaliidrogenati e negli oli di frittura, nella margarina (soprattutto nelle margarinedure), negli alimenti fritti di produzione industriale e nei prodotti da fornocontenenti shortening, margarina, oli o grassi parzialmente idrogenati comebiscotti, pane allo zucchero, frittelle, dolci, pasticcini, muffins, croissant, snacke alimenti fritti (in particolare patatine fritte e cibi pastellati), chips e crackers,confetteria. Ha degli effetti dannosi sulla salute quali ipercolesterolemia, ate-rogenicità, aumento del rischio cardiovascolare, alterazione della biosintesidegli AGPI a catena molto lunga e del metabolismo delle prostaglandine, pro-prietà cancerogene. \nTrigliceridiTriesteri del glicerolo, hanno importanza biologica come funzione di riserva\nenergetica, di isolamento e protezione.Il 95% del grasso della nostra alimentazione è sotto forma di trigliceridi che de-vono essere idrolizzati in monomeri. Quando si dosano i trigliceridi, non esistealcun modo di sapere se gli acidi grassi che li compongono sono saturi o insaturi. I carboni degli acidi grassi risultano molto ridotti, quindi possiedono moltaenergia chimica. I triacilgliceroli sono quindi delle molecole di stoccaggiodell’energia molto efficaci. La loro ossidazione produce circa 9 kcal/g., mentreper gli zuccheri l’ossidazione produce circa 4 kcal/g. I trigliceridi rappresentano la principale forma di trasporto nel sangue comelipoproteine. Si sottolinei il ruolo potente dei trigliceridi come indicatori di un rischio di in-farto del miocardio. Un’alimentazione ricca in grassi saturi può fare aumentarei trigliceridi sierici; importante è anche il ruolo di un eccesso di glucosio ema-tico, dovuto ad un eccesso di idrati di carbone che gioca un ruolo importantenel livello elevato di trigliceridi.\n27Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#28": "Colesterolo\nScoperto nella bile nel 18° secolo, di cui è il maggiore costituente, è il più ab-\nbondante degli steroli contenuto nei tessuti degli animali, o meglio lo si rin-viene in tutte le cellule animali (membrane cellulari), ma soprattutto nelcervello e nel midollo spinale. La quantità totale di colesterolo mediamentepresente nel corpo umano è di circa 200 g.E’ una molecola indispensabile alla vita delle cellule ed all’equilibrio degliorganismi. La stessa svolge molte funzioni biologiche di importanza capitale:• precursore degli acidi biliari, escreti tramite la vescicola biliare nel tubo\ndigerente, la cui azione detergente è necessaria all’assorbimento da partedell’intestino dei grassi alimentari;\n• costituisce lo scheletro degli ormoni steroidei molecola da dove derivano gli\normoni chiamati steroidei (ormoni maschili: androgeni; ormoni femminili:estrogeni, progesterone; e ormoni corticoidi: cortisone e cortisolo);\n• con l’aiuto dell’irradiazione solare trasforma la vitamina D nella pelle;• partecipa alla struttura delle membrane che circondano le cellule, composte\nper circa la metà da sostanze grasse;\n• è molto poco solubile;• abbonda nel tessuto nervoso. Inoltre, conferisce rigidità alle membrane cellulari e del mitocondrio e vienetrasportato ai tessuti extraepatici da lipoproteine quali LDL e VLDL.Il 30% del colesterolo si trova in forma libera (gruppo -OH su C3 non esteri-ficato), il 70% esterificato (gruppo -OH legato ad acido grasso).\n1.2.3 Proteine\nLe proteine rappresentano gli elementi strutturali e funzionali più importantinei sistemi viventi. Sono soggette ad un continuo processo di demolizione esintesi, il turnover proteico, attraverso il quale l’organismo è in grado di rin-novare continuamente le proteine logorate sostituendole con nuovo materialeproteico (globuli rossi, capelli, unghie, muscoli).Il turnover proteico è esteso a tutte le proteine, diminuisce dalla nascita all’età adultae richiede circa il 20% di energia del metabolismo basale. Quantitativamente corri-sponde a 3-4 volte l’introduzione di proteine ed è pari a 3-4 g di proteine/kg/die.Qualsiasi processo vitale dipende da questa classe di molecole. La caratteri-stica strutturale comune a tutte le proteine è di essere dei polimeri lineari di\namminoacidi della serie L uniti tra loro da un legame peptidico. Ciascuna pro-\nteina ha però una propria struttura tridimensionale che la rende capace di svol-\n28Fondamenti della Scienza dell’Alimentazione Capitolo I",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#29": "gere specifiche funzioni biologiche.\nSi stima che ne esistano più di 50.000 tipi di proteine umane e che il numerodi proteine distinte all’interno di una cellula vari tra le 3.000 e le 5.000. Nelsolo siero possono essere identificate più di 1.400 proteine.Le proteine svolgono numerose importanti funzioni riassunte nella Tabella 2.\nCatalizzatori di reazioni chimiche tutti gli enzimi sono proteine\nTrasporto emoglobina, mioglobina, albumina, \ntransferrina\nDeposito di materiale ferritina\nProteine dei sistemi contrattili actina-miosina, tubulina-dineina\nComponenti strutturali collagene, tessuto connettivo, \ncitoscheletro, pelle\nProtezione immunitaria gli anticorpi sono delle proteine \naltamente specifiche\nFunzione ormonale molti ormoni sono di natura proteica\n(insulina, glucagone)\nTossine alcuni veleni hanno una struttura proteica.\nControllo e regolazione espressione genica Istoni\nGenerazione e trasmissione impulso nervoso ad esempio la rodopsina, presente nei \nbastoncelli della retina, è una proteinache è in grado di funzionare come recettore per la luceTabella 2 - Funzioni delle proteine\n29Capitolo I Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#3": "3Fondamenti della Scienza dell’Alimentazione\nPrefazione\nIl Consiglio dell’Ordine Nazionale dei Biologi, al momento del suo insedia-\nmento, ha istituito dieci commissioni permanenti di studio. Nel corso dei mesi,queste si sono occupate di organizzare corsi di formazione, conferenze e con-vegni. In aggiunta, hanno avviato un’intensa attività editoriale destinata tantoagli addetti ai lavori quanto a un pubblico più generalista. Questo volume è stato elaborato dalla commissione “Nutrizione” dell’ONB,con l’intento di fornire strumenti utili al corretto svolgimento dell’attività pro-fessionale di Biologo Nutrizionista.Il Consiglio dell’Ordine e la commissione si sono posti come obiettivo quellodi contribuire ad accrescere le nozioni scientifiche degli addetti ai lavori, an-dando al di là delle competenze di base.Elaborare un profilo nutrizionale, infatti, necessita obbligatoriamente di unabuona conoscenza della biochimica degli alimenti e del loro impatto con or-gani, tessuti e cellule. Tali competenze, pertanto, sono imprescindibili al finedi migliorare lo stato di salute e il benessere degli individui.Sono molte le malattie che possono essere determinate da una dieta scorrettae il biologo deve, con professionalità, favorire scelte alimentari che possanocontribuire a prevenire importanti patologie.La commissione “Nutrizione”, con questo contributo scientifico, vuole dareun messaggio importante ai colleghi Nutrizionisti affinché esercitino al megliola professione.\nDr. Ermanno Calcatelli\nPresidente dell’Ordine Nazionale dei Biologi",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#30": "Bibliografia\n• Fidanza F., Liguori G., Nutrizione Umana. Idelson Editore, 1988 \n• Fidanza F. Ruoli e richieste di energia e nutrienti energetici . Gnocchi \n• Mariani Costantini A., Cannella C., Tomassi G. – Fondamenti di Nutrizione Umana. Il\nPensiero Scientifico Editore, 1999 \n• Cozzani I., Dainese E., Biochimica degli alimenti e della nutrizione. Piccin Editore, 2006• Murray M., Pizzorno J., Pizzorno L., Enciclopedia della Nutrizione. Dalla A alla Z tutti i\ncibi che guariscono . Tecniche Nuove\n• Riccardi, Pacioni, Giacco, Tivellese. Manuale di Nutrizione Applicata , Sorbona, III Edizione\n• Marandola P., Marotta F.. Il Manifesto della lunga vita . Sperling e Kupfer \n• Champe et al., Le basi della biochimica , Ed. Zanichelli \n•The role of virgin olive oil components in the modulation of endothelial function . Perona J.\net Coll. J Nutr Biochem. 2006 Jul;17(7):429-45). \n•L’acido oleico riduce anche il rischio di cancro (Molecular mechanisms of the effects of\nolive oil and other dietary lipids on cancer. Escrich E, et Coll. Mol Nutr Food Res. 2007Oct;51 (10):1279-92).\n•WHO (2003) Diet, Nutrition and the prevention of Chronic Diseases. Report of a Joint\nWHO/FAO Expert Consultation, WHO Techical Report Series 916. Geneva: WHO. \n• Disponibile sul sito: http://www.who.int/dietphysicalactivity/publications/trs916/en/• Stabler & Allen. Vitamin B12 deficiency as a worldwide problem . Ann. Rev. Nutr. 24,\n2004:299-326.\n• Rioux V . and Legrand P. (2007) Saturated fatty acids: simple molecular structures with\ncomplex cellular functions . Current Opinion in Clinical Nutrition and Metabolic Care\n10:752-58\nSitografia\nhttp://www.bda-ieo.it/index.aspx\nhttp://www.ministerosalute.ithttp://www.epicentro.iss.it/problemi/vitamine/epid.asphttp://www.iss.it/osnami/http://sinu.it/html/pag/larn_minerali.asphttp://www.efsa.europa.eu/it/scdocs/doc/nda_op_ej822_vit_k2_summary_it.pdfhttp://www.eufic.org/article/it/salute-e-stile-di\n30",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#31": "31Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#32": "CAPITOLO II\nACQUA E FIBRA ALIMENTARE\nRiassunto\nL’acqua è una componente essenziale del corpo umano, circa il 60-70% del\npeso corporeo. Svolge innumerevoli funzioni nei processi fisiologici e nellereazioni biochimiche all’interno dell’organismo. Per consentire che tutto ciòpossa correttamente avvenire e quindi conservare uno stato di salute buonobisogna mantenere un adeguato equilibrio nel bilancio idrico dell’organismoche è determinato dal bilanciamento tra entrate ed uscite. Il fabbisogno diacqua cambia per età e sesso ed aumenta in alcune condizioni fisiologichequali gravidanza ed allattamento così come in alcune condizioni patologichequali diarrea, vomito, stati febbrili.La fibra alimentare è ritenuta una componente importante della dieta umanaed esercita effetti di tipo funzionale e metabolico. E’ costituita da parti dellaparete cellulare vegetale che il nostro organismo non è in grado di digerire.La fibra alimentare è una miscela estremamente complessa di polisaccarididiversi che appartiene alla famiglia dei carboidrati, resiste all’idrolisi da partedegli enzimi gastrici e viene fermentata dalla microflora batterica del colon.Numerosi studi osservazionali suggeriscono che un consumo insufficientepossa contribuire a numerosi disturbi cronici come stipsi, diverticolite, emor-roidi, vene varicose, diabete, obesità, malattie cardiovascolari, tumori delcolon-retto e varie altre tipologie di tumore.\n32Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#33": "2.1 ACQUA\nL’organismo umano è formato principalmente di acqua che ne è costituente\nessenziale per il mantenimento della vita. Il contenuto di acqua nel corpo umano varia con l’età e il sesso. Neonato: 75 % circa del peso corporeo e si abbassa progressivamente dal 1°al 9° anno di etàUomo: 60-70% del p.c. Donna: 55-65% del p.c. Le differenze tra i sessi sono presenti già a partire dall’adolescenza. Nelladonna, infatti, è presente una maggiore percentuale di tessuto adiposo, tessutopovero in acqua, e ciò comporta una minore quantità di acqua. Con l’invec-chiamento il contenuto di acqua, sia come valore assoluto che come frazionepercentuale, si riduce. L’uomo medio è alto 175 cm e pesa circa 70 Kg. Di questo peso il 16% è co-stituito da proteine, il 13% da lipidi, il 5% da Sali minerali, l’1% da glucidi,le vitamine sono in tracce e circa il 60-70% è costituito da acqua. Quest’ultimaentra nella composizione dei muscoli e degli organi interni per il 75% circa,nel tessuto adiposo per il 10%, nello scheletro per oltre il 30%. E’ localizzataall’interno delle cellule per il 66% circa, nella linfa per il 2% circa, nel plasmaper il 7% circa e come acqua extracellulare ovvero negli spazi tra le celluleper il 25% circa.Una percentuale così alta di acqua nel corpo umano trova giustificazione nellenumerose funzioni che questa svolge nei processi fisiologici e nelle reazionibiochimiche all’interno dell’organismo:• Costituente principale del citoplasma delle cellule • Costituente principale di sangue, linfa, liquido cefalo rachidiano• Solvente di nutrienti, succhi digestivi, gas, elettroliti, colloidi• Veicolo per l’assorbimento dei principi nutritivi • Mezzo in cui avvengono le reazioni metaboliche e digestive• Veicolo per il trasporto di nutrienti, ormoni, elettroliti e secrezioni • Allontana le sostanze di rifiuto • Garantisce la giusta consistenza del contenuto intestinale• Indispensabile nel sistema di regolazione della temperatura corporea e in\nquello di disintossicazione \n• Svolge il ruolo di ammortizzazione nelle articolazioni e nei tessuti • Mantiene elastiche mucose e pelle attraverso il giusto grado di idratazione.\n33Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#34": "Appare ovvio, guardando a tutte le innumerevoli funzioni che l’acqua svolge,\nche per conservare uno stato di salute buono bisogna mantenere un adeguatoequilibrio nel bilancio idrico dell’organismo.Il bilancio idrico è dato dall’equilibrio tra entrate e uscite (uomo adulto, atti-\nvità fisica moderata, temperatura 18-20°C), così come rappresentato in tabella.\nNel nostro organismo l’equilibrio idrico è mantenuto attraverso due meccanismi:\n1) Il meccanismo della sete che regola la quantità di acqua da ingerire. E’ un\nmeccanismo con un tempo di risposta ritardato e a volte non interviene neitempi giusti per evitare gli effetti negativi dovuti alla perdita di acqua. Inparticolare nell’anziano è un meccanismo che non sempre è funzionante equindi non permette il rimpiazzo dell’acqua persa con conseguente disi-dratazione.\n2) Il riassorbimento dell’acqua a livello renale attraverso la regolazione della\nquantità di acqua eliminata con le urine.\nGià una piccola disidratazione pari all’1% del peso corporeo mette in difficoltàattività e performance fisiche dell’organismo. Con una percentuale di disidra-tazione dal 2% al 10% si assiste a sintomi sempre più importanti, da secchezzadella bocca e sensazione di sete ad alterazione della termoregolazione, a mu-cose secche ed asciutte, mal di testa, crampi muscolari, debolezza, maggioreirritabilità, malessere generale, allucinazioni, vomito, tachicardia fino perditadi conoscenza e pericolo per la stessa vita. Una disidratazione persistente aumenta inoltre il rischio di contrarre tumoridell’apparato urinario e del colon nonché di avere formazione di calcoli re-nali.\n2.1.1 LARN e ACQUA\nFabbisogno di acqua per adulti e anziani: circa 1 ml/Kcal/giorno.Fabbisogno di acqua per bambini : circa 1,5 ml/kcal/giorno. La quota di\nacqua per calorie è maggiore perché i bambini hanno maggior rischio di disi-dratarsi.Entrate Uscite\nBevande 1,5-2 lt Rene (urine) 0,5-1,5 lt\nAlimenti 0,5-1 lt Cute (sudore) 0,1-1 lt\nOssidazioni 0,3- 0,5 lt Vie aeree (respirazione) 0,4 - 0, 6 lt\n(acqua metabolica x formazione endogena)\nIntestino (feci) 0,05 – 0,2 lt\n34Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#35": "Per valutare il fabbisogno giornaliero di acqua si può anche applicare la for-\nmula:Peso corporeo x 0,03(per es. un uomo di 70 kg necessita di 2,1 l d’acqua al giorno: 70x0,03= 2,1)Durante la gravidanza e l’allattamento il fabbisogno idrico aumenta così comeanche in alcune condizioni patologiche come diarrea, vomito, stati febbrili, ecc.L’acqua metabolica prodotta dalla respirazione cellulare è pari a:\n• 0,56 g. per 1 g. di glucidi,• 1,07 g. per 1 g. di lipidi,• 0,39 g. per 1g. di proteine\nIn pratica, considerando un soggetto adulto che pesa di 70 kg, con un apportocalorico giornaliero di 2400 kcal costituito da: 70g di proteine (12%), 350 g.di carboidrati (58%) e 80 g. di lipidi (30%) si ottiene una produzione di 310ml di acqua endogena. Poiché il nostro metabolismo non produce acqua inquantità sufficiente a coprire il fabbisogno giornaliero occorre introdurla conalimenti e bevande.Negli alimenti l’acqua è presente in quantità diverse:\nData l’importanza dell’acqua nell’alimentazione umana, esperti della nutri-\nzione hanno elaborato la piramide dell’idratazione racchiudendo nell’ imma-gine della piramide, ormai nota ai più, le indicazioni per individui adulti, sanie moderatamente attivi circa il consumo di acqua e non solo allo scopo di sol-lecitare l’attenzione dell’opinione pubblica e di tutti coloro che operano perpromuovere salute riducendo al minimo gli effetti negativi di una scorrettaidratazione (Figura 1). \nfrutta, ortaggi, verdura e latte oltre l’85%   H2O\ncarne, pesce, uova, formaggi freschi 50-80%   H2O\npasta e riso cotti 60- 65%  H2O\npane e pizza 20-40%  H2O\nbiscotti, fette biscottate,  farina e frutta secca meno del 10%  H2O\n35Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#36": "Alla base della piramide, ovviamente, l’acqua. Almeno 5 bicchieri (da 200 ml\nciascuno) per un totale di circa 1 litro necessario per equilibrare mediamenteil bilancio idrico fisiologico.Al 2° livello: tè, caffe decaffeinato, oro, infusi e tisane, tutti senza zuccheroche possono integrare la restante quota idrica della giornata (0-3 bicchieri)Al 3° livello: latte, spremute di frutta fresca e succhi di frutta al 100%, e cen-trifugati (0-2 bicchieri)Al 4° livello: tutti gli altri tipi di succhi e la birra analcolica (0-2 bicchieri). Al 5° livello: il caffè sotto forma di espresso o americano da assumere di pre-ferenza senza zucchero (0-5 tazzine) Al 6° livello: le bevande idrosaline formulate per la reidratazione da consu-mare prima o dopo intensa attività fisica contenenti zuccheri, elettroliti, aromi(0-500 ml)Al 7° livello e quindi apice della piramide bibite analcoliche, energy-drink,sciroppi e soft drink ad alto contenuto di zuccheri (occasionalmente)La stratificazione nei diversi livelli della piramide serve, così come per la pi-ramide alimentare rispetto ai vari alimenti, a dare indicazioni sulle quantitàconsigliate per i vari tipi di bevande poiché alcune sono reidratanti ma ancheFigura 1: piramide dell’idratazione suggerita per la popolazione italiana (riadatta da Giam-\npiero M et al.; ADI Magazine 2011; 2: 105-115)\n36Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#37": "apportatrici di sostanze che richiedono attenzione nella quantità di consumo.\nRicapitolando, ogni giorno dovremmo reintegrare un paio di litri di liquidi ela precedenza va all’acqua e alle bevande non caloriche e non alcoliche, senzadimenticare che verdure, frutta e minestre contribuiscono, più di altri alimenti,al rifornimento quotidiano di liquidi.L’equilibrio idrico può essere mantenuto bevendo sia acqua del rubinetto cheacque minerali entrambe sicure e controllate tenendo conto che nelle secondesono contenuti sali minerali. Le acque minerali vengono classificate in basealla quantità e qualità di questi minerali quali carbonati, solfuri, cloruri e fosfatidi calcio, sodio, potassio, magnesio, ferro, bario, alluminio, silicio e manga-nese. Dal un punto di vista nutrizionale le acque minerali ricche in sali di calciopossono essere utili nella prevenzione dell’osteoporosi e nelle varie fasi dellavita. \nACQUE MINERALI. Totale di sali minerali (residuo fisso) nelle acque im-\nbottigliate\nDurante una moderata attività fisica attraverso la sudorazione, nella maggior\nparte delle persone, vengono persi 1-2 litri di liquidi per ora, con perdita anchedi sali minerali soprattutto sodio, potassio e cloro. Nel caso di attività non ago-nistica basterà una dieta equilibrata ricca in acqua, frutta, verdure per reintegrareacqua e sali minerali, mentre in caso di attività agonistica bisognerà ricorrere, aseconda dei casi e della prestazione, ad integratori anche idrosalini.\nBibliografia \n• INRAN. Linee Guida per una Sana Alimentazione Italiana. Revisione 2003 \n• Consigli Paolo. L’acqua pura e semplice. Tecniche Nuove• Giampietro M. e al. Piramide dell’idratazione suggerita per la popolazione italiana adulta\nsana. ADI Magazine 2011; 2: 105-115Tipologia Residuo fisso\nAcque minimamente mineralizzate meno di 50 mg/litro\nAcque oligominerali 50-500 mg/litro\nAcque minerali propriamente dette (acque medio minerali) 500-1500 mg/litro\nAcque fortemente mineralizzate più di 1500 mg/litro\n37Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#38": "2.2 Fibra alimentare\nIl concetto di fibra alimentare è cambiata notevolmente negli ultimi anni conil progredire delle conoscenze scientifiche. E’ ormai riconosciuto che la fibraalimentare comprende una gamma molto ampia di sostanze con un significatofisiologico maggiore di quanto si pensasse. E’ costituita soprattutto da partidella parete cellulare vegetale che il nostro organismo non è in grado di dige-rire. Pur non potendosi considerare un nutriente, la fibra alimentare esercitaeffetti di tipo funzionale e metabolico che la fanno ritenere un’importante com-ponente della dieta umana. Non esiste una definizione generalmente accettatadi fibra alimentare in Europa o nel mondo. Tuttavia, vi è consenso su defini-zioni basate sulle sue caratteristiche fisiologiche anche se con accenti diversida parte dei vari organismi internazionali.L’American Association of Cereal Chemist (AACC 2001) definisce le fibre\nalimentari come “parti commestibili di piante o analoghi di carboidrati resi-stenti alla digestione e all’assorbimento, con fermentazione completa o par-ziale nell’intestino crasso. Le fibre alimentari comprendono polisaccaridi,oligosaccaridi, lignina e sostanze vegetali associate. Esse promuovono effettifisiologici benefici tra cui l’effetto lassativo e la regolazione della colestero-lemia e della glicemia”L’Agence Francaise de Securitè Sanitaire des Aliments (AFSSA 2002) stabi-lisce che “ la fibra alimentare e costituita da: • polimeri di carboidrati con grado di polimerizzazione ≥ 3 di origine vegetale\ncon lignina o altri componenti differenti dai carboidrati quali, ad esempio,polifenoli, cere, saponine, fitati, cutina, fitosteroli \n• polimeri di carboidrati con grado di polimerizzazione ≥ 3, ottenuti con mezzi\nfisici, enzimatici o chimici o sintetici\nLa fibra alimentare non è né digerita né assorbita nell’intestino tenue e presentaalmeno una delle seguenti proprietà:\n- Stimola la fermentazione nel colon- Riduce i livelli di colesterolo pre-prandiali- Riduce la glicemia post-prandiale e / o livelli di insulina “\nIl Codex Alimentarius Commission (CAC 2006) definisce:\n“La fibra alimentare come polimeri di carboidrati con un grado di polimeriz-zazione non inferiore a 3, che non sono né digeriti né assorbiti nell’intestinotenue. Un grado di polimerizzazione non inferiore a 3 esclude mono e disac-caridi. La fibra alimentare è costituita da uno o più delle seguenti componenti:\n38Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#39": "• polimeri di carboidrati commestibili naturalmente presenti negli alimenti\nconsumati\n• polimeri di carboidrati, che sono stati ottenuti da materie prime alimentari\nmediante procedimenti fisici, enzimatici o mezzi chimici\n•polimeri di carboidrati sintetici\nEssa generalmente ha proprietà quali:\n• ridurre il tempo di transito intestinale ed aumentare la massa fecale• essere fermentata dalla microflora del colon• ridurre il colesterolo totale e / o livelli di colesterolo LDL• Ridurre la glicemia post-prandiale e / o i livelli di insulina”\nL’Health Council of The Netherland (2006) si esprime con la definizione:\n“La fibra alimentare sta ad indicare una serie di sostanze che non sono digerite\no assorbite nell’intestino tenue dell’uomo, e che hanno la struttura chimica deicarboidrati o di composti analoghi dei carboidrati, lignina e sostanze affini “Sebbene il concetto di fibra alimentare sia stato dibattuto per decenni e il di-battito continui ancora, i costituenti ormai considerati parte di essa non sonomolto diversi oggi da quelli discussi vari decenni fa e da tutte le definizioni sipuò evincere che la fibra alimentare è una miscela estremamente complessadi polisaccaridi diversi che appartiene alla famiglia dei carboidrati, resiste al-l’idrolisi da parte degli enzimi gastrici e che viene fermentata dalla microflorabatterica del colon.\nLa fibra alimentare può essere equiparata alla parete della cellula vegetale e\nrappresenta lo scheletro della pianta. \nLa costituzione scheletrica è determinata da:Componenti strutturali della parete cellulare, comprendenti Polisaccaridi\nstrutturali quali la cellulosa, l’emicellulosa, e le pectine. Altri costituenti sono\nrappresentati da Lignina, proteine, e materiale inorganico.Componenti non strutturali della parete cellulare, comprendenti Polisac-\ncaridi di varia origine, quali le mucillagini, le gomme, estratti di alghe, e po-\nlisaccaridi modificati.Le fibre alimentari, dal punto di vista della struttura chimica e dell’attività fi-\nsiologica sono state divise storicamente in idrosolubili ed insolubili anche se\n39Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#4": "4",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#40": "questi termini da alcuni anni sono stati definiti fuorvianti dall’Organizzazione\nMondiale della Sanità e dalla Food and Agricultural Organization perché espri-mono una distinzione fisiologica troppo semplicistica.\nÈ vero però che la fibra alimentare ha un’azione diversa a seconda della idro-\nsolubilità.\nFibre Insolubili: Cellulosa, Emicellulosa, Lignine\nLe fibre alimentari insolubili, presenti principalmente nella crusca di cereali,nelle verdure e negli ortaggi, assorbono acqua comportandosi come “agentidi rigonfiamento” e sono utilizzate solo in piccola parte dalla microflora. De-terminano aumento della massa fecale, accelerato transito intestinale, riduzionedel tempo di contatto con la mucosa intestinale di sostanze nocive. Sono quindiparticolarmente indicate nella regolazione delle funzioni intestinali.\nFibre Idrosolubili: Gomme, Mucillagini, Pectine, Galattomannani\nLe fibre alimentari solubili, presenti principalmente nei legumi e nella frutta, resi-stono alla digestione nel tratto superiore dell’intestino e vengono “degradate” dallamicroflora al ceco e al colon destro. Determinano rallentamento dello svuotamentogastrico e senso di sazietà, rallentato transito intestinale, aumento dell’eliminazionedegli acidi biliari, riduzione e regolazione dell’assorbimento di zuccheri e grassi.La fermentazione delle fibre solubili porta inoltre alla produzione di acidi grassi acatena corta (“scafs”, short chain fatty acids). Queste fibre sono indicate nell’ali-mentazione di soggetti con disturbi metabolici quali diabete, malattie cardiovasco-lari che traggono vantaggio da un assorbimento di nutrienti lento e/o ridotto e,poiché inducono senso di sazietà, anche nelle diete per la riduzione del peso.\nFanno parte delle fibre solubili i polisaccaridi non cellulosici che si dividono in:1) Olisaccaridi non digeribili: Galattomannani, PHGG, Fruttooligosaccaridi,\nFos, Xilani, Inulina\n2) Etero-Omo Polisaccaridi non digeribili: Gomma-Guar, Pectina, Agar-car-\nragenina, Alginati, Psyllium\nLa maggior parte degli alimenti di origine vegetale contiene sia fibre solubili\nche insolubili in proporzioni differenti. Elencati qui di seguito i più importanti componenti della fibra alimentare conuna breve descrizione. \n40Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#41": "2.2.1 Fibra insolubile\nCellulosa \nLa cellulosa è uno dei più importanti polisaccaridi, costituita esclusivamenteda unità di glucosio, fino a 10.000 unità per molecola unite tra loro da un le-game β(1➜4) glicosidico. La catena polimerica non è ramificata e le catenesono disposte parallelamente le une alle altre e legate tra loro per mezzo di le-gami ad idrogeno molto forti. Si formano così fibrille e catene molto lunghedifficili da dissolvere. Queste fibrille localmente sono molto ordinate e for-mano una struttura cristallina idrofoba. Principale fonte alimentare . La cellulosa è un componente principale della\nparete cellulare della maggior parte delle piante ed è quindi presente in frutta,verdura, legumi e cereali. Gran parte della fibra di crusca dei cereali è cellu-losa.Funzione. Trattiene l’acqua. Aumenta la massa fecale. Diminuisce la pressioneintracolicaRiduce il tempo di transito intestinale. Lega i sali minerali. E’ indicata nellastipsi e nella diverticolosi.\nEmicellulose\nLe emicellulose sono polisaccaridi contenenti zuccheri diversi dal glucosio esono associate con la cellulosa nelle pareti cellulari delle piante. Esse com-prendono molecole sia lineari che ramificate, più piccole della cellulosa, tipi-camente contenenti 50-200 unità di pentosi (xilosio e arabinosio) e unità diesosi (glucosio, galattosio, mannosio, ramnosio, glucuronico e acido galattu-ronico). Il termine emicellulose indica quindi un gruppo eterogeneo di sostanzeche sono presenti nei cibi vegetali in forme solubili in acqua e forme insolu-bili.Principale fonte alimentare . Crusca, cereali, verdura, legumi, frutta, noci.\nFunzione. Trattengono l’acqua ed aumentano la massa fecale. Regolarizzano\nil transito intestinale. Fanno diminuire la pressione intracolica. Legano i salibiliari. Ne è indicato il consumo nella sindrome dell’intestino irritabile.\nLignina \nLa parola lignina proviene dal termine latino lignum, che significa legno e per\nquesto motivo le piante che contengono una grande quantità di lignina sonodenominate legnose. È composta da una struttura polimerica di unità fenilpro-paniche e materiale non saccaridico. Essa svolge in tutti i vegetali la funzione\n41Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#42": "di legare e cementare tra loro le fibre per conferire ed esaltare la compattezza\ne la resistenza della pianta. Pur non essendo un polisaccaride la lignina è legatachimicamente alle emicellulose nella parete cellulare vegetale e quindi è inti-mamente associata ai polisaccaridi della parete cellulare delle piante. Dopo ipolisaccaridi la lignina è il polimero organico più abbondante nel mondo ve-getale. E’ presente in alimenti con un componente “legnoso” e negli stratiesterni dei cereali.Principale fonte alimentare. Frumento, verdura, frutta (fragole, pere, pesche,\nprugne)Funzione. Aumenta la massa fecale. Riduce il tempo di transito intestinale.\nLega i Sali biliari e gli ioni. E’ consigliata nella stipsi.\n2.2.2 Fibra solubilePectine\nLe pectine sono polisaccaridi solubili in acqua calda per poi trasformarsi ingel durante il raffreddamento. Sono composte principalmente da catene diacido galatturonico intervallate da unità di ramnosio e sono ramificate con ca-tene di pentosi ed esosi. Le pectine sono presenti nelle pareti della cellula enei tessuti intracellulari di frutta e verdura e sono utilizzati come agenti geli-ficanti e addensanti in vari alimenti.Principale fonte alimentare . Frutta, verdura, patate dolci\nFunzione. Aumenta il tempo di svuotamento gastrico. Modifica la produzione\ndi gas. Modifica la produzione di acidi grassi volatili. Riduce l’assorbimentodei nutrienti. Lega i sali minerali e gli ioni. Indicate in diverticolosi, obesità,diabete mellito, dislipidemie, litiasi biliare.\nβ-glucani\nI β-glucani sono polimeri costituiti da molecole di glucosio unite mediante le-gami glicosidici β(1-3) e β(1-4). Sono una componente importante della paretecellulare dei cereali e sono i principali componenti della frazione solubile dellafibra alimentare. Hanno un struttura lineare e sono di piccole dimensioni. Que-ste proprietà influenzano la loro solubilità, permettendo loro di formare solu-zioni viscose. Principale fonte alimentare . \nAvena, orzo ed in piccole quantità nel granoFunzione.Aumentano il tempo di svuotamento gastrico. Incrementano la peristalsi inte-\n42Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#43": "stinale. Giocano un ruolo nel contenimento del livello di colesterolo e di glu-\ncosio ematico.\nGomme e mucillagini\nGli idrocolloidi comprendono una vasta gamma di polisaccaridi viscosi. Essisono derivati   da essudati di piante (gomma arabica e adragante), semi (guar egomme locust) ed estratti di alghe (agar, carragenine e alginati). Le mucillaginisono presenti nelle cellule degli strati esterni di semi ad esempio nella famigliadelle Plantaginaceae tipo la piantagine ispaghula (psyllium). Questi idrocol-loidi sono utilizzati in piccole quantità come gelificanti, addensanti, stabiliz-zanti e emulsionanti in taluni prodotti alimentari. Principale fonte alimentare . \nAvena e legumiFunzione.Formano sostanze viscose e legano ioni ed altre sostanze. Aumentano il tempodi transito intestinale e danno sensazione di sazietà. Riducono l’assorbimentodei nutrienti e del colesterolo. Trovano indicazione nell’obesità, diabete mel-lito, dislipidemie. \nOligosaccaridi non digeribili .\nGli Oligosaccaridi non digeribili hanno un grado di polimerizzazione che va\nda 3 a 10. Possono essere sintetizzati chimicamente o derivare per idrolisi en-zimatica da monosaccaridi, disaccaridi o polisaccaridi. Vengono inclusi nelladefinizione di fibra alimentare perché, come risultato della loro non digeribi-lità, mostrano effetti fisiologici simili a quelli dei polisaccaridi. Essi sono in genere altamente fermentabili ed alcuni hanno proprietà prebio-tiche.I probiotici più noti sono i fruttani che includono i frutto-oligosaccaridi o oli-gofruttosi (FOS) ottenuti dall’idrolisi enzimatica dell’inulina e i loro analoghisintetici sono ottenuti per sintesi enzimatica dal saccarosio.L’inulina è costituita da polimeri del β-D- fruttosio, è solubile in acqua ed è\naccumulata nei vacuoli. Ricerche approfondite hanno dimostrato che l’assun-zione di quantità moderate di inulina determina un aumento significativo deibifidobatteri nel tratto intestinale con effetti benefici e ad una riduzione deibatteri indesiderabili. \nPrincipale fonte alimentare .\nCipolle, cicoria, topinanbur, porri, carciofi, asparagi, segale e frumento. \n43Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#44": "Funzione. \nStimolano selettivamente la crescita e/o l’attività di un limitato numero di bat-teri nel colon; modificano positivamente il rapporto tra microorganismi sim-bionti e patogeni. \n2.3 Ruolo fisiologico della fibra alimentare e benefici per la salute\nNumerosi studi osservazionali sulle fibre alimentari ne hanno evidenziato ef-\nfetti fisiologici e metabolici importanti per la salute dell’uomo suggerendo cheun consumo insufficiente possa contribuire a numerosi disturbi cronici comestipsi, diverticolite, emorroidi, vene varicose, diabete, obesità, malattie car-diovascolari, tumori del colon-retto e varie altre tipologie di tumore. Tutti que-sti disturbi hanno una eziologia multifattoriale ed è perciò complicato valutarequanto incide il consumo di fibre e se gli effetti benefici sono da attribuire aspecifici componenti della fibra o ad un modello alimentare totale. \nFermentazione colica\nLa fibra alimentare rappresenta per la flora batterica intestinale un substratofondamentale per la sua crescita e determina numerosi effetti:• Produzione di acidi grassi a catena corta (SCAFs): acetato, propionato, bu-\ntirrato che svolgono diverse funzioni, lassativa, riduzione della flora putre-fattiva e neutralizzazione dei prodotti del metabolismo putrefattivo, aumentodella digestione e della metabolizzazione del lattosio\n• Acidificazione del contenuto colico: un pH basso migliora il trofismo delle\ncellule del colon\n• Effetto prebiotico: ovvero è substrato per la crescita di specie batteriche be-\nnefiche\n• Produzione di energia, circa 1,6 kcal/g ( 6kJ/g) (LARN da British Nutrition\nFoundation 1990)\nInizio moduloMolti componenti della fibra alimentare sono parzialmente o completamentefermentati dalla microflora del colon. La flora batterica intestinale è un com-plesso ecosistema formato da più di 400 specie batteriche con considerevolivariazioni negli individui a seconda di fattori quali l’età e la dieta.Nello stomaco: 1000 batteri/grammoNel tenue: 100 milioni batteri/grammo\n44Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#45": "Nel colon: 1000 miliardi batteri /grammo.\nLa maggior parte dei batteri nel colon usano carboidrati come fonte di energia,ma non tutte le specie possono degradare i polisaccaridi ed alcuni batteri uti-lizzano i prodotti di degradazione iniziali di un’altra specie. I batteri del colonattraverso un’ampia gamma di enzimi producono idrogeno, metano, anidridecarbonica, acidi grassi a catena corta (principalmente acetato 60%, propionato25% e butirrato 15%) e lattato.Gli acidi grassi a catena corta (SCAFs) hanno un effetto trofico sulla mucosa\ndel colon, producono energia a livello del colonocita, hanno effetto selettivosulla flora batterica, hanno effetto preventivo della diarrea, producono ente-roglucagone e agiscono come immunomodulatori assorbendo procarcinogenie promuovendo l’attacco alle cellule maligne.Gli acidi grassi a catena corta prodotti a livello del colon raggiungono il fegatoe vengono trasformati in grassi, corpi chetonici e glucosio (neoglucogenesi)Le fibre alimentari hanno effetto prebiotico ovvero sono in grado di stimolare\nin maniera selettiva la crescita di alcune specie batteriche benefiche presenti alivello intestinale (Bacteroides, Bifidobacterium, Lattobacilli). Questa attivitàprebiotica determina importanti effetti sulla fisiologica attività intestinale:- Facilita la digestione e l’assorbimento di vari nutrienti- Produce nutrienti (vitamine del gruppo B) - Aumenta le dimensioni dei villi intestinali e quindi la superficie di assorbi-\nmento\n- Aumenta il turn-over delle cellule intestinali- Impedisce lo sviluppo di batteri patogeni\nEffetti della fibra sul transito intestinale. Le fibre alimentari sono in grado\ndi accelerare il transito intestinale con diversi meccanismi a seconda della loro\nsolubilità in acqua. La fibra insolubile assorbe acqua, determina un effetto massa e quindi si haun aumento del peso delle feci ed una distensione delle pareti del colon.La fibra solubile assorbe acqua e come abbiamo visto ha un effetto prebioticoquindi aumenta la massa batterica ed aumenta la massa fecale.STIPSI. La stipsi è una modificazione del funzionamento dell’intestino tale\nche l’espulsione delle feci è alterata nel ritmo giornaliero e/o richiede unosforzo eccessivo per essere portata a termine.Le cause possono essere molteplici di tipo motorio o funzionale, endocrino-metaboliche, legate a farmaci o a fattori psicosociali. Esiste inoltre una stipsiidiopatica.\n45Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#46": "L’alimentazione è il principale determinante il transito intestinale ed in parti-\ncolare l’assunzione di fibre il cui ammontare dovrebbe essere di circa 32-45g/die per raggiungere una massa fecale “critica” di 160-200 g/die necessariper minimizzare il rischio di costipazione. \nEffetti della fibra sulla diverticolosi . La malattia diverticolare è una condi-\nzione clinica complessa.\nDiverticolosi, diverticolite e malattia diverticolare sono forme cliniche diversedi una unica iniziale patologia. La diverticolosi del colon è caratterizzata daernie della parete del colon ed è normalmente asintomatica. I diverticoli pos-sono però causare dolore quando si infiammano ad esempio a seguito di azionebatterica portando ad una condizione detta diverticolite. Le cause della malattiadiverticolare sono diverse: predisposizione genetica, fattori ambientali, età (ri-dotta resistenza della parete, segmentazione del colon), ridotto apporto di fibrecon la dieta.Vi è evidenza da studi osservazionali e studi di intervento che l’assunzione difibre alimentari protegga dalla malattia ed allevi la sintomatologia. E’ parti-colarmente indicata la cellulosa presente nella crusca dei cereali. Questi effettiprotettivi comportano un aumento del peso delle feci, la diminuzione deltempo di transito e la diminuzione della pressione intracolica.\nEffetti della fibra sulle IBD (inflammatory bowel disease ).Le malattie in-\nfiammatorie croniche intestinali quali la colite ulcerosa ed il morbo di Chron\nsono patologie croniche ad eziologia multifattoriale con quadri clinici diversiche colpiscono prevalentemente i giovani. Le fibre alimentari potrebbero avereun ruolo nella dieta dei pazienti affetti da IBD in quanto capaci di “migliorare”la flora batterica intestinale. L’aumentata produzione di SCFAs, che hanno unruolo chiave nel mantenimento dell’omeostasi del colon, da parte delle fibrealimentari potrebbe avere un ruolo positivo nella gestione dei pazienti. L’uso di fibre alimentari è controindicato nelle forme acute e nelle forme ste-nosanti e fistolizzanti\nEffetti della fibra sulla digestione e l’assorbimento dei carboidrati.\nQuando si consumano pasti ricchi in fibre la tolleranza al glucosio migliora ela secrezione insulinica diminuisce sia in soggetti normali che diabetici noninsulino-dipendenti.Meccanismi ipotizzati per gli effetti dell’azione delle fibre sui carboidrati sonodiversi:\n46Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#47": "- ritardo del tempo di svuotamento gastrico\n- formazione di gel con sequestro di glucosio- alterazione del tempo di transito intestinale- isolamento dei carboidrati dagli enzimi digestivi- inibizione della digestione dei carboidrati complessi- produzione di SCFAs che aumentano l’utilizzazione del glucosio- aumento della sensibilità insulinica\nDiabete gestazionale. La dieta alimentare è un utile strumento di preven-\nzione/controllo nel trattamento del Diabete Gestazionale. Una dieta povera difibre è un fattore di rischio infatti si è visto che le fibre alimentari sono ingrado di ridurre il livello della glicemia post-prandiale nella gravidanza.\nDiabete. La prevalenza del Diabete Mellito tipo 2 aumenta in maniera pro-\ngressiva, specie nel sesso femminile, fino a 70 anni. Le cause sono molteplici:\ndisfunzione delle cellule β pancreatiche, ridotta secrezione di insulina, ridottasensibilità insulinica (insulino-resistenza), alterazioni endocrine, alterazionimetaboliche con le ben conosciute complicanze. La pianificazione alimentaree la distribuzione di specifici principi nutrizionali svolge un ruolo fondamen-tale nella terapia del diabete. Diete ricche in fibre (35gr/die) si sono rivelateefficaci nel migliorare il controllo del diabete con riduzione della glicemia adigiuno e dopo il pasto.\nEffetti della fibra sul metabolismo lipidico. \nUna dieta ricca di fibre alimentari ha un ruolo importante nel trattamento del-l’iperlipidemia, riducendo il colesterolo totale e la frazione LDL. Per spiegaretale effetto, son state formulate diverse ipotesi: - il legame tra fibre e acidi biliari rende questi ultimi indisponibili a formare\nle micelle necessarie per l’assorbimento dei grassi e del colesterolo\n- nei soggetti diabetici, migliorando il controllo glico-metabolico che agisce\nsul metabolismo lipidico, ne limita la sintesi\n- viene modulato il metabolismo dei grassi a livello epaticoI livelli di trigliceridi a digiuno e post-prandiali diminuiscono con il supple-mento di fibre solubili \nEffetti della fibra alimentare nella prevenzione del cancro del colon. L’ef-\nfetto della fibra alimentare sul cancro del colon-retto è stata oggetto di po-\nlemiche. La carcinogenesi è un processo biologico complesso che può essere\n47Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#48": "legata a mutazioni genetiche ereditarie, ma è sicuramente influenzata anche\nda fattori esterni, tra cui la dieta. La fibra ha effetti che potrebbero contribuiread una riduzione del rischio di malattia:\n- maggiore velocità del transito intestinale- diluizione dei carcinogeni e dei promotori della carcinogenesi- maggiore eliminazione - riduzione del loro contatto con la mucosa- produzione di SCFAs con effetto protettivo sulla mucosa- abbassamento del pH che riduce la carcinogenesi- migliore risposta immunitaria (sia aspecifica che specifica)- maggiore produzione di interleuchine ed IgA\nL’incidenza del cancro del colon-retto è minore in quei paesi che hanno una\ndieta alimentare prevalentemente a base di fibre.\nEffetti della fibra alimentare nella prevenzione di altre forma cancerose .\nDati osservazionali sul rapporto tra diete ricche in fibre ed insorgenza di di-\nverse forme di cancro non hanno dato risultati consistenti. Anche se molti studicaso-controllo hanno dimostrato una riduzione del rischio di cancro al senotra donne in post-menopausa che consumano diete ricche in fibre, la maggio-ranza degli studi prospettici non ha confermato questa associazione. Esistonoalcune prove, tuttavia, che l’assunzione di cereali integrali è protettivo controil cancro al seno, e che il rischio di cancro allo stomaco è correlato inversa-mente con consumo di grano intero. \nEffetti della fibra sulla prevenzione delle malattie cardiovascolari. \nLa dieta ha un ruolo nella prevenzione delle malattie cardiovascolari. Diversimeccanismi sono stati proposti per spiegare i possibili effetti protettivi dellafibra alimentare sul sistema cardiovascolare. Questi includono:- cambiamenti, ovvero riduzione, nell’assorbimento del colesterolo e nel rias-\nsorbimento della bile;\n- alterazioni nella produzione di lipoproteine nel fegato e cambiamenti nella\nclearance delle lipoproteine ematiche;\n- controllo della glicemia e sul metabolismo dell’insulina- diminuzione della densità energetica del cibo, controllo della fameTutti questi effetti possono tradursi in minori livelli plasmatici di colesterolototale e LDL e quindi ridurre alcuni dei fattori di rischio dell’aterosclerosi. Unaumento del consumo di fibre del tipo β-glucani, pectine e gomma di guar è\n48Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#49": "associato a riduzioni significative dei livelli di colesterolo nel sangue in sog-\ngetti normopeso, sovrappeso od obesi, così come in soggetti iperlipidemici,anche se molti studi di intervento hanno dimostrato che questo tipo di fibreagiscono in tal senso solo in quantità più elevate di quelle di una dieta abituale.I risultati di una meta-analisi su pazienti con livelli di colesterolo elevati hamostrato che un maggior apporto di queste fibre può essere un efficace ed utilecomplemento ad altri cambiamenti dietetici come la riduzione del consumodi grassi. Anche circa il consumo di avena, crusca di avena e psyllium ci sonodati sufficienti per consigliarne il consumo. L’assunzione di cereali integraliè inversamente associato con il rischio di malattia coronarica negli uomini enelle donne, e l’assunzione di frutta e verdura è inversamente associata al ri-schio nelle donne. Vi sono anche prove che l’aumento di assunzione di fibrealimentari, aumentando il consumo di cereali integrali, frutta e verdura, in unadieta a ridotto contenuto di grassi, riduce i livelli di trigliceridi, soprattutto trai soggetti con livelli inizialmente elevati.\nEffetti della fibra alimentare sull’obesità .\nL’obesità ed il sovrappeso sono un eccessivo deposito di tessuto adiposo nel-\nl’organismo come conseguenza dell’eccedenza dell’introito alimentare rispettoalla spesa energetica dell’organismo. Le cause possono essere diverse: - fattori genetici (spesso parenti obesi)- fattori ambientali (abitudini alimentari, lavoro)- fattori comportamentali- fattori ormonali (ipopituitarismo, ipotiroidismo, Morbo di Cushing, ipogo-\nnadismo)\nGli alimenti ricchi in fibra hanno una bassa densità energetica ed occupano volume,pertanto si ritiene che potrebbero promuovere sazietà e giocare un ruolo importantenel controllo del bilancio energetico e del peso corporeo. E’ stato suggerito chealimenti con un basso indice glicemico sono più sazianti di alimenti con alto indiceglicemico. Studi di intervento, indicano che lo svuotamento gastrico può essereritardato dal consumo di fibre solubili quali le pectine. Più interessante sembranoessere gli effetti sull’intestino tenue dove, formando gel, questo tipo di fibre ral-lentano l’assorbimento dei carboidrati, li rendono meno accessibili alla digestioneenzimatica e riducono il loro contatto con la mucosa intestinale. Tutto ciò aumentail senso di sazietà. I risultati variano a seconda del tipo di fibra e se aggiunta comeintegratore isolato o se presente in fonti alimentari.\n2.4 LARN 2012 e FIBRA ALIMENTARE\n49Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#5": "Introduzione\nLe scoperte degli ultimi decenni hanno messo in luce molti aspetti nuovi ed\ninteressanti riguardo al rapporto tra l’uomo e il cibo. E’ oramai chiaro che glialimenti non sono solo una fonte di molecole in grado di fornire, una voltametabolizzate, energia o nutrienti utili per la crescita ed il sostentamento delnostro organismo, ma piuttosto contengono molecole in grado d’influenzarel’espressione genica, il funzionamento di enzimi e proteine e, persino, il nostroatteggiamento nei confronti dei cibi stessi. Ciò è possibile grazie a complicatimeccanismi molecolari che permettono di integrare un numero elevatissimodi informazioni provenienti da organi e tessuti differenti, ognuno dei qualipresenta una propria specificità d’azione e funzioni fisiologiche ben definite.Ciò significa che la comprensione dei fenomeni fisiologici legati allanutrizione umana non possa prescindere dalla conoscenza di tali meccanismie del ruolo che i singoli componenti alimentari, nutrienti e non, svolgono nelnostro organismo.Tuttavia anche tale operazione, seppur perpetuata in modo rigoroso, puòrilevarsi del tutto inutile e non adeguata a descrivere le reali esigenzenutrizionali della maggior parte delle persone. L’uomo è infatti un animalemolto complesso, non standardizzabile o riconducibile facilmente ad unsemplice modello matematico. Il fenotipo umano è infatti il risultato dicombinazioni genotipiche uniche molte delle quali, seppur possano convergereverso fenotipi apparentemente simili, presenteranno sempre aspetti peculiaried unici.In ultima istanza, non dobbiamo dimenticare che altri fattori indipendenti qualigli stimoli provenienti dall’ambiente esterno e lo stile di vita possonoinfluenzare e pesantemente sia le nostre esigenze alimentari che il nostrocomportamento alimentare, contribuendo così a complicare, non poco, ilquadro generale.Il manuale di “Fondamenti della Scienza della Nutrizione” che la commissione“NUTRIZIONE” dell’Ordine dei Biologi propone, ha come obbiettivo quellodi affrontare la nutrizione umana da differenti punti di vista. L’obbiettivo èstato quello di generare, per la prima volta, un manuale che possa essereutilizzato come testo di riferimento da tutti coloro i quali intendono affrontarela professione del Biologo Nutrizionista.Il manuale, passo dopo passo, guida il lettore, attraverso un percorso lineareche permette di acquisire, progressivamente, tutti gli elementi utili adinterpretare la maggior parte dei complicati meccanismi che permettono di\n5Introduzione Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#50": "SDT- Obiettivi Nutrizionali per la Prevenzione\nPreferire alimenti naturalmente ricchi in fibre alimentari quali cereali integrali,legumi, frutta e verdura. Negli adulti consumare almeno 25g/die di fibra ali-mentare anche in caso di apporti energetici < 2000Kcal/dieAI- Assunzione adeguataEtà evolutiva (≥ 1 anno) 8,4g/1000 Kcal (2 MJ)\nRI- Intervallo di Riferimento per l’Assunzione di Macronutrienti\nAdulti 12,6-16,7 g/1000 Kcal (3,4 g/MJ)\nEffetti collaterali di dosi eccessive: distensione addominale, coliche addomi-\nnali per eccessiva produzione di gas e diarrea. La tolleranza è individuale.\nPRINCIPALI ALIMENTI RICCHI IN FIBRA\nLegumi: fagioli, fave, ceci, lenticchie, piselli Cereali e derivati : pasta, biscotti, pane e cereali da colazione (soprattutto se\nintegrali), prodotti da forno, orzo perlatoVerdura e ortaggi : carciofi, cavoli, cicoria, carote, melanzane, barbabietole,\nfunghi, agretti, finocchiFrutta fresca: pere, mele, fichi, banane, kiwi, lamponi, fichi d’India, ribesFrutta secca in guscio: noci, nocciole, mandorleFrutta essiccata: albicocche secche, fichi secchi, mele essiccate, uva passa,prugne secche, castagne secche\n50Fondamenti della Scienza dell’Alimentazione Capitolo II",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#51": "Percentuale di fibra per 100g di peso nei principali alimenti\nCEREALI %\nCrusca di grano 44,0\nFarina integrale 9,6\nFarina bianca 3,0\nFarina di soia 14,3\nRiso integrale lessato 5,5\nRiso bianco lessato 0,8\nPane integrale 8,5\nPane misto 5,1\nPane bianco 2,7\nAll Brain (crusca) 26,7\nCornflakes 11,0\nMuesli 7,4FRUTTA FRESCA E SECCA %\nMore 7,3\nUva 6,8\nMirtilli 4,2\nBanane 3,4\nPere 3,4\nFragole 2,3\nPrugne 2,1\nMele 2,0\nArance 2,0\nMandorle 14,3\nArachidi 8,2\nNocciole 6,2\nVERDURE %\nRavanelli 8,3\nSpinaci lessati 6,3\nCarote lessate 3,0\nCarote crude 2,9\nCavoletti di Bruxelles, \nbroccoli lessati 2,9\nPatate al forno con la buccia 2,0\nCavolfiore, verza lessati 1,8\nLattuga 1,5\nPomodori 1,5LEGUMI %\nPiselli congelati lessati 12,0\nFagioli lessati 7,4\nFave lessate 5,1\nLenticchie lessate 3,7\n51Capitolo II Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#52": "Bibliografia \n• American Association of Cereal Chemists. AACC Dietary Fiber Technical Committee. The\ndefinition of dietary fiber. Cereal Foods World 2001\n• Food and Agriculture Organization of the United Nations/World Health Organization.\nCarbohydrates in Human Nutrition. Report of a Joint FAO/WHO Expert Consultation,\nFAO/WHO, Rome, Italy, 1998\n• John F. Howlett et al.The definition of dietary fiber – discussions at the Ninth Vahouny Fiber\nSymposium: building scientific agreement - Food Nutr Res. 2010\n• Miller Jones J. Dietary fibre intake, disease prevention, and health promotion: An overview\nwith emphasis on evidence from epidemiology. In: Bioactive Carbohydrates for Food andFeed. JW van der Kamp ed. Academic Publishers, Wageningen, 2004, Netherlands\n• World Health Organization. Diet, nutrition and the prevention of chronic diseases. Report\nof a Joint WHO/FAO Expert Consultation. WHO Technical Report Series 916, Geneva, 2003\n• Watzl B, Girrbach S, Roller M. Inulin, oligofructose and immunomodulation. Br J Nutr.\n2005 Apr\n• Clayton BD, et al. BNF-TasK Force on Complex Carbohydrates in Foods• Chapman and Hall I ed; 11 New fetter Lane, London 1990. • Bassotti G, Iantorno G, Fiorella S et al. Colonic motility in man: features in normal subjects\nand in pz. with chronic idiopathic constipation. Am J Gastroenterol 1999.\n• Anderson JW, Chen WL. Plant fiber: carbohydrate and lipid metabolism. Am J Clin\nNutr1979\n• Eglash A, Lane CH, Scheider DM. Clinical inquires. What is the most beneficial diet for\nPatients with diverticulosis? J Fam Pract. 2006 Sep\n• HoweGR, Benitu E, Castelleto R et al. Dietary intake of fiber and decreased risk of cancer\nand rectum.Evidence from combined analysis of 13 case-control-studies. J Nat CancerInst.1992\n• SINU (Società Italiana di nutrizione Umana). LARN – Livelli di Assunzione di Riferimento\ndi Nutrienti ed energia per la popolazione italiana. Revisione 2012\n52",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#53": "53Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#54": "CAPITOLO III\nMetabolismo dei Carboidrati\nRiassunto\nI carboidrati o glucidi sono sostanze chimiche composte da carbonio, idro-\ngeno e ossigeno. I carboidrati più semplici sono i monosaccaridi mentre i di-saccaridi sono costituiti da 2 monosaccaridi. I polisaccaridi sono costituitida un numero elevato di monosaccaridiL’assunzione complessiva raccomandata di carboidrati si aggira sul 60%della razione alimentare, suddivisa tra zuccheri semplici (circa 15%) e poli-saccaridi (circa 45%).La loro metabolizzazione inizia nel cavo orale dove la ptialina è capace didemolire i legami alfa-1-4 ma non i legami a-1-6. Si formano così le primemolecole di destrine, maltosio e glucosio. Nello stomaco a casa degli acidigastrici che aumentano il pH la digestione viene momentaneamente sospesa.L’idrolisi poi riprende nell’intestino dove il pH è leggermente basico ad operadella alfa-milasi pancreatica. I prodotti finali della digestione dei carboidrati,i monosaccaridi glucosio, fruttosio, mannosi e galattosio, sono assorbiti dallecellule epiteliali intestinali ed entrano nel sangue. Il fattore condizionante ilmetabolismo glucidico nell’organismo è il mantenimento della glicemia entrovalori ben precisi, mediante l’attività antagonista di due ormoni, l’insulinaed il glucagone.La glicolisi, è il processo di ossidazione anaerobico che avviene nel citopla-sma per mezzo del quale le cellule possono ricavare energia dal glucosio senzala presenza di ossigeno. La glicolisi è una sequenza di reazioni che converteil glucosio in acido piruvico (piruvato), con concomitante produzione di ATP .Avviene nel citoplasma delle cellule di tutti i tessuti. E’ il preludio al ciclodell’acido citrico (ciclo di Krebs) ed alla catena di trasporto degli elettroni,con cui viene recuperata la maggior parte dell’energia contenuta nella mole-cola di glucosio. La gluconeogenesi è il processo di sintesi di glucosio a par-tire da precursori non glucidici, che consente il mantenimento dell’omeostasiglicemica, quando le riserve di glicogeno sono esaurite e/o l’apporto glucidicodella dieta è insufficiente. La gluconeogenesi avviene nel citoplasma delle cel-lule epatiche a partire dall’acido piruvico, principalmente in condizioni di di-giuno fra un pasto e l’altro. La glicogenosintesi è la formazione di glicogeno(polimero del glucosio) a partire dal glucosio. Questo processo avviene nel\n54Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#55": "muscolo e nel fegato dopo un pasto ricco di carboidrati.\nLa glicogenolisi rappresenta la via di degradazione del glicogeno per ottenereglucosio 6-fosfato: il glicogeno è trasformato in glucosio 1-fosfato dall’enzimafosforilasi e successivamente in glucosio 6-fosfato dall’enzima fosfoglucomu-tasi. La glicogenolisi avviene principalmente nel fegato per mantenere costantii livelli di glucosio ematico.Il Ciclo dei Pentosi o Shunt dell’Esoso Monofosfato è una via alternativa allaglicolisi per la demolizione della molecola del glucosio, avviene nel citopla-sma cellulare e riveste particolare importanza per il fegato, tessuto adiposo,rene, eritrociti dove ha la funzione di produrre NADPH per le biosintesi diacidi grassi, produrre ribosio e pentosi per la sintesi dei nucleotidi, produrreenergia attraverso la formazione di intermedi della via gli colitica.Il ciclo di Krebs, la catena di trasporto degli elettroni e la fosforilazione os-sidativa costituiscono il metabolismo ossidativo terminale, nel quale i prodottidel metabolismo intermedio vengono completamente bruciati a CO\n2ed H2O.\nIn condizioni di aerobiosi, l’acido piruvico prodotto durante la glicolisi entranel mitocondrio per essere completamente ossidato attraverso il ciclo diKrebs. Quando il glucosio viene ossidato completamente a CO\n2ed H2O, si ge-\nnerano in totale 30 o 32 ATP .\n55Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#56": "3.1 I Carboidrati o Glucidi\nI glucidi, chiamati anche (impropriamente) carboidrati, sono sostanze chimi-\nche composte da carbonio, idrogeno e ossigeno e possono essere definiti comederivati aldeidici o chetonici di alcoli polivalenti.I glucidi hanno formula elementare C\nn(H2O)n. Presentano dei gruppi -OH,\nquindi possono essere considerati alcoli polivalenti, e un gruppo aldeidico (al-dosi) o un gruppo chetonico (chetosi).I carboidrati più semplici sono i monosaccaridi. Ad esempio ribosio, galattosio\ne glucosio sono monosaccaridi aldosi; il fruttosio è un monosaccaride chetoso.Dal punto di vista biochimico, il glucosio può essere considerato il capostipitedi tutti i glucidi (Figura 1).\nFigura 1: struttura chimica del glucosio e del fruttosio\nPiù unità di monosaccaridi (da 2 a migliaia) possono legarsi con un legameglicosidico che si stabilisce tra un gruppo -OH di un monosaccaride in posi-zione 1 ed un gruppo -OH di un altro monosaccaride, con perdita di una mo-lecola di H\n2O.\nI disaccaridi sono costituiti da 2 monosaccaridi. Sono disaccaridi il saccarosio\n(glucosio + fruttosio), il lattosio (glucosio + galattosio), il maltosio (glucosio+ glucosio) (Figura 2).\n56Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#57": "Figura 2: struttura chimica di alcuni disaccaridi\nI polisaccaridi sono costituiti da un numero elevato di monosaccaridi. Sono\npolisaccaridi:\n3) glicogeno: catena ramificata di α-D-glucosio con legami α-1,4- ed α-1,6-\nglicosidici (Figura 3)\n4) cellulosa : catena lineare di β-D-glucosio con legami β-1,4-glicosidici\n5) amilosio: catena lineare di α-D-glucosio con legami α-1,4-glicosidici\n6) amilopectina : catena ramificata di α-D-glucosio con legami α-1,4- ed α-\n1,6-glicosidici\n7) amido: costituito da amilosio ed amilopectina\n57Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#58": "Figura 3: struttura chimica del glicogeno\n3.2 Classificazione\nI carboidrati si classificano in:-Monosaccaridi\nSono i glucidi più semplici e contengono da 3 a 9 atomi di carbonio; quellid’importanza biologica comprendono il glucosio, il fruttosio ed il galattosio.Il glucosio è scarsamente presente in natura, a parte piccolissime quantità nellafrutta e nella verdura. Il fruttosio è presente come tale nella frutta e nel miele. \n-DisaccaridiSi possono considerare come l’unione di due molecole di monosaccaridi me-diante legami glicosidici; quelli d’importanza biologica comprendono il sac-carosio, il lattosio e il maltosio.Il saccarosio è composto da glucosio + fruttosio e si trova nella frutta, spe-cialmente nella barbabietola e nella canna, da cui è estratto per produrre lozucchero da tavola.Il lattosio è contenuto nel latte ed è formato da glucosio + galattosio. Il mal-tosio (glucosio + glucosio) deriva dalla fermentazione (o dalla digestione)dell’amido.\n-OligosaccaridiIl termine oligosaccaridi è usato generalmente per i composti formati da 3 a10 monosaccaridi. A questo gruppo appartengono il raffinosio, lo stachiosio\n58Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#59": "ed il verbascosio non digeribili per l’uomo, composti da galattosio, glucosio\ne fruttosio e contenuti soprattutto nei legumi. La produzione di gas a seguitodella fermentazione di questi zuccheri nell’intestino crasso spiega il meteo-rismo provocato, soprattutto in alcuni soggetti, dal consumo di leguminose.\n-PolisaccaridiIl termine polisaccaridi è usato generalmente per i composti formati da piùdi 10 monosaccaridi.Essi si dividono in:• omopolisaccaridi, costituiti da un solo tipo di monosaccaride (esempio:\nglicogeno)\n• eteropolisaccaridi costituiti da più di un tipo di monosaccaridi (esempio:\nmucopolisaccaridi, presenti nei tessuti connettivi degli animali)\nI polisaccaridi più importanti per l’alimentazione umana sono l’amido e il gli-cogeno.L’amido costituisce la riserva energetica del mondo vegetale: le principali sor-\ngenti sono: i cereali (pane, pasta, riso) e le patate. E’ presente sotto forma digranuli a struttura semicristallina: la cottura dei cibi altera tale struttura (pro-cesso di gelatinizzazione), rendendo l’amido digeribile. Il raffreddamento deicibi, che conduce a parziali fenomeni di ricristallizzazione dell’amido, ne ri-duce parzialmente la digeribilità.Il glicogeno è d’origine animale. Negli alimenti (carne, fegato) il suo contenuto\ntuttavia è privo di significato nutrizionale essendo presente in minime quantità(lo stato di anossia che segue la morte dell’animale lo trasforma in acido lat-tico).Gli altri polisaccaridi non-amidacei sono ampiamente diffusi in natura, manon sono rilevanti a scopo nutrizionale, poiché non possono essere digeritidall’uomo per mancanza degli enzimi necessari.\n3.3 Funzioni dei glucidi\nI glucidi presentano una duplice funzione:\n- Plastica , entrano nella costituzione di strutture essenziali per gli organismi\nviventi\n-Energetica, forniscono all’organismo energia per le prestazioni funzionali \nI glucidi, sebbene nel mondo animale siano meno abbondanti delle proteine,\n59Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#6": "regolare il metabolismo umano, senza tuttavia mai eccedere con i tecnicismi\nscientifici o dilungarsi nella descrizione di particolari non strettamente utilialla formazione del Biologo Nutrizionista. La nutrizione costituisce il punto d’incontro di una moltitudine di disciplinebiomediche ma sempre più sta divenendo un campo di informazioni confusoin cui anche il professionista trova difficoltà ad orientarsi. L’attività delnutrizionista deve essere considerata nell’ottica di conoscenze scientificheaggiornate che spaziano dagli aspetti fisiologici, biochimici, genetici epatologici, al fine di garantire reali effetti benefici per la salute umana e per ilbenessere.Il testo rappresenta anche un mezzo pratico e di facile consultazione perinformazioni basilari rivolto non solo ai Nutrizionisti ma anche a tutti iProfessionisti della Biologia che potranno usufruire di uno strumento diindagine agile e semplice per affrontare in maniera rigorosa gli aspetti dellanutrizione umana.  \n6Fondamenti della Scienza dell’Alimentazione Introduzione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#60": "hanno un’importanza metabolica fondamentale, perché rappresentano la sor-\ngente primaria da cui le cellule ricavano l’energia. Sono quindi il materiale chi-mico indispensabile per lo svolgimento delle diverse forme di lavoro biologico.Nell’uomo, le cellule si alimentano sottraendo glucosio dal sangue, dove èpresente in concentrazione di 70-110 mg/100 ml. Quando il tasso ematico di-minuisce drasticamente (valori inferiori a 40 mg/100 ml), le cellule nervosene soffrono e si ha il coma ipoglicemico .\nQuando i glucidi non sono introdotti in misura sufficiente con l’alimentazione,il fegato trasforma gli amminoacidi o il glicerolo (ottenuto dalla degradazionedei grassi), in glucosio (gluconeogenesi). Il fegato è anche dotato di una riservadi glucosio polimerizzato in lunghe catene ramificate, il glicogeno , che è uti-\nlizzato attraverso la glicogenolisi per mantenere costante la glicemia. I glucidi hanno anche un ruolo strutturale. L’acido ialuronico, un eteropoli-saccaride, è un costituente essenziale del connettivo, del fluido sinoviale edell’umor vitreo. Le glicoproteine sono una classe di proteine coniugate con-tenenti da 1% ad 80% di sostanze glucidiche, che svolgono funzioni diverse.Ricordiamo inoltre che i glucidi entrano nella struttura di altri composti comenucleosidi, coenzimi, glicolipidi. \n3.4 Fabbisogno giornaliero\nDal momento che l’organismo ha la capacità di sintetizzare i glucidi da altre\nsostanze, i carboidrati non possono essere considerati nutrienti propriamenteessenziali; esiste tuttavia la necessità di mantenere il livello di glicemia entroun intervallo di valori adeguato al fabbisogno del sistema nervoso centrale edegli eritrociti (globuli rossi).Acido citrico Frutta Assorbiti come taliL’assunzione complessiva raccoman-data di carboidrati si aggira sul 60% della razione alimentare, suddivisa tra zuc-cheri semplici (circa 15%) e polisaccaridi (circa 45%). Gli alimenti contenenticarboidrati complessi, oltre a fornire energia a più lento rilascio rispetto a quellisemplici, apportano anche altri nutrienti fondamentali all’equilibrio generaledella dieta: questo aspetto è rilevante soprattutto quando è necessario mantenerel’apporto energetico complessivo entro limiti relativamente modesti.\n60Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#61": "3.5 Digestione dei carboidrati\nI più comuni carboidrati della dieta sono l’amido, il saccarosio ed il lattosio.\nL’amido, il polisaccaride di riserva delle piante, è formato da due componenti\nl’α-amilosio e l’amilopectina . L’amilosio e l’amilopectina sono formati en-\ntrambi da molecole di glucosio unite fra loro con legami glicosidici α-1,4, ma\nnell’amilopectina, ramificata, vi sono anche catene laterali unite alla catenaprincipale mediante legami glicosidici α-1,6. Più della metà dei carboidrati\ningeriti dall’uomo è costituita dall’amido.Nel cavo orale, l’ α-amilasi agisce spezzando i legami α-1,4 tra i residui di\nglucosio all’interno della molecola di amido, formando frammenti più sem-plici, le destrine. Le destrine prodotte sono le amilodestrine, le eritrodestrine,Prodotti terminaliGLUCIDI Principali sorgenti alimentaridella digestione\nPOLISACCARIDI\nCellulosa Gambo o foglie di vegetali Non digeribile\nPectine Frutta Non digeribile\nInulina Carciofi, cipolle, aglio Non digeribile\nAmido Tessuti di deposito vegetali \n(patate e altri tuberi, legumi) Glucosio\nDestrine - Glucosio\nGlicogeno Carne e pesce Glucosio\nDISACCARIDI\nSaccarosio Zucchero di canna e di bietola Glucosio e Fruttosio\nLattosio Latte e derivati Glucosio e Galattosio\nMaltosio Prodotti del malto, biscotti Glucosio\nMONOSACCARIDI\nGlucosio Frutta, miele, sciroppo Glucosio\nFruttosio Frutta e miele Fruttosio\nGalattosio - Galattosio\nDERIV ATI DA GLUCIDI\nAlcol etilico Bevande fermentate Assorbiti come tali\nAcido lattico Latte e derivati Assorbiti come tali\nAcido malico Frutta Assorbiti come tali\n61Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#62": "le acrodestrine e la destrina limite formata da tre residui, legati con legame\nglicosidico α-1,4, ed uno legato con legame glicosidico α-1,6.\nI prodotti digestivi entrano nello stomaco dove però l’acidità gastrica inibisce\nl’azione dell’ α-amilasi. Il contenuto dello stomaco passa quindi nell’intestino\ndove il bicarbonato secreto dal pancreas neutralizza l’acidità gastrica, facendoinnalzare il pH ai limiti ottimali per l’azione degli enzimi intestinali e pancreatici.Il pancreas secerne inoltre α -amilasi pancreatica che entra nel lume dell’intestino\ntenue e continua il processo digestivo. L’ α-amilasi pancreatica, come l’enzima\nsalivare, spezza i legami α-1,4 fra i residui di glucosio all’interno della catena.\nI prodotti dell’ α-amilasi pancreatica sono il disaccaride maltosio, il trisaccaride\nmaltotrioso e piccoli oligosaccaridi contenenti legami α-1,4 ed α -1,6.\nGli enzimi prodotti dalle cellule epiteliali intestinali e localizzati sull’orlettoa spazzola continuano il processo digestivo:- Una α-glucosidasi idrolizza residui di glucosio dall’estremità non riducente\ndegli oligosaccaridi. Questo enzima idrolizza anche i legami α-1,4 del\nmaltosio, rilasciando due molecole di glucosio.\n- Una α-destrinasi idrolizza i legami α-1,6, rilasciando residui di glucosio\ndagli oligosaccaridi ramificati. I disaccaridi della dieta sono digeriti daenzimi presenti sull’orletto a spazzola delle cellule epiteliali intestinali.\n- La saccarasi converte il saccarosio in glucosio e fruttosio.\n- La lattasi converte il lattosio in glucosio e galattosio.\nI prodotti finali della digestione dei carboidrati, i monosaccaridi glucosio, frut-\ntosio (derivato dall’idrolisi del saccarosio), mannosio (epimero del glucosio) ,\ngalattosio (derivato dall’idrolisi del lattosio), sono assorbiti dalle cellule epi-\nteliali intestinali ed entrano nel sangue. Polisaccaridi indigeribili come la cel-\nlulosa (che consiste di unità di glucosio legate da legami glicosidici α-1,4)\nfanno parte delle fibre alimentari, che passano attraverso l’intestino alle feci.\nAssorbimentoI monosaccaridi (glucosio, fruttosio, mannosio, galattosio) liberatisi a livello\nintestinale dopo i processi di digestione ed assorbimento dei carboidrati delladieta, passano dall’enterocita alla vena porta e quindi giungono al fegato.La membrana cellulare epatica è liberamente permeabile al glucosio (la suaconcentrazione negli epatociti è uguale a quella plasmatica) ed il suo trasportonella cellula è indipendente dalla disponibilità di insulina. Entrato nell’epato-cita, il glucosio e gli altri monosaccaridi sono trasformati in glucosio 6-fosfato.L’attività dell’enzima glucochinasi è stimolata dall’insulina favorendo l’uti-\n62Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#63": "lizzazione di glucosio. Una parte del glucosio attraversa il fegato e passa in\ncircolo, ma una volta fosforilato a glucosio 6-fosfato non è in grado di attra-versare la membrana plasmatica.\nConsumo di glucosio in condizioni basaliIl fattore condizionante il metabolismo glucidico nell’organismo è il mante-\nnimento della glicemia entro valori ben precisi, mediante l’attività antagonista\ndi due ormoni, l’insulina ed il glucagone.\nInsulina e glucagone\nL’insulina è un ormone proteico prodotto dalle cellule beta delle isole di Lan-gerhans ed è il principale regolatore del metabolismo intermedio agendo comeormone anticatabolico. Quando i livelli di insulina nel sangue sono bassi, mas-sima è la produzione di glucosio e minima la sua utilizzazione; essa, infatti, fun-ziona come ipoglicemizzante, permettendo l’assorbimento cellulare del glucosioematico. Nel fegato l’insulina stimola la glicogenosintesi, inibendo la glicoge-nolisi. Nel muscolo l’insulina favorisce la formazione del glicogeno, la sintesiproteica e l’immagazzinamento degli amminoacidi. Nel tessuto adiposo l’insu-lina ha sia un effetto lipogenico (stimola la formazione dei trigliceridi) sia uneffetto antilipolitico, ovvero inibisce la degradazione dei trigliceridi.La riserva di glicogeno epatico utilizzabile è circa 70-100 g, non sufficientiper mantenere la glicemia più a lungo di 24 ore, perciò è necessario ripristinarequesta riserva con l’assunzione di cibo. Il glicogeno muscolare non è utilizza-bile per regolare la glicemia, ne risulta disponibile per altri tessuti non essendoespresso a livello del tessuto muscolare, come anche nel cervello, l’enzimaglucosio-6-fosfatasi. Tale enzima defosforila il glucosio-6-fosfato riformandoglucosio che può, invece, attraversare la membrana cellulare.Durante il digiuno, l’azione dell’insulina è minima e massima è l’azione di unaltro ormone proteico prodotto dalle cellule alfa delle isole di Langerhans: ilglucagone. Il glucagone ha un effetto iperglicemizzante: stimola la liberazionedi glucosio dai tessuti. Nel tessuto adiposo il glucagone stimola la lipolisi, nelfegato la gluconeogenesi e la glicogenolisi; nei muscoli stimola la degrada-zione delle proteine in amminoacidi.Nell’uomo, a digiuno il tasso ematico del glucosio è 70-110 mg per 100 ml disangue. Il 50% del glucosio ematico è utilizzato dal cervello ed il resto daiglobuli rossi e bianchi e dal muscolo scheletrico. L’origine di tale glucosio èepatica, attraverso la glicogenolisi (75%) e la gluconeogenesi (25%). \n63Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#64": "Destino metabolico del glucosio dopo un pasto\nSubito dopo un pasto, i livelli di glucosio possono aumentare a 120-130 mg\nper 100 ml di sangue e possono raggiungere livelli anche superiori (fino a170). L’equilibrio basale è ristabilito dopo 90-120 minuti nel corso dei qualiil 15-20% del glucosio è utilizzato dai tessuti insulino-dipendenti (soprattuttotessuto adiposo e muscolo scheletrico). Circa il 25% è utilizzato dai tessuti in-sulino-indipendenti (cervello, globuli rossi), mentre il rimanente 55-60% èutilizzato dal fegato (glicogenosintesi, glicolisi, sintesi degli acidi grassi). Nel fegato, una quantità relativamente piccola di glucosio epatico è ossidataa CO\n2ed H2O attraverso il ciclo di Krebs, mentre la maggior parte dell’ATP\nrichiesto dal fegato deriva dall’ossidazione di acidi grassi ed amminoacidi. Inconfronto ad altri tessuti, quali muscolo e cervello, la quantità di glucosio me-tabolizzato nel fegato attraverso la via glicolitica è piccola: l’obiettivo dellaglicolisi epatica è quello di produrre acido piruvico da convertire poi in ace-til-CoA ed infine in acidi grassi (Figura 4).Circa la metà del glucosio metabolizzato nel fegato entra nello shunt deipentosi, che produce NADPH+H\n+per la sintesi degli acidi grassi.\nIl glucosio può essere utilizzato inoltre nella formazione di acido glucuronico,oltre che per la sintesi di mucopolisaccaridi, glicoproteine e glicolipidi.L’eccesso di glucosio epatico è in gran parte trasformato ed immagazzinatosotto forma di glicogeno attraverso la via della glicogenosintesi, che rappre-senta così la principale utilizzazione epatica del glucosio 6-fosfato.\n64Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#65": "Figura 4: Utilizzazione e produzione di glucosio 6-fosfato (glucosio 6-P) nel fegato.\nLa funzione principale del fegato nel metabolismo dei glucidi è legata alla sua\nattività glucostatica: è in grado di “tamponare” le variazioni della glicemia aseguito di modificazioni metaboliche e/o variazioni dell’apporto nutrizionale. Il fegato mantiene l’omeostasi glucidica attraverso:\n- Immagazzinamento di glucosio in forma di glicogeno.\n- Rilascio di glucosio dal glicogeno.- Utilizzazione del glucosio (glicolisi, shunt dell’esosomonofosfato).- Sintesi di glucosio mediante gluconeogenesi.- Conversione di glucidi in acidi grassi.\n3.6 GlicolisiLa glicolisi è una sequenza di reazioni che converte il glucosio in acido piru-\nvico (piruvato), con concomitante produzione di ATP (Figura 5). La glicolisisi compie nel citoplasma delle cellule di tutti i tessuti. E’ il preludio al ciclodell’acido citrico (ciclo di Krebs) ed alla catena di trasporto degli elettroni,con cui viene recuperata la maggior parte dell’energia contenuta nella mole-cola di glucosio.\n65Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#66": "3.6.1 Le reazioni della glicolisi\n• Il glucosio è convertito a glucosio 6-fosfato in una reazione che utilizza ATP\ne produce ADP. Enzimi: esochinasi e, nel fegato, glucochinasi, ambedue\nsoggetti a meccanismi di regolazione.\n• Il glucosio 6-fosfato è isomerizzato a fruttosio 6-fosfato. Enzima:\nfosfoglucoisomerasi.\n• Il fruttosio 6-fosfato è fosforilato dall’ATP. Si formano fruttosio 1,6-bifosfato\ne ADP. Enzima: fosfofruttochinasi .\n• Il fruttosio 1,6-bifosfato è scisso a formare i due trioso-fosfati, gliceraldeide\n3-fosfato e diidrossiacetone fosfato. Enzima: aldolasi.\n• Il diidrossiacetone fosfato è isomerizzato a gliceraldeide 3-fosfato. Enzima:\ntriosofosfato isomerasi.\nCome risultato netto delle reazioni sopra riportate, si formano due moli di gli-\nceraldeide 3-fosfato da una mole di glucosio con consumo di due moli di ATP.• La gliceraldeide 3-fosfato è ossidata dal NAD\n+e reagisce con fosfato\ninorganico (Pi). Si formano 1,3 bifosfoglicerato (acido 1,3 bifosfoglicerico)\ne NADH + H+. Enzima: gliceraldeide 3-fosfato deidrogenasi .\n• Il gruppo aldeidico della gliceraldeide 3-fosfato è ossidato ad acido che\nforma un’anidride ad alta energia con il fosfato inorganico. L’energia delgruppo anidridico del 1,3-bifosfoglicerato è sfruttata per produrre ATP daADP in una reazione al termine della quale si forma 3-fosfoglicerato.Enzima: fosfoglicerato chinasi .\n• Il gruppo fosfato del 3-fosfoglicerato è trasferito al carbonio 2, con\nformazione di 2-fosfoglicerato. Enzima: fosfogliceromutasi.\n• Al 2-fosfoglicerato viene sottratta una molecola di acqua, formando fosfoenol -\npiruvato, che presenta un gruppo fosforico ad alta energia. Enzima: enolasi.\n• Nell’ultima reazione glicolitica da fosfoenolpiruvato si forma piruvato, con\ncontemporanea formazione di ATP da ADP. Enzima: piruvato chinasi .\n3.6.2 Gli enzimi di regolazione della glicolisi\nEsochinasi\nL’esochinasi si trova nella maggior parte degli organi e tessuti ed ha il compitodi trasformare il glucosio in glucosio 6-fosfato da immettere nella via glicolitica.Ha una bassa Km per il glucosio, è inibita dal suo prodotto, il glucosio 6-fosfato:quindi è attiva quando il glucosio 6-fosfato viene rapidamente utilizzato.\n66Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#67": "Figura 5: reazioni della glicolisi\n67Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#68": "Glucochinasi\nLa glucochinasi si trova nel fegato e funziona ad una velocità apprezzabile su-bito dopo un pasto. Ha un’alta K\nmper il glucosio, quindi è molto attiva subito\ndopo un pasto, quando il livello di glucosio nella vena porta è alto, e relativa-mente non attiva durante le fasi di digiuno quando i livelli di glucosio sonobassi. La glucochinasi non è inibita da glucosio 6-fosfato.\nFosfofruttochinasi\nLa fosfofruttochinasi funziona a ritmo elevato quando le cellule necessitanodi ATP o, nel caso del fegato, quando il glucosio ematico è elevato. E’ inibitada acido citrico (prodotto nel ciclo di Krebs) ed ATP: quando la cellula pre-senta elevate concentrazioni di ATP, la glicolisi viene inibita.\nPiruvato chinasi\nLa piruvato chinasi è inibita nel fegato durante le fasi di digiuno, quando i li-velli dell’ormone glucagone sono alti. L’inibizione della piruvato chinasi fa-vorisce la gluconeogenesi.\n3.6.3 Destini metabolici dell’acido piruvico Conversione in Acetil-CoA\nIl principale destino metabolico dell’acido piruvico è la sua completa ossida-zione attraverso il ciclo di Krebs, nella matrice mitocondriale, dove è traspor-tato mediante carriers e trasformato in Acetil-CoA. La reazione è catalizzatadal complesso multienzimatico della piruvato deidrogenasi.\npiruvato deidrogenasiPiruvato + CoA + NAD+\nAcetil CoA+CO2+NADH+H+\npiruvato deidrogenasi\nFormazione di acido ossalaceticoL’acido piruvico può essere convertito ad acido ossalacetico dalla piruvato car-\nbossilasi, che si trova in tessuti come fegato e cervello, ma non nel muscolo.\nConversione ad alaninaL’acido piruvico può essere transaminato per formare l’amminoacido alanina(vedi metabolismo proteico).\n68Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#69": "Formazione di acido lattico\nDal piruvato si può formare acido lattico quando la quantità di ossigeno adisposizione è limitata, come durante un’intensa attività muscolare. L’acido lattico si ottiene per riduzione dell’acido piruvico utilizzando NADHprodotto nella glicolisi. La reazione è catalizzata dall’enzima lattico\ndeidrogenasi (LDH).\nlattico deidrogenasiPiruvato + NADH+ H+Lattato + NAD+\nLa riossidazione del NADH durante la conversione del piruvato a lattatopermette alla glicolisi di continuare a funzionare in condizioni di anaerobiosi.Se il NAD\n+non fosse rigenerato, la glicolisi si fermerebbe a gliceraldeide 3-\nfosfato, quindi non potrebbe essere prodotto ATP.Soltanto una piccola parte dell’energia del glucosio viene utilizzata nel casodella conversione anaerobia a lattato. Molta più energia può essere estratta incondizioni aerobie per mezzo del ciclo di Krebs e della catena respiratoria chesaranno trattati più avanti. Il vantaggio di questa via alternativa è la rigenerazione di NAD\n+che può far\ncontinuare il processo demolitivo del glucosio.Lo svantaggio risiede nel fatto che l’acido lattico è un catabolita tossico per lacellula. Lo smaltimento di questa molecola dalle cellule muscolari avvienemolto lentamente: il suo accumulo provoca affaticamento fisico, e, se la con-centrazione nel sangue supera il valore di 0,5 mg/ml, il processo di contrazionemuscolare si blocca e sopravviene un crampo muscolare.L’acido lattico viene trasportato al di fuori della cellula nel flusso sanguignoed inviato al fegato, dove viene riossidato ad acido piruvico e questo di nuovotrasformato in glucosio mediante la gluconeogenesi. Il glucosio dal fegatopassa nel sangue e da qui ancora nel muscolo, dove può avere inizio unulteriore processo glicolitico.Il processo metabolico seguito dall’acido lattico ha un andamento ciclico eprende il nome di Ciclo di Cori (Figura 6).\n69Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#7": "CAPITOLO I - I MACRO e MICRONUTRIENTI\n1.1 - I Micronutrienti1.1.1 I Sali Minerali1.1.2 VitamineVitamine idrosolubili Vitamine liposolubili \n1.2 I Macronutrienti\n1.2.1 CarboidratiClassificazione dei glucidi1.2.2 LipidiClassificazione e funzione dei lipidi1.2.3 ProteineClassificazione e funzione delle proteine Bibliografia\nCAPITOLO II - ACQUA E FIBRA ALIMENTARE\n2.1 Acqua2.2.1 Larn e acqua2.2 Fibra alimentare2.2.1 Fibra insolubile2.2.2 Fibra solubile2.3 Ruolo fisiologico della fibra alimentare e benefici per la saluteFermentazione colicaPrincipali alimenti ricchi in fibra2.4 LARN 2012 e FIBRA ALIMENTAREBibliografia\nCAPITOLO III - METABOLISMO DEI CARBOIDRATI\nI Carboidrati o GlucidiClassificazioneFunzioniFabbisognoDigestione dei CarboidratiAssorbimentoConsumo di glucosio in condizioni basaliDestino metabolico del glucosio dopo un pastoGlicolisi\n7Indice Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#70": "Gli eritrociti, cellule specializzate nel trasporto di ossigeno, non contengono\nmitocondri. In tali organelli citoplasmatici avvengono il ciclo di Krebs e lafosforilazione ossidativa, per cui l’unico modo per estrarre energia dal glucosioin queste cellule è la formazione di acido lattico da piruvato. \nConversione dell’acido piruvico ad etanolo\nNel lievito ed in parecchi microorganismi l’acido piruvico viene trasformatoin etanolo. Il primo passaggio è la decarbossilazione dell’acido piruvico adacetaldeide catalizzata dall’enzima piruvato decarbossilasi , l’acetaldeide è\npoi ridotta ad etanolo mediante una reazione di ossidoriduzione catalizzatadall’alcol deidrogenasi:\npiruvato decarbossilasiAcido piruvico CO2+ acetaldeide\nalcol deidrogenasiAcetaldeide + NADH +H+etanolo + NAD+\nLa conversione del glucosio ad etanolo è chiamata fermentazione alcolica .\nFigura 6: ciclo di Cori\n70Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#71": "3.6.4 Significato della glicolisi\nFunzione plastica\nLa glicolisi conduce alla formazione di molecole che hanno funzione strutturale.Il diidrossiacetone 3-fosfato, trasformato in glicerolo 3-fosfato, rappresenta ilpunto di partenza della lipogenesi da glucidi. L’acetil CoA avvia la sintesidegli acidi grassi che entrano nella costituzione di fosfolipidi, di glicolipidied esteri del colesterolo.\nFunzione preparatoria\nL’acetil CoA entra nel metabolismo ossidativo terminale.\nFunzione energetica\nLa glicolisi contribuisce alla formazione di ATP. Per ogni mole di glucosioche prende la via glicolitica, si producono 2 moli di acido piruvico, 2 moli diNADH e 2 moli di ATP \n3.7 GLUCONEOGENESI\nLa gluconeogenesi è il processo di sintesi di glucosio a partire da precursori\nnon glucidici, che consente il mantenimento dell’omeostasi glicemica, quandole riserve di glicogeno sono esaurite e/o l’apporto glucidico della dieta è in-sufficiente. La gluconeogenesi avviene nel citoplasma delle cellule epatiche apartire dall’acido piruvico, principalmente in condizioni di digiuno fra un pastoe l’altro (Figura 7).\nReazioni della gluconeogenesi\nIl piruvato (derivato dal lattato o da amminoacidi come l’alanina) è convertitoad ossalacetato dall’enzima piruvato carbossilasi, un enzima mitocondriale\nche richiede biotina ed ATP.L’ossalacetato non può attraversare la membrana mitocondriale, quindi è con-vertito in:Malato ad opera dell’enzima malico deidrogenasi:\nmalico deidrogenasiAc. ossalacetico+NADH+ H+\nmalico deidrogenasiAc. malico + NAD+\n71Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#72": "2) Aspartato ad opera dell’enzima glutammico ossalacetico transaminasi :\nGlutammico-ossalacetico transaminasiAc. ossalacetico + Ac. glutammico \nGlutammico-ossalacetico transaminasiAc. aspartico + Ac.α -chetoglutarico\nMalato ed aspartato attraversano la membrana mitocondriale interna e vengono\nriconvertiti ad ossalacetato nel citoplasma.\n• L’ossalacetato è decarbossilato e fosforilato dall’enzima fosfoenolpiruvato\n(PEP) carbossichinasi a formare fosfoenolpiruvato (PEP) e CO2. E’\nnecessaria energia sotto forma di GTP che viene convertito in GDP e Pi.\n• Il fosfoenolpiruvato va incontro alle tappe inverse della glicolisi formando\nalla fine, fruttosio 1,6-bifosfato.\n• Il fruttosio 1,6-bifosfato è convertito a fruttosio 6-fosfato dall’enzima\nfruttosio 1,6-bifosfatasi , che rilascia fosfato inorganico.\n• Il fruttosio 6-fosfato è convertito a glucosio 6-fosfato dalla stessa isomerasi\nutilizzata nella glicolisi. \n•L a  glucosio 6-fosfatasi , enzima presente solo nel fegato, idrolizza fosfato\ninorganico dal glucosio 6-fosfato, rilasciando glucosio libero nel sangue.\n72Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#73": "La sintesi di glucosio da due moli di acido piruvico richiede energia, fornita\ndall’idrolisi di 6 legami altamente energetici: 4 ottenuti dall’ATP e 2 dal GTP.Nello schema di reazioni soprariportato si intende che 2 moli di acido piruvicoportano alla formazione di 1 mole di fruttosio 1,6-bifosfato.I principali precursori della gluconeogenesi sono:• Acido lattico: proviene dagli eritrociti e dal muscolo in esercizio.\n• Amminoacidi: provengono dal muscolo per degradazione delle proteine\nmuscolari.\n• Glicerolo: deriva dalla degradazione dei trigliceridi del tessuto adiposo\n(Figura 8).\nFigura 7: reazioni che caratterizzano la gluconeogenesi epatica\n73Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#74": "3.7.1 Regolazione della gluconeogenesi\nIn condizioni di digiuno, l’ormone glucagone aumenta e stimola la gluconeo-\ngenesi.\nRegolazione della piruvato deidrogenasi\nIl glucagone stimola il rilascio di acidi grassi dal tessuto adiposo.Gli acidi grassi sono trasferiti al fegato ed ossidati producendo ATP, che pro-voca l’inattivazione dell’enzima piruvato deidrogenasi: il piruvato è così con-vertito ad ossalacetato invece che ad acetil-CoA.Figura 8: substrati utilizzati dal fegato per sintetizzare glucosio attraverso la gluconeogenesi.\n74Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#75": "Regolazione della piruvato chinasi\nIl glucagone provoca la fosforilazione della piruvato chinasi e quindi la suainattivazione: di conseguenza, il fosfoenolpiruvato formato dall’ossalacetatonon è convertito a piruvato, ma prende la via della gluconeogenesi.\nRegolazione della fosfofruttochinasi\nLa fosfofruttochinasi è inattiva poiché la concentrazione del suo inibitore,l’ATP, è elevata: il fruttosio 6-fosfato non è quindi convertito a fruttosio 1,6-bifosfato, bensì isomerizzato a glucosio 6-fosfato.\nRegolazione della glucochinasi \nNelle condizioni che favoriscono la gluconeogenesi, la concentrazione di glu-cosio è bassa: l’enzima è inattivo, dal momento che ha una elevata K\nmper il\nglucosio. \n3.8 Glicogenosintesi\nIl glicogeno, la forma più importante di riserva dei carboidrati, è un polimero\ndi D-glucosio con legami α-1,4 glicosidici, altamente ramificato attraverso\nlegami α-1,6 glicosidici, che si formano in media ogni 10 unità di glucosio\n(Figura 9).\n75Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#76": "La glicogenosintesi è la formazione di glicogeno dal glucosio. Questo processo\navviene nel muscolo e nel fegato dopo un pasto ricco di carboidrati.Le reazioni della glicogenosintesi sono le seguenti:\nglucochinasiGlucosio + ATP Glucosio 6-fosfato + ADP\nfosfoglucomutasiGlucosio 6-fosfato Glucosio 1-fosfato\nUDP-glucosio pirofosforilasiGlucosio 1-fosfato +UTP UDP-glucosio + PPi\npirofosfatasiPPi + H2O 2 Pi\n• Il glucosio entra nella cellula ed è fosforilato a glucosio 6-fosfato dalla\nesochinasi (o glucochinasi nel fegato). \n•L a fosfoglucomutasi converte il glucosio 6-fosfato in glucosio 1-fosfato.\n• Il glucosio 1-fosfato reagisce con UTP (uridina trifosfato), formando UDP-\nglucosio in una reazione catalizzata dalla UDP-glucosio pirofosforilasi. In\nquesta reazione viene rilasciato PPi (pirofosfato inorganico) che viene scissoda una pirofosfatasi in 2 molecole di Pi (fosfato inorganico) (Figura 10).Figura 9: struttura ramificata del glicogeno\n76Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#77": "glicogeno sintasiUDP-glucosio + glicogeno (n residui) \nglicogeno sintasiGlicogeno (n residui+1) + UDP\n• I residui di glucosio vengono trasferiti dall’UDP-glucosio al glicogeno\nprimer dall’enzima glicogeno sintasi (Figura 11). I primers sono molecole\ndi glicogeno parzialmente degradate nel fegato durante le fasi di digiuno e\nnel muscolo durante l’esercizio fisico intenso. Sono costituiti da unaproteina, la glicogenina dotata di attiviità catalitica. \nTale attività permette alla proteina nativa di catalizzare una reazione ditrasferimento del gruppo glicosidico dell’UDP-glucosio ad un suo residuodi tirosina in posizione 194. Così procede unendo ripetitivamente unità diglucosio mediante legame α-1,4 glicosidico fino ad ottenere una catena con\notto residui glicosidici per poi lasciare spazio alla glicogeno sintasi .Figura 10: sintesi dell’UDP-glucosio\n77Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#78": "• Infine, l’UDP viene rilasciato e può essere convertito in UTP mediante grazie\nall’azione di una nucleotide fosfotransferasi che utilizza una molecola di ATP.\nnucleodide difosfochinasiUDP + ATP UTP + ADP\nQuando una catena contiene 11 o più residui di glucosio, un tratto di 6-8 residui\nviene rimosso dalla catena e viene legato con legame α-1,6 ad un residuo di\nglucosio all’interno di una catena con legami α-1,4. Queste ramificazioni sono\nformate dall’ enzima ramificante, che rompe un legame α-1,4 e ne forma uno\ndi tipo α-1,6 (Figura 12).Figura 11: reazione catalizzata dall’enzima glicogeno sintasi\n78Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#79": "I nuovi punti di ramificazione si formano in seguito al trasporto di almeno 7\nunità glucidiche legate da legami α-1,4 da una sequenza di almeno 11 residui,\nper formare un legame α-1,6 con un residuo che disti almeno 4 residui di glu-\ncosio dalla ramificazione già esistente\n3.9 Glicogenolisi\nLa glicogenolisi rappresenta la via di degradazione del glicogeno per ottenere\nglucosio. La glicogenolisi è un processo complesso che prevede il coinvolgi-mento di diversi enzimi. La maggior parte delle molecole di glucosio contenutenel glicogeno viene trasformato in glucosio 1-fosfato grazie all’azione dal-l’enzima glicogeno fosforilasi\nglicogeno fosforilasiGlicogeno (n residui) Glicogeno (n-1 residui)\nPi\nGlucosio 1-PFigura 12: meccanismo di azione dell’enzima ramificante durante la glicogenosintesi.\n79Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#8": "Gluconeogenesi\nGlicogenosintesiGlicogenolisiCiclo dei PentosiMetabolismo Ossidativo terminaleCiclo di Krebs o dell’Acido CitricoCatena respiratoria e formazione di ATP nella fosforilazione ossidativaCenni di Patologia Clinica Bibliografia\nCAPITOLO IV - METABOLISMO DEI GRASSI\nLipidiI triacilgliceroliFosfolipidiDigestione ed assorbimento dei lipidiLe lipoproteineRegolazione della biosintesi di lipidiIl catabolismo dei lipidiSintesi e funzione fisiologica dei corpi chetoniciBibliografia\nCAPITOLO V - METABOLISMO DELLE PROTEINE\nProteine Assunzione dei protidiDigestione delle proteine - Assorbimento di aminoacidi e peptidiBibliografia\nCAPITOLO VI - RIPARTIZIONE DEI MACRONUTRIENTI \nIl concetto di dieta equilibrataIl concetto di dietaCaratteristiche di una dieta varia ed equilibrataDieta e attività fisicaPrincipi generali per impostare un profilo nutrizionale ripartizione dei macro-nutrienti Esempio di schema dieteticoBibliografia\n8Fondamenti della Scienza dell’Alimentazione Indice",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#80": "L’enzima glicogeno fosforilasi rimuove i residui di glucosio, uno alla volta,\ndal glicogeno. Questo enzima utilizza Pi per rompere i legami α-1,4,\nproducendo glucosio 1-fosfato (Figura 13).\nLa fosforilasi può agire solo fino a 4 unità di glucosio da un punto di ramifi-\ncazione• Le 4 unità che rimangono sulla ramificazione sono rimosse dall’ enzima\nderamificante , che ha sia un’attività transferasica che α-1,6 glicosidasica.\n• Tre dei 4 residui che rimangono sul punto di ramificazione sono rimossi\ncome trisaccaride ed attaccati all’estremità di un’altra catena: l’enzima è unatransferasi, che rompe un legame α-1,4 e forma un nuovo legame α-1,4.\n• Un’unità di glucosio rimane legata nel punto di ramificazione con legame\nα-1,6. Questo singolo residuo di glucosio viene idrolizzato dalla α-1,6\nglicosidasi , che forma glucosio libero (Figura 14).Figura 13: meccanismo d’azione dell’enzima glicogeno fosforilasi\n80Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#81": "Le molecole di glucosio 1-fosfato vengono successivamente trasformate in\nglucosio 6-fosfato dall’enzima fosfoglucomutasi. La glicogenolisi avviene\nprincipalmente nel fegato per mantenere costanti i livelli di glucosio ematico.Infatti nel fegato è presente l’enzima glucosio 6-fosfatasi che trasforma il glu-\ncosio 6-fosfato in glucosio. Il glucosio dagli epatociti va nel sangue.\nglucosio 6-fosfatasiGlucosio 6-fosfato Glucosio\nH2OP i\nL’enzima glucosio 6-fosfatasi è presente nel fegato, ma non nel muscolo, per-\ntanto il glicogeno muscolare non può fungere da riserva di glucosio ematico.Nel muscolo il glicogeno è degradato per produrre energia durante la contra-zione muscolare.Anche durante il digiuno prolungato non vengono mai esaurite del tutto le riservedi glicogeno, e la piccola quota rimanente è in grado di funzionare da “primer” perla sintesi di nuovo glicogeno, quando siano nuovamente disponibili carboidrati eso-geni. Nel caso si abbia la completa deplezione del glicogeno, una piccola proteinaglicosilata, la glicogenina , funge da primer per innescare la sintesi del glicogeno.\nFigura 14: meccanismo d’azione dell’enzima deramificante\n81Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#82": "3.9.1 Regolazione delle riserve di glicogeno\nDegradazione del glicogeno\n• Gli ormoni glucagone e adrenalina attivano l’enzima adenilato ciclasi della\nmembrana cellulare, che converte l’ATP in cAMP.\n• Il cAMP attiva una proteina chinasi (proteina chinasi A).\n• La proteina chinasi A fosforila la glicogeno sintasi ,causandone\nl’inattivazione ed impedendo la sintesi del glicogeno.\n• La stessa proteina chinasi A fosforila la fosforilasi chinasi , attivandola;\nquesto enzima a sua volta fosforila la fosforilasi b, che si converte nella sua\nforma attiva o fosforilasi a aumentando la degradazione del glicogeno.\nSintesi del glicogenoL’ormone insulina, la cui concentrazione è elevata dopo un pasto, stimola lasintesi di glicogeno sia nel fegato che nel muscolo.In stato di alimentazione, il livello di glucagone è basso ed il meccanismocAMP (AMP ciclico) dipendente non viene attivato.• Il cAMP è convertito ad AMP da una fosfodiesterasi .\n• Appena il cAMP diminuisce, viene inattivata la proteina chinasi.\n• Ne consegue la defosforilazione della fosforilasi chinasi e della fosforilasi\na, che diventano inattive.\n• L’insulina attiva le fosfatasi che defosforilano questi enzimi.\n• La fosfatasi defosforila anche la glicogeno sintasi e l’enzima diventa attivo.\nIl processo attivato dal cAMP è un meccanismo “a cascata” in cui il segnaleormonale originario è amplificato più volte.Una molecola di ormone, attraverso l’attivazione dell’enzima adenilato ciclasi,produce molte molecole di cAMP, che attivano l’enzima proteina chinasi.Una molecola attiva di proteina chinasi fosforila molte molecole di fosforilasichinasi, che convertono la fosforilasi b in fosforilasi a.Una molecola di fosforilasi a produce molte molecole di glucosio 1-fosfatodal glicogeno.Il risultato netto è che una molecola di ormone può generare decine di migliaiadi molecole di glucosio 1-fosfato.Nel fegato, il glucagone (durante il digiuno) e l’adrenalina (durante l’eserciziofisico) stimolano la degradazione del glicogeno. Il glucosio libero che vieneprodotto è utilizzato per mantenere i livelli di glucosio ematico.Nel muscolo, l’adrenalina stimola la lisi del glicogeno a glucosio 1-fosfato,poi convertito a glucosio 6-fosfato, che entra nella glicolisi e genera ATP per\n82Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#83": "la contrazione muscolare. Il muscolo è privo dell’enzima glucosio 6-fosfatasi,\nquindi non produce glucosio libero per contribuire al mantenimento della gli-cemia.Il muscolo non degrada glicogeno in risposta al glucagone.\n3.10 Ciclo del Pentosio Fosfato\nAccanto alla glicolisi sono state descritte altre vie di demolizione del glucosio.\nLa più importante è il ciclo dei pentosi o shunt dell’esoso monofosfato : avviene\nnel citoplasma cellulare e riveste particolare importanza per il fegato, tessutoadiposo, rene, eritrociti.E’ una via multifunzionale e si divide in:- Parte ossidativa ed irreversibile- Parte non-ossidativa e reversibile\nParte ossidativa ed irreversibile del ciclo\nDa glucosio 6-fosfato si formano NADPH e ribulosio 5-fosfato (Figura 15).\nFigura 15: parte ossidativa del ciclo del pentosio fosfato\n83Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#84": "• Il glucosio 6-fosfato è convertito a 6-fosfogluconolattone e il NADP+è\nridotto a NADPH + H+. Enzima: glucosio 6-fosfato deidrogenasi .\n• Il 6-fosfogluconolattone è idrolizzato a 6-fosfogluconato. Enzima:\ngluconolattonasi .\n• Il 6-fosfogluconato è decarbossilato ossidativamente. Si libera CO2e si\nforma un secondo NADPH + H+dal NADP+; gli altri atomi di carbonio\nrestano come ribulosio 5-fosfato. Enzima: 6-fosfogluconato deidrogenasi .\nNADPH viene utilizzato come donatore di idrogeno ed elettroni nelle biosin-\ntesi di acidi grassi e del colesterolo.\nIl ribulosio 5-fosfato fornisce il ribosio 5-fosfato per la sintesi dei nucleotidi\nnecessari per la formazione di RNA e DNA.Nelle reazioni ossidative, il glucosio 6-fosfato è decarbossilato ossidativa-mente, viene rilasciata CO\n2, si generano NADPH e ribulosio 5-fosfato.\nParte non ossidativa e reversibileDa ribulosio 5-fosfato si formano altri zuccheri a 5 atomi di carbonio che pos-sono essere convertiti in intermedi della via glicolitica.Nelle reazioni non ossidative il ribulosio 5-fosfato, prodotto nelle reazioni os-sidative, è convertito nel suo isomero ribosio 5-fosfato, che può essere usato\nper la sintesi dei nucleotidi. Il ribulosio 5-fosfato può essere convertito anchenel suo epimero xilulosio 5-fosfato .\nIl ribosio 5-fosfato e lo xilulosio 5-fosfato possono andare incontro ad una seriedi reazioni catalizzate dagli enzimi transchetolasi e transaldolasi, che trasferi-\nscono i carboni da un composto all’altro. Alla fine si formano due intermedidella glicolisi: il fruttosio 6-fosfato e la gliceraldeide 3-fosfato (Figura 16).\n84Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#85": "Il ribulosio 5-fosfato può essere isomerizzato a ribosio 5-fosfato. Enzima:\nfosforiboisomerasi.\n• Il ribulosio 5-fosfato può essere epimerizzato a xilulosio 5-fosfato. Enzima:\nribulosio fosfato epimerasi .\n• Un’unità a due atomi di carbonio può essere trasferita dallo xilulosio 5-fosfato\nal ribosio 5-fosfato per formare sedoeptulosio 7-fosfato. La gliceraldeide 3-fosfato si forma dai restanti atomi di carbonio dello xilulosio 5-fosfato. Enzima:\ntranschetolasi ; richiede tiaminapirofosfato come coenzima.\n• Un’unità a tre atomi di carbonio può essere trasferita dal sedoeptulosio 7-\nfosfato alla gliceraldeide 3-fosfato, formando fruttosio 6-fosfato. I quattrorestanti atomi di carbonio del sedoeptulosio 7-fosfato formano eritrosio 4-fosfato. Enzima: transaldolasi.\n• Un’unità a due atomi di carbonio può essere trasferita dallo xilulosio 5-\nfosfato all’eritrosio 4-fosfato a formare un’altra molecola di fruttosio6-fosfato. Gliceraldeide 3-fosfato si forma dai restanti atomi di carbonioFigura 16: reazioni non ossidative del ciclo dei PPP\n85Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#86": "dello xilulosio 5-fosfato. Enzima: transchetolasi.\nSignificato del ciclo dei pentosi\nDal ciclo dei pentosi si ha:\n✓Formazione di NADPH per le biosintesi di acidi grassi \n✓Biosintesi di pentosi e produzione di nucleotidi \n✓Ossidazione terminale con produzione di energia attraverso la formazionedi intermedi della via glicolitica \n3.11 Metabolismo Ossidativo Terminale\n3.11.1 Ciclo di Krebs o dell’Acido CitricoIn condizioni di aerobiosi, l’acido piruvico prodotto durante la glicolisi entra\nnel mitocondrio per essere completamente ossidato attraverso il ciclo di Krebs.Una volta entrato nel mitocondrio, l’acido piruvico subisce una decarbossila-zione ossidativa ad opera del complesso multienzimatico della piruvato dei-\ndrogenasi per formare Acetil CoA.\nTale complesso multienzimatico è costituito da 3 enzimi ognuno dei quali èaffiancato da un coenzima:\nTale complesso catalizza la reazione:                                                    \nPiruvato deidrogenasiPiruvato + CoA + NAD+\nPiruvato deidrogenasiAcetil CoA + CO2+ NADH + H+\nTale reazione (irreversibile) avviene nella matrice mitocondriale e rappresenta\nil punto di unione fra la glicolisi ed il ciclo di Krebs.Il complesso multienzimatico ha la seguente regolazione:\nInibizione da prodotto Acetil CoA ed NADH\nRegolazione a feedback GTP (inattivatore), AMP (attivatore)\nRegolazione per modificazione covalente Fosforilazione da parte di ATPEnzima Coenzima\nPiruvato deidrogenasi TPP (tiamina pirofosfato)\nDiidrolipoil transacetilasi Lipoamide\nDiidrolipoil deidrogenasi F AD\n86Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#87": "Il ciclo di Krebs, la catena di trasporto degli elettroni e la fosforilazione ossi-\ndativa costituiscono il metabolismo ossidativo terminale, nel quale i prodottidel metabolismo intermedio vengono completamente bruciati a CO\n2ed H2O.\nLa glicolisi, la β-ossidazione, la degradazione di numerosi amminoacidi,\nhanno come prodotto finale l’acetil CoA: questa molecola è essenziale perl’inizio del ciclo di Krebs, reagisce con l’acido ossalacetico ed entra nel ciclo.Alla fine del ciclo di reazioni, l’acido ossalacetico è rigenerato e può riprenderela sequenza di reazioni con un’altra molecola di acetil CoA.Durante il ciclo, l’acetil CoA è ossidato ad anidride carbonica, e contempora-neamente sono prodotti NADH + H\n+e FADH2.\nReazioni del ciclo di KrebsIl ciclo di Krebs consiste in 8 reazioni sequenziali, catalizzare da altrettantienzimi o complessi multienzimatici (Figura 17)• Acetil CoA si condensa con ossalacetato e si forma citrato. Enzima: citrato\nsintasi (1).\n• Il citrato è isomerizzato a isocitrato tramite un riarrangiamento della\nmolecola. Enzima: isocitrato isomerasi o aconitasi (2).\n• La prima decarbossilazione ossidativa avviene quando l’isocitrato è ossidato\na α-chetoglutarato. Si produce una molecola di CO\n2, NAD+è convertito in\nNADH + H+. Enzima: isocitrato deidrogenasi (3). L’H+prodotto è utilizzato\nnella stessa reazione per la decarbossilazione dell’intermedio ossalsuccinato.La stechiometria globale di questa reazione non vede quindi la produzionedi ioni H\n+.\n•L ’α-chetoglutarato è convertito a succinil CoA nella seconda\ndecarbossilazione ossidativa. Si produce una molecola di CO2, NAD+è\nconvertito in NADH + H+. Enzima: α-chetoglutarato deidrogenasi (4).\n• Il legame tioestere altamente energetico del succinilCoA è scisso e rilascia\nl’energia necessaria per la sintesi di GTP da GDP e fosfato inorganico (Pi);si forma succinato. Enzima: succinil CoA sintetasi (5).\n• Il succinato è ossidato ed il FAD diventa FADH\n2. Si forma fumarato.\nEnzima: succinato deidrogenasi (6).\n• L’aggiunta di una molecola di H2O scinde il doppio legame del fumarato e\nsi forma malato. Enzima: fumarasi (7).\n• Il malato è ossidato con trasformazione del NAD+in NADH + H+. Si\nrigenera l’ossalacetato e così si completa il ciclo. Enzima: malato\ndeidrogenasi (8).\n87Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#88": "Regolazione del ciclo di Krebs\nIl ciclo è regolato dalle necessità energetiche della cellula, cioè dalla concen-trazione di ATP.Quando la cellula richiede energia, vengono accelerate le reazioni della catenadi trasporto degli elettroni. NADH viene rapidamente ossidato ed aumenta lavelocità del ciclo di Krebs.Quando la concentrazione di ATP è elevata, diminuisce la velocità della catenadi trasporto degli elettroni, aumenta la concentrazione di NADH e viene inibitoil ciclo di Krebs.NADH + H\n+ed FADH2entrano nella catena di trasporto degli elettroni loca-\nlizzata nella membrana interna del mitocondrio.Figura 17: ciclo di Krebs\n88Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#89": "La reazione globale per il ciclo di Krebs è:\nAcetilCoA+3 NAD++F AD+GDP+Pi+2H2O \n2CO2+CoA+3NADH+2H++F ADH2+GTP\nCome tutti i fenomeni ossidativi anche il ciclo di Krebs libera una certa quan-\ntità di energia recuperata sotto forma di ATP: si ha la formazione diretta di\nATP(GTP) solo nella conversione di succinil CoA ad acido succinico. Il restodell’ATP si forma tramite la fosforilazione ossidativa con formazione di 2,5di ATP (o 1,5 per il F AD), per ogni mole di NAD+ (o F AD) ridotto. In totale si formano per ogni ciclo 9 moli di ATP tramite la fosforilazione os-sidativa ed 1 direttamente dal ciclo. Per ogni mole di acetil CoA degradata aCO2 ed H2O si producono quindi 10 moli di ATP .\nFunzione biosintetica del ciclo di Krebs\n- Alcuni metaboliti del ciclo di Krebs possono dare origine ad amminoacidi:\na) l’acido α-chetoglutarico dà origine all’acido glutammico per transamina-\nzione e alla glutammina\nb)il succinil CoA dà origine alla metionina, lisina, treonina e leucinac) l’acido ossalacetico dà origine all’acido aspartico per transaminazione e\nall’asparagina\n- Quando l’acido citrico (che si ottiene dalla condensazione di acido\nossalacetico ed acetil CoA) è in eccesso (per esempio dopo un pasto moltoricco di carboidrati), fuoriesce dai mitocondri e nel citosol produce acetilCoA, dal quale poi inizia la sintesi di acidi grassi.\n- Gli intermedi del ciclo di Krebs devono essere sostituiti se sottratti per le\nbiosintesi. Se, per esempio, l’acido ossalacetico è convertito in unamminoacido per la sintesi proteica, allora nuovo acido ossalacetico siformerà da acido piruvico. La reazione, catalizzata dalla piruvico\ncarbossilasi, è la seguente:\nPiruvato carbossilasiAc. piruvico + CO2+ATP \nPiruvato carbossilasiAc. ossalacetico + ADP + Pi\n89Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#9": "CAPITOLO VII – V ALUTAZIONE DELLO STATO NUTRIZIONALE \nLa valutazione dello stato nutrizionaleComposizione corporeaIndicatori antropometrici dello stato nutrizionale Plicometria Bioimpedenziometria Bibliografia\nCAPITOLO VIII – FABBISOGNO ENERGETICO\nMetabolismo basale (M.B.) CalorimetriaCalorimetria diretta Calorimetria indirettaTermogenesi indotta dalla dieta (TID) Costo energetico attività fisica (LAF) Bibliografia\nCAPITOLO IX - STRUMENTI DI INDAGINE ALIMENTARE\nStrumenti di indagine alimentareDiario di registrazione degli alimenti Il ricordo delle 24 oreIl questionario di frequenza degli alimentiStrumenti brevi per valutare la dietaStoria dieteticaElaborazione dei datiBibliografia\nCAPITOLO X - L.A.R.N. E LINEE GUIDA \nAdeguatezza nutrizionale della dietaI livelli di assunzione di riferimento di nutrienti e energia per la popolazioneitalianaCome si usano i LARNL’evoluzione delle linee guida per una sana alimentazioneAttualità e modernità delle linee guida per una sana alimentazione: dalle ca-renze nutrizionali alla alimentazione come prevenzione.Perché le revisioni periodiche delle Linee Guida?Le Linee Guida nei moderni strumenti di comunicazioneConclusioniBibliografia\n9Indice Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#90": "3.11.2 Catena respiratoria e formazione di ATP nella fosforilazione ossi-\ndativa\nNADH+ H+e FADH2formati nella glicolisi, nell’ossidazione degli acidi grassi\ne nel ciclo di Krebs, sono utilizzati per produrre ATP.\nGli elettroni sono trasferiti da NADH+H+e FADH2all’ossigeno molecolare\nattraverso un sistema di trasporto di elettroni, la catena respiratoria, stretta-\nmente associata alla fosforilazione ossidativa, che è il principale processo diformazione di ATP.La catena di trasporto degli elettroni è localizzata nella membrana internadei mitocondri ( Figura 18).\nCatena di trasporto degli elettroni.L’ossigeno molecolare è trasportato alle cellule dall’emoglobina attraverso icapillari sanguigni.La catena respiratoria è composta da quattro complessi enzimatici:\n- NADH-Q reduttasi\n- F ADH\n2reduttasi\n- Citocromo c reduttasi- Citocromo c ossidasi\nOgnuno dei complessi sfrutta l’energia derivante dal trasporto degli elettroni\nper pompare protoni nello spazio intermembrana (Figura 19).Figura 18: struttura dei un mitocondrio\n90Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#91": "La teoria chemiosmotica spiega la fosforilazione ossidativa mediante un mo-\nvimento di protoni.\nI principi di questa teoria sono:- La membrana mitocondriale interna è impermeabile agli ioni , in particolare\nagli ioni H\n+, che si accumulano all’esterno della membrana, causando una\ndifferenza di potenziale elettrochimico attraverso la membrana stessa\n- La differenza di potenziale attiva l’enzima di membrana ATP sintasi- I protoni possono rientrare nella matrice solo attraverso il complesso\ndell’ATP sintasi, dove viene generato ATP\nDurante il trasporto degli elettroni attraverso la catena respiratoria una partedell’energia viene dispersa sotto forma di calore.\nTrasporto di elettroni da NADH a O\n2\n• Gli elettroni del NADH vengono trasferiti al flavin mononucleotide (FMN)\npresente nel complesso della NADH-Q reduttasi (Complesso I).\n• FMN trasferisce i suoi elettroni al coenzima Q, tramite una serie di proteine\ncontenenti ferro-zolfo.\nL’energia prodotta da questo movimento di elettroni è utilizzata per pomparequattro protoni nello spazio intermembrana attraverso il Complesso I; con-temporaneamente si genera un potenziale elettrochimico.\n✓Gli elettroni sono trasferiti dal coenzima Q ai citocromi b e c1facenti parte\ndel complesso citocromo c reduttasi (Complesso III).Figura 19: catena di trasporto degli elettroni mitocondriale\n91Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#92": "✓Il citocromo c1cede gli elettroni al citocromo c.\nL’energia prodotta nel trasporto degli elettroni dal coenzima Q al citocromo\nc è utilizzata per pompare altri quattro protoni nello spazio intermembrana;contemporaneamente si genera un potenziale elettrochimico.\n✓Il citocromo c trasferisce gli elettroni al complesso citocromo c ossidasi(Complesso IV) costituito dai citocromi a e a\n3ed altri due protoni vengono\npompati nello spazio intermembrana.Il citocromo a\n3cede gli elettroni all’ossigeno molecolare che viene ridotto\nad H2O.\nL’energia prodotta dal trasferimento degli elettroni dal citocromo c all’ossigenomolecolare è utilizzata per pompare protoni nello spazio intermembrana; con-temporaneamente si genera un potenziale elettrochimico. L’ossidazione di unamole di NADH porta alla formazione di 2,5 moli di ATP.\nTrasferimento degli elettroni da FADH\n2\nLa rimozione degli elettroni da FADH2avviene in due tappe:\n✓FADH2trasferisce i suoi elettroni ad un centro ferro-zolfo (Complesso II),\nche a sua volta li cede al coenzima Q.\n✓Gli elettroni sono poi trasferiti al complesso citocromo c reduttasi, citocromoc ossidasi e all’ossigeno molecolare come per NADH.\n✓Dal momento che gli elettroni provenienti dal FADH2passano direttamente\nal coenzima Q (e non al complesso NADH-Q reduttasi), si producono solo1,5 moli ATP quando FADH2 si ossida.\nAgenti disaccoppiantiLo stretto accoppiamento che esiste fra il trasporto degli elettroni e la fosfori-lazione ossidativa nei mitocondri può essere rotto dal 2,4-dinitrofenolo e daalcuni altri composti aromatici acidi che trasportano protoni attraverso la mem-brana mitocondriale interna. In presenza di questi agenti disaccoppianti , il tra-\nsporto degli elettroni d NADH e FADH\n2all’ossigeno molecolare procede\nnormalmente, ma non si forma ATP ad opera dell’ATPasi mitocondriale, poi-ché la forza motrice protonica attraverso la membrana mitocondriale internaviene dissipata. Il disaccoppiamento della fosforilazione ossidativa può esserebiologicamente importante: ha la funzione di produrre calore per mantenerecostante la temperatura del corpo in animali in ibernazione ed in alcuni animaliappena nati incluso l’uomo. Il tessuto adiposo bruno, che è molto ricco di mi-tocondri, è specializzato in questo processo di termogenesi e la proteina di-saccoppiante si chiama termogenina.\nAgenti disaccoppianti sono: anestetici, antibiotici, coloranti, tossine batteriche,\n92Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#93": "ormoni come la tiroxina.\nEnergia generata per ossidazione del glucosio in CO2ed H2O\nQuando il glucosio viene ossidato completamente a CO2ed H2O, si generano\nin totale 30 o 32 ATP (Figura 20).\n• Due moli nette di ATP si formano dalla conversione di una mole di glucosio\na due moli di piruvato.\n• Le due moli di piruvato entrano nel mitocondrio e sono convertite in due moli\ndi acetil CoA, grazie all’azione dell’enzima piruvato deidrogenasi, producendodue moli di NADH, che danno 5 ATP per fosforilazione ossidativa.\n• Le due moli di acetil CoA sono ossidate nel ciclo di Krebs, generando un\ntotale di 20 moli di ATP.\n• Altro ATP (3 o 5 molecole) viene prodotto dalla riossidazione del NADH+H\n+\ngenerato nel citoplasma durante la glicolisi.\n• NADH+H+non può attraversare la membrana interna del mitocondrio,\nquindi gli elettroni sono trasferiti alla catena respiratoria mitocondrialeattraverso due sistemi navetta (shuttle).\nFigura 20: ATP generato attraverso la completa ossidazione del glucosio\n93Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#94": "3.12 Cenni di Patologia Clinica \nCarenza di Lattasi Intestinale\nLa lattasi è generalmente presente nei neonati e nei bambini, che sono quindi ingrado di scindere il lattosio presente nel latte e di assorbire, a livello intestinale,galattosio e glucosio. Con il progredire dello sviluppo, l’espressione e l’attivitàdella lattasi inizia a diminuire nella maggior parte delle persone intorno ai 2 annidi vita (non vi sono invece differenze significative di incidenza fra i due sessi)con una riduzione progressiva geneticamente programmata. I sintomi di intol-leranza al lattosio raramente si sviluppano prima dei 6 anni. Generalmente sonomantenuti a livelli efficaci di lattasi soltanto nelle popolazioni caucasiche, a con-dizione che si continui a inserire nella propria alimentazione latte o in generalealimenti che contengano lattosio. Il lattosio non digerito è ossidato dalla florabatterica intestinale producendo meteorismo addominale e diarrea.\nGalattosemia\nLa presenza di elevate concentrazioni di galattosio nel sangue può essere dovutaad una deficienza dell’enzima galattosio 1-fosfato uridil-trasferasi (enzima delmetabolismo del galattosio), che impedisce la conversione del galattosio in glu-cosio; galattosio e galattosio 1-fosfato si accumulano nei tessuti. L’eccesso digalattosio è ridotto a galattitolo, che può provocare cataratta; il galattosio 1-fo-sfato interferisce con il metabolismo del glucosio causando gravi danni ai variorgani e tessuti. Una forma più lieve di galattosemia è dovuta alla carenza del-l’enzima galattochinasi (enzima del metabolismo del galattosio).\nCarenza di piruvato chinasi\nLa carenza dell’enzima piruvato chinasi causa una diminuzione della produ-zione di ATP durante la glicolisi. I globuli rossi, che utilizzano come fonteenergetica la glicolisi, non hanno più a disposizione ATP per la pompa delsodio, perciò le membrane si lisano facilmente causando anemia emolitica.\nIntolleranza al fruttosio\nL’intolleranza al fruttosio si verifica in seguito a carenza dell’enzima fruttosio1-fosfato aldolasi (enzima del metabolismo del fruttosio). Il fruttosio 1-fosfatosi accumula e non si forma più glucosio, causando una severa ipoglicemia.Quando è carente l’enzima fruttochinasi (enzima del metabolismo del frutto-sio), il fruttosio non può essere metabolizzato velocemente, quindi aumenta alivello ematico ed è riscontrabile nelle urine.\n94Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#95": "Carenza di glucosio 6-fosfato deidrogenasi\nL’enzima glucosio 6-fosfato deidrogenasi catalizza la trasformazione del glu-cosio 6-fosfato in 6-fosfogluconolattone con concomitante produzione diNADPH. La carenza di questo enzima comporta un’insufficiente produzionedi NADPH, molecola importante per il mantenimento allo stato ridotto delglutatione. L’incapacità della cellula a riconvertire il glutatione ossidato inglutatione ridotto, comporta un danno ossidativo con lisi dei globuli rossi (cel-lule molto sensibili al danno ossidativo, perché non possono sintetizzare nuovienzimi al posto di quelli danneggiati, in quanto sono privi di nucleo) ed anemiaemolitica.\nMalattie del metabolismo del glicogeno (glicogenosi)\nSi tratta di malattie dismetaboliche in cui si ha un accumulo di glicogeno nel fe-gato o nel muscolo. Le carenze enzimatiche causa dell’accumulo sono soprattuttoa carico della degradazione del glicogeno e della sua conversione in glucosio.\nMalattia di McArdle\nUn difetto del metabolismo del glicogeno confinato al muscolo si realizzanella malattia di McArdle. L’enzima glicogeno fosforilasi è assente ed il pa-ziente non è in grado di mobilizzare il glicogeno a scopo energetico, quindilimitata è la capacità di svolgere esercizi fisici a causa di dolorosi crampi mu-scolari.\nMalattia di Von Gierke\nQuesta malattia è caratterizzata da una deficienza genetica dell’enzima glu-cosio 6-fosfatasi. Il fegato non è in grado di convertire il glucosio 6-fosfato inglucosio per rilasciarlo nel sangue. Nel fegato aumenta la concentrazione diglucosio 6-fosfato, la glicogeno fosforilasi è inibita ed è attivata la glicogenosintasi; di conseguenza si ha accumulo di glicogeno nel fegato ed epatomegaliamassiva.L’assenza di glucosio 6-fosfatasi determina ipoglicemia, in quanto non si puòformare glucosio da glucosio 6-fosfato. La malattia può essere curata con spe-cifici medicinali che inibiscono l’ingresso di glucosio negli epatociti, anchese nella maggioranza dei casi è necessario un trapianto di fegato.\nMalattia di Cori \nLa malattia di Cori è caratterizzata da una deficienza dell’enzima deramifi-cante α-1,6 glicosidasi. Come risultato, il muscolo ed il fegato accumulano\n95Capitolo III Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#96": "glicogeno con ramificazioni esterne molto corte ed il glicogeno non può essere\ncompletamente degradato. Questi pazienti tendono ad essere ipoglicemici. Lamalattia di Cori è trattata mediante un controllo della dieta, aumentando lafrequenza dei pasti e riducendone la quantità; l’intento è quello di diminuireil deposito di glicogeno.Sono conosciute anche altre malattie genetiche che riguardano il metabolismodel glicogeno (malattia di Andersen, malattia di Pompe, malattia di Hers), masono molto rare.\nDiabete Mellito           \nIl diabete mellito rappresenta un gruppo di disordini metabolici caratterizzatida un’iperglicemia cronica. E’ una patologia molto frequente che colpisce circa30 milioni di persone nel mondo. Il diabete mellito è una malattia irreversibilee sebbene i pazienti abbiano un normale stile di vita, a lungo termine si pos-sono verificare complicanze come malattie vascolari, retinopatie, nefropatie.Esistono due tipi di diabete:1. diabete di tipo 1 (10% dei pazienti diabetici)2. diabete di tipo 2 (90% dei pazienti diabetici)Il diabete di tipo 1 colpisce soprattutto i giovani adolescenti ed è dovuto alladistruzione autoimmune delle cellule beta delle isole di Langerhans. La causascatenante le reazioni autoimmunitarie è ancora sconosciuta, ma potrebbe es-sere un’infezione virale.Il diabete di tipo 1 sembra essere ereditario anche se i geni responsabili nonsono ancora stati completamente scoperti.I pazienti affetti da diabete di tipo 1 non sono in grado di produrre insulina equindi la devono introdurre dall’esterno.Il diabete di tipo 2 è molto comune e colpisce normalmente gli adulti; la ma-lattia può essere presente in forma subclinica per molti anni prima della dia-gnosi e l’incidenza aumenta notevolmente con l’età ed il grado di obesità.La maggioranza dei pazienti presenta una ridotta secrezione di insulina a causadi un danno alla funzionalità delle cellule beta, oppure una resistenza all’insulinastessa. Questa forma di diabete potrebbe avere una forte componente ereditaria,anche se ancora non sono stati identificati i geni responsabili della malattia.La carenza di insulina determina notevoli variazioni metaboliche; in primoluogo l’organismo non è in grado di utilizzare completamente i carboidrati in-trodotti con la dieta, quindi produce glucosio ematico sfruttando le riserve diglicogeno; i tessuti periferici, come il muscolo, assorbono una quantità minoredi glucosio. Per risparmiare glucosio, l’organismo utilizza i trigliceridi come\n96Fondamenti della Scienza dell’Alimentazione Capitolo III",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#97": "fonte energetica, con conseguente accumulo dei corpi chetonici: si instaura\nuno stato di acidosi metabolica associata a vomito e diarrea. Il fiato di questipazienti presenta il tipico odore di acetone. L’eccesso dei corpi chetonici èescreto con le urine.\nIpoglicemie\nSi parla di ipoglicemia quando in condizioni di digiuno, il livello del glucosioematico si abbassa sotto i 40-50 mg/dl.\nIpoglicemie esogene\nLa più comune ipoglicemia esogena è legata alla somministrazione di dosi ecces-sive di insulina o di farmaci orali ipoglicemizzanti nei soggetti affetti da diabete.Può verificarsi ipoglicemia dopo digiuno prolungato, soprattutto in seguito adun eccessivo esercizio fisico. Anche un’elevata ingestione di alcolici può pro-vocare ipoglicemia, perché aumentano nel fegato i livelli di NADH, che ini-biscono la gluconeogenesi.\nIpoglicemie endogene\nL’ipoglicemia endogena può derivare da un elevato assorbimento cellulare diglucosio, da alterazioni delle vie metaboliche della glicogenolisi e gluconeo-genesi, o da un’eccessiva secrezione di insulina (tumori benigni o malignidelle cellule beta delle isole di Langerhans chiamati insulinomi). Il tessutonervoso è il più vulnerabile all’ipoglicemia, poiché tale tessuto necessaria-mente utilizza glucosio ematico, non essendo in grado di sintetizzare glucosioo di immagazzinarlo in modo significativo.\nBibliografia\n• D. V oet, J.G. V oet, C.W. Pratt. Fondamenti di Biochimica. Edizioni Zanichelli. 3 Ed. 1200\npp. 2013\n• E. Marinello , R. Pagani. Elementi di Biochimica Medica. Edizioni ETS. 468 pp. 2009\n• G. Zubay, W.W. Parson, D.E. Vance. Principles of Biochemistry. Edizioni McGraw-Hill\nEducation, 992 pp. 1995\n• P. Kumar, M. Clarks. Clinical Medicine, Edizioni Elsevier Saunders. 8 Ed. 1304 pp. 2012• W.D. McArdle, F.I. Katch, V .L. Katch. Sports and Exercise Nutrition. Edizioni Lippincott\nRaven. 4 ed. 681 pp. 2012\n97Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#98": "CAPITOLO IV\nI LIPIDI ED IL METABOLISMO LIPIDICO\nRiassunto\nI lipidi sono composti molto importanti per l’uomo e comprendono differenti\nclassi di molecole, diverse tra loro per struttura chimica, abbondanza,distribuzione e funzione. Tra i lipidi più abbondanti ricordiamo i fosfolipidi,costituenti essenziali delle membrane biologiche ed i triglicilgliceroli, cherappresentano, senza dubbio, la principale riserva energetica dell’uomo. Altriancora, seppur meno abbondanti, svolgono una funzione ormonale, agisconoda agenti emulsionanti, da messaggeri o regolatori della fluidità dellemembrane cellulari. La maggior parte dei lipidi vengono assorbiti con glialimenti, veicolati nell’organismo tramite le lipoproteine ed accumulati negliadipociti. Tuttavia, in condizioni di surplus calorico, essi possono esseregenerati anche per sintesi endogena, partendo per esempio dai carboidrati ineccesso. In caso di necessità il nostro organismo è in grado di mobilizzare ilipidi dal tessuto adiposo e distribuirli alle cellule, le quali, metabolizzandoli,possono ricavare da essi una notevole quantità di energia. Sintesi edegradazione dei lipidi sono processi strettamente regolati dall’azione diormoni quali insulina, glucagone e adrenalina. \n98Fondamenti della Scienza dell’Alimentazione Capitolo IV",
    "data_test\\rootfolder\\varie\\Alimentazione\\Fond_Scienza_Aliment.pdf#99": "4.1 I lipidi \nI lipidi sono generalmente molecole organiche fortemente idrofobiche e quindiinsolubili in solventi acquosi. La classe dei lipidi comprende molecolestrutturalmente molto diverse tra loro quali i triacilgliceroli, le cere, gli steroli,i fosfolipidi e i glicolipidi. Nel nostro organismo i lipidi adempiono a differentifunzioni: rappresentano la principale riserva energetica dell’uomo, agisconoda isolanti termici, forniscono protezione meccanica, sono costituentiessenziali delle membrane cellulari, agiscono da molecole segnale, sonoprecursori di numerosi ormoni, e ottimi combustibili per la produzione dienergia.\n4.2 Gli acidi grassi\nIl carattere idrofobico dei lipidi è legato alla presenza, nella loro struttura,degli acidi grassi. Gli acidi grassi sono molecole sostanzialmente apolari,costituite da catene idrocarburiche di varia lunghezza e differente grado diinsaturazione. La loro formula generica è riportata nella Figura 1:\nCH\n3-(CH2)n-CH2-COO-\nFigura 1: formula di struttura di un acido grasso\nLa maggior parte degli acidi grassi presenti in natura contiene un numero paridi atomi di carbonio, compreso tra 16 e 22-24, ed un numero di doppi legamivariabile generalmente tra zero e quattro. Gli acidi grassi che non contengonodoppi legami vengono definiti saturi, quelli con un doppio legame,monoinsaturi e quelli con un numero maggiore di doppi legami, vengonodefiniti polinsaturi (PolyUnsaturated Fatty Acids, PUFA). Uno degli acidigrassi più diffuso negli alimenti è l’acido oleico, un acido grasso monoinsaturocostituito da 18 atomi di carbonio. Il doppio legame presente negli acidi grassiinsaturi può assumere configurazione “cis” o “trans”, a seconda che i dueatomi di idrogeno che si legano ai due atomi di carbonio, giacciano o menosullo stesso piano (vedi Figura 2). In natura la maggior parte degli acidi grassiinsaturi presentano doppi legami nella configurazione “cis” mentre quelli conconfigurazione “trans” sono più rari. Per esempio la carne bovina ed ovina, iformaggi ed il latte non contengono più del 2-8% di acidi grassi nellaconfigurazione “trans”.\n99Capitolo IV Fondamenti della Scienza dell’Alimentazione",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#0": "Diagrammi a Blocchi",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#1": "Cos’è un diagramma a blocchi?Il diagramma a blocchi (diagramma di flusso o flow chart) è uno schema a blocchi utilizzato per rappresentare gli algoritmi.\nSi tratta di una rappresentazione grafica che utilizza delle forme geometriche per descrivere gli algoritmi.",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#10": "Cos’è una variabile?Le variabili sono aree di memoria RAM dove vengono memorizzati i dati e che possono essere cambiati durante l’esecuzione di un’applicazione.\nLe costanti invece contengono un valore non modificabile.\nPer entrambe è opportuno dare dei nomi sensati, non troppo lunghi e non separati da spazi.",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#11": "Esercizi struttura sequenziale (1)Eseguire il prodotto tra due numeri;\nCalcolare l'ipotenusa date le misure dei cateti di un triangolo rettangolo\nDate 2 variabili, scambiarne il contenuto;\nCalcolare il numero minimo di banconote per un importo in euro, tenendo conto dei diversi tagli da 500, 200, 100, 50, 20, 10, 5 euro.\n\n",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#12": "Esercizi struttura condizionale (1)",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#13": "Esercizi struttura iterativa (1)",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#14": "Esercizi vettori (1)Caricamento di 10 numeri in un vettore;\nSomma degli elementi di un vettore;\nRicerca di un valore all’interno di un vettore.",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#2": "Cos’è un algoritmo? A cosa serve?Per algoritmo si intende una successione di passi (o istruzioni) che definiscono le operazioni da eseguire sui dati per ottenere i risultati.\nEsempi di algoritmi ne troviamo tantissimi, anche nella vita di tutti i giorni. Tipicamente è necessario un algoritmo a fronte di un problema, come ad esempio: andare a scuola; per risolvere questo problema dobbiamo seguire una sequenza ordinata e finita di passi (algoritmo), come ad esempio:\nSvegliarsi  Fare colazione  Vestirsi  Uscire di casa  Prendere l’autobus  Entrare in classe\nQuindi, l’insieme dei passi che consentono di risolvere un problema prende nome di algoritmo.",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#3": "Come si descrive un algoritmo?Ci sono tanti modi per rappresentare un algoritmo, un metodo molto utilizzato è quello basato sui diagrammi a blocchi, conosciuti anche con il nome di flow chart (letteralmente diagrammi di flusso).\nSono dunque utilizzati dei blocchi, cioè delle forme geometriche e ciascuna di essa ha un significato ben preciso.",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#4": "Quanti e quali blocchi abbiamo in un diagramma?I blocchi convenzionalmente utilizzati in un flow chart sono:\nEllisse\nParallelogramma\nRettangolo\nRombo",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#5": "EllisseL’ellisse è utilizzata semplicemente solo per indicare l’inizio e la fine di un diagramma a blocchi.\nQuindi ciascun diagramma inizierà con il blocco inizio e terminerà, dopo aver risolto il compito assegnato, con il blocco fine.",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#6": "ParallelogrammaIl parallelogramma è utilizzato per prendere dei dati in INPUT o per visualizzare dei dati in OUTPUT. \nNel caso in cui deve prendere dei dati in input è consigliabile inserire una I in alto a sinistra, seguita dai due punti. Similmente per l’output, che si è soliti indicare con una O in alto a sinistra, sempre seguita dai due punti (ma va bene una qualunque altra convenzione).",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#7": "RettangoloIl rettangolo è utilizzato per eseguire dei calcoli, ovvero per elaborare dei dati. \nAd esempio: per calcolare la somma tra due numeri, l’area di un rettangolo, la media fra tre numeri, …",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#8": "RomboIl rombo è utilizzato per le istruzioni condizionali, ovvero per porre una domanda. All’interno dunque viene fatto un test, per cui si valuta una condizione che può essere o vera o falsa, quindi si sceglie tra due strade diverse. \nUn esempio di semplice test potrebbe essere quello di vedere se un numero è positivo o negativo.",
    "data_test\\rootfolder\\varie\\DiagrammiABlocchi.pptx#9": "RomboIl rombo viene spesso utilizzato anche per i cicli while e do-while.",
    "data_test\\rootfolder\\varie\\HTML&CSS.pptx#0": "HTML e CSS",
    "data_test\\rootfolder\\varie\\HTML&CSS.pptx#1": "Tag base dell’HTML: <p>, <div> e <span><p>, <div> e <span> sono tre diversi tipi di contenitori (di testo o altro), e si comportano in modo diverso:\n<p> è un elemento di blocco e lascia spazio prima e dopo la propria chiusura;\n<div> è un elemento di blocco, non lascia spazio prima e dopo la propria chiusura, ma va a capo;\n<span> è un elemento inline e quindi non va a capo.",
    "data_test\\rootfolder\\varie\\HTML&CSS.pptx#2": "Tag base dell’HTML: <ul>, <ol> e <li><ul> e <ol> sono tag che descrivono l’inizio di una lista, in particolare:\n<ul> per le liste non ordinate;\n<ol> per le liste ordinate.\n<li> serve a descrivere l’inizio di un elemento della lista.\nEsempio di utilizzo:\n",
    "data_test\\rootfolder\\varie\\HTML&CSS.pptx#3": "Tag base dell’HTML: <table>, <tr>, <th> e <td><table> descrive l’inizio di una tabella, e contiene al suo interno i tag <tr>, che descrivono l’inizio di una riga; i tag <tr> a loro volta possono contenere due tag:\n<th> per descrivere una cella di «testata»;\n<td> per descrivere una generica cella di contenuto.",
    "data_test\\rootfolder\\varie\\HTML&CSS.pptx#4": "Tag base dell’HTML: <img><img> serve per inserire un’immagine all’interno della pagina, l’attributo «src» dei questo tag serve a specificare quale immagine caricare.",
    "data_test\\rootfolder\\varie\\HTML&CSS.pptx#5": "Tag base dell’HTML: <form>Un form (modulo) è una sezione di documento HTML che contiene elementi di controllo che l’utente può utilizzare per inserire dati o in generale per interagire. I dati inseriti possono essere poi inoltrati al server dove un agente può processarli. Gli elementi di controllo sono caratterizzati da un valore iniziale e da un valore corrente. Gli elementi di controllo possono essere: \nBottoni di azione\nCheckbox (caselle di spunta)\nRadio Button (bottoni mutuamente esclusivi)\nListe di selezione (lista di opzioni)\nCaselle di inserimento di testo",
    "data_test\\rootfolder\\varie\\HTML&CSS.pptx#6": "Esercizi",
    "data_test\\rootfolder\\varie\\homework\\id-homework-1.pptx#0": "Ingegneria dei dati 2022/2023\u000bHomework 1Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-1.pptx#1": "Homework 1Leggere l'articolo (divulgativo) di Andrew Ng \"Data-centric AI\" \u000b(https://spectrum.ieee.org/andrew-ng-data-centric-ai)\nIn una relazione di circa 300 parole: 1) descrivi quella consideri la tesi più importante dell'autore e 2) esprimi la tua posizione rispetto ad essa. \n\nTermini di consegna: inviare la relazione entro le ore 12:00 del14 ottobre 2022 attraverso il seguente modulo online:\u000b\u000bhttps://forms.office.com/r/PYP0ncXYqc ",
    "data_test\\rootfolder\\varie\\homework\\id-homework-2.pptx#0": "Ingegneria dei dati 2022/2023\u000bHomework 2\u000b(da svolgere individualmente)Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-2.pptx#1": "Homework 2A partire dal codice github dell'ing. Tommaso Teofili (https://github.com/tteofili/lucenex):\nscrivere un programma Java che indicizza i file .txt contenuti in una directory del proprio laptop. In particolare, si devono considerare due campi (e quindi creare due indici): il nome del file, il contenuto del file. Per ciascun campo utilizzare un analyzer appropriato\nscrivere un programma Java che legge una query da console, interroga l'indice e stampa il risultato. Usare una semplice sintassi per la query (ad esempio, una query inizia con la parola chiave nome o contentuto seguita da una sequenza di termini (eventualmente racchiusi tra virgolette per esprimere una phrase query)\ntestare il sistema con una decina di query diverse\n\nScrivere una relazione che, oltre a riportare l'url del proprio progetto su Github (o analogo) descriva:\ngli analyzer che si è scelto di utilizzare (motivando le scelte)\nil numero di file indicizzati e i tempi di indicizzazione\nle query usate per testare il sistema",
    "data_test\\rootfolder\\varie\\homework\\id-homework-2.pptx#2": "Homework 2\nTermini di consegna: inviare la relazione entro le ore 21:00 del 22 ottobre 2022 attraverso il seguente modulo online:\u000b\u000bhttps://forms.office.com/r/PYP0ncXYqc ",
    "data_test\\rootfolder\\varie\\homework\\id-homework-3.pptx#0": "Ingegneria dei dati\u000bHomework 3\u000b(da svolgere in gruppo)Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-3.pptx#1": "Homework 3Implementare l'algoritmo \"MergeList\" per la soluzione al problema \"Joinable Table Search\"\nUtilizzare la libreria Apache Lucene\nTestare la correttezza dell'algoritmo su un piccolo insieme di tabelle, appositamente costruito\nTestare l'efficacia e l'efficienza dell'algoritmo sulle tabelle contenute nel dataset \"tables\" del progetto Mentor: https://gitlab.com/Rm3UofA/Mentor/Datasets",
    "data_test\\rootfolder\\varie\\homework\\id-homework-3.pptx#2": "Homework 3Ogni team deve preparare \nuna presentazione di 5' per illustrare le caratteristiche del dataset \nuna presentazione di 10' minuti per illustrare la valutazione sperimentale della propria implementazione dell'algoritmo \"MergeList\"\nTermini di consegna: entro le ore 19:00 del 2 novembre 2022 ogni membro del team deve inviare le due presentazioni al docente compilando il seguente modulo (compilare il modulo due volte, una per ciascuna presentazione):\u000b\n             https://forms.office.com/r/PYP0ncXYqc \n\nQuattro team (scelti dal docente) presenteranno il proprio lavoro nella lezione del 3 novembre 2022\n\n\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-3.pptx#3": "Presentazione Caratteristiche del DatasetDurata 5'\nDeve riportare statistiche sul dataset \"tables\" che possano essere utili all'analisi del problema, all'implementazione dell'algoritmo di soluzione e alla sua valutazione. \nAd esempio:\nNumero di tabelle\nNumero medio di righe\nNumero medio di colonne\nNumero medio di valori nulli per tabella\nDistribuzione numero di righe (quante tabelle hanno 1, 2, 3, 4, etc. righe)\nDistribuzione numero di colonne (quante tabelle hanno 1, 2, 3, 4, etc. colonne)\nDistribuzione valori distinti (quante colonne hanno 1, 2, 3, 4, etc valori distinti)\nAltro a vostra scelta\nPresentare le statiche in maniera opportuna, anche attraverso l'uso di rappresentazioni grafiche\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-3.pptx#4": "Presentazione Valutazione SperimentaleDurata 10'\nDeve includere\nDescrizione ad alto livello dell'implementazione (classi e metodi principali)\nPrincipali problemi riscontrati nell'implementazione\nValutazione sperimentale:\nCon una descrizione chiara e precisa di obiettivi e metriche di ciascun esperimento",
    "data_test\\rootfolder\\varie\\homework\\id-homework-4.pptx#0": "Ingegneria dei dati\u000bHomework 4\u000b(da svolgere individualmente)Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-4.pptx#1": "Homework 4 - Esercizio 1Scegliere una tipologia di prodotti su amazon.it \u000b(ad esempio fotocamere, oppure prodotti senza glutine)\nScegliere un pagina con un prodotto della tipologia scelta\nIndividuare nella pagina almeno 5 caratteristiche del prodotto\nScrivere un'espressione XPath per estrarre il nome, il prezzo e il valore di ciascuna delle caratteristiche individuate al punto precedente\nVerificare che le espressioni XPath funzionino correttamente su almeno altre 10 pagine di prodotti della stessa categoria\nSe una regola XPath non funziona, correggerla affinchè funzioni correttamente su tutte e 10 le pagine\n\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-4.pptx#2": "Homework 4 - Esercizio 2Scegliere un tipo di entità di interesse (ad esempio, giocatori di basketball, aziende, università, etc.)\nCercare 5 sorgenti Web che pubblicano pagine di dettaglio di istanze dell'entità scelta (ad esempio siti web che pubblicano pagine di giocatori di basketball)\nSu ogni sorgente scegliere 5 pagine di dettaglio\nScrivere espressioni XPath per estrarre I valori di (almeno) 5 attributi rilevanti su tutte le pagine scelte\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-4.pptx#3": "Termini di consegnaOgni studente deve preparare individualmente una relazione in cui descrive l'attività svolta per portare a termine l'homework\nTermini di consegna: entro le ore 19:00 del 19 novembre 2022 inviare la relazione al docente compilando il seguente modulo:\u000b\n             https://forms.office.com/r/PYP0ncXYqc \n\n\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-5.pptx#0": "Ingegneria dei dati\u000bHomework 5\u000b(da svolgere in gruppo)Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-5.pptx#1": "Homework 5Obiettivo: creare un dataset strutturato con dati estratti da sorgenti Web\nCi interessano dati su una tipologia di entità: aziende\nPer semplicità, ci concentriamo su sorgenti in lingua inglese\nScrivere un programma di estrazione dati per almeno 1000 istanze da almeno 4 sorgenti web",
    "data_test\\rootfolder\\varie\\homework\\id-homework-5.pptx#2": "TecnologieE' possibile usare una delle seguenti tecnologie (ma è possibile usarne altre)\n\nIn Python: \nhttps://scrapy.org/ \nhttps://www.crummy.com/software/BeautifulSoup/\nIn Java: \nhttps://www.selenium.dev/ \nhttps://jsoup.org/ \n\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-5.pptx#3": "Termini di consegnaOgni team deve preparare una presentazione così strutturata\n1 minuto per illustrare come è stata scelta la tecnologia per implementare il sistema di estrazione\n2 minuti per illustrare l'architettura del sistema di estrazione dati realizzato\n3 minuti per illustrare le prestazioni del sistema di estrazione\n4 minuti per illustrare le caratteristiche delle sorgenti e le caratteristiche del dataset ottenuto\nTermini di consegna: \nentro le ore 19:00 del 9 dicembre 2022 inviare la presentazione al docente compilando il seguente modulo: https://forms.office.com/r/PYP0ncXYqc \nIl 13 dicembre, ogni team dovrà consegnare il dataset con i dati estratti al docente (in un file compresso). Ogni team può liberamente scegliere in che modo strutturare il dataset\nQuattro team (scelti dal docente) presenteranno il proprio lavoro nelle lezioni del 13 e del 15 dicembre 2022\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-6.pptx#0": "Ingegneria dei dati\u000bHomework 6\u000b(da svolgere individualmente)Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-6.pptx#1": "Homework 6Leggere uno tra questi due articoli: \nY. Suhara et at \"Annotating Columns with Pre-trained Language Models\" (https://arxiv.org/pdf/2104.01785.pdf) \nK. Koutras et at \"Valentine: Evaluating Matching Techniques for Dataset Discovery\"\u000b(https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458921) \nIn una relazione di circa 900 parole, descrivere: \u000b1) descrivere  il problema che affronta l'articolo, 2) le soluzioni che propone, 3) l'impostazione sperimentale per la valutazione dei risultati. \nLeggere l'articolo (scientifico):\nP. Konda et al. \"Magellan: Toward Building Entity Matching Management Systems\" \u000b(http://www.vldb.org/pvldb/vol9/p1197-pkonda.pdf)\nIn una relazione di circa 900 parole, descrivere: \u000b1) il problema che affronta l'articolo, 2) le soluzioni che propone, 3) l'impostazione sperimentale per la valutazione dei risultati. \n\nTermini di consegna: inviare le due relazioni entro le ore 18:00 del 5 gennaio 2023 attraverso il seguente modulo online:\u000bhttps://forms.office.com/r/PYP0ncXYqc ",
    "data_test\\rootfolder\\varie\\homework\\id-homework-7.pptx#0": "Ingegneria dei dati\u000bHomework 7\u000b(da svolgere individualmente)Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-7.pptx#1": "Homework 7L'obiettivo dell'homework è quello di valutare il sistema CERTA per la generazione di spiegazioni di diversi sistemi di Record Linkage basati su tecniche di deep learning\n\nSeguire le istruzioni riportate a questo indirizzo:\n\t https://gist.github.com/tteofili/eaaeaaa8af2d22005fe199f1dc8874ad \n\nTermini di consegna: entro le ore 18.00 del 21 gennaio 2023 caricare il file cvv nel seguente modulo:\n\thttps://forms.office.com/r/PYP0ncXYqc ",
    "data_test\\rootfolder\\varie\\homework\\id-homework-8-progetto-finale.pptx#0": "Ingegneria dei dati\u000bHomework 8\u000b(da svolgere in gruppo)Paolo Merialdo",
    "data_test\\rootfolder\\varie\\homework\\id-homework-8-progetto-finale.pptx#1": "Homework 8L'obiettivo dell'homework è quello di integrare le sorgenti dati collezionate da tutti i team nell'homework 5 e di arricchirle con le tecniche sviluppate nell'homework 3 \nAnalizzare le sorgenti dati e individuare le principali eterogeneità\nDefinire uno schema mediato opportuno ed allineare gli schemi delle sorgenti allo schema mediato. È possibile usare:\nUna soluzione custom (anche manuale)\nFlexMatcher https://flexmatcher.readthedocs.io/en/latest/ \nComa https://sourceforge.net/projects/coma-ce/  \nUno dei tool del progetto Valentine https://github.com/delftdata/valentine\nCalcolare il Record linkage. E' possibile usare:\nUna soluzione custom\nPython Record Linkage Toolkit https://recordlinkage.readthedocs.io/en/latest/ \nMagellan https://github.com/anhaidgroup/deepmatcher\nDeepMatcher (soluzione neural network) https://github.com/anhaidgroup/deepmatcher \nDitto (soluzione neural network) https://github.com/megagonlabs/ditto \nEMT (soluzione neural network molto simile a Ditto) https://github.com/brunnurs/entity-matching-transformer \nUn sistema non supervisionato  https://github.com/uestc-db/Unsupervised-Entity-Resolution oppure https://github.com/chu-data-lab/zeroer\nArricchire i dati integrati usando le tecniche (e il dataset di tabelle) sviluppate nell'homework 3\n",
    "data_test\\rootfolder\\varie\\homework\\id-homework-8-progetto-finale.pptx#2": "Termini di consegnaPreparare un documento scritto di 4 pagine e una presentazione di 15' che descrivano:\nLe caratteristiche salienti delle sorgenti\nI benefici potenziali di integrare i loro dati\nLo schema mediato\nLe soluzioni che avete scelto per integrare i dati\nLe prestazioni (in termini id precision, recall, F-measure, tempi di calcolo, sforzo umano)\nI dati tabulari che avete trovato per arricchire le informazioni integrate dalle sorgenti\nIl documento e la presentazione vanno consegnati caricandoli attraverso il modulo all'indirizzo:\n\thttps://forms.office.com/r/PYP0ncXYqc\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#0": "Caratteristiche del dataset tables - Homework 3 Federico Bianchi\t--  Matr. 534835\nAndrea de Donato  -- Matr. 536795\nPaolo Di Simone  -- Matr. 584638",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#1": "Formato del dataset",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#10": "Distribuzione numero di colonne",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#11": "Distribuzione numero di valori distinti per colonna",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#12": "Distribuzione percentuale di valori distinti per colonna",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#2": "Formato del dataset",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#3": "Formato delle celle\n\t- Celle vuote e «None»\n\t- Classificazione tipi di cella",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#4": "Formato delle celle\n\t- Frequenza di termini all’interno del dataset",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#5": "Formato delle celle\n\t- Frequenza di termini all’interno del dataset",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#6": "Formato delle righe\n\t- Righe con celle vuote e con celle «None»",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#7": "Formato delle colonne\n\t- Colonne con celle vuote e con celle «None»\n\t- Classificazione tipi di colonna",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#8": "Distribuzione numero di righe",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-AnalisiDelDataset.pptx#9": "Fun Fact\n\t- Tabelle con 100 righe: 5 076",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#0": "Ingegneria Dei Dati:\u000bValutazione Sperimentale Homework 3 Federico Bianchi\t--  Matr. 534835\nAndrea de Donato  -- Matr. 536795\nPaolo Di Simone  -- Matr. 584638dedo99/Homework3 (github.com)Link Repository Progetto:",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#1": "Organizzazione del progetto: packageIndex\nindicizzazione di tutto il file in inputModel\nmodello utilizzato per estrarre i dati dal datasetQuery\nesecuzione delle query ed estrazione dei documenti ritenuti compatibili",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#10": "Valutazione sperimentale del progetto (2)Tempo necessario per l’indicizzazione dell’intero file JSON contenente 550.271 tabelle (14,2 Gb): 297.882 s (c.a. 5 minuti)\n\nQuery d’esempio [«singlular», «plural», «fmou», «dual»], tempo di esecuzione:\nCon SimpleTextCodec: 18 minuti\nSenza SimpleTextCodec: 6 secondi",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#11": "Valutazione sperimentale del progetto (3)Per testare l’efficacia e l’efficienza del sistema sono state effettuate tre tipologie di test:\n\nTest al variare di k\n\nTest al variare della lunghezza della query\n\nPrecision, Recall, F1 e Accuracy su dataset di test",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#12": "Test al variare di k",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#13": "Test al variare della lunghezza della query",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#14": "Dataset di testÈ stato costruito un dataset per testare l’efficacia del sistema, con le seguenti caratteristiche:\n\n30 tabelle\n\nOgni tabella riporta informazioni su film, libri, autori, attori, …",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#15": "Dataset di test",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#16": "Dataset di test: distribuzione righe",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#17": "Dataset di test: distribuzione colonne",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#18": "Dataset di test: distribuzione valori distinti",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#19": "QuerySono state costruite 23 query di test, ottenute calcolando lo score Jaccard fra tutte le possibili coppie di colonne all’interno del dataset. Ogni query di test contiene:\n\nColonna di valori che rappresenta la query\n\nTop 3 colonne con score Jaccard più alto",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#2": "Le classiIndexCellModelCoordinatesQueryJSONObjectJSONIndexerQueryManager",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#20": "Query",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#21": "Precision, Recall, F1 e Accuracy",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#22": "QuerySolo per una query il sistema non restituisce alcun risultato corretto:\n[«USA», «USA», «USA», «USA», «USA», «USA», «USA»]\n\nLe altre query su cui il sistema fatica a restituire il risultato corretto sono molto simili:\n[«USA», «Italia», «Italia», «Francia», «Inghilterra», …]",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#3": "Classe JSONIndexer (1)La classe JSONIndexer è composta da due metodi:\n\n\nreadJsonStream(InputStream in, Codec codec): lettura dell’input dal file JSON\n\n\nindexJSONStream(JsonReader reader, Codec codec): estrazioni degli oggetti JSON (tabelle) dal reader, parsing in un oggetto Java e successiva indicizzazione. I documenti inseriti nell’indice corrispondono ciascuno ad una colonna di una tabella.\n\n\nUso della libreria Gson per convertire rappresentazioni JSON in oggetti Java e viceversa.",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#4": "Classe JSONIndexer (2)Definizione di un Tokenizer CustomCreazione dei documentiAnalyzer analyzer = CustomAnalyzer.builder()\u000b        .withTokenizer(PatternTokenizerFactory.NAME, \"pattern\", \"~\", \"group\", \"-1\")\u000b        .build();for(Cell c : obj.getCells())\u000b    if (!c.getHeader()) {\u000b        if(colonnaXvalori.containsKey(obj.getId() + \"_\" + c.getCoordinates().getColumn().toString())) {\u000b            String value = colonnaXvalori.get(obj.getId() + \"_\" + c.getCoordinates().getColumn().toString());\u000b            colonnaXvalori.put(obj.getId() + \"_\" + c.getCoordinates().getColumn().toString(), value + \"~\" + c.getCleanedText());\u000b        } else\u000b            colonnaXvalori.put(obj.getId() + \"_\" + c.getCoordinates().getColumn().toString(), c.getCleanedText());\u000b    }",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#5": "Classi JsonObject, Cell e CoordinatesLe classi JsonObject, Cell e Coordinates sono stati realizzate per rendere agevole la trasformazione da un oggetto Json ad un oggetto Java.\n\nLe seguenti classi sono dotate di variabili di istanza, relative alle sole informazioni necessarie rispetto al completo contenuto dell’oggetto Json, e i corrispondenti metodi setter e getter.public class JSONObject {\n String id;\n Cell[] cells;\n}public class Cell {\n Boolean isHeader;\n String cleanedText;\n Coordinates Coordinates;\n}public class Coordinates {\n Double row;\n Double column;\n}",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#6": "Classe QueryManager\nmergeList(int n, String[] queryString) restituisce le prime n colonne tra tutte le tabelle che hanno corrispondenze con il maggior numero di termini nella query\n\nexecuteQuery(String field, String[] queryString) genera una mappa (idTabella_idcolonna -> numero corrispondenze) scansionando tutti gli elementi presenti nella query\n\nsortMapByValues(Map<String, Integer> columnsXcount) effettua l’ordinamento della mappa sull’intero contenuto nel campo valore\n\nrunQuery(IndexSearcher searcher, Query query,  Map<String, Integer> columnsXcount) restituisce per ciascun elemento della query una mappa con le colonne delle tabelle in cui è presente\n\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#7": "Problemi riscontratiIndividuare la corretta rappresentazione dei documenti nell’indice\n\nIndividuare un modo corretto di tokenizzare i documenti (nello specifico il campo ‘value’)\n\nTempi di indicizzazione",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#8": "ObiettiviImplementare nel modo più efficiente l’indicizzazione di una grande quantità di dati\n\nRestituire, a seguito di una query, le tabelle con la relativa colonna in cui sono state incontrate delle corrispondenze senza includere nel conteggio eventuali ripetizioni dello stesso termine nella colonna \n\nRestituire in ordine decrescente i risultati sulla base del numero di corrispondenze ottenute",
    "data_test\\rootfolder\\varie\\relazioni\\HW3-ValutazioneSperimentale.pptx#9": "Valutazione sperimentale del progetto (1)Tutti i risultati sono stati ottenuti utilizzando un calcolatore con le seguenti specifiche:\n\nProcessore Intel core i7 di 8° gen\n\nRAM 8Gb\n\nSSD 512Gb",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#0": "Web Scraping:\u000bScraping business information\u000b\u000b\u000b\u000bDipartimento di Ingegneria\nCorso di Laurea Magistrale in Ingegneria InformaticaAnno Accademico \n2022-2023\n13 Dicembre 2022Corso\u000b Ingegneria dei datiProfessore\nPaolo MerialdoStudenti\nPaolo Di Simone\nPietro Baroni\nMatteo WisselGitHub: Web Scraping- Homework5",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#1": "Purpose del progettoIntroduzioneAutomatizzazione della ricerca ed estrazione di informazioni relative ad aziende\n\n\nUtilità\n\nCreazione di un dataset relativo ad aziende per diversi task:\u000b\nAddestramento modello AI\u000b\nAnalytics\n\nData IntegrationScopo",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#10": "Sorgente: gov.ukLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#11": "Sorgente: gov.ukLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#12": "Efficacia X-Paths: Data ConsistencyAnalisi pattern\n\nAddress, Business code, Business name (E-business), Date, ecc…\n\nAnalisi frequenza valori celle \u000b\nLegal form, Status, ecc…\n\n\n\nAnalisi frequenza token\u000b \nBusiness name, ecc…Lilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#13": "Efficacia X-Paths: Analisi PatternTable: GOV.UK\n\nField: Address\n\nExample values: '38SpringfieldRoad Gillingham Kent England ME71YJ’\n\n\n\n    \nRegex: \n\t([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|\n\t(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|\n\t([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\s?[0-9][A-Za-z]{2})\nUK Postal codeLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#14": "Efficacia X-Paths: Analisi PatternIndirizzi non conformi\n\nAustrasse429490 Vaduz Liechtenstein\n\nPasiadou5KatoLakatamia 2332Nicosia Nicosia Cyprus\n\nLaChausseeStreet PortLouis MauritiusLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#15": "Efficacia X-Paths: Frequenza ValoriTable: E-Business\n\nField: Legal form\n\nExample values:\nPrivate limited company, Public limited company, Non-profit association, ecc…\n\n\n    \nLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#16": "Efficacia X-Paths: Frequenza ValoriLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#17": "Efficacia X-Paths: Frequenza TokenTable: GOV.UK\n\nField: Name\n\nExample values: P & A PROPERTY (WESTON) LIMITED, P A JONES LIMITED, P A H \t\t       CARPENTRY & JOINERY LTD  \n\n\t\n    \nLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#18": "Efficacia X-Paths: Frequenza TokenTable: GOV.UK\n\nField: Name\n\nMost frequent tokens:\nLimited, LTD,LTD., Services, …\n\n\n    \nAnalisi\n\n1466 su 1469 hanno nel loro nome \n      i 10 token più frequenti\n1 ) LIMITED -> 728\n2 ) LTD -> 533\n3 ) SERVICES -> 94\n4 ) ELECTRICAL -> 38\n5 ) CONSTRUCTION -> 37\n6 ) BUSINESS -> 36\n7 ) CORP. -> 35\n8 ) PROPERTIES -> 34\n9 ) HOMES -> 27\n10 ) LTD. -> 26\nLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#19": "Test sistema: WorkflowIl sistema è stato testato nel seguente modo:\nCampionamento casuale del dataset estratto:\nCompaniesmarketcap  30 URL\nInfoclipper  50 URL\nGovUK  29 URL\nEbusiness  30 URL\nEstrazione manuale dei dati contenuti nel campione selezionato\nConfronto dati estratti dal sistema e dati ottenuti manualmente\nLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#2": "Ricerca sitiFiltraggio sitiSchema datiParsing dati &\nData ConsistencyAcquisizione datiAnalisi datiRoad mapIntroduzione",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#20": "Test sistema: companiesmarketcap.comLilla SystemErrori dovuti esclusivamente alla variabilità giornaliera dei campi\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#21": "Test sistema: e-BusinessLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#22": "Test sistema: info-clipper.comLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#23": "Test sistema: info-clipper.comLilla SystemPostalcode\nLe differenze sono dovute al fatto che le celle formattate come intere eliminano gli 0 a sinistra della stringa  {187’, 00187’} \nState\nNel nostro sistema nel campo state inseriamo anche la sigla dello stato, nei test l’utente non inserisce nel campo State la sigla  {'California(CA)', 'California'} ",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#24": "Test sistema: gov.ukLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#25": "Test sistema: gov.ukLilla SystemCompany ID\nLe differenze sono dovute al fatto che le celle formattate come intere eliminano gli 0 a sinistra della stringa  {'6174105', '06174105’} \nCompany Status\nGli errori sono dovuti al cambiamento di status dell’azienda  {'Active', 'Dissolved'}\nDissolution Date\nL’azienda nel frattempo è stata dissolta  {'nan', 14 February 2023 '} \n",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#26": "Efficienza temporale: Estrazione datiTempo complessivo\n\nTempo di request \n\nTempo di estrazione del dato (navigazione del dom via X-Path)\nLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#27": "Efficienza temporale: RequestLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#28": "Efficienza temporale: Estrazione datiLilla System//div[@class = \"company-code\"]//*[@id=\"cmkt\"]/div[3]/div[1]/div[2]/div[3]/div[1]/a/text()",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#29": "Dataset: companiesmarketcap.comLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#3": "\nBeautiful soup\n\nRequest\n\nLXML & Etree\nWeb ScrapingPandas & numpy                    \t\nMatplotlib\n\nGeopandas\nData ProfilingTecnologieLilla SystemPython",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#30": "Dataset: companiesmarketcap.com   Analisi campi\n\nName: Nome dell’azienda\nCompany Code: Codice identificativo delle società quotate in borsa (WMT, AMZN, UPS, KR, …)\nMarketcap: Somma del valore totale delle azioni in circolo\nShare Price: Costo singola azione\nEarnings: Profitto annuo\nRevenue: Ricavi annui\nShares: Numero totale di azioni in circolo\nEmployees: Numero totale di dipendenti\n\n\n  Numeriche generali\n\nTotale istanze: 1400\nTotale colonne: 10\n\n\nCelle totali: 14000\nValori nulli: 21Lilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#31": "Dataset: companiesmarketcap.comLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#32": "Dataset: companiesmarketcap.comLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#33": "Dataset: e-BusinessLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#34": "Dataset: e-BusinessAnalisi campi\n\nName: Nome dell’azienda\nCompany code: Codice identificativo dell’azienda\nLegal Form: Forma giuridica dell’azienda\nStatus: Status dell’azienda (Deleted, Entered into the register, ecc…)\nRegistration Date: Data di inserimento dell’azienda nel registro\nCapital: Capitale dell’azienda\nAddress: Indirizzo sede dell’azienda\nDeletion Time: Data di eliminazione dell’azienda dal registro\n\n  Numeriche generali\n\nTotale istanze: 1469\nTotale colonne: 10\n\n\n\nCelle totali: 14690\nValori nulli: 2120 Lilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#35": "Dataset: e-BusinessLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#36": "Dataset: e-BusinessESTONIALilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#37": "Dataset: info-clipper.comAnalisi campi\n\nName: Nome dell’azienda\nTrade Name: Nome commerciale dell’azienda\nAddress: Indirizzo sede dell’azienda\nCity: Città \nPostalcode: Codice postale nei formati UK, USA, Italia, Estonia\nState: Stato Americano di residenza o Nazione di residenza\nCountry: Nazione di residenza\nLocation type: Tipo di sede (es. Headquarter, Secondary Office, ecc…)\n\n\n\n  Numeriche generali\n\nTotale istanze: 1504\nTotale colonne: 10\n\n\n\nCelle totali: 15040\nValori nulli: 1289Lilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#38": "Dataset: info-clipper.com\n\n\nLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#39": "Dataset: info-clipper.com\n\n\nSTATI UNITILilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#4": "Lilla\nSystemGeneral web pageRequest: get informationResponse: list of linksRequest: get all linksResponse:  informationCreate datasetSpecific web pageDatasetData Parsing and\nData ConsistencyArchitetturaLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#40": "Dataset: info-clipper.com\n\n\nESTONIALilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#41": "Dataset: info-clipper.com\n\n\nITALIALilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#42": "Dataset: info-clipper.com\n\n\nINGHILTERRALilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#43": "Analisi campi\n\nName: Nome dell’azienda\nCompany ID: Codice identificativo dell’azienda\nCompany Status: Status dell’azienda (Active, Dissolved, Registered, Liquidated)\nCompany Type: Tipo dell’azienda (Overseas Entity, Private Limited Company, ecc…)\nRegistration Date: Data di registrazione di aziende estere\nIncorporation Date: Data di inserimento delle aziende inglesi nel registro\nDissolution Date: Data di dissoluzione dell’azienda\nOffice Address: Indirizzo dell’azienda\n\n\nDataset: gov.uk  Numeriche generali\n\nTotale istanze: 1331\nTotale colonne: 10\n\n\n\nCelle totali: 13310\nValori nulli: 2555Lilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#44": "Dataset: gov.ukLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#45": "Dataset: gov.ukREGNO UNITOLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#46": "Dataset: gov.ukINGHILTERRALilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#47": "Member: Pietro Baroni\nMatricola: 536373\nTask: Algoritmi\nLinkedin: Pietro BaroniMember: Paolo Di Simone\nMatricola: 584638\nTask: Analytics\nLinkedin: Paolo Di SimoneMember: Matteo Wissel\nMatricola: 534693 \nTask: Algoritmi\u000bLinkedin: Matteo WisselTeamLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#48": "GRAZIE PER L’ATTENZIONERoma, 13  Dicembre 2022GitHub: Web Scraping- Homework5",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#5": "Sorgente: companiesmarketcap.comLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#6": "Sorgente: ariregister.rik.ee (e-Business)Lilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#7": "Sorgente: ariregister.rik.ee (e-Business)Lilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#8": "Sorgente: info-clipper.comLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW5-WebScarping.pptx#9": "Sorgente: info-clipper.comLilla System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#0": "Data integration:\u000bArlecchino system\u000b\u000b\u000b\u000bDipartimento di Ingegneria\nCorso di Laurea Magistrale in Ingegneria InformaticaAnno Accademico \n2022-2023\n22 Febbraio 2023Corso\u000b Ingegneria dei datiProfessore\nPaolo MerialdoStudenti\nPaolo Di Simone\nPietro Baroni\nMatteo WisselArlecchino System\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#1": "Purpose del progettoIntroduzioneUse Case\n\nIntegrazione di dataset aziendali\u000bScopo\n\nImplementazione di un sistema di Data IntegrationDataset\n\nDataset_Corso_Ingegneria_dei_Dati_2022/23\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#10": "ParsingArlecchino SystemParsing nomi colonne\u000b\nTrasformazione in lower case  \nEliminazione caratteri speciali (-, /, …)\nEliminazione di skip words (of, the, del, di, …)  Parsing valori celle \n\nParsing di stringhe: \nTrasformazione in lower case\nEliminazione caratteri speciali (-, /, …)\nEliminazione di skip words (of, the, del, di, …)\nEsempio: Amazon -> amazon\n  \nParsing di valori monetari:\nNormalizzazione valori \nInserimento unità di misura\nEsempio: $102 million -> doll_ 0.102 b \n\nParsing valori percentuali\nParsing valori rank\nParsing valori date\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#11": "Schema matching: Formulazione problemaArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#12": "Schema Matching: Matching Module (SMM)Arlecchino SystemPre-processing module\n\nInput: colonne e samples di valori \nOutput: un dizionario parziale di sinonimi\nUtilità: 1. riduzione del search space del JaccardModule\n    2. inferisce informazioni al JaccardModule  \nJaccardModule\n\nInput: dizionario sinonimi_preprocessing\nOutput: dizionario sinonimi_finale\nUtilità: trova le reali correlazioni semantiche tra le colonneUser",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#13": "SMM: MotivazioniArlecchino SystemAnalisi senza pre-processingAnalisi con pre-processingSample data: 1000 per colonna\u000b\nColonne totali: 18\u000b\nNumero medio di sinonimi reali: 1.6\u000b \n\nStima confronti totali: ≈ 153\nNumero medio confronti per colonna: ≈ 18\nNumero medio di confronti inutili: ≈ 17\nEsempio \ncluster cbinsightsSample data: 1000 per colonna\n\nColonne totali: 18\n\nNumero medio di sinonimi reali: 1.6\n\nStima confronti totali: 14\nNumero medio confronti per colonna:  2.5\nNumero medio di confronti inutili: 0.84\n\nFattore di riduzione: ≈ 12",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#14": "Pre-processing: Name SimilarityArlecchino SystemName similarity di colonne\n\nInput: (column_names, dizionario_sinonimi_pregressi)\nOutput: un dizionario parziale di sinonimi\nUtilità: individua i sinonimi schema-wise \n\t(dettati da similarità di nome)Logica:\t\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#15": "Pre-processing: Data CorrelationArlecchino System\n\nData similarity di colonne [1]\n\nInput: (sample_dati_colonne)\nOutput: un dizionario sinonimi4cluster\nUtilità: individua i sinonimi data-wise\n\t(dettati da similarità di dati) [1] Schema Matching using Machine LearningTanvi Sahay, Ankita Mehta, Shruti Jadon",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#16": "SMM: Features EngineeringArlecchino SystemFeatures selezionate\nMin_val\nMax_val\nAvg\nVariance\nStandard_Dev\nIs_incremental\nIs_year Type_of_string (1 perc_, 2 rank_, 3 link, 4-5 monetari, 6 resto)\nAVG_monetary_value\nAVG_len_of_field\nVAR_len_of_field\nSDEV_len_of_field\nRatio_white_space\nRatio_numeric_values\nIs_country (1 se country, 0 altrimenti)\nIs_sector (1 se sector, 0 altrimenti)1. Type_of_data (0 string, 1 integer, 2 date)for Integerfor stringsfor date",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#17": "Pre-processing: Analisi OutputArlecchino SystemDizionario sinonimi_preprocessing (largo)\n\nUnione dei dizionari di sinonimi prodotti dai due step di \u000bpre-processing\nScopo: limitare eventuali errori (high recall)\u000b\nEsempio (companiesmarketcap):\u000b\nToken: market_cap\n\nTrue_sinonimi\n\tmarket_cap->{marketcap, market_capitalization, pricecap, …}\u000b\nsinonimi4NameCorr\n\tmarket_cap->{marketcap, market_capitalization,…}\n\nsinonimi4Clusters\n\tmarket_cap->{marketcap, pricecap, …}\n\nsinonimi_preprocessing\n\tmarket_cap->{marketcap, market_capitalization, pricecap, …}\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#18": "SMM: JaccardModuleArlecchino SystemJaccardModule\n\nInput: sinonimi_preprocessing\n\nLogica: per ogni colonna c presente nel dizionario dei sinonimi_preprocessing, il sistema genera un file .csv contenente tutte le colonne giudicate sinonimi",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#19": "SMM: OutputArlecchino SystemDizionario sinonimi finali\n\nRisultato finale del Matching module\u000b\nPer ogni colonna dello schema mediato è definita  una lista di  possibili sinonimi \u000b(colonne semanticamente simili)\n\nL’utente seleziona i match opportuni eliminando eventuali errori del sistema\n\nAl netto della validazione dell’utente, il sistema aggiorna il dizionario dei sinonimi pregressi\u000b\n\n\nUser",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#2": "Caratteristiche dei sorgenti: ClusterSorgenti\t",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#20": "Schema MediatoArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#21": "Record LinkageArlecchino SystemInput: schema mediato (184.587 record)\n\nOutput: dataset finale\n\nScopo: Trovare nella tabella in input i record relativi alla stessa entità ed unirli in un unico record\n\n Record Linkage\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#22": "Record Linkage: BlockingArlecchino SystemNumero di confronti iniziali: 34.072.360.569 Numero di confronti post-blocking: 2.177.713Tempi: 76 min 36 secBlocking step1. overlap di una parola nel nome delle aziende\n\n2. overlap di una parola nel paese delle aziende (se presente)\n\n3. Indice di Levenshtein < 0.7 tra i nomi\n delle aziende\n\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#23": "Record Linkage: Training SetArlecchino SystemTotale record: 1300\n\nDivisi in training set, test set e validation set (ratio 3:1:1)\n\nLabel Match: 695\n\nLabel No-Match: 605\n\nTraining set",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#24": "Record Linkage: Model TrainingArlecchino SystemIperparametri:\nEpoche = 10\nDimensione batch = 16 \nStatistiche:\nTempo impiegato: 10 min 29 secModello utilizzato: Matching Model di Deep MatcherModel Training",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#25": "Record Linkage: PredictionArlecchino SystemStatistiche:\nPredizioni effettuate: 2.177.713\nPredizioni Match: 1.480.727\nTempo impiegato: 14 ore e 45 minuti\nUtilizzo del modello addestrato per eseguire le predizioni sulle coppie non bloccatePrediction",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#26": "Record Linkage: JoinArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#27": "Schema IntegratoArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#28": "ArricchimentoColonna usate come input:\nNome dell’azienda\nCEO dell’aziendaTabelle con più occorrenze in output:\nList of S&P 500 companies\nList of largest companies by revenue\nList of largest European manufacturing companies by revenue\nList of multinationals with research and development centres in Israel\nList of largest Nordic companies\nAutomotive industryPMF \u000bsystem\u000b(HW3)Top table ids Dataset utenteinput (Schema mediato, \n[name, ceo] )Indice HW3Arlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#29": "Schema ArricchitoArlecchino System142 celle riempite",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#3": "Caratteristiche dei sorgenti: ClusterSorgenti\t",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#30": "Testing SMM: MetricheArlecchino SystemLogica\n\n Confronto tra i sinonimi computati per una colonna dello schema mediato S e i sinonimi veri S’\n\nMetriche\n\nNumero di confronti inutili effettuato per sorgente\n\nSimilarità tra sinonimi computati:\nPrecision\nRecall\nF1",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#31": "Testing: Pre-Processing per clustering (Conf.)Arlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#32": "Testing: performance SMM clusterArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#33": "Testing: Pre-Processing per clusteringArlecchino SystemRisultati\n",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#34": "Testing: JaccardModule for cluster Arlecchino SystemConfigurazione\n\nThreshold Jaccard*: 0.1\n\nThreshold edit: 0.5 ",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#35": "Testing: Pre-processing for schema mediatoArlecchino SystemRisultati",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#36": "Testing: performance SMM schema finaleArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#37": "Testing: Schema MediatoArlecchino SystemConfigurazione\n\nThreshold Jaccard*: 0.1\n\nThreshold edit: 0..5Pre-processingJaccardModule",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#38": "Testing: Record LinkageArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#39": "MiglioramentiArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#4": "Caratteristiche dei sorgenti: ClusterSorgenti\t",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#40": "Member: Pietro Baroni\nMatricola: 536373\nLinkedin: Pietro BaroniMember: Paolo Di Simone\nMatricola: 584638\nLinkedin: Paolo Di SimoneMember: Matteo Wissel\nMatricola: 534693\u000bLinkedin: Matteo WisselTeamArlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#41": "GRAZIE PER L’ATTENZIONERoma, 22 Febbraio 2023GitHub: Arlecchino System",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#5": "Analisi dei sorgentiDati finanziari\nDati giuridici\nDati geografici\nDati di personale\nEtichette tipologie datiSorgenti\t",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#6": "Struttura dello schema mediato Schema mediatoVisione unificata delle informazioni\n\n\nSchema mediato\nBenefici",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#7": "Arlecchino SystemArlecchino System…",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#8": "TecnologieArlecchino SystemSchema Matching\n\nPreprocessing Module - custom\nJaccardModule - customRecord Linkage\n\nMagellan (py_entitymatching)\nDeepMatcherData Enrichment\n\n Sistema HW3",
    "data_test\\rootfolder\\varie\\relazioni\\HW8-DataIntegration.pptx#9": "ArchitetturaArlecchino SystemParsing&CleaningSchema MatchingRecord Linkage"
}