{
    "020-linguaggi-e-grammatiche-24.pdf": {
        "1": "9/28/2021\n102-linguaggio -e-grammatiche -24\n1Linguaggi e Grammatiche\n2Linguaggi e informatica\n\u2022ubiquitari nelle applicazioni\n\u2013linguaggi di programmazione\n\u2022compilatori ed interpreti\n\u2013linguaggi di comunicazione\n\u2022protocolli per il dialogo traentit\u00e0 omologhe\n\u2013linguaggi per intefacce\n\u2022specifica di sequenze di operazioni\n\u2022paradigmatici nella teoria\n\u2013molti importanti problemi teorici sono riconducibili a \nquello dell\u2019appartenenza di una stringa ad un \nlinguaggio1\n2",
        "2": "9/28/2021\n202-linguaggio -e-grammatiche -24\n3Tre approcci diversi\n\u2022approccio insiemistico\n\u2013utile per determinare le \npropriet\u00e0 elementari dei\nlinguaggi\n\u2022approccio generativo\n\u2013grammatiche formali\n\u2022approccio riconoscitivo\n\u2013automi riconoscitori\n4Concetti matematici di base\n\u2022Insiemi\n\u2022Relazioni\n\u2022Funzioni3\n4",
        "3": "9/28/2021\n302-linguaggio -e-grammatiche -24\n5Insiemi\n\u2022consideriamo insiemi finiti e insiemi infiniti\n\u2022|A| = cardinalit\u00e0 dell\u2019insieme (finito ) A\n\u2022alcuni insiemi infiniti di numeri:\nN naturali (contiene zero)\nN+ naturali positivi\nZ interi relativi\nZ+interi positivi\nZ- interi negativiQ razionali relativi\nQ+ razionali positivi\nQ-razionali negativi\nR reali\nR+ reali positivi\nR-reali negativi\n6Sottoinsiemi e insiemi uguali\n\u2022datidue insiemi A e B, se\nx \uf0ceB \uf0dex \uf0ceA\nallora B \u00e8 sottoinsieme di A, e siscrive B \uf0cdA\n\u2022ogni insieme \u00e8 sottoinsieme di se stesso\n\u2022l\u2019insieme vuoto\uf0c6\u00e8 sottoinsieme di ogni insieme\n\u2022se A e B sono finiti , allora B \uf0cdA \uf0de|B| \uf0a3|A|\n\u2022A e B insiemi uguali\nA=B \uf0db(x \uf0ceA \uf0dbx \uf0ceB)\nsipu\u00f2scrivere anche\nA=B \uf0db(A \uf0cdB \uf0d9B \uf0cdA)\n\u2022A \u00e8 sottoinsieme proprio di B (A \uf0ccB) se \n(A \uf0cdB) \uf0d9(A \uf0b9B)5\n6",
        "4": "9/28/2021\n402-linguaggio -e-grammatiche -24\n7Insieme delle parti\nA\nP(A)l\u2019insieme dei sottoinsiemi di A \u00e8 detto l\u2019 insieme delle parti di A e si \nindica con P(A) o 2A\nse A \u00e8 finito e |A| = n allora |P(A)| = 2n\n8Operazioni tra insiemi\n\u2022unione C = A \uf0c8B\n\u2013se A e B sono finiti |C| \uf0a3|A|+|B|\n\u2013commutativa e associativa\n\u2022intersezione C=A \uf0c7B\n\u2013se A e B sono finiti |C| \uf0a3min{|A|, |B|}\n\u2013commutativa e associativa\n\u2013l\u2019intersezione \u00e8 distributiva rispetto all\u2019unione\n\u2022partizione di A\n\u2013insieme di n sottoinsiemi di A tali che \nA1\uf0c8A2\uf0c8\u2026 \uf0c8An= A\ni \uf0b9j \uf0deAi\uf0c7Aj= \uf0c6A B\nA B\nA7\n8",
        "5": "9/28/2021\n502-linguaggio -e-grammatiche -24\n9Operazioni tra insiemi\n\u2022complemento di B rispetto ad A\nC = A -B = {x| x\uf0ceA \uf0d9x\uf0cfB}\n\u2022differenza simmetrica o somma disgiunta\nA+B=A\uf0c8B-(A \uf0c7B)\n\u2022prodotto cartesiano C=A \uf0b4B\nC= {\uf03cx,y\uf03e| x\uf0ceA \uf0d9y\uf0ceB}\n\u2013insieme di tutte le possibili coppie ordinate\n\u2013il prodotto cartesiano \u00e8 associativo ma non commutativoA BA B\n10Relazioni\n\u2022siano A1, A2, \u2026, Ann insiemi \n(non necessariamente distinti)\n\u2022una relazione n -aria \u00e8 un sottoinsieme di \nesempio :\n\u2013la relazione \u201c minore di \u201d definita sui naturali \u00e8 l\u2019insieme \nR \uf0cdN\uf0b4N = N2, dove R= {\uf03cx,y\uf03e| x\uf03cy}A1\uf0b4A2\uf0b4\u2026\uf0b4An\nR \uf0cdA1\uf0b4A2\uf0b4\u2026\uf0b4An9\n10",
        "6": "9/28/2021\n602-linguaggio -e-grammatiche -24\n11Relazione d\u2019ordine\n\u2022R\uf0cdA2=A \uf0b4A \u00e8 una relazione d\u2019ordine se valgono le \nseguenti propriet\u00e0:\n1.riflessivit\u00e0\n\uf03cx,x\uf03e\uf0ceR\n2.antisimmetria\n\uf03cx,y\uf03e\uf0ceR\uf0d9\uf03cy,x\uf03e\uf0ceR\uf0dex=y\n3.transitivit\u00e0\n\uf03cx,y\uf03e\uf0ceR\uf0d9\uf03cy,z\uf03e\uf0ceR\uf0de\uf03cx,z\uf03e\uf0ceR\nun insieme su cui \u00e8 definita una relazione d\u2019ordine si dice \nparzialmente ordinato o poset (\u201cpartially ordered set\u201d)\nesempio : la relazione \u201c \uf0a3\u201d \u00e8 una relazione d\u2019ordine su N\n12Relazione d\u2019ordine totale\n\u2022una relazione d\u2019ordine R \uf0cdA2\u00e8 detta totale se\n\uf03cx,y\uf03e\uf0ceA2 \uf0de\uf03cx,y\uf03e\uf0ceR \uf0da\uf03cy,x\uf03e\uf0ceR\nesempio :\nla relazione \u201c \uf0a3\u201d \u00e8 una relazione d\u2019ordine totale su N\n1 \uf0a32 \uf0a33 \uf0a34 \uf0a35 \uf0a36 \uf0a37 \uf0a38 \u202611\n12",
        "7": "9/28/2021\n702-linguaggio -e-grammatiche -24\n13Relazione di equivalenza\n\u2022R\uf0cdA2= A\uf0b4A \u00e8 una relazione di equivalenza se valgono \nle seguenti propriet\u00e0:\n1.riflessivit\u00e0\n\uf03cx,x\uf03e\uf0ceR\n2.simmetria\n\uf03cx,y\uf03e\uf0ceR\uf0de\uf03cy,x\uf03e\uf0ceR\n3.transitivit\u00e0\n\uf03cx,y\uf03e\uf0ceR\uf0d9\uf03cy,z\uf03e\uf0ceR\uf0de\uf03cx,z\uf03e\uf0ceR \nesempio : la relazione \u201c= \u201d\u00e8 una relazione di equivalenza su R\n14Relazione di equivalenza\n\u2022un insieme A su cui \u00e8 definita una relazione di \nequivalenza si pu\u00f2 partizionare in sottoinsiemi \nmassimali di equivalenza , detti classi di \nequivalenza\n\u2022l\u2019insieme delle classi di equivalenza di A \u00e8 \ndetto insieme quoziente e si denota A/R \n\u2022un elemento di A/R si denota con [a]\n\u2022il numero di classi di A/R si chiama indice di R13\n14",
        "8": "9/28/2021\n802-linguaggio -e-grammatiche -24\n15Esempio di relazione di equivalenza\n\u2022consideriamo la relazione Eksu N\nn \uf0bak m\nse esistono q, q \u2032, r (con r<k) tali che\nn=qk+r    e    m=q\u2032k+r\n\u2022Ek\u00e8 una relazione di equivalenza\n\u2022le sue classi sono le classi resto rispetto alla \ndivisione per k\n16Operazioni su relazioni\n\u2022unione\nR1\uf0c8R2={\uf03cx,y\uf03e|\uf03cx,y\uf03e\uf0ceR1\uf0da\uf03cx,y\uf03e\uf0ceR2}\n\u2022complementazione\nR={\uf03cx,y\uf03e|\uf03cx,y\uf03e\uf0cfR}\n\u2022chiusura transitiva\nR+  ={\uf03cx,y\uf03e|\n\uf024y1, \u2026,yn\uf0ceA, n\uf0b32, y1=x, yn=y\ntali che\n\uf03cyi,yi+1\uf03e\uf0ceR, i=1, \u2026,n -1} \n\u2022chiusura transitiva e riflessiva\nR*=R+\uf0c8{\uf03cx,x\uf03e|x\uf0ceA}15\n16",
        "9": "9/28/2021\n902-linguaggio -e-grammatiche -24\n17Chiusure di relazioni\n2\n31\n54\n2\n31\n542\n31\n54chiusura transitivachiusura transitiva\ne riflessiva\n18Funzioni\nR \uf0cdX1 \uf0b4\u2026 \uf0b4Xn\n\u00e8 una relazione funzionale se\n\uf022\uf03cx1, \u2026, xn-1\uf03e\uf0ceX1 \uf0b4\u2026 \uf0b4Xn-1\nesiste al pi\u00f9 un elemento xn \uf0ceXntale che\n\uf03cx1, \u2026, xn-1, xn\uf03e\uf0ceR\nsi chiama funzione la legge che associa \uf03cx1, \u2026, xn-1\uf03ead xn\nf(x1, \u2026, xn-1) = xn\nf: X1 \uf0b4\u2026 \uf0b4Xn-1\u2192Xn\nX1 \uf0b4\u2026 \uf0b4Xn-1\u00e8 il tipodella funzione17\n18",
        "10": "9/28/2021\n1002-linguaggio -e-grammatiche -24\n19Funzioni: dominio codominio\ndom(f) = dominio di f\nsottoinsieme di X1 \uf0b4\u2026 \uf0b4Xn-1\ndom(f) = { \uf03cx1, \u2026, xn-1\uf03e\uf0ceX1 \uf0b4\u2026 \uf0b4Xn-1|\n\uf024xn \uf0ceXnf(x1, \u2026, xn-1) = xn}\ncod(f) = codominio di f\nsottoinsieme di Xn\ncod(f) = { xn \uf0ceXn|\n\uf024\uf03cx1, \u2026, xn-1\uf03e\uf0ceX1 \uf0b4\u2026 \uf0b4Xn-1\nf(x1, \u2026, xn-1) = xn}\n20Funzioni: fibra\ndato un xn\nf-1(xn) = controimmagine o fibra di xn\nsottoinsieme di X1 \uf0b4\u2026 \uf0b4Xn-1\nf-1(xn) = {\uf03cx1, \u2026, xn-1\uf03e\uf0ceX1 \uf0b4\u2026 \uf0b4Xn-1|\n\uf03cx1, \u2026, xn-1\uf03e\uf0cedom(f)\n\uf0d9\nf(x1, \u2026, xn-1) = xn}19\n20",
        "11": "9/28/2021\n1102-linguaggio -e-grammatiche -24\n21Funzione\ndominiocodominioX1 \uf0b4\u2026 \uf0b4Xn-1 Xn\n22Funzione totale\ndominiocodominioX1 \uf0b4\u2026 \uf0b4Xn-1 Xn\u2022una funzione f \u00e8 totale se dom(f) = X1, \u2026, Xn-121\n22",
        "12": "9/28/2021\n1202-linguaggio -e-grammatiche -24\n23Funzione suriettiva\ndominiocodominioX1 \uf0b4\u2026 \uf0b4Xn-1 Xn\u2022una funzione f \u00e8 suriettiva se cod(f) = Xn\n24Funzione iniettiva\ndominiocodominioX1 \uf0b4\u2026 \uf0b4Xn-1 Xn\u2022una funzione f \u00e8 iniettiva se |f-1(xn)|=123\n24",
        "13": "9/28/2021\n1302-linguaggio -e-grammatiche -24\n25Funzione biiettiva\ndominiocodominioX1 \uf0b4\u2026 \uf0b4Xn-1 Xn\u2022una funzione f \u00e8 biiettiva (biiezione) se \u00e8 \niniettiva, suriettiva e totale\n26Alfabeto\n\u2022un alfabeto \u00e8 un insieme finito non vuoto di \nsimboli (caratteri)\n\u2022esempi:\n{\u2018M\u2019, \u2018C\u2019, \u2018L\u2019, \u2018X\u2019, \u2018I\u2019, \u2018V\u2019}\n{\u20180\u2019, \u20181\u2019}\n{\u20180\u2019, \u20181\u2019, \u20182\u2019, \u20183\u2019, \u20184\u2019, \u20185\u2019, \u20186\u2019, \u20187\u2019, \u20188\u2019, \u20189\u2019}\n{\u2018a\u2019, \u2018b\u2019, \u2018c\u2019, \u2018d\u2019, \u2018e\u2019, \u2018f\u2019, \u2018g\u2019, \u2018h\u2019, \u2018i\u2019, \u2018l\u2019, \u2018m\u2019, \n\u2018n\u2019, \u2018o\u2019, \u2018p\u2019, \u2018q\u2019, \u2018r\u2019, \u2018s\u2019, \u2018t\u2019, \u2018u\u2019, \u2018v\u2019, \u2018z\u2019}25\n26",
        "14": "9/28/2021\n1402-linguaggio -e-grammatiche -24\n27Concatenazione\n\u2022dato un alfabeto \uf053, definiamo l\u2019operazione binaria \nconcatenazione (denotata con \u201c \u25e6\u201d)\na\u25e6b= abcon a, b\uf0ce\uf053\n\u2022indichiamo con anla concatenazione di acon se stessa n \nvolte\nesempio: a4 = a\u25e6a\u25e6a\u25e6a= aaaa\n\u2022l\u2019operazione \u201c \u25e6\u201d \u00e8 associativa ma non commutativa\n\u2022dati \uf053e \u201c\u25e6\u201d definiamo \uf053+come l\u2019insieme delle stringhe \n(parole) di lunghezza finita\n\u2022se a \uf053+aggiungiamo la stringa vuota \uf065= \u201c\u201d otteniamo \uf053*\n28Linguaggio\n\u2022un linguaggio \u00e8 un sottoinsieme di \uf053*\n\u2022\uf053*\u00e8 detto linguaggio universale\n\u2022il linguaggio vuoto \uf04cnon contiene stringhe \n(nota che \uf04ccoincide con l\u2019insieme vuoto \uf0c6)\n\u2013attenzione:\n\uf04c\uf0ba\uf0c6\n\uf04c\uf0b9{\uf065}27\n28",
        "15": "9/28/2021\n1502-linguaggio -e-grammatiche -24\n29Operazioni sui linguaggi\nL1e L2linguaggi\n\u2022unione\nL1\uf0c8L2= {x\uf0ce\uf053*| x\uf0ceL1\uf0dax \uf0ceL2}\nL1\uf0c8\uf04c= L1\n\u2022intersezione\nL1\uf0c7L2= {x\uf0ce\uf053*| x\uf0ceL1\uf0d9x \uf0ceL2}\nL1\uf0c7\uf04c= \uf04c\n\u2022complementazione\nL1={x\uf0ce\uf053*| x\uf0cfL1}\n30Operazioni sui linguaggi\nL1e L2linguaggi\n\u2022concatenazione o prodotto\nL1\u25e6L2= {x\uf0ce\uf053*|\n\uf024x1\uf0ceL1\uf0d9\uf024x2\uf0ceL2tali che x = x1\u25e6x2}\nL \u25e6{\uf065} = {\uf065} \u25e6L = L\nesempio :\nL1= {an | n\uf0b31};  L2= {bm | m\uf0b31};   L1\u25e6L2= {an bm | n,m\uf0b31}\n\u2022potenza Lhdi un linguaggio L\nL0= {\uf065}\nLh= L \u25e6Lh-1, per h \uf0b3129\n30",
        "16": "9/28/2021\n1602-linguaggio -e-grammatiche -24\n31Operatore di Kleene\n\u2022chiusura riflessiva e transitiva di un linguaggio\nL*= Lh\n\uf065\uf0ceL* \uf04c*={\uf065}\nesempio : L={aa} L*={a2n|n\uf0b30}\n\u2022chiusura transitiva (non riflessiva) di un \nlinguaggio\nL+= Lh\nesempio : L={aa} L+={a2n|n\uf0b31}\nL*=L+ \uf0c8{\uf065}\uf0a5\uf0c8\nh=0\n\uf0a5\uf0c8\nh=1\n32Espressioni regolari\n\u2022\u00e8 uno strumento per descrivere linguaggi (vedremo nel seguito quali)\n\u2022dato un alfabeto \uf053, si definisce espressione regolare ogni stringa r\nr\uf0ce(\uf053\uf0c8{+, *, (, ), \u25e6, \uf0c6})+\n\u2022tale che:\n1.r=\uf0c6oppure\n2.r\uf0ce\uf053oppure\n3.r=(s+t) oppure r=(s\u25e6t) oppure r=s*, con se tespressioni regolari\nespressione linguaggio\n\uf0c6 \uf04c\na\uf0ce\uf053 {a}\n(s+t) L(s)\uf0c8L(t)\n(s\u25e6t) L(s)\u25e6L(t)\ns*L(s)*semantica31\n32",
        "17": "9/28/2021\n1702-linguaggio -e-grammatiche -24\n33Espressioni regolari\ni linguaggi rappresentabili con espressioni regolari sono una \ninteressante sottoclasse st\u00e8 forma sintetica di s\u25e6tforma sintetica\nespressioni sintetiche si ottengno definendo delle \nprecedenze tra gli operatori: *> \u25e6> +\nesempio : \n(a+(b(cd))) = a+bcdforma sinteticaesempio : \n(a+b)*rappresenta L=({ a}\uf0c8{b})*\nesempio : \n(a+b)*arappresenta L={x|x \uf0ce{a,b}*\uf0d9\u201cx termina con a\u201d}\n34Esercizio\ndata una mappa stradale, scrivere un\u2019espressione regolare che \ndefinisca tutti i percorsi possibili tra A e B\nadec +adg+fec+fg= \n= ad(ec+g) + f(ec+g) = \n= (ad+f)(ec+g)A Ba d c e\nf g\n33\n34",
        "18": "9/28/2021\n1802-linguaggio -e-grammatiche -24\n35Esercizio\ndata una mappa stradale, scrivere un\u2019espressione regolare che \ndefinisca tutti i percorsi possibili tra A e B\nf(ebd)*gA Bd e\nf gb\n36Esercizio\ndata una mappa stradale, scrivere un\u2019espressione regolare che \ndefinisca tutti i percorsi possibili tra A e B\na(deb)*decA Ba d c eb\n35\n36",
        "19": "9/28/2021\n1902-linguaggio -e-grammatiche -24\n37Esercizio\ndata una mappa stradale, scrivere un\u2019espressione regolare che \ndefinisca tutti i percorsi possibili tra A e B\na(deb)*dgA Ba d e\ngb\n38Esercizio\ndata una mappa stradale, scrivere un\u2019espressione regolare che \ndefinisca tutti i percorsi possibili tra A e B\nf(edb)*ecA Bd c e\nfb\n37\n38",
        "20": "9/28/2021\n2002-linguaggio -e-grammatiche -24\n39Esercizio\ndata una mappa stradale, scrivere un\u2019espressione regolare che \ndefinisca tutti i percorsi possibili tra A e B\nf(ebd)*g+ f(ebd)*ec+ a(deb)*dec+ a(deb)*dg=\n= f(ebd)*(g+ec) + a(deb)*d(g+ec) =\n= (f(ebd)*+a(deb)*d) (g+ec) =\n= (f+ad)(ebd)*(g+ec)A Ba d c e\nf gb\n40Le grammatiche formali\n1940 Post e Markof, riscrittura e derivazione di \nstringhe\n1950 Chomsky, classificazione delle grammatiche \nnell\u2019ambito degli studi sul linguaggio \nnaturale\n1960 Backus Naur Form per descrivere Algol\u2022approccio generativo alla descrizione dei \nlinguaggi\n\u2022metodo di costruzione delle stringhe basato sulla \nriscrittura39\n40",
        "21": "9/28/2021\n2102-linguaggio -e-grammatiche -24\n41Grammatiche formali\n\u2022grammatiche di Chomsky\n\u2022\uf065-produzioni\n\u2022riconoscimento di linguaggi\n42Grammatiche di Chomsky\nunagrammatica \u00e8 una quadrupla \nG=<VT, VN, P, S>\n\u2022VT \uf0cd\uf053\u00e8 l\u2019alfabeto (finito) di simboli terminali\n\u2022VN\u00e8 un insieme (finito) di simboli non terminali , o \ncategorie sintattiche , tale che VN\uf0c7\uf053= \uf0c6\n\u2022P, detto insieme delle produzioni , \u00e8 una relazione \nbinaria finita su\n(VT\uf0c8VN)*\u25e6VN\u25e6(VT\uf0c8VN)*\uf0b4(VT\uf0c8VN)*\n\u2022S\uf0ceVN\u00e8 l\u2019assioma<\uf061,\uf062>\uf0ceP si indica generalmente con \uf061\u2192\uf062forma sintetica41\n42",
        "22": "9/28/2021\n2202-linguaggio -e-grammatiche -24\n43Esempio\nuna grammatica definisce implicitamente tutte le stringhe \ndi terminali generabili a partire dall\u2019assioma tramite una \nsequenza di riscritture\nesempio:\nG=<{a, b, c}, {S, B, C}, P, S>, con P composto da:\n\uf075S \u2192 aS\uf076S \u2192 B \uf077B \u2192 bB\n\uf078B \u2192 bC\uf079C \u2192 cC\uf07aC \u2192 c\ngenera L(G) = { anbmch| n\uf0b30, m, h\uf0b31}\n\uf061\u2192 \uf0621\uf061\u2192 \uf0622 \u2026\uf061\u2192 \uf062n\nviene anche indicato con\n\uf061\u2192 \uf0621| \uf0622|\u2026|\uf062nforma sintetica\n44Linguaggio generato\n\u2022derivazione diretta : relazione su\n(V*\u25e6VN\u25e6V*) \uf0b4V*\n<\uf06a,\uf079> appartiene alla relazione (si scrive \uf06a\uf0de\uf079 ) se\n\uf024\uf061\uf0ce V*\u25e6VN\u25e6V*ed \uf024\uf062,\uf067,\uf064\uf0ceV*t.c. \uf06a=\uf067\uf061\uf064\uf079=\uf067\uf062\uf064e \uf061\u2192\uf062\uf0ceP\n\uf06ae \uf079sono dette forme di frase\n\u2022derivazione : chiusura riflessiva e transitiva della \nderivazione diretta, si rappresenta con \uf0de*\n\u2022il linguaggio generato da G \u00e8 L(G) = {x|x \uf0ceVT*\uf0d9S\uf0de*x}\n\u2022due grammatiche G1e G2sono equivalenti se \nL(G1)=L(G2)\ntalvolta \uf0deal posto di \uf0de*forma sinteticaVT\uf0c8VNviene talvolta indicato con Vforma sintetica43\n44",
        "23": "9/28/2021\n2302-linguaggio -e-grammatiche -24\n45Grammatiche formali\nesempio : generazione di { anbncn|n\uf0b31}\ngrammatica G=<{ a, b, c},{S,B,C,F,G},P,S>\ncon P composto da\n\uf075S \u2192 aSBC \uf076CB \u2192 BC\n\uf077SB \u2192 bF\uf078FB \u2192 bF\n\uf079FC \u2192 cG\uf07aGC \u2192 cG\n\uf07bG \u2192 \uf065\nper generare aabbcc\nS\uf0de\uf075aSBC \uf0de\uf075aaSBCBC \uf0de\uf076aaSBBCC\n\uf0de\uf077aabFBCC \uf0de\uf078aabb FCC \uf0de\uf079aabbc GC\n\uf0de\uf07aaabbcc G \uf0de\uf07baabbcc\n46Grammatiche formali\nosservazione : non \u00e8 detto che una sequenza di derivazioni \nporti ad una stringa del linguaggio\nesempio : \nla grammatica G=<{ a, b, c}, {S, A}, P, S> con\nS \u2192 aSc| A\nA \u2192 bAc| \uf065\ngenera {anbmcn+m|n,m\uf0b30}\nesempio : \nla grammatica G=<{a, b, c}, {S, A}, P, S> con\nS \u2192 Ab\nA \u2192 Sa\ngenera \uf04c45\n46",
        "24": "9/28/2021\n2402-linguaggio -e-grammatiche -24\n47Grammatiche di Chomsky\n\u2022di tipo 0, non limitate\n\u2022di tipo 1, context sensitive, contestuali\n\u2022di tipo 2, context free (CF), non contestuali\n\u2022di tipo 3, lineari destre (RL), regolari\n48Grammatiche di Chomsky\ndi tipo 0, non limitate\n\u2022sono le meno restrittive\n\u2022produzioni del tipo\n\uf061\u2192\uf062, \uf061\uf0ceV*\u25e6VN\u25e6V*, \uf062\uf0ceV*\nammettono anche derivazioni che accorciano stringhe\nlinguaggi di tipo 0\nesempio :\nil linguaggio { anbn|n\uf0b31} \u00e8 di tipo 0 in quanto generato da\nS \u2192 aAB B \u2192 b\naA \u2192 aaAbaAb\u2192 ab\naAA \u2192 aA 47\n48",
        "25": "9/28/2021\n2502-linguaggio -e-grammatiche -24\n49Grammatiche di Chomsky\ndi tipo 1, context sensitive, contestuali\n\u2022produzioni che non riducano la lunghezza delle forme di frase\n\uf061\u2192\uf062, |\uf061|\u2264|\uf062|, \uf061\uf0ceV*\u25e6VN\u25e6V*, \uf062\uf0ceV*\nlinguaggi di tipo 1\nesempio :\nil linguaggio { anbncn|n\uf0b31} \u00e8 di tipo 0 in quanto generato da\nS \u2192 aSBC CB \u2192 BC \nSB \u2192 bF FB \u2192 bF\nFC \u2192 cG GC \u2192 cG\ncG \u2192 c\nma \u00e8 anche di tipo 1, infatti \u00e8 generato anche da\nS \u2192 aSBc| aBc\ncB \u2192 Bc\nbB \u2192 bb\naB \u2192 ab\n50Generazione di stringhe di anbncn\n(1)S \u2192 aSBc | aBc    (2)cB \u2192 Bc \n(3)bB \u2192 bb (4)aB \u2192 ab\nS\uf0deaSBc\n\uf0deaaSBcBc\n\uf0deaaaSBcBcBc\n\uf0deaaaaBcBcB cBc\n\uf0deaaaaBcB cBBcc \n\uf0deaaaaBcBB cBcc \n\uf0deaaaaB cBBBccc \n\uf0deaaaaBB cBBccc \n\uf0deaaaaBBB cBccc \uf0deaaaaBBBBcccc\n\uf0deaaaabBBBcccc\n\uf0deaaaab bBBcccc\n\uf0deaaaabb bBcccc\n\uf0deaaaabbbbcccc49\n50",
        "26": "9/28/2021\n2602-linguaggio -e-grammatiche -24\n51Grammatiche di Chomsky\ndi tipo 2, context free (CF), non contestuali\n\u2022produzioni del tipo\nA\u2192\uf067, A\uf0ceVN, \uf067\uf0ceV+\nlinguaggi di tipo 2\nesempio :\nil linguaggio { anbn|n\uf0b31} \u00e8 di tipo 0 in quanto generato da\nS \u2192 aAb\naA \u2192 aaAb\nA \u2192 \uf065\nma \u00e8 anche di tipo 2, infatti \u00e8 generato anche da\nS \u2192 aSb| ab\n52Esempi di linguaggi di tipo 2\nlinguaggio delle espressioni aritmetiche con la variabile i (come per \nesempio \u201ci*i+(i*i+(i))*i*i \u201d, oppure \u201c((i+i)*i) \u201d). \nL\u2019assioma \u00e8 E. \nE \u2192 E+T | T\nT \u2192 T*F | F\nF \u2192 i| (E)\ngrammatica delle parentesi ben bilanciate (esempio \u201c (((())))() \u201d)\nS \u2192 ()| SS | (S)\nda quale sequenza di produzioni \u00e8 generata \u201c ()((()())) \u201d ?\ngrammatica delle stringhe palindrome (esempio \u201celle\u201d, \u201cereggere\u201d)51\n52",
        "27": "9/28/2021\n2702-linguaggio -e-grammatiche -24\n53Grammatiche di Chomsky\ndi tipo 3, lineari destre (RL), regolari\n\u2022produzioni del tipo\nA\u2192\uf064, A\uf0ceVN, \uf064\uf0ce(VT\u25e6VN)\uf0c8VT\n\u2022linguaggi di tipo 3\nesempio :\nil linguaggio {anb|n\uf0b30} \u00e8 di tipo 3 in quanto generato da\nS \u2192 aS\nS \u2192 b\nsi possono anche definire grammatiche lineari sinistre (LL) con \nA\u2192\uf064, A\uf0ceVN, \uf064\uf0ce(VN\u25e6VT)\uf0c8VT\nesempio : il linguaggio { anb|n\uf0b30} \u00e8 anche generato da\nS \u2192 Tb| b\nT \u2192 a| Ta\nteorema : i linguaggi generati da grammatiche LL e RL coincidono\n54Grammatiche di Chomsky\nun linguaggio \u00e8 strettamente di tipo n se esiste una \ngrammatica di tipo n che lo genera e non esiste una \ngrammatica di tipo m>n che lo genera\nesempio : il linguaggio { anbn|n\uf0b31} \u00e8 generato da \nuna grammatica di tipo 2 e non \u00e8 generato da \nnessuna grammatica di tipo 353\n54",
        "28": "9/28/2021\n2802-linguaggio -e-grammatiche -24\n55Grammatiche di Chomsky\ncontenimento tra i linguaggitipo 0\ntipo 1\ntipo 2\ntipo 3\n56Grammatiche di Chomsky\nquadro riassuntivo della classificazione delle \ngrammatiche secondo Chomskytipo produzioni vincoli\ntipo 0\nnon limitate\uf061\u2192\uf062 \uf061\uf0ceV*\u25e6VN\u25e6V*, \uf062\uf0ceV*\ntipo 1\ncontestuali\uf061\u2192\uf062|\uf061|\u2264|\uf062|\n\uf061\uf0ceV*\u25e6VN\u25e6V*, \uf062\uf0ceV*\ntipo 2\nnon contestualiA\u2192\uf067 A\uf0ceVN, \uf067\uf0ceV+\ntipo 3\nregolariA\u2192\uf064 A\uf0ceVN, \uf064\uf0ce(VT\u25e6VN)\uf0c8VT55\n56",
        "29": "9/28/2021\n2902-linguaggio -e-grammatiche -24\n57\uf065-produzioni\n\u2022con grammatiche di tipo1, 2, 3 non \u00e8 possibile\ngenerare la stringa vuota\uf065\n\u2013per generare \uf065occorre una produzione \uf061\u2192\uf065che\nviene detta\uf065-produzione\n\u2013per Chomsky tutti ilinguaggi checontengono \uf065-\nproduzioni sono linguaggi di tipo0\n\u2022qual \u00e8 l\u2019impatto sui corrispondenti linguaggi\ndelle\uf065-produzioni nelle grammatiche ?\n\u2013se ammettiamo \uf065-produzioni dobbiamo fare \nattenzione , altrimenti rischiamo di snaturare la \ngerarchia di Chomsky\n58\uf065-produzioni : variazione della gerarchia\ncon le seguenti modifiche , ilinguaggi generati dale \ndiverse tipologie di grammatiche rimangono inalterati , \nsalvo per la possibilit\u00e0 di generare la stringa vuota\ntipo\uf065-produzioni ammesse\n0 tutte (per definizione)\n1 solo sull\u2019assioma quando quest\u2019ultimo non \ncompare mai a destra di una produzione\n2 tutte\n3 tutte57\n58",
        "30": "9/28/2021\n3002-linguaggio -e-grammatiche -24\n59Esempi di grammatiche\n\u2022il linguaggio {w \u25e6w| w \uf0ce(a+b)*}\n\u2022\u00e8 generato dalla grammatica contestuale\nS \u2192 T | \uf065\nT \u2192 aAT | bBT | A0a| B0bAa\u2192 aA\nAb\u2192 bA\nBa\u2192 aB\nBb\u2192 bBAA0\u2192 A0a\nBA0\u2192 A0b\nAB0\u2192 B0a\nBB0\u2192 B0b(1) (2) (3)\nA0\u2192 a\nB0\u2192 b(4)\n\u2022le (1) generano insieme caratteri della prima e della seconda \nstringa; A0(B0) \u00e8 l\u2019ultimo carattere della prima stringa\n\u2022le (2) e le (3) separano la prima stringa dalla seconda\n\u2022le (4) chiudono la generazione, se sono applicate troppo presto \nil processo diverge\n60Esempi di grammatiche\n\u2022il linguaggio {(x #)*| x = permutazione di (a,b,c)} (che contiene \nper esempio le stringhe \uf065, abc# , acb# , bac#cab# , ecc)\n\u2022ma \u00e8 generato anche dalla grammatica CF:\nS \u2192 E#S | \uf065 E \u2192 abc | acb | cba | cab | bac | bca\n\u2022ed anche dalla grammatica regolare:\nS \u2192 aX | bY | cZ | \uf065\nX \u2192 bX\uf0a2| cX\uf0b2\nX\uf0a2\u2192 cR\nX\uf0b2\u2192 bRR \u2192 #S\nY \u2192 aY\uf0a2| cY\uf0b2\nY\uf0a2\u2192 cR\nY\uf0b2\u2192 aRZ \u2192 aZ\uf0a2| bZ\uf0b2\nZ\uf0a2\u2192 bR\nZ\uf0b2\u2192 aRS \u2192 S\u2019 | \uf065 A \u2192 a\nB \u2192 b\nC \u2192 cAB \u2192 BA\nAC \u2192 CA\nBC \u2192 CBS\u2019 \u2192 ABC#\nS\u2019 \u2192 ABC#S\u2019\u00e8 generato dalla grammatica contestuale:59\n60",
        "31": "9/28/2021\n3102-linguaggio -e-grammatiche -24\n61Forma normale di Backus\n\u2022la BNF \u00e8 una notazione CF con alcuni accorgimenti \nsintattici che ne aumentano la leggibilit\u00e0\n<sequenza istruzioni> ::= <istruzione>; \n{<istruzione>;}\n<istruzione if> ::= if ( <condizione> ) \n<istruzione> [else <istruzione>]esempio\npu\u00f2 essere riscritto:\nQ \u2192 I;| I;Q\npu\u00f2 essere riscritto:\nF \u2192 if( C )I else I | if( C )I\n62Riconoscimento dei linguaggi\n\u2022esistono linguaggi a cui non corrisponde alcun algoritmo \ndi decisione\n\u2022i linguaggi di tipo 3 sono riconosciuti da dispositivi con \nmemoria costante in tempo lineare (automi a stati finiti)\n\u2022i linguaggi strettamente di tipo 2 sono riconosciuti da \ndispositivi non deterministici con pila in tempo lineare \n(automi a pila non deterministici)problema : \nstabilire se una stringa appartiene ad un dato linguaggio61\n62",
        "32": "9/28/2021\n3202-linguaggio -e-grammatiche -24\n63Riconoscimento dei linguaggi\n\u2022i linguaggi strettamente di tipo 1 sono riconosciuti da \ndispositivi non deterministici con memoria che cresce \nlinearmente con la lunghezza della stringa da esaminare \n(automi non deterministici \u201clinear bounded\u201d)\n\u2022i linguaggi strettamente di tipo 0 sono riconosciuti da \nmacchine di Turing con memoria e tempo illimitati\n\u2013\u00e8 possibile che non esista un algoritmo di decisione ma un \nprocesso semidecisionale, in cui, se la stringa non fa parte del \nlinguaggio non \u00e8 detto che la computazione termini\n63"
    },
    "022-espressioni-regolari-07.pdf": {
        "1": "1Esercizi di Informatica Teorica\nEspressioni regolari\nqueste esercitazioni sono il frutto del lavoro di molte persone, tra le quali \nLuca Cabibbo, Walter Didi mo e Giuseppe Di Battista",
        "2": "2\nNotazione sulla difficolt\u00e0 degli esercizi\nfacile\nnon difficile\ndifficile\nmolto difficile\n",
        "3": "3Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "4": "4Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "5": "5Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "6": "6Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "7": "7Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "8": "8Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "9": "9Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "10": "10Espressioni regolari\nesercizio 1\ndire se le seguenti affermazioni sono vere o false:\n1.a L(\u2205*) = \u2205\n1.b baa \u2208L(a*b*a*b*)\n1.c abcd \u2208L( (a (cd)*b)*)\n1.d L(a*b*)\u2229L(b*a*)=  L ( a*+b*)\n1.e L((ab)*)\u2229L((cd)*)=  \u2205\n1.f L((abb + a)*a) = L(a(bba + a)*)\n1.g L((a+b)*) = L((a*b*)*)\n",
        "11": "11Espressioni regolari\nesercizio 2\nquali linguaggi sono descritti dalle seguenti espressioni regolari? \n2.a 1(0+1)*\n2.b (0+1)*1(0+1)*\n",
        "12": "12Espressioni regolari\nesercizio 3\nscrivere le espressioni regolari corrispondenti ai seguenti linguaggi \nsu \u03a3=  { 0, 1}\n3.a tutte le sequenze alternate (cio\u00e8 che non contengono n\u00e9 00\nn\u00e911) di 0e 1che iniziano e finiscono per 1o che iniziano \ne finiscono per 0\n3.b tutte le sequenze con un numero pari di 0",
        "13": "13Espressioni regolari\nesercizio 4\nscrivere l\u2019espressione regolare che descrive il complemento dei \nseguenti linguaggi su \u03a3=  { 0, 1}\n4.a 1(0+1)*\n4.b 0*+1*\n",
        "14": "14Espressioni regolari\nesercizio 5\nsemplificare le seguenti espressioni regolari su \u03a3={a, b, c}\n5.a (a*b+b*cb)*\n5.b ((a*b*)*(b*a*)*)*\n",
        "15": "15Espressioni regolari\nesercizio 6\ndeterminare le espressioni re golari per i seguenti linguaggi\n6.a i numeri naturali in notazione binaria \n6.b i numeri binari su 4 bit \n6.c i numeri naturali in base 10\n6.d i numeri naturali pari \n6.e i numeri pari in base 3\n",
        "16": "16Soluzioni\nsoluzione esercizio 3\n3.a (10)*1+(01)*0\n3.b 1*(01*01*)*\nsoluzione esercizio 4\n4.a (0(0+1)*)*\n4.b ((1+0)*0(1+0)*1(1+0)*)+((1+0)*1(1+0)*0(1+0)*) \noppure \n(0+1)*(01+10)(0+1)*",
        "17": "17Soluzioni\nsoluzione esercizio 6\n6.a i numeri naturali in notazione binaria \n0+1(0+1)*\n6.b i numeri binari su 4 bit \n(0+1) (0+1) (0+1) (0+1)\n6.c i numeri naturali in base 10\n0+(1+2+3+4+5+6+7+8+9)(0+1+2+3+4+\n5+6+7+8+9)*",
        "18": "18Soluzioni\n6.d i numeri naturali pari \n(0+2+4+6+8)+(1+2+..+9)(0+1+..+9)*(0+2+4+6\n+8)\n6.e i numeri pari in base 3\nsi noti che i numeri pari in base tre sono tutte e \nsole quelle sequenze di cifre in { 0,1,2} con un \nnumero pari di 1\nSia \u03b1= (2(0+2)*) e \u03b2= (1(0+2)*1(0+2)*)\nIl linguaggio \u00e8 rappresentabile da:\n\u03b1\u03b1*\u03b2*+  \u03b1*\u03b2\u03b2*+  0"
    },
    "025-cardinalita-transfinite-06.pdf": {
        "1": "1Informatica Teorica\nCardinalit\u00e0 transfinite",
        "2": "2Pidgeonhole principle\nteorema : \ndati due insiemi A e B tali che\n0 < |B| < |A| < \u221e\nnon esiste una funzione f: A \u2192B che sia totale e \niniettiva\ndimostrazione : \nbasata sulla cardinalit\u00e0 di B e per induzione",
        "3": "3Pidgeonhole principle\nma questa \nassegnazione \nnon \u00e8 totale!",
        "4": "4Pidgeonhole principle\n\u2026 e questa non \n\u00e8 iniettiva!\na/1\n",
        "5": "5Dimostrazione (pidgeonhole principle)\n\u2022 dimostrazione per induzione\n\u2013 passo base: |B|=1\n\u2013 passo induttivo: |B|>1\n\u2022 passo base (|B|=1)\nB={b}, |A|>1, es. A={a1,a2}\nse f \u00e8 totale, allora f(a1)=b e f(a2)=b\nallora f non \u00e8 iniettiva perch\u00e9 |f -1(b)|>1",
        "6": "6Dimostrazione (pidgeonhole principle)\n\u2022 passo induttivo: |B|>1\nsupponiamo sia vero per |B| = n ed |A| \u2265n+1\ndimostriamo che \u00e8 vero per |B| = n+1 e |A| \u2265n+2\nipotizziamo per assurdo che esista una funzione totale \niniettiva f e scegliamo un qualunque elemento b di B\nse |f -1(b)|\u22652 \u21d2contraddizione \u21d2teorema dimostrato\nse |f -1(b)|\u22641 consideriamo\nA\u2032=A-{f -1(b)}   e    B \u2032=B-{b} \n|A\u2032| \u2265n+1 > |B\u2032| = n\napplichiamo l\u2019ipotesi induttiva \u21d2contraddizione ",
        "7": "Considerazioni sul pidgeonhole principle \n\u2022 il pidgeonhole principle mette in relazione la \nnumerosit\u00e0 degli insiemi con le propriet\u00e0 delle funzioni che hanno gli insiemi come domini o codomini\n\u2022 in particolare se esiste una funzione biettiva \nf: A\u2192B\n\u2013 esiste una funzione totale ed iniettiva f: A \u2192B \n\u2013 esiste una funzione totale ed iniettiva f-1: B\u2192A \n\u2013 per il pidgeonhole principle non pu\u00f2 essere |B| > |A| \nn\u00e9 |A| > |B|\n7",
        "8": "8Cardinalit\u00e0 di insiemi infiniti\n\u2022 due insiemi sono equinumerosi se esiste una \nbiiezione tra essi\n\u2022 la relazione di equinumerosit\u00e0 \u00e8 una relazione \ndi equivalenza\n\u2022 possiamo ora dare una definizione rigorosa di \ncardinalit\u00e0 di un insieme finito A:\n|A|=0 se A= \u2205\n|A|=n se A \u00e8 equinumeroso a {0, 1, \u2026, n-1}",
        "9": "9Numerabilit\u00e0\n\u2022 insiemi numerabili\n\u2013 un insieme \u00e8 numerabile se \u00e8 equinumeroso a N\n\u2013 un insieme ha cardinalit\u00e0 aleph zero (\u05d00) se \u00e8\nequinumeroso a N, cio\u00e8 se \u00e8 numerabile\n\u2022 insiemi contabili\n\u2013 un insieme \u00e8 contabile se \u00e8 finito o numerabile\n\u2013 sottoinsiemi di insiemi contabili sono contabili",
        "10": "10Numerabilit\u00e0: \u05d00+ k = \u05d00\nteorema : \nper ogni intero k, l\u2019insieme Nkdegli interi \nmaggiori o uguali a k \u00e8 numerabile\ndimostrazione : \nbiiezione con N\nN : 01234\u2026\nNk:k+0 k+1 k+2 k+3 k+4 \u2026",
        "11": "11Numerabilit\u00e0 degli interi relativi\nteorema : \nl\u2019insieme Z degli interi relativi \u00e8 numerabile\ndimostrazione : \nbiiezione con N\nZ:01-12-23-34-4\u2026\nN : 012345678\u2026",
        "12": "12Numerabilit\u00e0 dei numeri pari ( \u05d00 + \u05d00= \u05d00)\nteorema : \nl\u2019insieme P dei numeri pari \u00e8 numerabile\ndimostrazione : \nbiiezione con N\nP:024681 0 1 2 1 4 1 6 \u2026\nN : 012345678\u2026",
        "13": "13Numerabilit\u00e0: \u05d00\u00d7\u05d00 = \u05d00\nteorema : \nl\u2019insieme N2delle coppie di naturali \u00e8 numerabile\ndimostrazione : \ntecnica usata da Cantor per mostrare la \nnumerabilit\u00e0 di Q\n0    1    2    3    4\n00    1    3    6    10\n12    4    7    11\n25    8    12 \n39   13\n414osservazione : \nper ogni n \u2208N, se A \u00e8 numerabile, \nanche An\u00e8 numerabile",
        "14": "14Insiemi non numerabili\nper dimostrare la non numerabilit\u00e0 di un insieme si \nusa la tecnica di diagonalizzazione di Cantor\nteorema :  R non \u00e8 numerabile\ndimostrazione : \n1. dimostriamo che R \u00e8 equinumeroso a (0,1)\n2. dimostriamo che (0,1) non \u00e8 numerabile",
        "15": "15Insiemi non numerabili\n(0,1) e R sono equinumerosi:  una biiezione \u00e8 data, per\nesempio, dalla funzione y =\n00.10.20.30.40.50.60.70.80.91\n- 1 0 - 9- 8- 7- 6- 5- 4- 3- 2- 1 0 1 2 3 4 5 6 7 8 9 1 0(2x+1)1\ny\nx",
        "16": "16Insiemi non numerabili\n\u2022 Supponiamo per assurdo che una enumerazione \ndi (0,1) esista, denotiamo con \u03a6il\u2019iesimo \nelemento di (0,1)\n\u2022 consideriamo r \u2208(0,1) che ha come i-esima cifra \ndella mantissa (i=1, 2, \u2026) un valore diverso da 0, \nda 9, e dal valore della i-esima cifra di \u03a6i",
        "17": "17Insiemi non numerabili\nr, detto elemento diagonale , non fa parte della \nenumerazione, in quanto differisce da ogni elemento della enumerazione in almeno una cifra, e ci\u00f2 \u00e8\nassurdocifre delle mantisse di \u03a6\ni:\n1234567\u2026\n\u03a615104396\u2026\n\u03a622410000\u2026\n\u03a637985377\u2026\n\u03a640046031\u2026\nr 6517 \u2026\u2026\u2026\u2026",
        "18": "Nota sulla scelta delle cifre di r\n\u2022 le cifre dell\u2019elemento diagonale rsono scelte in \nmodo da essere diverse da 0 e da 9\n\u2013 non si pu\u00f2 generare la mantissa 0000\u2026 che non \nappartiene all\u2019insieme\n\u2013 non si possono generare numeri terminanti con 9 \nperiodico che corrispondono ad una seconda \nrappresentazione di un numero non-periodico\n\u2022 0.999\u2026 coincide con 1\n\u2022 0.123999\u2026 coincide con 0.124\n18",
        "19": "19Insiemi non numerabili\nteorema :  P(N) non \u00e8 numerabile\ndimostrazione :\nsupponiamo per assurdo che lo sia\nsia P1, P2, \u2026, Pi, \u2026 una sua enumerazione\na ciascun Piassociamo la sequenza\nbi0, bi1, bi2, \u2026, dove\nbij=0 se j\u2209Pi\nbij=1 se j\u2208Pi",
        "20": "20Insiemi non numerabili\ncostruiamo ora l\u2019insieme P(diagonale) con \nsequenza p0, p1, \u2026, pk,\u2026 dove\npk= 1 - bkk\nPdifferisce da ogni Pi, in quanto\ni\u2208P\u21d4i\u2209Pi\nosservazione : la non numerabilit\u00e0 di P(N) vale \nanche per l\u2019insieme delle parti di ogni insieme di \ncardinalit\u00e0 \u05d00",
        "21": "21Cardinalit\u00e0 transfinite\nteorema : R \u00e8 equinumeroso a P(N) ed \u00e8 quindi \ncontinuo\ndimostrazione : \n\u00e8 sufficiente mostrare che la propriet\u00e0 vale per i \nreali in (0,1), vista la biiezione tra R e (0,1)\nuso della rappresentazione binaria della mantissa e \ndel concetto di funzione caratteristica",
        "22": "22Cardinalit\u00e0 transfinite \u2013 notazione aleph\n\u2022 se un insieme finito ha cardinalit\u00e0 n, il suo \ninsieme delle parti ha cardinalit\u00e0 2n\n\u2022 analogamente, se un insieme infinito ha \ncardinalit\u00e0 \u05d00denotiamo con 2\u05d00la \ncardinalit\u00e0 del suo insieme delle parti\n\u2022 gli insiemi con cardinalit\u00e0 2\u05d00sono detti \ncontinui\n\u2022 Cantor ha dimostrato che esistono infiniti \ncardinali transfiniti ( \u05d00, 2\u05d00, 22\u05d00, \u2026)",
        "23": "Conseguenze della teoria\n\u2022 vedremo come considerazioni relative alla cardinalit\u00e0 di \ninsiemi infiniti daranno interessanti spunti sull\u2019idea di \ncalcolabilit\u00e0\n\u2022 per il momento ci limitiamo alla seguente riflessione\n\u2013 un linguaggio \u00e8 un sottoinsieme di \u03a3*\n\u2022 qual \u00e8 la cardinalit\u00e0 di \u03a3*?\n\u2022 qual \u00e8 la cardinalit\u00e0 di P( \u03a3*)?\n\u2022 quanti linguaggi esistono?\n\u2013 un programma in un linguaggio di programmazione qualsiasi \npu\u00f2 essere considerato come una sequenza finita di caratteri\n\u2022 quanti sono i possibili progr ammi che possiamo scrivere?\n23"
    },
    "Database schema mapping using Machine learning with feature selection.pdf": {
        "1": "Database Schema Matching Using Machine\nLearning with Feature Selection\nJacob Berlin and Amihai Motro\nInformation and Software Engineering Department\nGeorge Mason University, Fairfax, VA 22030\nfjberlin, ami g@gmu.edu\nAbstract. Schema matching, the problem of \ufb01nding mappings between\nthe attributes of two semantically related database schemas, is an im-\nportant aspect of many database applications such as schema integra-\ntion, data warehousing, and electronic commerce. Unfortunately, schema\nmatching remains largely a manual, labor-intensive process. Further-\nmore, the e\ufb00ort required is typically linear in the number of schemas\nto be matched; the next pair of schemas to match is not any easier than\nthe previous pair. In this paper we describe a system, called Automatch,\nthat uses machine learning techniques to automate schema matching.\nBased primarily on Bayesian learning, the system acquires probabilistic\nknowledge from examples that have been provided by domain experts.\nThis knowledge is stored in a knowledge base called the attribute dic-\ntionary . When presented with a pair of new schemas that need to be\nmatched (and their corresponding database instances), Automatch uses\nthe attribute dictionary to \ufb01nd an optimal matching. We also report\ninitial results from the Automatch project.\n1 Introduction\nSchema matching is the problem of \ufb01nding mappings between the attributes of\ntwo semantically related database schemas. The schema matching problem is an\nimportant, current issue for many database applications such as schema integra-\ntion, data warehousing, and electronic commerce [12, 15]. Unfortunately, schema\nmatching remains largely a manual, labor-intensive process. Furthermore, the\ne\ufb00ort required is typically linear in the number of schemas to be matched; the\nnext pair of schemas to match is not any easier than the previous pair. Thus,\ndatabase applications that require schema matching are limited to environments\nin which the set of member information sources is small and stable. These ap-\nplications would scale-up to much larger communities of member sources if the\nschema matching \u201cbottleneck\u201d was broken by automating the matching process.\nIn this paper we discuss such a system, called Automatch, for automating\nthe schema matching process. Based primarily on Bayesian learning, the sys-\ntem acquires probabilistic knowledge from examples of schemas that have been\n\u201cmapped\u201d by domain experts into a knowledge base of database attributes called\ntheattribute dictionary . Roughly speaking, this dictionary characterizes di\ufb00erent",
        "2": "attributes by means of their possible values and the probability estimates of these\nvalues. Furthermore, the dictionary may be extended to contain any attribute\nmetadata that has a probabilistic interpretation (e.g. attribute names or string\npatterns).\nWhen presented with a pair of \u201cclient\u201d schemas that need to be matched (and\ntheir corresponding database instances), Automatch matches them \u201cthrough\u201d its\ndictionary. Using probabilistic methods, an attempt is made to match every at-\ntribute of one client schema with every attribute of the other client schema,\nresulting in individual \u201cscores.\u201d An optimization process based on a Minimum\nCost Maximum Flow network algorithm \ufb01nds the overall optimal matching be-\ntween the two client schemas, with respect to the sum of the individual attribute\nmatching scores.\nTo overcome the problem of very large dictionaries caused by very large\nattribute domains, Automatch employs statistical feature selection techniques\nto learn an e\ufb03cient representation of the examples. That is, each attribute is\nrepresented with a minimal set of most informative values. Thus the attribute\ndictionary is made human understandable through aggressive reduction in the\nnumber of values. Although the example schemas may contain many thousands\nof values, we are able to focus learning on a very small subset, consisting of as\nfew as 10% of the initial values.\nThe results of our initial experimentation with Automatch are encouraging\nas they show performance that exceeds 70% (measured as the harmonic mean\nof the soundness and the completeness of the matching process). Although the\nattribute dictionary was built for Automatch, we conjecture that it could be\nemployed as a knowledge asset in other schema matching systems.\nThe remainder of this paper is organized as follows. Section 3 describes the\nbasic methodology of Automatch; in particular, the probabilistic information\nin the acquired knowledge base and how it is used to infer optimal matchings\nbetween \u201cclient\u201d schemas. Section 4 describes alternative methods for reducing\nthe size of the knowledge base through feature selection. Section 5 explains\nthe experiment and its conclusions. Section 6 summarizes the contributions and\nsuggests future research directions. We begin with a brief discussion of other\npublished approaches and how they are related to Automatch.\n2 Related Work\nA thorough discussion of schema matching techniques and implementations can\nbe found in [6, 11, 15]. Here we mention two such approaches and compare them\nto Automatch. Automated schema matching can be classi\ufb01ed as rule based and\nlearner based [6].\nThe Artemis system [5] is a rule-based approach for schema integration. This\nsystem determines the a\ufb03nity of attributes from two schemas in a pair-wise fash-\nion. A\ufb03nity is based on comparisons of attribute names, structure, and domain\ntypes and is scored on a [0,1] interval. The process relies on thesauri to deter-\nmine semantic relationships. The system uses hierarchical clustering based on",
        "3": "a\ufb03nity values to group together related attributes. Finally, a set of uni\ufb01cation\nrules are employed to interactively guide a user through the construction of an\nintegrated schema. In contrast with Automatch, Artemis considers schema in-\nformation; Automatch considers instance information. Furthermore, knowledge\nin Artemis is \u201cpre-coded\u201d in the thesaurus and uni\ufb01cation rules; knowledge in\nAutomatch is learned from examples.\nSemInt [9, 10] is a learner-based system that uses neural networks to identify\nsimilar attributes from di\ufb00erent schemas. This system uses a combination of\nschema and instance information. Schema information includes such information\nas data types, \ufb01eld length, and constraint information. Instance information\nincludes such information as value distributions, character ratios, numeric mean\nand variance.\nFor each type of information the system exploits, it determines a numerical\nvalue on a [0 ;1] interval. A tuple of these numerical values for one attribute\nis the signature of the attribute. The system uses these signatures to cluster\nsimilar attributes within the same schema. The system then uses the signatures\nof the cluster centers to train a neural network to output an attribute category\nbased on the input signatures. Given a new schema, the system determines the\nsignature of each schema attribute using the same type of schema and instance\ninformation used for training. These signatures are then applied to the neural\nnetwork to determine the category of the respective attributes. In contrast with\nAutomatch, SemInt uses a \ufb01xed set of features for learning; Automatch combines\nfeature selection with learning to \ufb01nd an optimal set of features for a given\nproblem domain. Furthermore, SemInt discovers matches to attribute clusters;\nAutomatch discovers matches to individual attributes.\n3 Methodology\nThis section describes the basic methodology of Automatch, providing details of\nits data structures and algorithms. It begins with an intuitive description of the\napproach and a formal description of the problem.\n3.1 The Overall Approach\nAutomatch is based on a knowledge base about schema attributes which is con-\nstructed from examples. When presented with two new \u201cclient\u201d schemas that\nneed to be matched (and their corresponding database instances), Automatch\nchecks every client attribute against its attribute dictionary, obtaining individual\n\u201cmatching scores\u201d for each pair of client attribute and dictionary attribute.\nThese client-dictionary attribute scores are combined to generate client-client\nattribute scores. To illustrate, assume Bis an attribute of one client scheme, C\nis an attribute of the other client scheme, and Ais an attribute of the dictionary,\nand assume that the matching of BtoAis scored w1and the matching of Cto\nAis scored w2; then the matching B$Creceives the score w1+w2.1\n1We combine the individual scores by their sum, but other combinations are also\npossible; for example, their product.",
        "4": "In turn, these individual client-client attribute scores are combined to gen-\nerate overall schema-schema matching scores. To illustrate, assume schemas\nR1=fB1; B2gandR2=fC1; C2gand assume the client-client attribute scores:\nw1:B1$C1,w2:B1$C2,w3:B2$C1, and w4:B2$C2. The schema\nmatching fB1$C2; B2$C1gis then scored w2+w3. Other schema matchings\nare scored similarly.\nIn a subsequent optimization process, Automatch \ufb01nds the schema matching\nwith the highest schema-schema score.\n3.2 Formalization of the Problem\nOur formalization is based on the relational model. However, we are con\ufb01dent\nthat the methods can be extended to other models, such as the object-oriented\nor the semi-structured models. A database schema is simply a \ufb01nite set of at-\ntributes fA1; : : : ; A ng. Given two database schemas R1=fB1; : : : ; B pgand\nR2=fC1; : : : ; C qg, amatching is a mapping between a subset of R1and a\nsubset of R2.\nWe assume a knowledge base about database attributes, called the attribute\ndictionary and denoted D. In this knowledge base, each attribute is characterized\nby a select set of possible values and their probability estimates.\nIn addition, we assume a scoring function fthat, given (1) the attribute dic-\ntionary D, (2) a pair of database schemas R1andR2, (3) a pair of corresponding\ndatabase instances r1andr2, and (4) a matching between R1andR2, issues a\nvalue (a real number), that indicates the \u201cgoodness\u201d of the matching.\nThe problem is then to \ufb01nd the bestmatching for two given schemas R1and\nR2. This abstract description leaves two major issues to be discussed in detail:\n1. The nature of the attribute dictionary Dand the scoring function f.\n2. The optimization of f(i.e., \ufb01nding the best schema matching).\nThese two issues are discussed in the next two subsections.\n3.3 The Attribute Dictionary and the Scoring Function\nThe attribute dictionary Dconsists of a \ufb01nite set of schema attributes fA1; : : : ; A rg.\nEach attribute in the attribute dictionary is characterized by a set of possible\nvalues and their probability estimates . The attribute dictionary serves as a knowl-\nedge base that accumulates information about attributes. All attempts to match\nattributes of client schemas refer to this knowledge base. We use Bayesian learn-\ning to populate the attribute dictionary with example values provided by domain\nexperts.\nRecall from the intuitive description in Section 3.1 that the \ufb01rst task is to\ndetermine client-dictionary attribute scores.\nLetXbe a client attribute, let Adenote a dictionary attribute, and let V\ndenote a set of values that are observed in X(these values are derived from the\ninstance of the client schema to which Xbelongs).",
        "5": "LetP(A) be the prior probability that Xmaps to A(before observing any\nvalues of X), let P(V) represent the unconditional probability of observing values\nVinX, and let P(VjA) represent the conditional probability of observing the\nvalues V, given that Xmaps to A. Bayes Theorem states that\nP(AjV) =P(VjA)\u00a2P(A)\nP(V): (1)\nP(AjV) is referred to as the posterior probability that Xmaps to A, because\nit re\ufb02ects the probability that a mapping of XtoAholds after the values V\nhave been observed. This posterior probability serves as the score of the client\nattribute Xand the dictionary attribute A.\nLetting Vbe a sequence of values ( v1; : : : ; v n), and assuming conditional\nindependence of values given the mapping, the client-dictionary attribute score\nis\nM(X; A) =P(A)\nP(V)\u00a2nY\nk=1P(vkjA): (2)\nAlthough the attribute values may not be conditionally independent, such an\nassumption has been shown to be an acceptable approach, aimed at reducing the\nnumber of probabilities to a tractable amount while not sacri\ufb01cing optimality [7,\n8, 13].\nTo build the attribute dictionary for each attribute Awe must learn and store\nthe probability estimates P(A),P(:A),P(vjA), and P(vj:A) for all dictionary\nattributes Aand values v. Note that we do not need to learn P(V) because this\nterm is determined by the requirement that M(X; A) +M(X;:A) = 1.\nP(A), the probability that a client attribute Xmaps to A, is estimated by the\nproportion of examples provided by the domain expert that have been mapped\ntoA.P(vjA), the probability that attribute value voccurs given that a mapping\ntoAholds, is estimated by counting the occurrences of vin the set of examples\nprovided by the domain expert. The remaining terms are learned in a similar\nfashion. For numeric data values, we assume a normal distribution and use the\nnormal probability density function to estimate the conditional probabilities.\nA thorough discussion of the algorithms for estimating these terms is reported\nin [4]. A critical selection process that reduces the number of values vthat are\nmaintained for each attribute Ais discussed in Section 4.\n3.4 Optimal Schema Matching\nAssume now two given schemas R1andR2with their corresponding instances\nr1andr2, and let Ddenote the attribute dictionary.\nThe scores M(X; A) from Equation 2 are calculated for each attribute Xin\nthe given schema and for each attribute Aof the dictionary. A threshold is then\nadopted, and scores that are below this threshold are interpreted as evidence that\nXshould not be mapped to A. These results may be represented in a weighted\ntripartite graph in which nodes correspond to attributes, edges correspond to\nmatches, and edge weights correspond to the posterior probabilities.",
        "6": "Figure 1 shows such a graph for a simple case in which R1=fB1; B2g,\nR2=fC1; C2g, and D=fA1; A2; A3g. This example shows a fulltripartite\ngraph (every node in the left or right partitions is connected to every node in\nthe center partition), but the use of a threshold implies that in general the graph\nneed not be full.\nFig. 1. Weighted tripartite graph for representing individual attribute-attribute scores.\nRecall from the intuitive description in Section 3.1, that the client-dictionary\nattribute scores wiare combined to generate client-client attribute scores. Note,\nhowever, that every two client attributes may be matched through every dictio-\nnary attribute. In the example, B1andC1may be matched through A1(with\nscore w1+w7), through A2(with score w3+w9) and through A3(with score\nw5+w11). Note that associating a dictionary attribute with every attribute\nmatch is like providing a common type for the matching attribute pair.\nIn turn, client-client attribute scores are used in generating overall schema-\nschema scores. In the example, the schema matching comprising of B1A1$C2\nandB2A3$C1receives the score w1+w8+w6+w11. Obviously, the number\nof possible matchings between R1andR2is too high for a simple process that\nenumerates all the matchings and scores each.",
        "7": "One obvious approach for matching R1andR2is to choose for each client\nattribute the most probable dictionary attribute. For instance, in the example,\nthe highest of w1,w3andw5will determine whether B1is mapped to A1,A2\norA3. Then a mapping can be established between those schema attributes\nthat share a node in the attribute dictionary. In the example, assume that the\nhighest of w1,w3andw5isw3; (i.e., B1is best mapped to A2), and assume\nthat the highest of w8,w10andw12isw10(i.e., C2is best mapped to A2); the\nconclusion would then be that B1is best matched with C2. The problem with\nsuch an approach is that it easily leads to ambiguity. In the example, if the\noptimal mappings correspond to the edges with weights w3,w4,w9andw10,\nwe have established a match between the schemas, but the attribute mapping\nis ambiguous. Furthermore, the approach easily leads to no match; e.g., if the\noptimal mappings correspond to the edges with weights w1,w6,w9, and w10.\nTo avoid these pitfalls, we impose an additional constraint on the matching\nofR1andR2. Speci\ufb01cally, we limit our search to schema mappings in which the\npaths between attributes in R1andR2are free of intersections. That is, two\nattributes of a client scheme never map to the same dictionary attribute. The\nresulting problem can then be solved using e\ufb03cient \ufb02ow network techniques.\nTowards this, we must \ufb01rst extend the tripartite graph in several ways.\nFirst, we add two nodes to the graph: a source node Son the left, which is\nthen connected to all the R1nodes, and a target node Ton the right, which\nis then connected to all the R2nodes. Next, we split each attribute dictionary\nnode Ainto two nodes, AinandAout. Each Ain\niis connected to its corresponding\nAout\ninode. Next, we reconnect the edges from R1andR2to the appropriate Ain\norAoutnode. Finally, each edge is given direction, capacity, and cost. All edges\nare directed away from the source node Sand towards the target node T. The\ncapacity for each edge is 1 (thus, the \ufb02ow through an edge will be either 0 or 1).\nThe cost of each of the new edges added to the graph is 0. The cost of each of\nthe old edges is the negation of the edge weight. Figure 2 shows the new graph\nfor the example of Figure 1. Edge capacities and costs were omitted for clarity.\nThe reason for the negation of the weights is that we will be using an algo-\nrithm that searches for a minimum when we actually wish to \ufb01nd the maximum\n(\ufb01nding a maximum is equivalent to \ufb01nding the minimum of the negation). With\nthese modi\ufb01cations, we can now \ufb01nd a matching between the schemas R1and\nR2that conforms to our constraints by using a Minimum Cost Maximum Flow\nnetwork algorithm [1]. In the current implementation of Automatch, we use the\nLEDA software package for this purpose [2].\nSpeci\ufb01cally for Figure 2, since the source has two outgoing edges of capacity\n1 and the target has two incoming edges of capacity 1 (i.e., two attributes are\nmatched on each side), the maximum \ufb02ow is 2. Thus, we seek to \ufb01nd the edges\nin the graph that have the minimum cost while supporting a maximum \ufb02ow of\n2. The edges in this set correspond to the optimal mapping of attributes of R1\ntoR2.\nNote that when the client schemas do not have the same number of attributes,\nsome of the attributes of the larger schema will be matchless. Moreover, since",
        "8": "Fig. 2. Minimum-Cost-Maximum-Flow graph for \ufb01nding optimal schema matching.\nthe tripartite is not necessarily full, the optimal matching may leave attributes\nin the smaller schema matchless as well. This is not an undesirable consequence,\nas it simply indicates that the client schemas include attributes that are unique\nto their schemas.\n4 Optimal Selection of Dictionary Values\nRecall that the attribute dictionary of Automatch represents each attribute with\na set of possible values and their probability estimates. For schema attributes\nthat contain text, the number of needed probabilities is proportional to the num-\nber of unique values of this attribute. An attribute such as CustomerName could\nassume thousands of values, thus imposing considerable space and processing re-\nquirements. Furthermore, not all of these probabilities are equally informative.\nIndeed, many of them are either uninformative (irrelevant) or misleading (noise).\nA critical consideration in our methods is to reduce the dictionary repre-\nsentation of attributes while retaining the most informative values. In machine\nlearning terminology these values are called features and the reduction process\nis called feature selection . To reduce the size of the Automatch dictionary, we\nhave tested and compared three statistical feature selection strategies: Mutual\nInformation, Information Gain, and Likelihood Ratio. The former two strategies",
        "9": "are commonly used for feature selection; to our knowledge the latter strategy\nhas not been used for this purpose.\nWe will discuss each feature selection strategy in turn. Common to all these\napproaches is that each feature is assigned a \u201cscore.\u201d These feature scores can\nbe calculated from the probability estimates in the attribute dictionary. In all\nof these approaches, higher scores are better. Once these scores have been cal-\nculated for a given approach, a percentage of the highest scoring features is\nretained with ties broken arbitrarily.\nFinally, we must normalize the probabilities of the remaining features to sum\nto unity. Thus, statistical feature selection imposes very little overhead in our\napproach. In contrast, other machine learning approaches (e.g. neural networks,\nrule learners, etc.) must execute their respective learning algorithms after feature\nselection is completed.\n4.1 Mutual Information\nMutual information has been used previously as a feature selection strategy in\ninformation retrieval tasks such as [16]. The mutual information of a value vand\nan attribute Ais de\ufb01ned as\nMI(v; A) = logP(v^A)\nP(v)\u00a2P(A): (3)\nWhen vandAare independent, the mutual information of vandAis zero.\nIntuitively, P(v) is a measure of the event that a value voccurs in the client at-\ntribute X, and P(A) is a measure of the event the client attribute Xis mapped to\nthe dictionary attribute A. Hence, MI(v; A) is a measure of the co-occurrence of\nthese two events. For example, if the events are independent (their co-occurrence\nis unbiased), then the mutual information is 0.\nFor the purpose of characterizing dictionary attributes, we wish to retain the\nvalues that have the greatest score regardless of whether they favor Aor:A.\nTherefore, we score values in the MI approach using this formula:\nMImax(v; A) = max\u00bd\nlogP(v^A)\nP(v)\u00a2P(A);logP(v^ :A)\nP(v)\u00a2P(:A)\u00be\n: (4)\nThe values vwith the highest MImax(v; A) are chosen as the characterization\nof attribute A. The actual number of values chosen is discussed in Section 5.2.\n4.2 Information Gain\nInformation gain is often used in machine learning to determine the value of\na particular feature [13]. Given a client attribute Xand a dictionary attribute\nA, the issue is whether Xmaps to Aor not. This issue may be formatted as a\nbinary message : 1 if yes, 0 if no.\nDenote P(A) the probability that Xmaps to A. Assume \ufb01rst that our only\nknowledge is the proportion of attributes that are mapped to A(how \u201cpopular\u201d",
        "10": "Ais as a target of mappings). The entropy (information content) of the message\nis then\nH=\u00a1(P(A)\u00a2logP(A) +P(:A)\u00a2logP(:A)): (5)\nAssume now that we know a new fact: v2X. The new entropy (information\ncontent) of the message is\nH1=\u00a1(P(Ajv)\u00a2logP(Ajv) +P(:Ajv)\u00a2logP(:Ajv)): (6)\nAssume now that we know an alternative fact: v62X. The new entropy\n(information content) of the message is\nH2=\u00a1(P(Aj :v)\u00a2logP(Aj :v) +P(:Aj :v)\u00a2logP(:Aj :v)):(7)\nH1andH2may be combined using P(v), the probability that vis in X.\nThen the entropy (information content) of the message is\nH0=P(v)\u00a2H1+p(:v)\u00a2H2: (8)\nTheinformation gained by knowing the presence or absence of vis\nIG(v; A) =H\u00a1H0: (9)\n4.3 Likelihood Ratio\nThe likelihood ratio for a value vand attribute A, de\ufb01ned as P(vjA)=P(vj:A),\nmeasures the retrospective support given to Aby the occurrence of v[14]. The\nlikelihood ratio produces scores on the interval (0 ;1). It has a value of 1 if\nthe feature provides no support. Likelihood ratios greater than 1 indicate that\nthe feature supports A; likelihood ratios less than 1 indicate that the feature\nsupports :A.\nFor the task at hand, we wish to retain the features that provide the most\nsupport regardless of whether they favor Aor:A. The features that favor Aare\non the interval (1 ;1), with higher values indicating stronger support, whereas\nthe features that favor :Aare on the interval (0 ;1), with lower values indicating\nstronger support. Consequently, it is di\ufb03cult to use the likelihood ratio as de-\n\ufb01ned, because higher scores are not necessarily better. For this reason, we use an\nadjustment that inverts the likelihood ratios that support :A, placing them on\nthe same scale as likelihood ratios that support A, and then choose the stronger\nof the supports:\nLR(v; A) = max\u00bdP(vjA)\nP(vj:A);P(vj:A)\nP(vjA)\u00be\n: (10)\nThis strategy produces scores on the interval (1 ;1) and higher scores are\nalways better.",
        "11": "5 Experimentation\n5.1 Setting Up the Experiment\nTo experiment with the methods discussed in this paper, we built an attribute\ndictionary for computer retail information with the following attributes: Desktop-\nManufacturer, MonitorManufacturer, PrinterManufacturer, DesktopModel, Mon-\nitorModel, PrinterModel, DesktopCost, PeripheralCost, Inventory .\nData for this experiment was taken from the web sites of 15 di\ufb00erent com-\nputer retailers (e.g. Gateway, Outpost, etc). A total of 22 relations were ex-\ntracted. The data was collected o\ufb00-line from HTML web pages and imported\ninto relational database tables accessible through the ODBC protocol.\nTo experiment with this data, we used a procedure from data mining called\nstrati\ufb01ed cross-validation which we brie\ufb02y describe (see [17] for a complete de-\nscription). Each of the 22 schemas was manually mapped into our attribute dic-\ntionary. We then partitioned these 22 schemas into three folds of approximately\nequal size. Using two folds for learning and one fold for testing, we repeated\nthe experiment for the three possible combinations of folds. For the test fold, we\nchose two schemas at a time (for all possible combinations) and used Automatch\nto match the schemas. We used the manually constructed mappings to judge the\nmappings which Automatch concluded.\n5.2 Measuring Performance\nTo measure performance, each schema-matching result was interpreted as set\nof mapping decisions for pairs of schema attributes hR1(Bi); R2(Cj)i, where i\nranges over all the attributes of R1andjranges over all the attributes of R2.\nEach of these attribute mapping decisions falls into one of four sets, A,B,C,\nandD, where\nA= True Positives (decision to map R1(Bi) toR2(Cj) is correct).\nB= False Negatives (decision to not map R1(Bi) toR2(Cj) is incorrect).\nC= False Positives (decision to map R1(Bi) toR2(Cj) is incorrect).\nD= True Negatives (decision to not map R1(Bi) toR2(Cj) is correct).\nThe ratio jAj=(jAj+jCj) is the proportion of true positives among the cases\nthought to be positive; i.e., it measures the accuracy of Automatch when it\ndecides True. The ratio jAj=(jAj+jBj) is the proportion of positives detected\nby Automatch among the complete set of positives; i.e., it measures the ability\nto detect positives. Speci\ufb01cally to our application, the former ratio measures\nthesoundness of the discovery process, and the latter ratio measures its com-\npleteness . These two ratios are known from the \ufb01eld of information retrieval\nasprecision andrecall , but we shall refer to them here as the soundness and\ncompleteness of the schema matching process.\nTo simplify the comparison of the three feature selection approaches, we\ncombined soundness and completeness into a single performance measure using\ntheir harmonic mean . The harmonic mean of precision and recall is often used",
        "12": "in information retrieval whenever a single performance measure is preferred [3].\nThe harmonic mean for our mapping problem is calculated as\nF(x) = 2\u00a2S(x)\u00a2C(x)\nS(x) +C(x)(11)\nwhere S(x) and C(x) are the soundness and completeness of the discovery process\nat a given percent reduction xin the feature space. The harmonic mean assumes\nhigh values only when both soundness and completeness are high. Thus, maxi-\nmizing the harmonic mean can be thought of as the best compromise between\nsoundness and completeness.\nTo measure the performance of each of the feature selection strategies that\nwere discussed in Section 4, we determine the harmonic mean of soundness and\ncompleteness for each strategy as we increase the percentage of the feature space\nthat is discarded. We reduce the feature space in increments of 5 percent until\n95 percent of the feature space has been discarded.\n5.3 Interpreting the Results\nFirst we measured the performance of Automatch without any attempt at op-\ntimizing the dictionary through feature selection; that is, we use the Bayesian\napproach to score matches (Section 3.3) and the \ufb02ow graph approach to optimize\nmatches (Section 3.4). Using cross validation, we achieved a performance of 66%\n(measured as the harmonic mean of soundness and completeness). In a separate\nexperiment, we used random guessing to match the same schemas and achieved\na performance of 10%.\nNext, we compared the three feature selection strategies of Section 4 and\nassessed their impact on schema matching. Figure 3 shows the performance for\nschema matching for each of the feature selection strategies. The x-axis is the\npercentage of low-scoring features that have been discarded, and the y-axis is the\nperformance, measured as the harmonic mean of soundness and completeness.\nThe leftmost point in the graph corresponds to our \ufb01rst experiment with no\nfeature selection.\nInitially, with 5% feature reduction, all the feature selection strategies im-\nprove performance by at least 6%. The strategies then perform comparably up to\n60% reduction. At levels of reduction over 80%, IG and LR continue to produce\nimproved matching performance (relative to no feature selection) while MI falls\nbelow performance with no feature selection.\nAll three feature selection strategies improve performance when compared to\nthe initial performance with no feature selection (though the level to which they\nsustain this improvement varies). This observation indicates that allof these\napproaches are acceptable for reducing the feature space. Furthermore, if we are\nseeking the most ambitious reduction in the feature space, LR is preferable to\nIG which is preferable to MI.",
        "13": "0.500.550.600.650.700.75\n0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90LR IG MIFig. 3. Harmonic mean of soundness and completeness ( y-axis) as the feature set is\nreduced in increments of 5 percent ( x-axis).\n6 Conclusion\nIn this paper we described an automated solution for the well-known problem\nof database schema matching. Our approach uses Bayesian machine learning,\nstatistical feature selection, and the Minimum Cost Maximum Flow network\nalgorithm to \ufb01nd an optimal matching of attributes between two semantically\nrelated schemas.\nOur signi\ufb01cant \ufb01ndings and contributions in this paper were:\n\u2013The Automatch system is a new and viable approach to eliminate the schema-\nmatching bottleneck present in modern database applications. Our results\nare encouraging as they show performance that exceeds 70% (measured as\nthe harmonic mean of the soundness and the completeness of the attribute\nmatching process).\n\u2013Statistical feature selection can be used to improve the performance of Au-\ntomatch. The improvement is in three areas: (1) in the storage require-\nments for the auxiliary knowledge base, (2) in the computational costs of\nthe matching algorithm, and (3) in the quality (soundness and completeness)\nof the results. We estimate that statistical feature selection can be used to\nimprove the performance of other automated schema-matching approaches\n(such as [6, 10]) that must deal with high-dimensional feature spaces.\n\u2013Statistical feature selection incurs little overhead in Automatch since we\nare using a probabilistic learning approach. Learning after feature selection\nconsists simply of normalizing the probabilities of the remaining features.\nIn contrast, other machine learning approaches (e.g. neural networks, rule",
        "14": "learners, etc.) must execute their respective learning algorithms after feature\nselection is completed.\nWhile the performance of 70% in these experiments is promising, user inter-\naction is still necessary to complete the matching process. In our future research,\nwe plan on building a user interface that allows a domain expert to adjust the\nattribute mappings that have been proposed by Automatch. Furthermore, the in-\nterface will allow for iterative adjustment (i.e., after the user adjusts some of the\nmappings, we can re-apply Automatch for the remaining unmapped attributes).\nAn important bene\ufb01t of user interaction in Automatch is that the system\nwill be able to learn continuously. As new matches are provided through the\nuser interface, the learner will be able to combine this information with what has\nalready been learned. Note that this is signi\ufb01cantly di\ufb00erent than re-executing\nthe entire learning algorithm. Such continuous learning is possible due to the\nstatistical nature of the learning algorithm. As new matches are validated by a\nuser, we can learn from these additional examples by updating the frequency\ncounts of the features.\nFinally, while this initial experimentation is encouraging, it is admittedly of\na limited scale. Additional experimentation is planned to validate these prelim-\ninary conclusions.\nAcknowledgement: The authors wish to thank Joseph (Se\ufb03) Naor for his\nimportant suggestions in the area of network \ufb02ows.\nReferences\n1. Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. Network Flows:\nTheory, Algorithms, and Applications . Prentice Hall, 1993.\n2. Algorithmic Solutions. The LEDA Users Manual (Version 4.2.1) , 2001.\n3. Ricardo Baeza-Yates and Berthier Ribeiro-Neto. Modern Information Retrieval .\nACM Press, 1999.\n4. Jacob Berlin and Amihai Motro. Autoplex: Automated discovery of content for\nvirtual databases. In Proceedings of the Ninth International Conference on Coop-\nerative Information Systems , pages 108\u2013122, 2001.\n5. Silvana Castano and Valeria De Antonellis. A schema analysis and reconciliation\ntool environment for heterogeneous databases. In Proceedings of the International\nDatabase Engineering and Applications Symposium , pages 53\u201362, 1999.\n6. AnHai Doan, Pedro Domingos, and Alon Y. Halevy. Reconciling schemas of dis-\nparate data sources: A machine-learning approach. In Proceedings ACM Special\nInterest Group for the Management of Data (SIGMOD) , 2001.\n7. Pedro Domingos and Michael Pazzani. Conditions for the optimality of the simple\nbayesian classi\ufb01er. In Proceedings of the 13th International Conference on Machine\nLearning , pages 105\u2013112, 1996.\n8. Pat Langley, Wayne Iba, and Kevin Thompson. An analysis of bayesian classi\ufb01ers.\nInProceedings of the Tenth National Conference on Arti\ufb01cial Intelligence , pages\n223\u2013228, 1992.\n9. Wen-Syan Li and Chris Clifton. Semantic integration in heterogeneous databases\nusing neural networks. In Proceedings of 20th International Conference on Very\nLarge Data Bases , pages 1\u201312, 1994.",
        "15": "10. Wen-Syan Li and Chris Clifton. Semint: A tool for identifying attribute corre-\nspondences in heterogeneous databases using neural networks. Data & Knowledge\nEngineering , 33(1):49\u201384, 2000.\n11. Jayant Madhavan, Philip A. Bernstein, and Erhard Rahm. Generic schema match-\ning with cupid. In Proceedings of the 27th International Conferences on Very Large\nDatabases , pages 49\u201358, 2001.\n12. Ren\u00b4 ee Miller, Laura Haas, and Mauricio Hern\u00b4 andez. Schema mapping as query\ndiscovery. In Proceedings of the 26th International Conferences on Very Large\nDatabases , pages 77\u201388, 2000.\n13. Tom Mitchell. Machine Learning . McGraw-Hill, 1997.\n14. Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\nInference . Morgan Kaufmann, 1988.\n15. Erhard Rahm and Philip Bernstein. On matching schemas automatically. Technical\nReport MSR-TR-2001-17, Microsoft, Redmond, WA, February 2001.\n16. Mehran Sahami, Susan Dumais, David Heckerman, and Eric Horvitz. A bayesian\napproach to \ufb01ltering junk e-mail. AAAI-98 Workshop on Learning for Text Cate-\ngorization , 1998.\n17. Ian H. Witten and Eibe Frank. Data Mining: Practical Machine Learning Tools\nand Techniques with Java Implementations . Morgan Kaufmann, 2000."
    },
    "Deep Learning Based Approach to Unstructured Record Linkage.pdf": {
        "1": "Deep Learning Based Approach to Unstructured Record Linkage\nJurek-Loughrey, A. (2021). Deep Learning Based Approach to Unstructured Record Linkage. International \nJournal of Web Information Systems . https://doi.org/10.1108/IJWIS-05-2021-0058 \nPublished in:\nInternational Journal of Web Information Systems\nDocument Version:\nPeer reviewed version\nQueen's University Belfast - Research Portal:\nLink to publication record in Queen's University Belfast Research Portal\nPublisher rights\nCopyright 2021 Emerald Publishing Limited. This work is made available online in accordance with the publisher\u2019s policies. Please refer to\nany applicable terms of use of the publisher.\nGeneral rights\nCopyright for the publications made accessible via the Queen's University Belfast Research Portal is retained by the author(s) and / or other\ncopyright owners and it is a condition of accessing these publications that users recognise and abide by the legal requirements associated\nwith these rights.\nTake down policy\nThe Research Portal is Queen's institutional repository that provides access to Queen's research output. Every effort has been made to\nensure that content in the Research Portal does not infringe any person's rights, or applicable UK laws. If you discover content in the\nResearch Portal that you believe breaches copyright or violates any law, please contact openaccess@qub.ac.uk.\nDownload date:14. gen. 2023",
        "2": "Deep Learning Based Approach to Unstructured\nRecord Linkage\nNo Author Given\nNo Institute Given\nAbstract\nPurpose\nIn the world of big data, data integration technology is crucial for maximising\nthe capability of data-driven decision making. Integrating data from multiple\nsources drastically expands the power of information and allows us to address\nquestions that are impossible to answer using a single data source. Record Link-\nage (RL) is a task of identifying and linking records from multiple sources that\ndescribe the same real world object (e.g. person), and it plays a crucial role in\nthe data integration process. RL is challenging as it is uncommon for di\u000berent\ndata sources to share a unique identi\fer. Hence the records must be matched\nbased on the comparison of their corresponding values. Most of the existing RL\ntechniques assume that records across di\u000berent data sources are structured and\nrepresented by the same scheme (i.e. set of attributes). Given the increasing\namount of heterogeneous data sources, those assumptions are rather unrealistic.\nThe purpose of this paper is to propose a novel RL model for unstructured data.\nMethodology\nIn our previous work [16] we proposed a novel approach to linking unstructured\ndata based on the application of the Siamese Multilayer Perceptron model. It\nwas demonstrated that our method performed on par with other approaches\nthat make constraining assumptions regarding the data. This paper expands our\nprevious work originally presented at iiWAS2020 [16] by exploring new architec-\ntures of the Siamese Neural Network, which improves the generalisation of the\nRL model and makes it less sensitive to parameter selection.\nFindings\nThe experimental results con\frm that the new Autoencoder based architec-\nture of the Siamese Neural Network obtains better results in comparison to the\nSiamese Multilayer Perceptron model proposed in [16]. Better results have been\nachieved in three out of four datasets. Furthermore, it has been demonstrated\nthat the second proposed (hybrid) architecture based on integrating the Siamese\nAutoencoder with a Multilayer Perceptron model, makes the model more stable\nin terms of the parameter selection.\nOriginality\nTo address the problem of unstructured RL, this paper presents a new deep\nlearning based approach to improve the generalisation of the Siamese Multilayer\nPreceptron model and makes it less sensitive to parameter selection.\nKeywords: Record Linkage, Unstructured Data, Siamese Neural Network",
        "3": "1 Introduction\nSociety worldwide is generating more and more data giving rise to the \\data\ndeluge\" problem. Making sense of such data is necessary for making strategically\nimportant decisions by government bodies, security, healthcare, \fnancial entities,\nto name a few. Often to enable decision-making, data from di\u000berent sources have\nto be integrated [7]. For instance, integrating records from law enforcement watch\nlists with data coming from \fnancial institutions, car rental companies, airlines,\nimmigration agencies, and residency agencies could help to prevent terrorist\nattacks. As a part of the data integration process, records (from two or more\ndata sources) that refer to the same real world entity (e.g. person) need to be\nlinked. In many cases, datasets do not share a unique identi\fer (e.g. National\nInsurance Number), thus the process of linking records needs to be performed by\nmatching their corresponding attributes. This becomes challenging due to issues\nsuch as di\u000berent data formats, language ambiguity and abbreviations. Commonly\napplied RL methods require assistance from a domain expert to carefully hand-\ncraft bespoke domain-speci\fc linking rules that aid in determining the linkage\nlikelihood of a candidate record pair [9]. This requires deep topical expertise\nin the domain and continuous maintenance to cope with any changes in the\ncharacter of the data, which is a costly proposition in many realistic scenarios.\nGiven its pivotal importance and challenges, there has been strong interest in\nRL in the last decade within the computer science domain [7] [11]. In particular,\nthe application of Machine Learning (ML) o\u000bers a promising approach, which\ncan be applied as an alternative to manual rule building [20]. However, the\nexisting ML-based approaches to RL are based on the assumption that the data\nobtained from di\u000berent sources is structured and represented by overlapping sets\nof attributes [25][32][9][30] [15] [18] [8] [28]. This is very restrictive in terms of\nreal world applications, given the increasing number of unstructured data sources\nsuch as social media channels, for example. Consequently, using ML methods for\nRL tasks becomes more challenging and it has been limited to structured data\nonly. In our previous work [16], we introduced a new approach to unstructured\nRL based on an application of the Siamese Neural Network and text embedding\nmodels. It was demonstrated that the proposed model performed on par with\nML-based methods yet it does not make any assumptions with respect to the\ndata. Following the experimental evaluation, we learnt that there was still room\nfor improvement in terms of the generalisation of the model. The model also\ntended to be sensitive with respect to the parameter selection. In this work\nwe tried to address the limitations of the model proposed in [16]. The main\ncontributions of this work are as follows. We proposed a new architecture of\nthe Siamese Neural Network, which improves the generalisation of he model in\n[16]. We further modi\fed the architecture in order to obtain a model that is less\nsensitive to parameter selection.",
        "4": "1.1 Overview of the Record Linkage Task\nAn illustrative example of a RL task is presented in Table 1 containing records\nfrom two digital libraries, DBLP and ACM . In this case, RL can be applied\nto detect which of the record pairs represent the same publication, which in\nthis case should be (ACM1, DB1) and (ACM2, DB2). Any other pair should\nbe considered as non-match. Formally, for two sets of records SandT, RL is\nTable 1: An Example of RL.\nID Title Authors Venue\nACM1 A compact B-tree Peter Bumbulis, Ivan\nT. BowmanInternational Confer-\nence on Management\nof Data\nACM2 A theory of redo re-\ncoveryDavid Lomet, Mark\nTuttleInternational Confer-\nence on Management\nof Data\nDB1 A compact B-tree Ivan T. Bowman, Pe-\nter BumbulisSIGMOD Conference\nDB2 A theory of redo re-\ncoveryMark R. Tuttle,\nDavid B. LometSIGMOD Conference\nDB3 Enhanced Abstract\nData Types in\nObject-Relational\nDatabasesPraveen Seshadri VLDB J.\nDB4 Parametric Query\nOptimizationRaymond T. Ng,\nTimos K. Sellis,\nYannis E. Ioannidis,\nKyuseok ShimVLDB J.\nde\fned as a task of identifying pairs of records ( s;t)2S\u0002Tthat correspond to\nthe same real world entity. In the RL process, each pair of records from S\u0002T\nis being classi\fed as a match or non-match.\n1.2 Machine Learning Based Approach to Record Linkage\nThe majority of existing work in the space of ML and RL has focused on struc-\ntured data sources where records are represented by overlapping sets of attributes\n(e.g. Table 1). For two structured data sources with overlapping attributes, the\nsimilarity between any two records can be determined by comparing their corre-\nsponding attributes using a similarity measure. Formally, given a pair of records\nfs= (s:f1;:::;s:f N);t= (t:f1;:::;t:f N)g(wheref1;:::;f Nrepresent the over-\nlapping attributes), a similarity measure mquanti\fes the similarity between two\nattributes values of sandtand can be formulated as:\nm:Fi\u0002Fi![0;1] (1)",
        "5": "whereFidenotes the domain of attribute fi. The similarity measure returns\na numeric value ranging between 0 and 1, referred to as a similarity value.\nm(s:fi;t:fi) = 1 indicates that the pair of records have the exact same values\non the attribute fi, whilem(s:fi;t:fi) = 0 indicates that there is no similarity\nbetween the values s:fiandt:fi. Some of the commonly used similarity measures\ninclude Jaro [14], Jaro-Winkler [33], Jaccard [13], Q-Gram [29], and Levenshtein\nedit distance [22]. It has been demonstrated [5] that depending on the type of\ndata, di\u000berent similarity measures have di\u000berent levels of accuracy. Moreover,\nthere does not exist a single similarity measure that is optimal for all data sets\n[1]. Once the similarity values are determined for all candidate record pairs,\neach pair of records can be represented as a comparison vector of length N.\nNotationally, for two records s= (s:f1;:::;s:f N) andt= (t:f1;:::;t:f N) and a\nsimilarity measure m, a comparison vector for sandtis formulated as:\nm(s;t) =<m(s:f1;t:f1);:::;m (s:fN;t:fN)> (2)\nEach element of the comparison vector represents a numeric similarity value\ncalculated with a similarity measure on the corresponding pair of attributes\nof the records sandt. For a labelled dataset (i.e. each comparison vector is\nlabelled as 1 - representing a matching record pair or 0 - representing non-\nmatching records), the RL task can be then considered as a comparison vector\nclassi\fcation problem [9]. Notationally:\n[S;T;m ] =)LR:\u0000 !V!0;1 (3)\nwhere\u0000 !V=fm(si;tj) :si2S;tj2Tgis the set of comparison vectors gener-\nated for each record pair from S\u0002T. Using supervised learning for training a\ncomparison vector classi\fcation model has been proved to be very e\u000bective in\nthe past. However, ML methods can only be applied with structured data where\nrecords from di\u000berent sources are represented by a common set of attributes.\n1.3 Challenges\nThe key challenge of ML based approaches to RL is the fact that they can not\nhandle unstructured and heterogeneous data sources. The majority of existing\ntechniques assume that the data obtained from di\u000berent sources is structured\nand represented by an overlapping set of attributes. This is a major limitation\nas nowadays the data is being obtained in many various formats including struc-\ntured i.e. prede\fned data model (e.g. relational database), semi-structured (e.g.\nExtensive Mark-up Language, LATEX, web data, scienti\fc data), and unstruc-\ntured i.e. no prede\fned data model, usually text (e.g. text documents, email\nmessages, research publications) records. Consequently, integration and linkage\nof unstructured and heterogeneous data types still remains a challenge.",
        "6": "2 Unstructured Record Linkage\n2.1 Problem De\fnition\nConsider two datasets of records S=fs1;:::;s ngandT=ft1;:::;t mg, where\neach dataset may contain structured or unstructured records. We further assume\nthat the records across SandTare not represented by the same scheme, i.e.\nare not represented by an overlapping set of attributes. For each record from\nSandT, we useLto denote a pre-trained language model (e.g. Word2Vec\n[23], GloVe [27] or BERT[6]), which can be used to embed the records into\nnumerical vectors of the same dimension. For unstructured data such as emails,\na pre-trained language model is applied to vectorise each of the records. For\nstructured data (i.e. with pre-de\fned attributes), \frst an embedding vector can\nbe obtained for each attribute value individually. The embedding of a record\ncan then be calculated as a combined vector of all attribute embeddings (e.g. by\naveraging or concatenation). Here, we address the task of supervised RL, that\nof leveragingfS;Tgand a language model Lto train a classi\fcation model for\npredicting a pair of records fsi;tjgas a match or non-match. Notationally:\n[S;T;L ]Supervised= = = = = = =)\nLearningRL:S\u0002T!f0;1g (4)\nThe output of RL would be a 0 =1 label, where 0 indicates that a pair of records\ndoes not refer to the same entity while 1 means that the two records are a match.\n2.2 Existing Solution\nIn our previous work [16], we proposed a new approach to RL which can be\napplied to unstructured data. The proposed model was a Siamese Multilayer\nPerceptron whose architecture is presented in Figure 1. The model's architecture\nis composed of two twin Multilayer Perceptron (MLP) networks which share\nthe same structure (i.e. number of layers and neurons) and parameters. The\nmodel takes as an input a pair of records represented by their embedded vectors\nobtained from a selected language model ( x1andx2). Each of the two networks\nproduces a high level feature representation of the input vectors ( o1ando2). The\ntraining dataset is composed of pairs of records (i.e. vector embeddings) and the\nlabels indicating whether a pair is a match or non-match. The training objective\nof the model is to learn a new representation of the records that minimise the\nEuclidean distance between matching records and maximise it for non-matching\nrecords. In the training process the contrastive loss function is used. This is a\ndistance based function, as opposed to prediction based functions (e.g. Cross\nEntropy) commonly used in classi\fcation tasks. The contrastive loss tries to\nensure that semantically similar records are embedded close together. Given a\npair of embedding vectors ( x1;x2), it is calculated as:\nL=y\u0002k\u001a(x1)\u0000\u001a(x2)k2+(1\u0000y)\u0002max(margin\u0000k\u001a(x1)\u0000\u001a(x2)k;0)2(5)",
        "7": "whereyis the ground truth relation between the original records and \u001arepre-\nsents the function of the MLP model. In this case y= 1 ifx1andx2represent\nembeddings of two matching records and y= 0 otherwise. The margin param-\neter is used to tighten the constraint in the learning process. If two records do\nnot represent the same entity (i.e. are not matching) then the distance between\nthem should be at least the margin . In the classi\fcation process, a new pair of\nembedded vectors is passed as the input to the model and their new representa-\ntions are provided as the output. Following this, the Euclidean distance between\nthe two output vectors is calculated. Based on their distance and a pre-de\fned\nthreshold, the pair of records is classi\fed as a match or as a non-match. The\nempirical evaluation of the proposed Siamese Multilayer Perceptron (SMLP)\ndemonstrated that the model performed on par with other approaches, which\nmake constraining assumptions regarding the data.\nFig. 1: Siamese Multilayer Perceptron with contrastive loss function applied for\nthe RL task.\n2.3 Proposed Approach\nIn this work we explore di\u000berent architectures of Siamese Neural Networks (SNN)\ntrying to improve the performance of the approached proposed in [16]. In partic-\nular, we explored two di\u000berent directions. First, we proposed to use Autoencoder\ninstead of the MLP architecture for the creation of the SNN. Autoencoder is an",
        "8": "Arti\fcial Neural Network (ANN), which allows us to learn e\u000ecient data encod-\nings in an unsupervised manner. The purpose of using Autoencoder is to learn a\nnew representation of records by training the network to ignore any noise in the\ndata. We hypothesise that combining the Autoencoder reconstruction loss with\nthe contrastive loss function will lead to a better generalisation of the model and\nhence improve its performance on new (validation) data. As the second direc-\ntion we proposed to use a hybrid approach based on combination of a Siamese\nAutoencoder and MLP in order to make the model less sensitive to the selection\nof its two parameters (i.e. margin andclassificationthreshold ). Following the\ntraining of the Siamese Autoencoder we used the new data representation to\nfurther train a supervised MLP with binary cross entropy loss function. The\ntwo aforementioned SNN architectures are explained in detail in the following\nsections.\n2.4 Siamese Autoencoder\nThe architecture of the proposed model is presented in Figure 2. The model is\nbuilt with two identical Autoencoders (i.e. sharing the same architecture and\nparameters) and it requires a training dataset containing record pairs labelled\nas match/non-match. The input is composed of two vectors of the same dimen-\nsion representing embeddings of a pair of records. Similarly as with the model\nproposed in [16], the goal of the training process is to learn new representations\nof the vectors, which minimise the distance between matching record pairs and\nmaximise it for non-matching records. For this purpose, the contrastive loss ( L3)\nis being calculated on the bottlenecks (encoded inputs) of the two Autoencoders.\nIn addition to this, the reconstruction losses of the two Autoencoders ( L1and\nL2) are being calculated and included in the \fnal loss function of the Siamese\nmodel. Given a pair of embedding vectors ( x1;x2), the \fnal hybrid loss function\nof the proposed Siamese Autoencoder model is calculated as per Equation 6.\nLhybrid =\u000b\u0002(kx1\u0000'(\u001a(x1))k2+kx2\u0000'(\u001a(x2))k2)+\ny\u0002k\u001a(x1)\u0000\u001a(x2)k2+(1\u0000y)\u0002max(margin\u0000k\u001a(x1)\u0000\u001a(x2)k;0)2(6)\nThe \frst component of the loss function from Equation 6 represents the recon-\nstruction losses of the twin Autoencoder models, where \u001aand'represent the\nfunctions of the Encoder and the Decoder respectively. The second component\nrefers to the contrastive loss calculated on the embedded representations of x1\nandx2calculated by the twin Autoencoders. \u000bis a parameter indicating the\nweight of the reconstruction loss.\nThe motivation behind using the hybrid loss function is to downgrade the\nimpact of the contrastive loss function on the training process by combining\nit with the reconstruction losses of the Autoencoder models. We hypothesise\nthat training the model for the additional task will improve the generalisation\nof the RL model. As it was demonstrated in [21], adding the unsupervised task\nof reconstructing the input can improve the generalisation performance of a\nsupervised task.",
        "9": "Following the training of the model with a set the record pairs labelled as\nmatch/non-match, only the Encoder model is used in the classi\fcation process.\nFor a new record pair, their embeddings are \frst obtained from the language\nmodel. Following this, each embedded vector is passed through the Encoder\nmodel providing two vectors as output. In the \fnal step, the Euclidean distance is\ncalculated between the output vectors and depending on a pre-de\fned threshold\nthe pair of records is classi\fed as a match or non-match. Both parameters of the\nmodel (i.e.margin andclassificationthreshold ) need to be optimised with the\napplication of a validation dataset.\nFig. 2: Siamese Autoencoder applied for the RL task.\n2.5 Siamese Autoencoder with Multilayer Perceptron\nIt has been demonstrated in our experimental evaluation (presented later in the\npaper) that the Siamese Autoencoder model is sensitive with the respect to the\nselection of its two parameters, namely margin andclassification threshold .\nTrying to address this issue we proposed a modi\fed version of the model which is\ntrained in two phases. In the \frst phase the Siamese Autoencoder introduced in\nthe previous section is trained with a set of record pairs labelled as match/non-\nmatch and the hybrid loss function. In the second phase, the new representations\nof the records (trained by the Siamese Autoencoder) is further \fne-tuned with a\nMLP model for a supervised classi\fcation task (i.e. classifying a pair of records",
        "10": "as a match or a non-match). The architecture of the model is presented in Figure\n3. A pair of vectors (representing a record pair) is \frst passed through the previ-\nously trained Encoder model providing two embedded vectors as an output. The\ntwo output vectors are combined into a single vector by computing the absolute\nvalues of the di\u000berences between their corresponding entrances. This representa-\ntion of record pairs with their labels is further applied to train a MLP model for\na supervised classi\fcation task (i.e. classifying a pair of records as a match or a\nnon-match) with binary cross-entropy loss function. Combining representations\nof two records into a single vector and training a MLP model to classify such\na record pair representation allows us to eliminate the classificationthreshold\nparameter, which is necessary while classifying a record pair based on their dis-\ntance. Furthermore, by performing the second training phase we can reduce the\nimpact of the margin selection in the training of the Siamese Autoencoder on\nthe \fnal performance of the model.\nFollowing the training of the model, a new pair of vectors (i.e. records) is\n\frst passed through the Encoder model in order to obtain their embedded rep-\nresentation. The two outputs are then combined and passed as an input to the\nMLP model. Based on the output of the MLP, the record pair is classi\fed and\na match or a non-match.\nFig. 3: Siamese Autoencoder with Multilayer Perceptron applied for the RL task.",
        "11": "3 Experimental Evaluation\nIn this section we present the experimental evaluation of the two RL models\narchitectures presented in the previous section. We conduct a set of experiments\nin order to answer the following key questions:\n{Is the Autoencoder based Siamese Neural Network with hybrid loss function\nmore suitable for the RL task than the MLP based architecture proposed in\n[16]?\n{How does combining the Siamese Autoencoder model with the MLP model\nimpact its sensitivity to the parameter selection and the \fnal classi\fcation\nperformance?\n{How does the performance of the proposed Siamese model architectures com-\npare against the results obtained by the baseline approaches?\n3.1 Experimental Setup\nDatasets. The experiments are conducted with four datasets commonly used\nby the RL community. The properties of each dataset are listed in Table 2. The\nRestaurant1dataset contains records of 864 restaurants, each with \fve \felds\n(name, address, city, phone, type). The Cora1dataset is a collection of 1,295 ci-\ntations to computer science papers represented by 4 \felds (author, title, venue,\nyear). The DBLP-ACM and DBLP-Scholar are bibliographic datasets of com-\nputer science bibliography records represented by four attributes (title, authors,\nvenue, year).\nFor the evaluation purposes of the proposed approaches, all datasets have\nbeen unstructured so that each record is represented by unstructured text com-\nposed of all attributes values merged together.\nBlocking. In order to simplify the linkage process, we \frst perform blocking\nto get rid of the obvious non-matching record pairs. This is a standard pre-\nprocessing step with any RL method. In this work we use a schema-agnostic\nblocking method proposed in [26], which is one of the state-of-the-art unsuper-\nvised blocking techniques applicable to unstructured datasets. The records are\n\frst divided into overlapping blocks according to their common tokens. Follow-\ning this, record pairs are either retained or removed based on their number of\nshared common blocks. In the linkage process, only record pairs from the same\nblock are compared.\n1https://www.cs.utexas.edu/users/ml/riddle/data.html\n2http://dbs.uni-leipzig.de/en/research/projects/object_matching/fever/\nbenchmark_datasets_for_entity_resolution",
        "12": "Table 2: Properties table for evaluated datasets.\nNameNumber of\nAttributesNumber of\nRecordsNumber of\nRecords PairsNumber of\nMatchesTask\nRestaurant 5 864 372,816 112 Deduplication\nCora 4 1,295 837,865 17,184 Deduplication\nDBLP-ACM 4 2,616+2,294 6,001,104 2,224 Linkage\nDBLP-Scholar 4 2,616+64,263 168,181,505 5,347 Linkage\n3.2 Siamese Autoencoder\nIn this section we evaluate the proposed Siamese Autoencoder RL (referred\nto as SA) model in comparison to the Siamese MLP (referred to as SMLP)\nproposed in [16]. The Encoder and Decoder of the SA model have the same\narchitectures including one hidden layer. The dimension of the bottleneck is\nset to 50. The SMLP is composed of two twin MLP networks containing one\nhidden layer. For both architectures, we use Leaky Recti\fed Linear Units (leaky\nReLu) as the activation function in the hidden layer. SMLP uses the contrastive\nloss function, while for SA we apply the hybrid loss combining the contrastive\nand reconstruction losses. The parameters of the contrastive loss function are\noptimised with the application of a validation dataset.\nThe loss function is combined with a standard back propagation algorithm\nwhere the gradient is added across the twin networks due to the tied weights.\nWe use Adam optimiser with a learning rate of \u0011= 0:001 and mini batch size of\n256. For the weights we apply He initialisation with uniform distribution. Biases\nare initialised to be zero. We apply l1weights regularisation and dropout with\nthe keep probability set to 0.5. We train each network for 100 epochs. For the\ngeneration of the record embeddings (i.e. vectors provided as the inputs to both\nof the models) we applied BERT [6], one of the state-of-the-art language models.\nThe results in a form of F-measure obtained by the two models with each of\nthe four datasets are demonstrated in Table 3. For each dataset, the experiments\nwere conducted with 10-cross validation. It can be observed that the Autoen-\ncoder based Siamese Neural Network outperformed the MLP based architecture\non three out of four datasets. Even though the di\u000berence is rather small, SA\nperforms consistently better than SMLP. This indicates that as we expected,\nadding the unsupervised task of input reconstruction to the supervised RL task\nimproves the generalisation of the model.\nTable 3: F-measure obtained by the SA and the SMLP models.\nDataset: Restaurant Cora DBLP-ACM DBLP-Scholar\nSMLP 0.97 0.999 0.955 0.849\nSA 0.975 0.999 0.965 0.857",
        "13": "3.3 Siamese Autoencoder with Multilayer Perceptron\nIn this section we evaluate the performance of the second proposed architec-\nture that combines the SA with the supervised MLP model (AS+MLP). The\nAS+MLP model does not require the classification threshold parameter as it\nis trained to classify each pair of records based on the combination of their em-\nbedded representations rather than based on the distance between them (as in\ncase of the SA). It still however relies on the margin parameters, which is used\nduring training the SA model. In order to investigate how sensitive the SA and\nthe SA+MLP are to the selection of their parameters values, we evaluate both\nof the models using a range of di\u000berent parameters values. For the margin pa-\nrameter we use values ranging from 2.5 to 4.5 with the increment of 0.1. For the\nclassification threshold we applied values form 1 to 4 incrementing the values\nby 0.1. The summary of the results (F-measure) obtained by both models across\ndi\u000berent values of the parameters are presented in Table 4. For each model and\neach dataset we reported the maximum, minimum and average of F-measure\nachieved across all parameters combinations.\nIt can be observed from the results that the SA can obtain a higher maxi-\nmum F-measure in comparison to the SA+MLP. The di\u000berence is particularly\nvisible for the Restaurant and DBLP-Scholar datasets. However, at the same\ntime the SA+MLP obtained better average performance in 3 out of 4 datasets.\nIt also obtained a greater minimum value of F-measure for each dataset with a\nsigni\fcant di\u000berence in 3 out of 4 datasets. The AS+MLP is clearly less sensitive\nto the selection of its parameter as the average di\u000berence between the maximum\nand minimum F-measure obtained across di\u000berent datasets is 0.034. At the same\ntime, the average di\u000berence in performance obtained by the SA across di\u000berent\nparameter values is 0.243. This indicates that the SA is capable of providing\nbetter performance in RL tasks when validation data is available in order to\noptimise its parameters. The SA+MLP performs slightly worse however, it is\nmuch more stable and may be a good alternative when we don't have enough\ndata to carefully select the parameters values.\nTable 4: Comparison of the results (F-measure) obtained by the SA and the\nSA+MLP models for di\u000berent margin andclassificationthreshold parameters.\nDataset SA SA+MLP\nDataset Min. Ave. Max. Min. Ave. Max.\nRestaurant 0.764 0.895 0.975 0.814 0.875 0.942\nCora 0.991 0.997 0.999 0.998 0.999 0.999\nDBLP-ACM 0.559 0.88 0.965 0.945 0.956 0.963\nDBLP-Scholar 0.510 0.751 0.857 0.762 0.788 0.812",
        "14": "3.4 Comparison with Baseline RL Methods\nIn our experiments we compare the proposed SNN models with three di\u000berent\napproaches to RL.\nMachine Learning based approach. We apply four machine learning algo-\nrithms for training classi\fcation models. Those include Support Vector Machine\n(SVM) with linear kernel (SVM-L), SVM with polynomial kernel (SVM-P), SVM\nwith RBF kernel (SVM-R) and Random Forest (RF) (number of trees = 500,\nmaximum depth = 16). We use \fve di\u000berent similarity measures for generat-\ning the comparison vectors (Jaro, Smith-Waterman, Q-Gram, Jaro-Winkler and\nLevenshtein edit distance).\nAs the ML based RL models require the data to be structured and repre-\nsented by the same set of attributes across all data sources, the structure of each\nof the datasets used in the experiments was kept when applied with any of the\naforementioned ML methods.\nDistributed representation of records (DeepER). This is a recently pro-\nposed approach which applies a distributed representation of words [8] for con-\nstructing a distributed representation of records. For each token (word) within\nan attribute value its distributed representation is obtained from one of the pre-\ntrained embedding dictionaries. A distributed representation of the attribute\nis then constructed by averaging embeddings of all its tokens. Following this,\nfor each pair of records, their comparison vector is generated by computing co-\nsine similarity between embeddings of their corresponding attributes. Finally,\nthe aforementioned four classi\fcation models are trained with the comparison\nvectors labelled as a match or non-match. For a new pair of records, their com-\nparison vector is \frst calculated following the same procedure, which is then\npassed to the ML model and classi\fed as a match or no-match.\nThis model also assumes that the data is structured and the records are rep-\nresented by the same attributes.\nTF-IDF rule based approach. This is a RL method [10] which uses Log\nTF-IDF (Term Frequency Inverse Document Frequency) values for measuring\nthe similarity between records. The Log TF-IDF measure [17] is formally de-\n\fned as:\nsim(ti;tj) =X\nq2ti\\tjw(ti;q)\u0001w(tj;q); (7)\nwhere\nw(t;q) =w0(t;q)rP\nq2tw0(t;q)2; (8)\nand\nw0(t;q) = log(tf(t;q) + 1)\u0001log(jRj\ndf(q;R)+ 1) (9)\nwhere (t1;t2) represents a record pair, w(t;q) is the normalised TF-IDF weight",
        "15": "of a termqin a record t,tf(t;q) represents the term frequency of qint,jRjis\nthe total number of records in the dataset R,df(q) is the document frequency\nof the term qin the cohort. For each record pair from Rtheir similarity is\ncalculated according to Equation 7. If the obtained similarity is greater than a\nprede\fned threshold the records are classi\fed as a match. The optimal value of\nthe threshold is usually determined using labelled data. Note that this is our\nonly competitor that can be applied with unstructured data and it does not\nrequire for the records to be represented by the same attributes.\nThe results obtained by the three baseline methods on each of the four\ndatasets are demonstrated in Table 5. We compare the baseline methods with\nthe two models proposed in this paper, the SA and SA+ the MLP. For the ML\nbased models, the best results obtained across all learning methods and sim-\nilarity metrics used for constructing the comparison vectors are reported. For\nthe ML based method and the DeepER models, the data had to be kept in its\nstructured format. TF-IDF method is the only competitor that is applicable to\nunstructured data.\nWhen comparing the SA with the TF-IDF based method, we observe that our\nproposed model performed much better for each of the datasets (the SA+MLP\nwas slightly outperformed by the TF-IDF in two datasets). In comparison to\nthe DeepER, the SA performed better in three out of four cases. The only\ndataset where the DeepER obtained slightly higher F-measure than the SA is\nDBLP-ACM dataset. The DeepER outperformed the SA+MLP in two out of\nfour datasets. Finally, the ML based method outperformed the SA in two out\nof four cases and the SA+MLP in three out of four cases. We can note that\nfor the Cora dataset, both the SA and the SA+MLP signi\fcantly outperformed\nany of the competitors. The obtained results demonstrate that the proposed SA\napproach to RL can be successfully applied with unstructured data. It has been\nslightly outperformed by the other two methods in some cases, however, it has\na great advantage over them since it does not make any assumptions regarding\nthe schema of the data. It can be easily applied with structured, semi-structured\nor unstructured records. It also allows for the data sources to be heterogeneous,\ni.e. represented in di\u000berent formats. At the same time, the DeepER and the ML\nbased methods require for the data to be structured and represented by overlap-\nping attributes. The SA+MLP performed slightly worse but it was still on par\nwith the other approaches.\n4 Relevant Work\nRecord Linkage. The two key research directions that have been studied in the\nspace of RL include: (1) the determination of time-e\u000ecient algorithms for RL\n[4], and (2) the development of methods for e\u000bective discovery of links [10] [3].\nThe former focuses on improving the speed of the RL process through reduction\nof the number of record comparisons (referred to as blocking). The latter \feld\nfocuses on developing techniques for e\u000bective link discovery. In this work we focus\non the second research problem, which is identifying links between records.",
        "16": "Table 5: Comparison of the performance (F-measure) of the SA and the\nSA+MLP models with the state-of-the-art ML, distributed tuple representation\nand the TFIDF based approaches.\nDataset SASA+MLP Best-ML DeepER TFIDF\nRestaurant 0.975 0.942 0.97 0.972 0.947\nCora 0.999 0.999 0.946 0.955 0.809\nDBLP-ACM 0.965 0.963 0.978 0.973 0.923\nDBLP-Scholar 0.857 0.812 0.889 0.791 0.833\nWork on RL models can be categorised as based on (1) declarative rules and\n(2) Machine Learning methods. With the \frst family of approaches generic rules\nare applied using similarity measures and thresholds in order to identify those\npairs of records that are similar enough to be considered as matches [12] [31]\n[1]. An obvious advantage of those techniques is the fact that they can provide\ninterpretable solutions, which is not the case with a ML based approach. In\nsome work it was proposed to learn the rules by appropriate selection of the\nsimilarity functions and the thresholds [12] [31]. Even though it was possible to\nautomate some of the steps, those methods still heavily rely on the expertise\nand knowledge of the data. Alternatively, ML based methods can automatically\ntrain a model to classify a comparison vector generated for a pair of records\nas a match or non-match [9]. Popular ML based approaches include genetic\nprogramming [25], active learning [32], SVM [9], self-learning [30] [15] [18]. In\nsome recent work a distributed representation of records using word embeddings\nhas been proposed for the task of RL [8] [28]. In [8] two techniques of computing\ndistributed representation of attributes values were explored. The \frst one based\non averaging embeddings of all tokens within an attribute, and the second using\na recurrent neural network to convert each tuple into a numeric vector. Following\nthis, a comparison vector was calculated for each pair of records. The similarity\nvalues between the corresponding attributes values (embeddings) were obtained\nusing cosine similarity. Comparison vectors were further applied with the ML\nmethods to train a RL model. In [28] word embeddings were applied to determine\nsimilarity between online user-generated content.\nEven though ML based methods have been proved to work very well in the\nRL tasks, the strong assumption they make regarding the structure of the data\nmakes them less and less applicable to increasing volumes of heterogeneous data\nsources.\nSiamese Neural Networks. Siamese ANN were \frst proposed to solve a sig-\nnature veri\fcation task as an image matching problem [2]. A SNN is an ANN,\nwhich is composed of two or more identical sub-networks (i.e. the same archi-\ntecture and weights) that take di\u000berent input vectors but are joined by a loss\nfunction at the top. Each of the network computes a high level representation of",
        "17": "the input vectors. The aim of the learning process is to \fnd a similarity between\nthe input vectors by comparing their high level representations. The learning\ncan be performed using triplet or contrastive loss functions. It has been demon-\nstrated that SNN can learn useful data descriptors that can be further used to\ncompare between the inputs of the respective subnetworks. Its inputs can be\nanything from numerical data (in this case the subnetworks are usually formed\nby fully-connected layers), image data (with Convolutional Neural Networks as\nsubnetworks) or even sequential data such as sentences or time signals (with\nRecurrent Neural Networks as subnetworks). SNNs have been successfully ap-\nplied for a one-shot imagine recognition problem [19]. Siamese Recurrent Neural\nNetworks with word embeddings have also been applied for learning sentence\nsimilarity [24].\n5 Conclusion\nIn this work we propose a new Autoencoder-based SNN architecture for linking\nunstructured data. As opposed to previously introduced MLP-based Siamese\nmodel, with this approach a hybrid loss function is applied, which incorporates\ncontrastive and reconstruction losses. The motivation behind the proposed de-\nsign is to improve the generalisation of the supervised model by training the\nmodel for additional unsupervised task. For computing the vector representa-\ntions of records (i.e. records embeddings) we applied the BERT language model.\nThe experimental results demonstrate that the new proposed architecture of\nthe Siamese model performs better in comparison to the SNN based on MLP\nproposed in [16].\nSNN with contrastive loss function relies on two parameters, margin used\nduring the training and classification threshold used while classifying a pair\nof records as a match or a non-match. By evaluating the model with di\u000berent\nvalues of the two parameters we were able to demonstrate that the model is\nvery unstable in terms of the parameters selection. In order to alleviate this\nproblem we propose another network architecture by integrating the Siamese\nAutoencoder with a MLP model. The Siamese Autoencoder is trained with the\nhybrid loss function and the new embedded representation of the records is\nfurther used for training the MLP for the record pairs classi\fcation task. In this\nway we eliminate the need for the classification threshold parameter and also\nmake the model much more robust to the selection of the margin parameter.\nEven though the classi\fcation model performs slightly worse than the distance\nbased model, it is a good alternative in case when we don't have a su\u000ecient\nvalidation dataset for optimisation of the parameters.\nThe two proposed approaches were also compared to three di\u000berent RL meth-\nods including ML based models, another distributed records representation based\napproach and a rule based TF-IDF model. The proposed model outperformed\nthe TF-IDF based method when applied to unstructured data. It also performed\non par with the other two models, which required the data to be structured.",
        "18": "As the next step of this work we want to explore the integration of the\nproposed model with an embedding based blocking technique in order to provide\nan end-to-end linkage solution. The distributed representation of records used\nwith our approach allows for addressing the blocking problem in a di\u000berent\nmanner, which has not yet been explored by the RL community.\nReferences\n1. Bilenko, M., Mooney, R., Cohen, W., Ravikumar, P., Fienberg, S.: Adaptive name\nmatching in information integration. IEEE Intelligent Systems 18(5), 16{23 (2003)\n2. Bromley, J., Guyon, I., LeCun, Y., S\u007f ackinger, E., Shah, R.: Signature veri\fcation\nusing a\" siamese\" time delay neural network. In: Advances in neural information\nprocessing systems. pp. 737{744 (1994)\n3. Christen, P.: Data matching: concepts and techniques for record linkage, entity\nresolution, and duplicate detection. Springer Science & Business Media (2012)\n4. Christen, P.: A survey of indexing techniques for scalable record linkage and dedu-\nplication. IEEE transactions on knowledge and data engineering 24(9), 1537{1555\n(2012)\n5. Cohen, W., Ravikumar, P., Fienberg, S.: A comparison of string metrics for match-\ning names and records. In: Kdd workshop on data cleaning and object consolida-\ntion. vol. 3, pp. 73{78 (2003)\n6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n7. Dong, X.L., Srivastava, D.: Big data integration. In: 2013 IEEE 29th international\nconference on data engineering (ICDE). pp. 1245{1248. IEEE (2013)\n8. Ebraheem, M., Thirumuruganathan, S., Joty, S., Ouzzani, M., Tang, N.: Dis-\ntributed representations of tuples for entity resolution. Proceedings of the VLDB\nEndowment 11(11), 1454{1467 (2018)\n9. Elfeky, M.G., Verykios, V.S., Elmagarmid, A.K.: Tailor: A record linkage toolbox.\nIn: Proceedings 18th International Conference on Data Engineering. pp. 17{28.\nIEEE (2002)\n10. Elmagarmid, A.K., Ipeirotis, P.G., Verykios, V.S.: Duplicate record detection: A\nsurvey. IEEE Transactions on knowledge and data engineering 19(1), 1{16 (2007)\n11. Getoor, L., Machanavajjhala, A.: Entity resolution: theory, practice & open chal-\nlenges. Proceedings of the VLDB Endowment 5(12), 2018{2019 (2012)\n12. Isele, R., Bizer, C.: Learning expressive linkage rules using genetic programming.\nProceedings of the VLDB Endowment 5(11), 1638{1649 (2012)\n13. Jaccard, P.: Distribution de la \rore alpine dans le bassin des dranses et dans\nquelques r\u0013 egions voisines. Bull. Soc. Vaud. Sci. Nat. 37, 241{272 (1901)\n14. Jaro, M.A.: Advances in record-linkage methodology as applied to matching the\n1985 census of tampa, \rorida. Journal of the American Statistical Association\n84(406), 414{420 (1989)\n15. Jurek, A., Hong, J., Chi, Y., Liu, W.: A novel ensemble learning approach to\nunsupervised record linkage. Information Systems 71, 40{54 (2017)\n16. Jurek-Loughrey, A.: Deep learning based approach to unstructured record linkage.\nIn: Proceedings of the 22nd International Conference on Information Integration\nand Web-based Applications & Services. pp. 417{425 (2020)",
        "19": "17. Kejriwal, M., Miranker, D.P.: An unsupervised algorithm for learning blocking\nschemes. In: 2013 IEEE 13th International Conference on Data Mining. pp. 340{\n349. IEEE (2013)\n18. Kejriwal, M., Miranker, D.P.: Semi-supervised instance matching using boosted\nclassi\fers. In: European Semantic Web Conference. pp. 388{402. Springer (2015)\n19. Koch, G., Zemel, R., Salakhutdinov, R.: Siamese neural networks for one-shot\nimage recognition. In: ICML deep learning workshop. vol. 2 (2015)\n20. K\u007f opcke, H., Thor, A., Rahm, E.: Evaluation of entity resolution approaches on\nreal-world match problems. Proceedings of the VLDB Endowment 3(1-2), 484{493\n(2010)\n21. Le, L., Patterson, A., White, M.: Supervised autoencoders: Improving generaliza-\ntion performance with unsupervised regularizers. Advances in neural information\nprocessing systems 31, 107{117 (2018)\n22. Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions, and\nreversals. In: Soviet physics doklady. vol. 10, pp. 707{710 (1966)\n23. Mitchell, J., Lapata, M.: Composition in distributional models of semantics. Cog-\nnitive science 34(8), 1388{1429 (2010)\n24. Mueller, J., Thyagarajan, A.: Siamese recurrent architectures for learning sentence\nsimilarity. In: Thirtieth AAAI Conference on Arti\fcial Intelligence (2016)\n25. Ngomo, A.C.N., Lyko, K.: Unsupervised learning of link speci\fcations: determin-\nistic vs. non-deterministic. In: Proceedings of the 8th International Conference on\nOntology Matching-Volume 1111. pp. 25{36. CEUR-WS. org (2013)\n26. O'Hare, K., Jurek-Loughrey, A., Pires, C.: High-value token-blocking: E\u000ecient\nblocking method for record linkage. ACM Transactions on Knowledge Discovery\nfrom Data (2021)\n27. Pennington, J., Socher, R., Manning, C.: Glove: Global vectors for word repre-\nsentation. In: Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP). pp. 1532{1543 (2014)\n28. Schneider, A.T., Mukherjee, A., Dragut, E.C.: Leveraging social media signals for\nrecord linkage. In: Proceedings of the 2018 World Wide Web Conference on World\nWide Web. pp. 1195{1204. International World Wide Web Conferences Steering\nCommittee (2018)\n29. Shannon, C.E.: A mathematical theory of communication. ACM SIGMOBILE Mo-\nbile Computing and Communications Review 5(1), 3{55 (2001)\n30. Sherif, M.A., Ngomo, A.C.N., Lehmann, J.: W ombat{a generalization approach\nfor automatic link discovery. In: European Semantic Web Conference. pp. 103{119.\nSpringer (2017)\n31. Wang, J., Li, G., Yu, J.X., Feng, J.: Entity matching: How similar is similar.\nProceedings of the VLDB Endowment 4(10), 622{633 (2011)\n32. Wang, Q., Vatsalan, D., Christen, P.: E\u000ecient interactive training selection for\nlarge-scale entity resolution. In: Paci\fc-Asia Conference on Knowledge Discovery\nand Data Mining. pp. 562{573. Springer (2015)\n33. Winkler, W.E.: String comparator metrics and enhanced decision rules in the\nfellegi-sunter model of record linkage. pp. 354{359 (1990)"
    }
}