{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wissel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "import pickle\n",
    "from gensim.models import CoherenceModel\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy        #per la lemmizzazione delle parole\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_setting_data():\n",
    "           \n",
    "\n",
    "           \n",
    "            base_path=os.path.join(\"model_settings\",\"model_config.json\")\n",
    "            # Open the JSON file and load its contents\n",
    "            with open(base_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                clusters=int(data[\"number_clusers\"])\n",
    "                return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"model_\"+str(read_setting_data())\n",
    "model_path=model+\"/model.lda\"\n",
    "dic_path=model+\"/id2token.mm\"\n",
    "lda_model = gensim.models.LdaModel.load(model_path)\n",
    "dictionary = corpora.Dictionary.load(dic_path)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #pipeline di pulizia\n",
    "def remove_stops(text,stops):\n",
    "        #rimozione data\n",
    "        pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b'\n",
    "        text= re.sub(pattern, '', text)\n",
    "        text=text.replace(\"\\n\",\" \")\n",
    "        text=text.replace(\"-\",\" \")\n",
    "        text=text.strip()\n",
    "        text=text.lower()\n",
    "        words=text.split()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #rimozione stopwords\n",
    "        final=[]\n",
    "        for word in words:\n",
    "            if word not in stops:\n",
    "                if len(word)<16 and len(word)>1:\n",
    "                    final.append(word)\n",
    "        final=\" \".join(final)\n",
    "\n",
    "        #punti\n",
    "        final=final.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "        #rimozione numeri\n",
    "        final=\"\".join([i for i in final if not i.isdigit()])\n",
    "\n",
    "        #eliminazione doppi \" \"\n",
    "        while \"  \" in final:\n",
    "            final=final.replace(\"  \",\" \")\n",
    "        \n",
    "        return (final)\n",
    "\n",
    "def remove_custom_punct(text):\n",
    "        translator = str.maketrans('', '', ''.join(string.punctuation))\n",
    "        return text.translate(translator)\n",
    "\n",
    "def remove_doubleCharWords(line):\n",
    "        words=line.split()\n",
    "        final=[]\n",
    "        for i in words:\n",
    "            if len(i)>2:\n",
    "                words=remove_custom_punct(i)\n",
    "                final.append(words)\n",
    "            \n",
    "                \n",
    "        final=\" \".join(final)\n",
    "\n",
    "        return final\n",
    "        \n",
    "def lemming_text( text):\n",
    "        # Load the Italian language model\n",
    "        nlp = spacy.load(\"it_core_news_sm\")\n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text)\n",
    "        # Lemmatize each token in the text\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        # Print the lemmas\n",
    "        text=\" \".join(lemmas)\n",
    "        return text\n",
    "    \n",
    "            \n",
    "def find_none_spaced_words(text):\n",
    "       \n",
    "       \n",
    "            \n",
    "        text_with_space = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", text)\n",
    "\n",
    "        \n",
    "        return text_with_space\n",
    "\n",
    "def delete_double_spaces(text):\n",
    "        text_without_double_spaces = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text_without_double_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(i):\n",
    "    \n",
    "            new_text=[]\n",
    "            stops=stopwords.words(\"italian\")\n",
    "            stops.extend(stopwords.words(\"english\"))\n",
    "        \n",
    "\n",
    "            if type(i)==type(\"s\"):\n",
    "                #rimozione di tutte le parole che non hanno lunghezza maggiore di 2\n",
    "                parsed_text=find_none_spaced_words(i)              #andiamo a dividere eventuali parole distinte, concatenate (es. HelloWord->Hello World)\n",
    "                parsed_text=remove_stops(parsed_text,stops)                  #rimozione stopwords\n",
    "                parsed_text=remove_doubleCharWords(parsed_text)    #rimozione parole e simboli superflui\n",
    "                parsed_text=delete_double_spaces(parsed_text)\n",
    "                parsed_text=lemming_text(parsed_text)              #lemming delle parole\n",
    "                \n",
    "            else:\n",
    "                parsed_text=\"\"\n",
    "                \n",
    "            return parsed_text\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Preprocess the new text\n",
    "new_text = \"complessit√† asistotica \"\n",
    "new_text = parse_query(new_text).split()\n",
    "new_text_bow = dictionary.doc2bow(new_text)\n",
    "    \n",
    "# Get the topics related to the new text\n",
    "topics = lda_model.get_document_topics(new_text_bow)\n",
    "\n",
    " # Print the topics\n",
    "for topic in topics:\n",
    "                print(\"Topic {}: {:.2%}\".format(topic[0], topic[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
