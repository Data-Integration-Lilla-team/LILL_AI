{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66d4364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41e5269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(\"dataExtraction\", \"data_test\", \"new_data\", \"040-algoritmi-link-state-packet-01.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4a50f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_page_text = {}\n",
    "reader = PdfReader(file)\n",
    "n_pages = len(reader.pages)\n",
    "for i in range(n_pages):\n",
    "    page = reader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    dict_page_text[str(i)] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"json_data.json\", \"w\", encoding=\"utf8\") as outfile:\n",
    "    json.dump(dict_page_text, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61818a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18362a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70a55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(\"dataExtraction\", \"data_test\", \"new_data\", \"HW8-DataIntegration_WISSEL.pptx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c236690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_page_text = {}\n",
    "presentation = Presentation(file)\n",
    "for idx, slide in enumerate(presentation.slides):\n",
    "    text = \"\"\n",
    "    for shape in slide.shapes:\n",
    "        if hasattr(shape, \"text\"):\n",
    "            text += shape.text\n",
    "    dict_page_text[str(idx)] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b0f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"json_data.json\", \"w\", encoding=\"utf8\") as outfile:\n",
    "    json.dump(dict_page_text, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f49162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19638761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "247a2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(\"dataExtraction\", \"data_test\", \"new_data\", \"HW8-Relazione_WISSEL.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51576ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(file_path)\n",
    "pages = []\n",
    "current_page = []\n",
    "\n",
    "for paragraph in doc.paragraphs:\n",
    "    current_page.append(paragraph.text)\n",
    "    if paragraph.runs:\n",
    "        if paragraph.runs[0].element.xml.startswith('<w:br'):\n",
    "            pages.append('\\n'.join(current_page))\n",
    "            current_page = []\n",
    "\n",
    "# Aggiungi l'ultima pagina, se presente\n",
    "if current_page:\n",
    "    pages.append('\\n'.join(current_page))\n",
    "\n",
    "return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "094b6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_page_text = {}\n",
    "document = Document(file)\n",
    "for idx, page in enumerate(document.paragraphs):\n",
    "    text = page.text\n",
    "    dict_page_text[str(idx)] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24dbcf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'Data Integration: Arlecchino System',\n",
       " '1': '\\t\\t\\t\\t\\t',\n",
       " '2': 'Matteo Wissel, Paolo Di Simone, Pietro Baroni',\n",
       " '3': 'Dipartimento di Ingegneria Informatica Roma 3',\n",
       " '4': 'Roma 22/02/23',\n",
       " '5': '',\n",
       " '6': 'Introduzione',\n",
       " '7': 'Data Integration',\n",
       " '8': 'La qualità delle decisioni prese in un contesto sempre più Data-Driven è direttamente influenzato delle informazioni possedute e dalla facilità di acquisizione di esse. Tuttavia, molto spesso, tali informazioni sono memorizzate in sorgenti distinte. ',\n",
       " '9': 'A tal proposito negli anni sono stati presentati numerose soluzioni di Data-Integration. Tali sistemi hanno l’obiettivo di integrare informazioni provenienti da sorgenti distinte in base alla relazione semantica tra le diverse colonne delle tabelle di partenza.',\n",
       " '10': 'Use Case',\n",
       " '11': 'L’obiettivo principale del progetto è stata la creazione di uno schema unico contenente informazioni derivanti dai dataset sviluppati da 11 team distinti durante l’Homework 5 del corso di Ingegneria dei Dati mediante tecniche di Web-Scraping. Tali dataset contengono informazioni eterogenee relative ad aziende memorizzate in formati distinti.',\n",
       " '12': 'A tale scopo è stato implementato un sistema ad-hoc, Arlecchino System, capace di eseguire tutti i passaggi di Data Integration (Schema Matching e Record Linkage) indipendentemente dalle tabelle in input.',\n",
       " '13': '',\n",
       " '14': 'Dataset',\n",
       " '15': 'Il Dataset di partenza è costituito da 44 tabelle (4 per ogni team) provenienti da 14 pagine web distinte.',\n",
       " '16': 'Clustering del Dataset',\n",
       " '17': 'In quanto numerose tabelle del Dataset sono state generate estraendo dati da una medesima sorgente web, per l’implementazione della pipe-line di Data Integration, abbiamo ritenuto opportuno raggruppare le singole tabelle rispetto ai siti web sorgenti (Figura 1).',\n",
       " '18': '',\n",
       " '19': 'Figura 1-Tabelle per cluster',\n",
       " '20': 'Analisi dei sorgenti',\n",
       " '21': 'Poiché l’obiettivo principale del progetto è individuare le relazioni semantiche tra le diverse tabelle, per avere una visione più dettagliata del Dataset si è ritenuto opportuno classificare i clusters rispetto al contenuto informativo delle loro tabelle.',\n",
       " '22': 'Le labels scelte sono quattro: ',\n",
       " '23': 'Dati finanziari;',\n",
       " '24': 'Dati geografici;',\n",
       " '25': 'Dati relativi al personale;',\n",
       " '26': 'Dati giuridici.',\n",
       " '27': '',\n",
       " '28': '',\n",
       " '29': '',\n",
       " '30': '',\n",
       " '31': 'Come si può vedere dalla Figura 2, non esiste un cluster che contenga tutti e quattro i tipi di informazione, questo implica che, per avere una visione unificata delle quattro tipologie di informazioni ricercate, è necessario intervenire integrando le tabelle del Dataset.',\n",
       " '32': 'Schema Mediato Target',\n",
       " '33': 'Per le motivazioni elencate nel capitolo precedente lo schema mediato finale proposto (Figura 3) è tale da racchiudere informazioni derivanti da ognuna delle quattro aree di interesse e permettere quindi di avere una visione unificata dei dati interessati. ',\n",
       " '34': '',\n",
       " '35': 'Figura 3-Schema Mediato',\n",
       " '36': 'Architettura',\n",
       " '37': 'Il sistema implementato per la creazione della tabella integrata (Schema mediato) è riportato in Figura 4.',\n",
       " '38': '',\n",
       " '39': 'Figura 4-Architettura Arlecchino',\n",
       " '40': '',\n",
       " '41': '',\n",
       " '42': 'L’architettura di Arlecchino può essere suddivisa nei seguenti tre moduli:',\n",
       " '43': 'Parsing & Cleaning Module: modulo specializzato nella pulizia e standardizzazione dei dati presenti nelle tabelle di partenza;',\n",
       " '44': 'Matching Module: modulo custom specializzato nell’operazione di Schema Matching;',\n",
       " '45': 'Record Linkage Module: modulo specializzato nelle operazioni di Record Linkage (Blocking, Linkage e Join).',\n",
       " '46': 'Funzionamento',\n",
       " '47': 'Arlecchino System persegue il task di Data Integration applicando in modo iterativo il Matching Module. Quest’ultimo, infatti, viene prima applicato su ogni cluster per generare uno schema mediato di cluster (visione unificata dei dati interessanti presenti all’interno di un singolo cluster), lo stesso modulo viene poi utilizzato per integrare i singoli schemi mediati di cluster ed ottenere lo schema mediato finale. Le istanze di Matching Module che operano sui cluster inoltre, inferiscono informazioni sul problema all’istanza finale, in quanto alimentano un dizionario unificato dei sinonimi individuati nei singoli cluster (dizionario sinonimi pregressi) ',\n",
       " '48': '',\n",
       " '49': 'Una volta generato lo schema mediato finale, quest’ ultimo viene dato in input al Record Linkage Module il quale genera lo schema Integrato finale. ',\n",
       " '50': '',\n",
       " '51': 'In questa relazione approfondiamo il Matching Module ed il Record Linkage Module.',\n",
       " '52': '',\n",
       " '53': 'Schema matching – Matching Module',\n",
       " '54': 'Formulazione problema',\n",
       " '55': 'Il problema di Schema Matching è stato formulato nel seguente modo:',\n",
       " '56': '',\n",
       " '57': 'Analizzando la funzione f(c) si nota come domino e codominio (search space) hanno entrambi cardinalità O(N) ed il numero totale di confronti potenziali è O(N²).',\n",
       " '58': 'Architettura Matching Module',\n",
       " '59': 'L’operazione di schema matching è stata svolta interamente dal Matching Module, un modulo custom responsabile nell’individuazione dei sinonimi delle colonne in input mediante analisi data-wise e column-wise. La Figura 5 riassume l’architettura del modulo.',\n",
       " '60': '',\n",
       " '61': 'Figura 5-Matchin Module',\n",
       " '62': 'Il modulo è caratterizzato da due componenti: Preprocessing e JaccardModule. Ognuno dei moduli ha l’intento di individuare per ogni colonna la lista di sinonimi considerando similarità di nome della colonna e similarità a livello di dato.',\n",
       " '63': 'Preprocessing ',\n",
       " '64': 'La componente Preprocessing ha il compito di generare un dizionario parziale di sinonimi tale da ridurre il search space. L’obbiettivo principale è quello di ridurre il numero di confronti operati da JaccardModule (collo di bottiglia della pipeline) massimizzando la Recall e definendo per ogni colonna una lista di possibili sinonimi computati unendo i sinonimi identificati dai blocchi NameSimilarity e DataCorrelation.',\n",
       " '65': 'Questi due ricercano relazioni semantiche tra colonne operando in domini distinti:',\n",
       " '66': 'La NameSimilarity opera a livello di schema (Figura 6)',\n",
       " '67': '',\n",
       " '68': 'Figura 6-Name Similarity module',\n",
       " '69': ' ',\n",
       " '70': 'In questa fase si fa inoltre uso del dizionario dei sinonimi pregressi, un dizionario contenente tutti i sinonimi individuati e validati dal sistema fino a quel punto.',\n",
       " '71': 'La DataCorrelation opera a livello di dati. Trasforma ogni colonna in un vettore di 17 features ed individua i sinonimi mediante utilizzo di un modello ML non supervisionato (K-Means)',\n",
       " '72': 'E’ stato empiricamente testato che il numero di confronti operato dal JaccardModule in presenza di Preprocessing si riduce di un fattore medio di 8.',\n",
       " '73': 'JaccardModule',\n",
       " '74': 'I sinonimi computati nello step di Preprocessing vengono dati in input al JaccardModule, quest’ultimo genera un file .csv per ogni colonna c presente nel dizionario dei sinonimi in input contenente tutte le colonne reputate sinonimi. Per ogni file, il modulo individua i sinonimi veri operando anche in questo caso sia schema-wise che data-wise. Nello specifico, la logica può essere riassunta come in Figura 7.',\n",
       " '75': '',\n",
       " '76': 'Figura 7-Logica JaccardModule',\n",
       " '77': 'Output',\n",
       " '78': 'L’output del MatchingModule è un dizionario di sinonimi in cui, per ogni colonna definita nello schema mediato, sono presenti una lista di possibili colonne semanticamente uguali (sinonimi). L’utente seleziona i match eliminando eventuali errori del sistema. ',\n",
       " '79': 'Al netto della validazione dell’utente, il sistema opera un update del dizionario dei sinonimi pregressi inserendo le nuove relazioni semantiche individuate nell’esecuzione corrente.',\n",
       " '80': 'Infine, viene creato lo schema mediato inserendo nelle colonne i dati provenienti dai relativi sinonimi. Nella Figura 8 sono riportate le statistiche relative allo schema Mediato computato in termini di valori univoci e nulli per ogni colonna. ',\n",
       " '81': '',\n",
       " '82': 'Figura 8-Statistiche Schema Mediato',\n",
       " '83': '',\n",
       " '84': 'Record linkage - Record Linkage Module ',\n",
       " '85': 'Proseguendo l’analisi della pipeline, l’ultimo modulo è il Record Linkage Module, la cui architettura è riassunta in Figura 9.',\n",
       " '86': '',\n",
       " '87': 'Figura 9-Architettura Record Linkage Module',\n",
       " '88': 'Quest’ultimo riceve in input lo schema mediato computato dal MatchingModule sul quale vengono effettuate un insieme di operazioni volte ad individuare quali record fanno riferimento alla stessa entità. L’output di tali operazioni sarà utilizzato per la costruzione della tabella finale.',\n",
       " '89': 'Il Record Linkage Module è stato implementato combinando due librerie principali:',\n",
       " '90': 'py_entitymatching(Magellan);',\n",
       " '91': 'Deepmatcher.',\n",
       " '92': 'Blocking',\n",
       " '93': 'Per determinare se i record dello schema mediato siano relativi ad una stessa entità in fase di record linkage si devono eseguire confronti tra tutti i record della tabella. Nel nostro caso il numero di confronti è di oltre 34 miliardi quindi, per ridurre la complessità computazionale di tale operazione, sono stati impiegati due tipologie di blockers, offerte dalla libreri py_entitymatching.',\n",
       " '94': 'Nello specifico:',\n",
       " '95': 'overlap blocker: elimina dai confronti tutti record che in un determinato campo non hanno almeno un numero, scelto dall’utente, di parole in comune;',\n",
       " '96': 'rule based blocker: utilizza regole custom specifiche per il dominio in esame.',\n",
       " '97': 'Le operazioni di blocking che abbiamo eseguito sono state:',\n",
       " '98': 'Blocking di coppie che non hanno overlap di una parola nel nome dell’azienda;',\n",
       " '99': 'Blocking di coppie che non hanno overlap di una parola nello stato dell’azienda (se presente);',\n",
       " '100': 'Blocking di coppie nelle quali la distanza di Levenshtein tra i nomi delle aziende è minore di 0.7.',\n",
       " '101': 'Tali operazioni hanno permesso di ridurre notevolemente il numero di confronti. Nello specifico:',\n",
       " '102': 'Confronti pre-blocking = 34.072.360.569;',\n",
       " '103': 'Confronti post-blocking = 2.177.713.',\n",
       " '104': 'Modello',\n",
       " '105': 'Una volta determinate le possibili coppie di record relative ad una stessa azienda, il modulo implementa le operazioni di Entity Matching. Queste sono state implementate utilizzando la libreria deepmatcher, la quale fornisce modelli di reti neurali specializzati in task di EM.',\n",
       " '106': 'Tutti le soluzioni DL al problema di entity matching sono divise in tre parti: ',\n",
       " '107': 'Attribute Embedding;',\n",
       " '108': 'Attribute Similarity Representation;',\n",
       " '109': 'Classification. ',\n",
       " '110': 'Anche nel modello utilizzato, detto Matching Model, sono presenti queste tre fasi: la prima è eseguita da fastText, ovvero un modello pre-addestrato per l’apprendimento di embeddings di parole, sull’input del modello. La seconda nel modello scelto è divisa in due sezioni l’Attribute Summarizer e l’Attribute Comparator, tra le varie versioni proposte dagli autori l’attribute summarizer scelto è quello ibrido ovvero un modello implementato mediante una RNN bidirezionale. La terza, invece, è eseguita da un layer Softmax. Abbiamo scelto l’attribute summarizer ibrido perché confrontandolo con gli altri sviluppati gli autori hanno convenuto che sia quello con maggior potere rappresentazionale.',\n",
       " '111': 'Training',\n",
       " '112': 'Per l’addestramento del modello, sono state estratte casualmente 1300 coppie di record.\\nQueste sono state classificate manualmente in: ',\n",
       " '113': '1, se la coppia si riferisce alla stessa entità (match);',\n",
       " '114': '0, se i due record si riferiscono ad entità diverse (no-match).',\n",
       " '115': 'Le coppie sono state divise training, test, e validation set in rapporto 3:1:1. La distribuzione dei valori della label di tali record è abbastanza uniforme, ovvero sono presenti 695 label match e 605 label no-match.',\n",
       " '116': 'Addestramento',\n",
       " '117': 'Il modello di EM è stato addestrato utilizzando i seguenti iperparametri;',\n",
       " '118': 'Epoche: 10;',\n",
       " '119': 'Batch-size: 16;',\n",
       " '120': 'Salvataggio del modello con F1 più alta calcolata sul validation set.',\n",
       " '121': 'Dopo aver addestrato il modello lo abbiamo impiegato per la predizione delle coppie di record definite dopo il blocking, questa operazione è durata circa 14 ore e 45 minuti, ed il risultato è stato un numero di match pari a 1.480.727 su 2.177.713 predizioni.',\n",
       " '122': 'Join',\n",
       " '123': 'Definiti i record relativi alla stessa azienda, questi sono stati unificati in un unico record mediante una funzione custom di join la quale inserisce nel campo x del record finale il valore con più occorrenze tra i record match. Le statistiche relative alla tabella generata prima della fase di arricchimento (Data Enrichment) sono riassunte in Figura 10.',\n",
       " '124': '',\n",
       " '125': 'Figura 10-Statistiche schema mediato pre-arricchimento',\n",
       " '126': '',\n",
       " '127': 'Arricchimento',\n",
       " '128': 'Ottenuta la tabella finale, questa è stata arricchita utilizzato l’algoritmo di merge implementato durante l’Homework 3 sull’indice ottenuto nel medesimo homework.',\n",
       " '129': 'La ricerca nell’indice è stata svolta dando come input le colonne più significative della tabella, ovvero quelle che contengono dati che possono ricondurre più facilmente all’entità stessa ovvero Name e Ceo. Grazie a questa operazione sono stati inseriti 142 nuovi valori nelle colonne Country, Revenue, Profit, Ceo, City.',\n",
       " '130': '',\n",
       " '131': 'Prestazioni',\n",
       " '132': 'Vengono riportate in questa sezione i risultati raccolti in fase di testing del sistema Arlecchino. Nello specifico, sia per il Matching Module che per il Record Linkage Model, le metriche di valutazione sono:',\n",
       " '133': 'Precision: (indice del lavoro umano in fase di validazione dei sinonimi);',\n",
       " '134': 'Recall;',\n",
       " '135': 'F1.',\n",
       " '136': '',\n",
       " '137': ''}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca95994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806bad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
